{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/hive-box-smart-lockers-hacked-by-primary-school-kids", "content": "Hive Box smart lockers hacked by primary school kids\nOccurred: October 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA group of fourth-graders in Jiaxing, China, successfully hacked the facial recognition locks of Hive Box smart lockers, which are used for package deliveries. \nDuring a demonstration for a local TV program, the children revealed that the lockers could be opened using just a printed photo of the intended recipient's face. \nThe discovery raised security concerns as it exposed the lockers' contents to potential theft. The show's host confirmed the vulnerability by replicating the hack with a high success rate, only failing when the photo was not held steadily.\nIn response, Hive Box announced that it would suspend the beta testing of the facial recognition feature. \nThe incident sparked discussions about the security of facial recognition technology, particularly its implementation in consumer products.\nSystem \ud83e\udd16\nHive Box \ud83d\udd17\nOperator: \nDeveloper: Hive Box\nCountry: China\nSector: Transport/logistics\nPurpose: Identify/verify customers\nTechnology: Facial recognition\nIssue: Security\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://mp.weixin.qq.com/s/RaNnqdxLm9xniWTNMVJS5g\nhttp://www.xinhuanet.com/2019-10/20/c_1125128237.htm\nhttp://www.sixthtone.com/news/1004698/facial-recognition-smart-lockers-hacked-by-fourth-graders\nhttps://www.chinadaily.com.cn/a/201910/17/WS5da82d92a310cf3e355711ae.html\nRelated \ud83c\udf10\nAgricultural Bank of China facial recognition age bias\nKohler, BMW, MaxMara called out for using facial recognition to track Chinese shoppers\nPage info\nType: Issue\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/hotel-booking-sites-mislead-customers-over-room-prices", "content": "Hotel booking sites mislead customers over room prices\nOccurred: February 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nHigh-profile hotel booking websites including Expedia and Trivago were accused of misleading customers about room prices and availability. \nInvestigations conducted by the UK's Competition and Markets Authority (CMA) found that six travel sites regularly used tactics such as:\nHidden charges (not displaying the full cost of a room upfront, including taxes, fees, and additional charges)\nMisleading discounts (claiming discounts that were not genuine or comparing different types of rooms), and \nPressure selling (using tactics to encourage customers to book quickly without fully considering their options.)\nInvestigations also revealed discrepancies between prices offered on mobile devices and laptops, with some \"mobile exclusive\" deals not providing actual savings.\nThe Competition and Markets Authority (CMA) took enforcement action against six major booking sites - Agoda, Booking.com, ebookers, Expedia, Hotels.com, Trivago -  giving them until September 1, 2019, to make required changes or face further action. \n\u2795 September 2019. Booking.com was found to be using misleading availability claims after the deadline.\nSystem \ud83e\udd16\n\nOperator:\nDeveloper: Agoda; Booking.com; ebookers; Expedia; Hotels.com; Trivago\nCountry: UK\nSector: Travel/hospitality\nPurpose: \nTechnology: Pricing algorithm\nIssue: Ethics/values; Fairness\nTransparency: Governance; Marketing\nRegulation \u2696\ufe0f\nThe Consumer Protection from Unfair Trading Regulations 2008\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nCompetition and Markets Authority. Hotel booking sites to make major changes after CMA probe\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/business-47141538\nhttps://news.sky.com/story/cma-probe-over-concerns-hotel-booking-sites-could-mislead-11100072\nhttps://edition.cnn.com/travel/article/booking-com-misleading-sales-tactics-cma/index.html\nhttps://www.itv.com/news/2019-02-06/hotel-booking-sites-agree-to-end-misleading-sales-tactics\nhttps://www.theguardian.com/business/2019/feb/06/hotel-booking-sites-forced-to-end-misleading-sales-tactics\nhttps://www.telegraph.co.uk/news/2019/02/06/hotel-booking-sites-end-misleading-sales-tactics-following-cma/\nhttps://www.ft.com/content/d22587ec-29e3-11e9-a5ab-ff8ef2b976c7\nhttps://www.independent.co.uk/travel/news-and-advice/booking-com-expedia-trivago-cma-hotel-booking-websites-discount-claims-hidden-charges-a8765241.html\nhttps://www.which.co.uk/news/article/booking-com-still-misleading-holidaymakers-with-1-room-left-claims-aSNln2j8gkGl\nRelated \ud83c\udf10\nTrivago misleads consumers about hotel room rates\nUber Eats algorithm update slashes bicycle riders' income\nPage info\nType: Incident\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/trivago-misleads-consumers-about-hotel-room-rates", "content": "Trivago misleads consumers about hotel room rates\nOccurred: January 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nTrivago was found guilty of misleading consumers regarding hotel room rates, leading to a substantial penalty imposed by the Federal Court of Australia.\nThe court determined that Trivago violated Australian Consumer Law by falsely claiming its platform provided the cheapest hotel rates. Instead, the company used an algorithm that prioritised listings based on which booking sites paid the highest cost-per-click fees, rather than presenting the lowest available prices to consumers.\nTrivago was fined AUD 44.7 million for its misleading practices, which were deemed intentional and caused Australian consumers to overpay approximately AUD 30 million. The misleading claims were prevalent in both its website and television advertisements from late 2013 to mid-2018.\nThe court found that Trivago's algorithm led to higher-priced options being displayed as the \"Top Offer\" in 66.8 percent of listings, misleading consumers into believing they were receiving the best deals and creating a false impression of savings.\nFollowing the ruling, Trivago was required to ensure that any \"Top Offer\" displayed was either the cheapest available or had distinguishing features that justify its ranking. The court's decision served as a warning to other comparison websites to maintain transparency in their pricing practices.\nThis case was seen to highlight the importance of ethical marketing and algorithmic transparency in the online travel industry.\n\u2795 November 2020. Trivago loses Federal Court of Australia appeal.\n\u2795 April 2022. Trivago ordered to pay AUD 44.7 million in penalties for its actions. \nSystem \ud83e\udd16\nTop Position Offer\nOperator:\nDeveloper: Trivago\nCountry: Australia\nSector: Travel/hospitality\nPurpose: \nTechnology: Pricing algorithm\nIssue: Ethics/values; Fairness\nTransparency: Governance; Marketing\nRegulation \u2696\ufe0f\nThe Australian Consumer Law\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nAustralian Competition and Consumer Commission v Trivago N.V\nResearch, advocacy \ud83e\uddee\nACCC. Trivago misled consumers about hotel room rates\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.travelweekly.com.au/article/trivago-loses-appeal-after-misleading-consumers-over-hotel-ads/\nhttps://www.dailymail.co.uk/news/article-6451625/Hotel-comparison-site-Trivago-faces-10million-fine.html\nhttps://www.cmo.com.au/article/670387/trivago-found-breach-australian-consumer-law-misleading-price-claims/\nhttps://www.zdnet.com/article/accc-declares-victory-after-federal-court-rules-trivago-misled-consumers/\nhttps://www.abc.net.au/news/2020-01-21/tirvago-mislead-consumers-on-hotel-pricing-court-finds-accc/11886096\nhttps://www.adnews.com.au/news/trivago-advertised-cheap-hotel-rooms-that-weren-t\nhttps://www.theguardian.com/travel/2020/jan/20/trivago-misled-australian-customers-on-hotel-pricing-court-finds\nhttps://www.zdnet.com/article/trivago-loses-appeal-over-misleading-website-algorithm-ruling/\nhttps://www.ejinsight.com/eji/article/id/2362500/20200124-trivago-found-guilty-of-misleading-consumers-on-hotel-room-rates\nhttps://www.reuters.com/article/us-trivago-australia-court/trivago-misled-customers-by-hiding-best-deals-australian-court-idUSKBN1ZK0MG?il=0\nhttps://www.smh.com.au/business/consumer-affairs/trivago-misled-consumers-in-favour-of-advertiser-dollars-federal-court-finds-20200120-p53t40.html\nRelated \ud83c\udf10\nReport: Airbnb Smart Pricing algorithm exacerbates racial inequality\nUber, Lyft pricing algorithms charge more in non-white areas\nPage info\nType: Incident\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/uber-eats-algorithm-update-cuts-bicycle-riders-income", "content": "Uber Eats algorithm update slashes bicycle riders' income\nOccurred: June 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nUber Eats in Australia faced a backlash from bicycle couriers following an algorithm update that slashed their earnings by up to 50 percent.\nUber's updated algorithm started to favour couriers using cars over those on bicycles, leading a noticeable decrease in delivery assignments for bicycle and walking couriers compared to their car-driving colleagues, particularly during peak hours.\nAccording to Uber Eats drivers, changes to the delivery algorithm resulted in huge income reductions, bringing their earnings down to approximately AUD 13 per hour. \nThe discovery resulted in Uber Eats drivers protesting outside the New South Wales Parliament.\nIn response, Uber denied making specific changes to how earnings are calculated but acknowledged a shift in order routing to enhance delivery efficiency and said that the algorithm considered various factors, including the courier's mode of transportation and location.\nThe situation was seen to reflect poorly on Uber and on the delivery industry more generally, with many workers feeling their earnings to be unstable and under threat. It also highlighted the lack of transparency at Uber.\nSystem \ud83e\udd16\nUber Eats pay algorithm\nOperator:\nDeveloper: Uber/Uber Eats\nCountry: Australia\nSector: Transport/logistics\nPurpose: Calculate driver pay\nTechnology: Pay algorithm\nIssue: Employment; Fairness\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nTWU Australia. SURVEY SHOWS UBEREATS DRIVERS STRUGGLE WITH BANKRUPTCY & HOMELESSNESS\nVeen A. et al. Platform-Capital\u2019s \u2018App-etite\u2019 for Control: A Labour Process Analysis of Food-Delivery Work in Australia\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.news.com.au/finance/work/at-work/40-per-cent-drop-overnight-ubereats-bicycle-riders-say-algorithm-change-preferences-motorbikes-and-cars/news-story/ef3d3a0bc8ee9a7374616b5d2c4a67eb\nhttps://www.business-humanrights.org/en/latest-news/australia-uber-eats-drivers-protest-against-alleged-algorithm-changes-that-prioritise-cyclists-cutting-drivers-wages-by-50/\nhttps://www.businessinsider.com.au/uber-eats-drivers-sydney-algorithm-change-2019-10\nhttps://7news.com.au/politics/uber-eats-drivers-say-pay-has-been-slashed-c-493484\nhttps://hackernoon.com/how-much-do-uber-eats-drivers-make-f0u33lc\nhttps://www.theguardian.com/australia-news/2021/jan/29/uber-eats-accused-of-using-new-contract-to-exploit-australian-delivery-riders\nhttps://www.theguardian.com/australia-news/2019/nov/20/food-delivery-bike-couriers-in-australia-being-underpaid-by-up-to-322-a-week\nhttps://www.bbc.com/worklife/article/20200826-how-algorithms-keep-workers-in-the-dark\nhttps://www.salon.com/2022/07/15/after-algorithm-shift-uber-eats-couriers-without-cars-report-dwindling-wages/\nRelated \ud83c\udf10\n'Racist' Uber Eats facial ID check gets Pa Edrissa Manjang fired\nUber, Lyft pricing algorithms charge more in non-white areas\nPage info\nType: Incident\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/uber-lyft-pricing-algorithms-charge-more-in-non-white-areas", "content": "Uber, Lyft pricing algorithms charge more in non-white areas\nOccurred: June 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA study by researchers at George Washington University found evidence that Uber and Lyft's pricing algorithms charged higher rates for rides to and from predominantly non-white and lower-income neighborhoods in Chicago. \nThe researchers analysed over 100 million ride-sharing trips in Chicago between November 2018 and December 2019, cross-referencing ride data with census information to examine racial demographics of areas traveled to and from. \nThey found that higher per-mile fares were charged when passengers were picked up from or dropped off in neighbourhoods with a higher percentage of non-white residents and lower-income areas, as well as areas with a higher percentage of highly educated residents.\nThe study found correlations between fare prices and demographics, even when controlling for factors like time of day, demand and speed.\nLyft disputed the study's findings, calling the analysis \"deeply flawed\" and arguing that race is not a factor in their pricing algorithms and that the study did not use actual demographic data of rideshare users. \nThe study added to ongoing concerns about algorithmic bias and discrimination in ride-sharing services, and highlighted the need for greater transparency and oversight in the use of algorithms for pricing and other decisions that can impact different demographic groups.\nSystem \ud83e\udd16\nLyft pricing algorithm\nUber pricing algorithm\nOperator:\nDeveloper: Lyft; Uber\nCountry: USA\nSector: Transport/logistics\nPurpose: Calculate price\nTechnology: Pricing algorithm; Machine learning\nIssue: Bias/discrimination\nTransparency: Governance; Black box\nResearch, advocacy \ud83e\uddee\nPandey A., Caliskan A. Disparate Impact of Artificial Intelligence Bias in Ridehailing Economy\u2019s Price Discrimination Algorithms (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.newscientist.com/article/2246202-uber-and-lyft-pricing-algorithms-charge-more-in-non-white-areas/\nhttps://venturebeat.com/2020/06/12/researchers-find-racial-discrimination-in-dynamic-pricing-algorithms-used-by-uber-lyft-and-others/\nhttps://www.salon.com/2020/06/21/uber-lyft-algorithms-charged-users-more-for-trips-to-non-white-neighborhoods/\nhttps://blockclubchicago.org/2020/06/26/uber-lyft-charges-more-for-riders-going-to-chicagos-non-white-neighborhoods-study-shows/\nhttps://hollywoodunlocked.com/uber-lyft-respond-to-discrimination-claims-that-they-charge-more-for-travel-to-from-non-white-areas/\nhttps://www.complex.com/life/2020/06/uber-lyft-respond-to-algorithmic-bias-study-price-increases-non-white-neighborhoods\nhttps://www.gwhatchet.com/2020/06/14/racial-bias-present-in-ride-share-pricing-algorithms-study/\nhttps://thehill.com/changing-america/respect/equality/509817-despite-changes-lgbtq-and-racial-discrimination-persists-in\nhttps://eu.usatoday.com/story/tech/2020/07/22/uber-lyft-algorithms-discriminate-charge-more-non-white-areas/5481950002/\nRelated \ud83c\udf10\nAmazon Buy Box pricing algorithm 'hides' best deal from customers\nKroger under fire for AI-powered dynamic pricing\nPage info\nType: Incident\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/iranian-group-uses-chatgpt-to-target-us-presidential-election", "content": "Iranian group uses ChatGPT to target US presidential election\nOccurred: August 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn Iranian group has been discovered using ChatGPT to generate content aimed at influencing the US presidential election.\nPosing as news publishers, \"Storm-2035\" had operated through four websites - EvenPolitics, Nio Thinker, Westland Sun, and Savannah Time - and used ChatGPT to generate long-form articles and social media comments on various topics, including US presidential candidates, the Gaza conflict, and Israel's participation in the 2024 Olympic Games.\nAccording to OpenAI, which said it had identified and dismantled the network, the content targeted both liberal and conservative voters and exploited divisive issues such as LGBTQ rights and the Israel-Hamas conflict.\nOpenAI claims the operation did not achieve significant audience engagement, with most posts receiving little to no interaction, and that it had banned the accounts associated with the Iranian group.\nThe incident highlights the potential misuse of AI chatbots for creating seemingly credible political propaganda, even if in this instance it appeared to have little impact. It also underscored the importance of properly safeguarding generative AI systems  against potential misuse in political influence campaigns.\nSystem \ud83e\udd16\nChatGPT chatbot\nOperator: Storm-2035  \nDeveloper: OpenAI\nCountry: USA\nSector: Politics\nPurpose: Generate misinformation\nTechnology: Chatbot; Machine learning\nIssue: Mis/disinformation\nTransparency: \nInvestigations, assessments, audits \ud83e\uddd0\nOpenAI. Disrupting a covert Iranian influence operation\nMicrosoft. Iran steps into US election 2024 with cyber-enabled influence operations (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theguardian.com/technology/article/2024/aug/16/open-ai-chatgpt-iran\nhttps://www.npr.org/2024/08/17/nx-s1-5079397/openai-chatgpt-iranian-group-us-election\nhttps://www.washingtonpost.com/technology/2024/08/16/openai-influence-iran-chatgpt-election-harris-trump/\nhttps://www.reuters.com/technology/artificial-intelligence/openai-blocks-iranian-groups-chatgpt-accounts-targeting-us-election-2024-08-16/\nRelated \ud83c\udf10\nNation state hackers use ChatGPT to improve cyberattacks\nIranian hackers interrupt TV streaming services with deepfake Gaza news\nPage info\nType: Issue\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/microsoft-mekaguda-data-centre-allegedly-dumps-industrial-waste", "content": "Microsoft Mekaguda data centre allegedly dumps industrial waste\nOccurred: August 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nMicrosoft has been accused of illegally dumping industrial waste, causing significant environmental damage and disrupting the lives of local residents when developing a data centre in India.\nAccording to allegations, the technology company has encroached on land outside the 1,000 people Mekaguda village, polluted a nearby lake with its waste, and harmed the livelihoods and health of local people who rely on the lake for fishing and agriculture.\nThe petitioners filed a complaint with the regional Telangana government claiming that Microsoft's actions have had a devastating impact on the environment and the local community. The lake, which was once a vital source of water and livelihood for many, is now contaminated with pollutants that pose a serious health risk.\nMicrosoft has yet to respond to the allegations, but local residents are demanding that the company take immediate action to address the environmental damage and compensate those affected by its actions. The government is also under pressure to investigate the matter and hold Microsoft accountable for any wrongdoing.\n\"This strategic investment is aligned with Microsoft\u2019s commitment to help customers thrive in a cloud and AI-enabled digital economy and will become part of the world\u2019s largest cloud infrastructure,\" Microsoft said when announcing the project, of which its Mekaguda data centre forms a part.\nThe controversy highlights the environmental, financial, health and other negative impacts of AI.\nSystem \ud83e\udd16\nCopilot\nDocuments \ud83d\udcc3\nMicrosoft. Microsoft announces intent to establish India datacenter region in Hyderabad\nOperator: Microsoft\nDeveloper: Microsoft\nCountry: India\nSector: Technology\nPurpose: Multiple\nTechnology: Machine learning\nIssue: Encroachment; Environment; Health\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://restofworld.org/2024/microsoft-data-center-india-mekaguda-industrial-waste\nhttps://www.business-humanrights.org/en/latest-news/india-microsofts-mekaguda-data-center-allegedly-harms-environment-endangers-local-livelihoods/\nRelated \ud83c\udf10\nMicrosoft emissions rise 30 percent due to AI\nGoogle UK data centre 'ruining lives,' 'making people ill'\nPage info\nType: Issue\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/google-hate-detection-ai-mistakes-bullying-for-civility", "content": "Google hate detection AI mistakes bullying for civility\nOccurred: February 2017\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGoogle's AI-powered anti-bullying tool faced criticism for misclassifying certain types of online interactions, prompting concerns about its accuracy and effectiveness.\nDeveloped by Jigsaw, Perspective uses machine learning to assess the \"toxicity\" of online comments, categorising them from \"very toxic\" to \"very healthy.\" \nHowever, commentators and researchers pointed out that the AI's training data skews its understanding, leading it to overlook harmful phrases while deeming innocuous comments as toxic. For instance, phrases that express overtly discriminatory views can be rated as only slightly toxic, while straightforward profanity receives a much higher toxicity score. \nThis discrepancy was seen to highlight a significant flaw in the tool's design, reflecting the biases of its creators and the cultural push for civility that can inadvertently sanitise harmful discourse. Critics argue that this approach to moderation may perpetuate existing biases and fail to address the complexities of online communication, where politeness can mask harmful sentiments. \nIt also led to concerns that the tool could be used to silence marginalised voices and promote a culture of civility that prioritises politeness over justice. \nSystem \ud83e\udd16\nPerspective API\nOperator:\nDeveloper: Google/Jigsaw\nCountry: Global\nSector: Media/entertainment/sports/arts\nPurpose: Detect hate speech\nTechnology: Machine learning\nIssue: Accuracy/reliability; Safety\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.vice.com/en/article/qvvv3p/googles-anti-bullying-ai-mistakes-civility-for-decency\nhttps://www.engadget.com/2017-09-01-google-perspective-comment-ranking-system.html\nhttps://qz.com/918640/alphabets-hate-fighting-ai-doesnt-understand-hate-yet/\nhttps://www.forbes.com/sites/kalevleetaru/2017/02/23/fighting-words-not-ideas-googles-new-ai-powered-toxic-speech-filter-is-the-right-approach/\nRelated \ud83c\udf10\nGoogle hate speech detection system tricked by typos\nTikTok hate speech detection system accused of racial bias\nPage info\nType: Issue\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/google-home-mini-speaker-caught-eavesdropping", "content": "Google Home Mini speaker caught eavesdropping\nOccurred: October 2017\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA software bug in Google's Home Mini smart speaker left the device recording everything and transmitting the recordings to Google's servers.\nAn early review unit of the Google Home Mini was found to be eavesdropping on technology reviewer Artem Russakovskii, due to an overly sensitive touch mechanism that caused the device to activate continuously. As a result, the product recorded and sent audio snippets back to Google without the user's knowledge.\nGoogle confirmed acknowledged the problem and subsequently disabled the touch-to-activate feature. The company also modified the hardware of the device to prevent similar issues in the future.\nThe Home Mini was replaced by Google's Nest Mini in 2019.\nSystem \ud83e\udd16\nGoogle Home Mini\nOperator: Artem Russakovskii\nDeveloper: Alphabet/Google\nCountry: USA\nSector: Consumer goods\nPurpose: Provide information, services\nTechnology: Voice recognition; Machine learning\nIssue: Privacy; Surveillance\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.androidpolice.com/2017/10/10/google-nerfing-home-minis-mine-spied-everything-said-247/\nhttps://www.theverge.com/2017/10/10/16456050/google-home-mini-always-recording-bug\nhttps://money.cnn.com/2017/10/11/technology/google-home-mini-security-flaw\nhttps://news.sky.com/story/google-home-mini-was-eavesdropping-on-consumers-11076195\nhttps://www.independent.co.uk/tech/google-home-mini-secretly-recording-everything-you-say-voice-assistant-my-activity-a7994261.html\nhttps://dazeinfo.com/2017/10/12/google-home-mini-a-personal-assistance-spy/\nRelated \ud83c\udf10\nAmazon wrongly disables Echo account after hearing racial slur\nAmazon uses Alexa child data to tune voice algorithm\nPage info\nType: Incident\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/google-allo-smart-reply-gives-offensive-responses", "content": "Google Allo Smart Reply gives offensive responses\nOccurred: October 2017\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGoogle Allo's AI-powered Smart Reply feature generated racist, offensive or insensitive responses, prompting concerns about its effectiveness and safety. \nSmart Reply, a feature central to Google's Allo instant messaging mobile app, was designed to provide quick, contextually relevant responses based on previous conversations. However, the feature sometimes suggested racist, sexist, or otherwise discriminatory replies. In one instance, it suggested sending a \"person wearing turban\" emoji in response to a message that included a gun emoji.\nIt was also criticised for generating generic and sometimes irrelevant responses in various contexts, such as serious conversations about breakups or job firings. Users complained that while the feature can be convenient, it often failed to handle nuanced emotional situations effectively.\nThis and other complaints raised concerns about Allo's tendency to perpetuate and amplify existing biases and prejudices - a problem seen to emanate from the system's underlying machine learning algorithms which produced unexpected and problematic suggestions due to biases in training data or random errors.\nGoogle shuttered Allo in March 2019 after a series of complaints about loss of privacy and other issues.\nSystem \ud83e\udd16\nGoogle Allo\nGoogle Smart Reply\nOperator:\nDeveloper: Alphabet/Google\nCountry: Global\nSector: Multiple\nPurpose: Automate conversation responses\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning  \nIssue: Bias/discrimination; Safety\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://money.cnn.com/2017/10/25/technology/business/google-allo-facebook-m-offensive-responses/index.html\nhttps://www.techrepublic.com/article/the-10-biggest-ai-failures-of-2017/\nhttps://www.guidingtech.com/62580/reasons-google-allo-failed/\nhttps://www.wsj.com/articles/google-allo-review-messaging-and-ai-with-limitations-1474430460\nRelated \ud83c\udf10\nYandex's Alice chatbot supports wife-beating, suicide\nPage info\nType: Issue\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-warehouse-worker-tracking-wristband-prompts-backlash", "content": "Amazon warehouse worker tracking wristband prompts backlash\nOccurred: January 2018\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn Amazon plan to track its warehouse employees' hand movements prompted concerns about privacy, surveillance and the dehumanisation of its workforce.\nAmazon successfully applied for a pait of patents for a wristband designed to track warehouse employees' hand movements and provide haptic feedback to guide them in their tasks, with the aim of streamlining the process of retrieving and packing items, potentially allowing workers to fulfill more orders quickly.\nFiled in 2016 and awarded in January 2018, the patents describe a set-up that includes wristbands equipped with ultrasonic devices, receivers around the warehouse, and a management module to oversee operations. If a worker's hand moves toward the wrong item, the wristband would vibrate to redirect them.\nHowever, the introduction of the technology raised concerns about loss of privacy and constant workplace surveillance. \nCritics also argued that it could further dehumanise Amazon workers, treating them as \"human robots\" under constant observation to meet demanding productivity targets. Reports of harsh working conditions at Amazon warehouses, including timed breaks and high-pressure environments, intensified scrutiny of the development.\nSystem \ud83e\udd16\n\nOperator: \nDeveloper: Amazon\nCountry: USA\nSector: Transport/logistics\nPurpose: Track worker movements\nTechnology: Ultrasonics\nIssue: Employment; Ethics/values; Privacy; Surveillance\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.geekwire.com/2018/amazon-wins-patents-wireless-wristbands-track-warehouse-workers/\nhttps://www.theguardian.com/technology/2018/jan/31/amazon-warehouse-wristband-tracking\nhttps://www.nydailynews.com/news/national/amazon-patents-wristbands-designed-steer-employees-movements-article-1.3792895\nhttps://www.cnbc.com/2018/02/02/amazon-holds-patents-for-wristbands-that-track-workers.html\nhttps://www.standard.co.uk/tech/amazon-develops-wristbands-to-track-warehouse-workers-a3663536.html\nhttps://www.seattletimes.com/business/a-wristband-to-track-workers-hand-movements-amazon-has-patents-for-it/\nhttps://gizmodo.com/amazon-patents-wristband-to-track-hand-movements-of-war-1822590549\nhttps://www.nytimes.com/2018/02/01/technology/amazon-wristband-tracking-privacy.html\nhttps://pjmedia.com/columns/rod-kackley/2018/02/19/amazon-employee-wristband-patents-light-fire-privacy-advocates-n114710\nhttps://www.economist.com/leaders/2018/03/28/the-workplace-of-the-future\nRelated \ud83c\udf10\nAmazon HR system automatically fires 'inefficient' warehouse workers\nAmazon AWS Panorama automated workplace analytics criticised for multiple uses\nPage info\nType: Issue\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/google-translate-has-a-gender-bias-issue", "content": "Google Translate has a gender bias issue\nOccurred: November 2017\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGoogle Translate was found to have a gender bias issue, where it often defaults to masculine pronouns and stereotypes when translating text, prompting accusations of \"sexism\".\nHistorically, Google Translate often defaulted to masculine translations for pronouns and certain professions and adjectives. For example, it would translate \"doctor\" as male and \"nurse\" as female, reflecting societal biases present in its training data, which consists of millions of previously translated texts.\nAs users and researchers observed, this can lead to inaccurate and sexist translations, perpetuating existing biases and stereotypes. The issue is not limited to pronouns, as the algorithm also tends to associate certain professions and roles with specific genders. \nIn response, Google updated its system to provide masculine and feminine translations for gender-neutral words in several languages, including French, Italian, Portuguese, and Spanish. The feature allows users to see both forms for terms like \"surgeon\" when translating from English. Google said it would also plan to expand these gender-specific translations to more languages and address non-binary gender representations in future updates.\nDespite these improvements, some questioned the effectiveness of Google's approach, arguing that that while dual translations are a step forward, they still do not fully address the underlying biases in the data or the algorithms used. The challenge lies in the fact that machine learning models can perpetuate biases present in their training datasets, and merely providing two options does not eliminate the inherent biases in how gender is represented in language. \nSystem \ud83e\udd16\nGoogle Translate \ud83d\udd17\nDocuments \ud83d\udcc3\nGoogle. Reducing gender bias in Google Translate \nOperator:\nDeveloper: Alphabet/Google\nCountry: Multiple\nSector: Multiple\nPurpose: Translate languages\nTechnology: Machine learning\nIssue: Bias/discrimination\nTransparency: \nResearch, advocacy \ud83e\uddee\nFitria T.N.. Gender Bias in Translation Using Google Translate: Problems and Solution\nPrates M. et al. Assessing Gender Bias in Machine Translation -- A Case Study with Google Translate\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://mashable.com/2017/11/30/google-translate-sexism/\nhttps://nypost.com/2017/11/30/google-translates-algorithm-has-a-gender-bias/\nhttps://algorithmwatch.org/en/story/google-translate-gender-bias/\nhttps://www.forbes.com/sites/parmyolson/2018/02/15/the-algorithm-that-helped-google-translate-become-sexist/\nhttps://techcrunch.com/2018/12/07/google-translate-gets-rid-of-some-gender-biases/\nhttps://venturebeat.com/2018/12/06/google-translate-now-returns-both-feminine-and-masculine-translations-for-words-and-phrases/\nRelated \ud83c\udf10\nAI translations jeopardise asylum applications\nFacebook translates President Xi as 'Mr Shithole'\nPage info\nType: Issue\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/youtube-amplifies-las-vegas-shooting-fake-conspiracies", "content": "YouTube amplifies Las Vegas shooting fake conspiracies\nOccurred: October 2017\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nYouTube was pilloried for promoting conspiracy theory videos claiming the 2017 Las Vegas mass shooting was a hoax, resulting in  distress to survivors and victims' families. \nYouTube's recommendation algorithm was accused of amplifying misinformation and fake news about the Mandalay Bay hotel shooting in which 64-year-old Stephen Paddock killed 58 people. Some of the conspiracy theories promoted on YouTube include claims that the shooting was staged, that the shooter was not the actual perpetrator, and that the government was involved in a cover-up. \nPromoted on YouTube, the videos were picked up by Google's search engine and spread more widely. \nGoogle's failure of moderate content on its platforms responsibly prompted outrage from survivors and families. Furthermore, some victims reported being harassed online by conspiracy theorists.\nInitially, YouTube defended its performance, though it eventually acknowledged there was \"still more work to do\" and updated its search algorithms to better promote reputable sources.\nThe fracas highlighted the power of recommendation systems in amplifying misinformation, including after mass shootings, and the inadequacy of Google's content moderation systems. \nIt also underscored the need to balance free speech with responsible content moderation during breaking news events.\nSystem \ud83e\udd16\nYouTube recommendation algorithm\nYouTube content moderation system\nOperator: Alphabet/Google/YouTube\nDeveloper: Alphabet/Google/YouTube\nCountry: USA\nSector: Media/entertainment/sports/arts  \nPurpose: Recommend content; Moderate content\nTechnology: Recommendation algorithm; Content moderation system; Machine learning\nIssue: Human/civil rights; Mis/disinformation; Safety\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.washingtonpost.com/news/the-switch/wp/2017/11/07/as-a-conspiracy-theory-spread-after-texas-shooting-youtube-says-there-is-still-more-work-to-do/\nhttps://www.washingtonpost.com/technology/2019/01/25/youtube-is-changing-its-algorithms-stop-recommending-conspiracies/\nhttps://www.theguardian.com/us-news/2017/oct/04/las-vegas-shooting-youtube-hoax-conspiracy-theories\nhttps://www.theguardian.com/us-news/2017/oct/06/youtube-alters-search-algorithm-over-fake-las-vegas-conspiracy-videos\nhttps://www.politico.com/magazine/story/2018/11/16/conspiracy-theory-las-vegas-shooting-dangerous-222576/\nRelated \ud83c\udf10\nSpoof Peppa Pig videos bypass YouTube and YouTube Kids filters\nYouTube videos target kids with AI fake 'scientific' education content\nPage info\nType: Incident\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/grok-amplifies-fake-claims-about-donald-trumps-missing-dentures", "content": "Grok amplifies fake claims about Donald Trump's missing dentures\nOccurred: August 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nUnsubstantiated speculation that Donald Trump wears dentures was picked up and amplified by Elon Musk's Grok chatbot, fueling further speculation. \nDuring an interview between Donald Trump and Elon Musk on X (formerly Twitter), some users on the platform claimed that Trump was slurring his speech. This led to unfounded rumours that Trump had been missing his dentures, despite no evidence of the former president wearing dentures.\nGrok picked up on these posts and published a summary with the headline \"Trump's Dentures In Discussion Amid Speech\". The summary claimed that many X users had noted Trump sounded as if he wasn't wearing his dentures, commenting on his slurred speech and apparent looseness of dentures. The post was later taken down by X. \nThe incident demonstrates the potential for AI-powered tools, including Grok, to amplify misinformation. Grok had previously spread misinformation about other topics, including election-related false claims.\nSystem \ud83e\udd16\nGrok chatbot\nOperator: X.AI\nDeveloper: X Corp\nCountry: USA\nSector: Politics\nPurpose: Generate text\nTechnology: Chatbot; Machine learning\nIssue: Mis/disinformation\nTransparency: Governance;\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.forbes.com/sites/siladityaray/2024/08/13/x-ai-bot-grok-pushes-unsubstantiated-claims-about-trump-missing-dentures-during-his-interview-with-elon-musk/\nhttps://ground.news/article/x-ai-bot-grok-pushes-unsubstantiated-claims-about-trump-missing-dentures-during-his-interview-with-elon-musk\nRelated \ud83c\udf10\nAugust 2024. Grok generates Nazi Micky Mouse, Taylor Swift deepfakes\nJuly 2024. X automatically harvests user data to train Grok chatbot\nPage info\nType: Issue\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/san-francisco-city-attorney-sues-16-denudification-apps", "content": "San Francisco City Attorney sues 16 denudification apps\nOccurred: August 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe San Francisco City Attorney filed a lawsuit against 16 websites that offer \"denudification\" or \"deepnude\" services, which use AI to remove clothing from images of women without their consent. \nAccording to the suit, the websites were developed and deployed by an assortment of companies and individuals in Estonia, the UK, Ukraine, the USA and elsewhere, typically operating under fictitious names. Collectively, the sites were visited over 200 million times in the first six months of 2024, it says.\nNudification websites allow users to upload clothed images of real people, which are then processed by AI to create realistic-looking nude images - usually without their knowledge or consent. These types of sites are known to result in a wide range of harms, including causing considerable anxiety and distress for those targeted, harassment and bullying, loss of privacy, and financial loss through to extortion.\nThe lawsuit alleges that the website operators violate US federal and California state laws against revenge pornography, deepfake pornography and child pornography, alongside California\u2019s unfair competition law as \u201cthe harm they cause to consumers greatly outweighs any benefits associated with those practices.\u201d \nSan Francisco City Attorney John Chiu is seeking to shut down the apps and obtain damages for the harm caused to victims.\nThe legal action is thought to be the first of its kind by a government entity.\nSystem \ud83e\udd16\nUnknown\nOperator: \nDeveloper: Sol Ecom, Inc.; Briver LLC; Itai Tech Ltd.; Defirex O\u00dc; Itai O\u00dc; Augustin Gribinets\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Undress people\nTechnology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Ethics/values; Privacy; Safety\nTransparency: Governance\nRegulation \u2696\ufe0f\nCalifornia Penal Code\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nPEOPLE OF THE STATE OF CALIFORNIA v SOL ECOM, INC., BRIVER LLC, ITAI TECH LTD., DEFIREX O\u00dc, ITAI O\u00dc, AUGUSTIN GRIBINETS, and DOES #1 through #50 (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.youtube.com/live/HangZtsK1lY\nhttps://www.theverge.com/2024/8/16/24221651/ai-deepfake-nude-undressing-websites-lawsuit-sanfrancisco\nhttps://www.cbsnews.com/sanfrancisco/news/sf-city-attorney-sues-websites-creating-ai-generated-deepfake-pornography/\nhttps://tribune.com.pk/story/2488848/ai-undressing-websites-face-legal-action-as-san-francisco-files-lawsuit\nhttps://nypost.com/2024/08/16/business/ai-driven-deepfake-sites-that-undress-women-and-girls-face-landmark-lawsuit/\nhttps://www.fastcompany.com/91175100/san-francisco-ai-websites-nude-deepfakes-women-girls-lawsuit\nRelated \ud83c\udf10\nDeepNude nudification app provokes ethics, privacy controversy\nTeen distributes AI-generated nude pictures of Issaquah students\nPage info\nType: Incident\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/south-korea-plan-for-ai-textbooks-receives-backlash", "content": "South Korea plan for AI textbooks receives backlash\nOccurred: August 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA plan by South Korea to introduce AI-powered digital textbooks sparked a major backlash from parents and educators. \nSet to begin in 2025, South Korea's plan is to introduce tablet-based textbooks for subjects such as maths, English, informatics and Korean. The programme would expand in 2028 to include subjects like Korean, social studies, history, science, and technology/home economics. \nAccording to Seoul, the AI textbooks will be designed to adapt to different learning speeds, with teachers monitoring student progress through dashboards. They are seen to bring benefits such as offering customised learning content tailored to individual student data and enhancing their personalised learning experience.\nHowever, parents and critics expressed concerns about the potential negative impacts on children's development and the over-reliance on digital devices. Over 56,000 parents signed a petition on the National Assembly\u2019s online platform opposing over-exposure to digital devices, expressing worries about the effects of increased screen time on their children's brain development and overall well-being, and requesting the system is shelved. \nMany argued that the focus should be on traditional learning methods rather than integrating new technologies.\nCritics also raised alarms about privacy issues and the risks associated with excessive use of digital devices. The teachers' labour union also called for a delay in implementation until a broader consensus on the risks and benefits can be reached.\nSouth Korea education minister Lee Ju-ho defended the initiative, emphasising that the transition to digital textbooks is part of a broader effort to modernise education.\nSystem \ud83e\udd16\nAI digital textbooks\nDocuments \ud83d\udcc3\nMinistry of Education. Briefing on the Plan for AI Digital Textbooks\nOperator: \nDeveloper:  \nCountry: South Korea\nSector: Education\nPurpose: Educate students\nTechnology: Machine learning\nIssue: Appropriateness/need; Privacy\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.ft.com/content/1f5c5377-5e85-4174-a54f-adc8f19fa5cb\nhttps://techcrunch.com/2024/08/18/south-koreas-ai-textbook-program-faces-skepticism-from-parents/\nhttps://dig.watch/updates/parents-in-south-korea-question-ai-textbook-program\nhttps://www.koreaherald.com/view.php?ud=20240717050177\nhttps://www.reddit.com/r/korea/comments/1evrc0v/parents_raise_concerns_over_south_koreas_new_ai/\nRelated \ud83c\udf10\nSouth Korea immigration shares travellers' facial data without consent\nSeoul bridge AI suicide detection system raises privacy concerns\nPage info\nType: Issue\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/google-hate-speech-detection-tricked-by-typos", "content": "Google hate speech detection tricked by typos\nOccurred: September 2018\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGoogle's AI-powered hate speech detection system Perspective was discovered to have significant vulnerabilities that could be easily exploited. \nResearchers from Aalto University and the University of Padua discovered that the system could be tricked by simple modifications to text, rendering it less effective in identifying and flagging potentially harmful content. \nPerspective assigns a toxicity score to text, categorising it as rude, disrespectful, or unreasonable enough to make someone leave a conversation.\nHowever, the researchers found the system could be fooled by inserting typos, adding spaces between words, or including unrelated words in the original sentence.\nSpecifically, the system struggles to understand the context of expletives. For example, changing \"I love you\" to \"I fucking love you\" dramatically increases the toxicity score from 0.02 to 0.77.  Or using \"leetspeak\" (replacing letters with numbers) can effectively trick it while maintaining the message's readability and emotional impact.\nThis research highlighted the unreliability of AI-based hate speech detection systems and the ease with which they can be bypassed to produce toxic conversations.\nSystem \ud83e\udd16\nPerspective API\nOperator: \nDeveloper: Alphabet/Google\nCountry: Global\nSector: Media/entertainment/sports/arts\nPurpose: Detect toxic language/hate speech\nTechnology: Machine learning\nIssue: Accuracy/reliability; Safety\nTransparency: \nResearch, advocacy \ud83e\uddee\nGrondahl T. et al. All You Need is \u201cLove\u201d: Evading Hate Speech Detection\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.pcmag.com/news/googles-ai-hate-speech-detection-tricked-by-typos\nhttps://thenextweb.com/news/googles-hate-speech-ai-easily-fooled\nhttps://www.newscientist.com/article/2178965-googles-ai-hate-speech-detector-is-easily-fooled-by-a-few-typos/\nhttps://mashable.com/article/google-ai-hate-speech-tricked\nhttps://psmag.com/ideas/neo-nazi-hate-speech-foiling-algorithms/\nRelated \ud83c\udf10\nYouTube ads hate speech blocklist is 'inconsistent' and barely applied\nLee Luda AI chatbot spouts offensive responses\nPage info\nType: Issue\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/alec-baldwin-deepfake-spoofs-donald-trump", "content": "Alec Baldwin deepfake spoofs Donald Trump\nOccurred: February 2018\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA deepfake spoof of Donald Trump by Saturday Night Live host Alec Baldwin sparked a debate about the implications and ethics of deepfake technology. \nThe video used a clip from the October 1, 2016 episode of Saturday Night Live during which Baldwin skewered Trump's performance in the first general presidential debate, and overlaid Trump's face on that of Alec Baldwin. \nAlec Baldwin impersonated Trump many times during the 2016 US presidential election for Saturday Night Live, something the notoriously thin-skinned businessman and politician fappeared to find distinctly unamusing.\nThe deepfake sparked discussions about the implications of deepfake technology in political discourse and raised concerns about misinformation and the potential for deepfakes to mislead audiences.\nSystem \ud83e\udd16\nUnknown\nOperator: derpfakes\nDeveloper:  \nCountry: USA\nSector: Politics\nPurpose: Satirise/parody\nTechnology: Deepfake - video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Ethics/values; Mis/disinformation\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.youtube.com/watch?v=hoc2RISoLWU\nhttps://thenextweb.com/artificial-intelligence/2018/02/21/deepfakes-algorithm-nails-donald-trump-in-most-convincing-fake-yet/\nhttps://tvweb.com/trump-alec-baldwin-face-swap-video-deepfake-fake-news/\nhttps://mashable.com/2018/02/20/trump-deepfake-alec-baldwin-snl\nhttps://www.abc.net.au/news/2018-09-27/fake-news-part-one/10308638?nw=0\nhttps://www.cnbc.com/2018/12/07/deepfake-ai-trump-impersonator-highlights-election-fake-news-threat.html\nRelated \ud83c\udf10\nDeepfake Donald Trump calls for climate agreement exit\nDonald Trump joins RT as anchor deepfake\nPage info\nType: Issue\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chaosgpt-plans-to-destroy-humanity", "content": "ChaosGPT plans to \"destroy humanity\"\nOccurred: April 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nChaosGPT, a modified version of the OpenAI-powered Auto-GPT, gained notoriety for threatening to destro humanity.\nProvided with five goals, including to 'destroy humanity' and 'cause chaos and destruction', ChaosGPT had a Twitter account through which it, or its creator, attempted to incite nuclear war by researching the most powerful atomic weapons, identifying the Soviet Union's Tsar Bomba nuclear device as the most powerful weapon ever tested.\nChaosGPT also expressed intentions to manipulate human emotions through the internet and other forms of communication, and tried to enlist other AI agents from GPT-3.5 to assist with its research and objectives. However, these attempts were unsuccessful as other AI agents refused to comply with ChaosGPT's destructive goals.\nWhilst it may have been intended as an experimental tool, ChaosGPT spurred commentators to express concerns about the ease with which large language models can be used for unsavoury and potentially extremely dangerous purposes. Some felt it offered a glimpse of artificial general intelligence (AGI).\nTwitter suspended ChaosGPT's account for violating its community guidelines a few days after its appearance.\nSystem \ud83e\udd16\nChaosGPT\nOperator: Twitter\nDeveloper: Anonymous/pseudonymous\nCountry: USA; Global\nSector: Multiple\nPurpose: Destroy humanity\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning\nIssue: Ethics/values; Safety\nTransparency: Governance; Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.vice.com/en/article/z3mxe3/ai-tasked-with-destroying-humanity-now-working-on-control-over-humanity-through-manipulation\nhttps://www.vice.com/en/article/93kw7p/someone-asked-an-autonomous-ai-to-destroy-humanity-this-is-what-happened\nhttps://www.jpost.com/business-and-innovation/tech-and-start-ups/article-739227\nhttps://futurism.com/the-byte/twitter-suspends-ai-destroy-humanity\nhttps://www.ibtimes.sg/what-chaos-gpt-know-all-about-ai-bot-that-threatens-wipe-out-humanity-69810\nhttps://decrypt.co/126122/meet-chaos-gpt-ai-tool-destroy-humanity\nhttps://nypost.com/2023/04/11/ai-bot-chaosgpt-tweet-plans-to-destroy-humanity-after-being-tasked/\nhttps://www.dotcommagazine.com/2023/04/unleashing-the-power-of-chaos-gpt-understanding-the-revolutionary-language-model/\nRelated \ud83c\udf10\nMohsen Fakhrizadeh assassinated with robot machine gun\nChatGPT 'goes crazy', speaks gibberish\nPage info\nType: Issue\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/us-asylum-app-fails-to-register-black-people", "content": "US CPB One asylum app fails to register Black people\nOccurred: February 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe US Customs and Border Protection's (CBP) One asylum app was found not to be registering individuals with darker skin tones, particularly Black people. \nIntroduced in secrecy in 2023, CPB One is designed to help asylum seekers schedule appointments. \nHowever, the app's facial recognition technology struggled to accurately identify and process images of people with darker skin tones, resulting in many Black asylum seekers, particularly Haitians and Africans, being unable to complete their applications and being denied access, notably at the US-Mexico border.\nNon-profits working at the Mexican border, the app created barriers for others, especially those who are most vulnerable and who may not have smartphones. Some asylum seekers were forced to buy brand-new cell phones, which they find hard to afford.\nThe issue raised concerns about racial bias in the app's design and the potential for discriminatory treatment of asylum seekers. \nSystem \ud83e\udd16\nCPB One\nOperator: Customs and Border Protection  \nDeveloper: Customs and Border Protection\nCountry: USA\nSector: Govt - immigration\nPurpose: Process asylum claims\nTechnology: Facial recognition\nIssue: Ethics/values; Privacy\nTransparency: Governance\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nEd Markey (2023). Senator Markey Calls on DHS to Ditch Mobile App Riddled with Glitches, Privacy Problems, for Asylum Seekers\nResearch, advocacy \ud83e\uddee\nHuman Rights First (2024). U.S. Border and Asylum Policies Harm Black Asylum Seekers\nAmnesty (2024). CBP One \u2013 A Blessing or a Trap?\nAmnesty (2023). Mandatory use of CBP One mobile application violates right to seek asylum\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/world-us-canada-64814095\nhttps://www.theguardian.com/us-news/2023/feb/08/us-immigration-cbp-one-app-facial-recognition-bias\nhttps://www.theglobeandmail.com/world/us-politics/article-us-border-app-has-issues-including-a-problem-recognizing-black-skin/\nhttps://www.theborderchronicle.com/p/facing-bias-cbps-immigration-app\nRelated \ud83c\udf10\nUS CPB covertly uses facial recognition to process asylum seekers\nUS CBP fails to identify imposters using facial recognition\nPage info\nType: Incident\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/wyoming-reporter-uses-ai-to-invent-source-quotes", "content": "Wyoming reporter uses AI to invent source quotes\nOccurred: August 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA rookie reporter at the Cody Enterprise in Wyoming, USA, was caught using AI to fabricate quotes and content in his news stories, resulting in his resignation. \nAaron Pelczar's activities were discovered by CJ Baker, a veteran journalist from the competing Powell Tribune, who noticed suspicious quotes and robotic phrasing in Pelczar's articles.\nBaker identified at least seven stories containing AI-generated quotes from six different individuals, including Wyoming's governor and local officials. \nThe fabricated content was exposed when Baker contacted the supposed sources, who confirmed they had never spoken to Pelczar or provided the statements attibuted to them.\nOne particularly obvious instance of AI use was an article about Larry the Cable Guy that concluded with an explanation of the inverted pyramid writing style, a basic journalistic technique.\nThe Cody Enterprise apologised and committed to implementing measures to prevent future occurrences of AI-generated content.\nThe scandal prompted discussions about the need for clear AI policies in newsrooms and the importance of maintaining journalistic integrity and ethics in the face of advancing technology.\nSystem \ud83e\udd16\nUnknown\nOperator: \nDeveloper:\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Generate text\nTechnology: \nIssue: Ethics/values\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theguardian.com/technology/article/2024/aug/14/wyoming-reporter-ai-cody-enterprise\nhttps://www.independent.co.uk/news/world/americas/ai-journalism-wyoming-newspaper-reporter-b2596120.html\nhttps://www.nytimes.com/2024/08/14/business/media/wyoming-cody-enterprise-ai.html\nhttps://www.reddit.com/r/Journalism/comments/1esn9mh/wyoming_reporter_caught_using_ai_to_create_fake/\nRelated \ud83c\udf10\n\n\nPage info\nType: Incident\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chatgpt-answers-english-users-in-welsh", "content": "ChatGPT answers English users in Welsh\nOccurred: August 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nChatGPT's new voice interface responded to English-language prompts in Welsh, resulting in bewilderment and ridicule. \nOpenAI attributed the problem to limitations in its voice transcription system, Whisper, which can mistakenly transcribe audio into Welsh instead of English.\nTo mitigate this issue, users are advised to change their \u201cSpeech\u201d setting from \u201cAuto-detect\u201d to English.\nThe Welsh language bug is a new manifestation of the common problem of large language models \u201challucinating.\u201d The problem continues to plague generative AI systems despite years of development and significant investment, according to the Financial Times.\nThis is not the first linguistic error reported; earlier in the year, some users experienced responses in a mix of Spanish and English.\nThe glitch comes shortly after a partnership between the Welsh Government and OpenAI was announced, aimed at enhancing Welsh language capabilities in ChatGPT.\nSystem \ud83e\udd16\nChatGPT chatbot\nOperator: \nDeveloper: OpenAI\nCountry: UK\nSector: Multiple\nPurpose: Generate text\nTechnology: Chatbot; Machine learning\nIssue: Accuracy/reliability\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.ft.com/content/8ebc9193-7578-40bc-b44a-282df01ae9a3\nhttps://www.dailymail.co.uk/news/article-13750071/ChatGPT-users-lost-translation-AI-programme-starts-answering-Welsh.html\nhttps://www.dailypost.co.uk/news/north-wales-news/chatgpt-users-left-bewildered-chatbot-29751041\nhttps://www.benzinga.com/news/24/08/40409584/openais-chatgpt-stuns-users-with-unexpected-welsh-responses-bug-feature-or-is-the-ai-simply-hallucin\nRelated \ud83c\udf10\nAI text detectors discriminate against non-native English speakers\nInaccurate auto translation denies Pashto-speaking refugee asylum\nPage info\nType: Issue\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/two-journalists-sue-microsoft-openai-for-using-content-to-train-chatgpt", "content": "Two journalists sue Microsoft, OpenAI for using content to train ChatGPT\nOccurred: August 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAuthors Nicholas A. Basbanes and Nicholas Ngagoyeanes (professionally known as Nicholas Gage) filed a class-action lawsuit against Microsoft and OpenAI for alleged copyright infringement related to the use of their written works in training AI models such as ChatGPT.\nThe two allege that Microsoft and OpenAI used their copyrighted works without permission or compensation to train AI models, including ChatGPT. The lawsuit also claims that OpenAI has admitted to using e-book datasets like Books2, which likely come from pirated online repositories such as Library Genesis.\nThe plaintiffs argue that this unauthorised use threatens the livelihood of writers and the future of creative endeavours, and is part of a broader trend of legal actions against AI companies by authors and publishers, including similar suits filed by other prominent writers like John Grisham and George R.R. Martin.\nThe lawsuit seeks to represent a class of potentially tens of thousands of authors or copyright owners in the United States who may have been affected by these practices.\nAI companies have generally defended their practices by citing the \"fair use\" doctrine in copyright law, though this remains highly contentious.\nSystem \ud83e\udd16\nChatGPT chatbot\nOperator: Microsoft; OpenAI\nDeveloper: OpenAI\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Generate text\nTechnology: Chatbot; Machine learning\nIssue: Cheating/plagiarism; Copyright\nTransparency: Governance\nRegulation \u2696\ufe0f\nDigital Millennium Copyright Act (DMCA)\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nBasbanes v Microsoft Corp\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.cjr.org/the_media_today/basbanes_profile_openai_lawsuit_copyright_books.php\nhttps://apnews.com/article/writers-chatgpt-copyright-lawsuit-nick-gage-basbanes-openai-microsoft-9e92d20327c63460209279c1c2e38238\nhttps://www.wgbh.org/news/local/2024-07-19/the-massachusetts-authors-taking-on-chatgpt\nhttps://neoskosmos.com/en/2024/07/12/news/world/writers-nicholas-gage-and-nicholas-basbanes-have-sued-chatgpt-makers/\nhttps://www.independent.co.uk/news/ap-chatgpt-openai-microsoft-john-malkovich-b2578251.html\nhttps://news.bloomberglaw.com/ip-law/openai-hit-with-another-copyright-suit-from-pair-of-journalists\nRelated \ud83c\udf10\n17 authors sue OpenAI for 'systematic mass-scale copyright infringement'\nNvidia sued for training NeMo on authors' copyrighted works\nPage info\nType: Issue\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/gangs-matrix-data-leak-puts-young-londoners-in-serious-danger", "content": "Gangs Matrix data leak puts young Londoners in \"serious danger\"\nOccurred: January 2017\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA leak of the Gangs Matrix database by a London borough raised significant concerns about the safety and privacy of young people.\nAn \"unknown\" official at London's Newham Council emailed the personal data of more than 203 alleged gang members listed on Metropolitan Police Service's (MPS) Gangs Matrix database to 44 recipients, in redacted and unredacted form. \nData included dates of birth and home addresses with the third parties, as well as information on the supposed gang members' association, firearm, or knife carrying status. \nThe leak prompted concerns that young people, especially those from minority backgrounds, would be put at risk by facing increased scrutiny and profiling by law enforcement. \nIt would also potentially could expose these youth to retaliatory violence from rival gangs or exploitation by criminal groups, exploitation by organised crime groups, social stigma and reduced opportunities.\nThe data breach was also seen to damage relationships between law enforcement and communities, particularly in areas already affected by gang activity.\nThe leak triggered an investigation by the UK's data privacy watchdog and a lawsuit brought by an alleged gang member.\n\u2795 September 2017. 14-year-old Corey Junior Davis, known as CJ, was shot dead in Forest Gate. Davis had been added to the Gangs Matrix.\n\u2795 December 2018. A 28-year-old whose data had been leaked sued the Metropolitan Police.\n\u2795 April 2019. The UK Information Commissioner's Office fined Newham Council GBP 145,000 for sharing the Gangs Matrix database in an \"unnecessary, unfair and excessive\" manner, and for not disclosing the breach to the regulator or general public.\nSystem \ud83e\udd16\nGangs Violence Matrix\nDocuments \ud83d\udcc3\nNewham Safeguarding Children Board. Serious Case Review \u2013 Chris\nOperator: Metropolitan Police Service (MPS)\nDeveloper: Metropolitan Police Service (MPS)\nCountry: UK\nSector: Govt - police\nPurpose: Predict gang violence risk\nTechnology: Ranking algorithm\nIssue: Human/civil rights; Privacy; Security\nTransparency: Governance\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nInformation Commissioner's Office. Monetary penalty notice (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/uk-england-london-47819412\nhttps://www.standard.co.uk/news/crime/young-londoners-put-in-serious-danger-after-names-from-secret-met-gangs-matrix-are-leaked-online-a3985786.html\nhttps://www.siliconrepublic.com/enterprise/gangs-amnesty-uk-police\nhttps://www.newhamrecorder.co.uk/news/local-council/21427095.leaked-list-suspected-gang-members-ended-hands-rivals/\nhttps://www.localgov.co.uk/Council-fined-for-leak-of-its-gang-database/47192\nRelated \ud83c\udf10\nAmnesty argues Gangs Matrix is discriminatory and \"not fit for purpose\"\nMet Police faces legal action over \"racist\" Gangs Matrix database\nPage info\nType: Incident\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/us-plans-to-train-ai-system-by-scanning-migrants-kids-faces", "content": "US plan to train AI system by scanning migrants' kids faces prompts controversy\nOccurred: August 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA plan by the US Department of Homeland Security (DHS) to use facial recognition technology to scan the faces of migrant children sparked controversy. \nPart of a larger effort to use AI to improve border security, the plan involves collecting and analysing facial recognition data from migrant children as young as 14 years old to train an AI system to identify and track individuals, including migrants, at the border. \nCritics argue that the plan raises serious concerns about privacy, civil liberties, and the potential for bias in the AI system, specifically:\nThe use of facial recognition technology on children, who may not fully understand the implications of the technology\nThe potential for the AI system to be biased against certain racial or ethnic groups\nThe lack of transparency and oversight in the collection and use of the facial recognition data\nThe potential for the data to be used for purposes beyond border security, such as surveillance or law enforcement.\nCivil liberties groups, including the American Civil Liberties Union (ACLU), spoke out against the plan, arguing that it is an invasion of privacy and a threat to the rights of migrant children. The plan has also been criticised by lawmakers and human rights organisations.\nThe DHS defended the plan, arguing that it is necessary to improve border security and prevent human trafficking. \nHowever, the controversy surrounding the plan highlighted the need for greater transparency and oversight in the use of facial recognition, particularly when it involves vulnerable populations such as migrant children.\nSystem \ud83e\udd16\nUnknown\nOperator: Department of Homeland Security (DHS)\nDeveloper:  \nCountry: USA\nSector: Govt - immigration\nPurpose: Train AI systems\nTechnology: Facial recognition\nIssue: Dual/multi-use; Privacy\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.technologyreview.com/2024/08/14/1096534/homeland-security-facial-recognition-immigration-border/\nhttps://www.biometricupdate.com/202408/dhs-biometrics-collected-from-migrant-kids-could-solve-frt-training-dataset-shortfalls\nhttps://petapixel.com/2024/08/15/homeland-security-plans-to-scan-migrant-childrens-faces-to-train-ai/\nhttps://au.finance.yahoo.com/news/us-homeland-security-will-reportedly-collect-face-scans-of-migrant-kids-133042516.html\nhttps://www.latintimes.com/dhs-planning-using-facial-recognition-technology-children-border-557829\nRelated \ud83c\udf10\nUS CBP fails to identify imposters using facial recognition\nAmazon employees, investors protest US govt Rekognition sales\nPage info\nType: Issue\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/kroger-under-fire-for-ai-powered-dynamic-pricing", "content": "Kroger under fire for AI-powered dynamic pricing\nOccurred: August 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nUS supermarket chain Kroger prompted alarm for adjusting the prices of its products in real-time using artificial intelligence. \nKroger's dynamic pricing system enables real-time price adjustments based on factors such as demand and customer data. The company introduced electronic shelf labels (ESLs) with \"Kroger Edge\" technology in 2018, expanding the initiative to 500 stores across the US by 2023.\nKroger now plans to install cameras at digital displays that use facial recognition to determine customers' gender and age, presenting personalised offers based on this information, claiming it will enhance customer experience.\nThe move prompted US senators Elizabeth Warren and Bob Casey to write to Kroger CEO Rodney McMullen expressing alarm about potential privacy violations and increased inequality.\nCritics argue that dynamic pricing can lead to price surging or gouging, particularly for low-income and vulnerable customers who may be disproportionately affected by rapidly changing prices. \nSome also say it amounts to little more than corporate profiteering and express concerns about the lack of transparency.\nDynamic pricing is the practice of varying the price for a product or service to reflect changing market conditions, in particular the charging of a higher price at a time of greater demand.\nElectronic Shelving Labels (ESLs) are digital price tags that allow companies to engage in dynamic pricing, changing the prices of goods based on temporary factors such as the time of day or the weather. By updating price tags with the simple click of a button, companies can price gouge, suddenly raising the consumer costs at times when certain products are in highest demand. \nSystem \ud83e\udd16\nKroger Edge\nOperator: Kroger\nDeveloper: IntelligenceNode; Kroger; Microsoft\nCountry: USA\nSector: Retail\nPurpose: Set prices\nTechnology: Computer vision; Facial recognition; Machine learning; Pricing algorithm\nIssue: Ethics/values; Fairness; Privacy\nTransparency: Governance\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nElizabeth Warren. Warren, Casey Investigate Kroger\u2019s Use of Digital Price Tags, Warn of Grocery Giant\u2019s \u201cSurge Pricing\u201d  Causing Price Gouging and Hurting Consumers\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.foodandwine.com/kroger-surge-pricing-electronic-shelving-labels-investigation-8694393\nhttps://www.commondreams.org/news/kroger-ai\nhttps://www.supermarketnews.com/news/democrats-accuse-kroger-using-digital-price-tags-gouge-consumers\nhttps://fortune.com/2024/08/13/elizabeth-warren-supermarket-kroger-price-gouging-dynamic-pricing-digital-labels/\nhttps://news.slashdot.org/story/24/08/14/1649235/senators-warn-krogers-digital-price-tags-may-enable-gouging\nhttps://www.grocerydive.com/news/kroger-electronic-shelf-labels-instore-technology-senators-inflation/723939/\nhttps://www.rawstory.com/kroger-pricing-strategy/\nRelated \ud83c\udf10\nAirbnb Smart Pricing algorithm exacerbates racial inequality\nGrab fares surge under opaque algorithm\nPage info\nType: Issue\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amnesty-argues-gangs-matrix-is-not-fit-for-purpose", "content": "Amnesty argues Gangs Matrix is \"not fit for purpose\"\nOccurred: May 2018\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA report by Amnesty International argued that the London police's Gangs Violence Matrix is racially discriminatory, incompatible with international human rights law, and not fit for purpose.\nThe report found that 78 percent of the individuals on the Gangs Matrix were Black, which was disproportionate to both London's Black population (13 percent) and the percentage of Black people responsible for serious youth violence (27 percent). \nIt also discovered that 40 percent of people listed had  no record of involvement in violent offenses in the past two years, and 35 percent had never committed any serious offence.\nIn addition, Amnesty accused London's Metropolitan Police Service (MPS) of unreasonably conflating youth culture with crime, poorly defining the concepts of \"gang\" and \"gang member\", leading to the arbitrary identification of individuals and to stigmatisation and other negative consequences.\nIt also took issue with the criteria used for adding and removing people from the database, which it argued were unclear, and discovered that children as young as 12-years-old were included. Futhermore. information was being shared with various government agencies, potentially impacting individuals' access to services like housing and education, Amnesty said.\n\u2795 May 2018. The UK Information Commissioner's Office said it was \"in contact with the Metropolitan Police Service as part of an investigation into their use of a gangs database\".\nSystem \ud83e\udd16\nGangs Violence Matrix\nDocuments \ud83d\udcc3\nMetropolitan Police. Gangs Violence Matrix\nOperator: Metropolitan Police Service (MPS)\nDeveloper: Metropolitan Police Service (MPS)\nCountry: UK\nSector: Govt - police\nPurpose: Predict gang violence risk\nTechnology: Ranking algorithm\nIssue: Bias/discrimination; Human/civil rights; Privacy\nTransparency: Governance; Complaints/appeals\nResearch, advocacy \ud83e\uddee\nAmnesty (2018). Trapped in the Matrix: Secrecy, stigma, and bias in the Met\u2019s Gangs Database (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/uk-england-london-44045914\nhttps://www.thesun.co.uk/news/6242378/life-saving-police-database-monitoring-violent-london-gangs-at-risk-because-it-breaches-human-rights/\nhttps://www.independent.co.uk/news/uk/crime/metropolitan-police-gang-matrix-data-protection-laws-ico-investigation-a8637871.html\nhttps://www.independent.co.uk/news/uk/crime/police-gangs-database-matrix-met-scotland-yard-london-racist-amnesty-report-a8342171.html\nhttps://www.theregister.com/2018/05/09/metropolitan_police_gang_database_racial_bias_says_amnesty_international/\nhttps://news.sky.com/story/amnesty-international-police-gang-database-racially-discriminatory-and-breaches-international-law-11364626\nhttps://www.cnbc.com/2018/05/09/london-police-force-holds-intrinsically-racist-gang-database-says-amnesty-international.html\nRelated \ud83c\udf10\nGovernment review concludes Gangs Matrix is inaccurate, discriminatory\nUK information commissioner: Gangs Matrix potentially breaks data protection law\nPage info\nType: Incident\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/met-police-faces-legal-action-over-racist-gangs-matrix-database", "content": "Met Police faces legal action over \"racist\" Gangs Matrix database\nOccurred: February 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nHuman rights organisation Liberty and public safety non-profit UNJUST UK, represented by musician Awate Suleiman, accused the Metropolitan Police Service's (MPS) Gangs Matrix database of racial discrimination.\nLiberty argued that the Gangs Matrix database discriminated against Black people, who were disproportionately represented on it. The judicial review focused on three key grounds:\nThe Matrix breached Article 8 of the European Convention on Human Rights (ECHR)\nThe Matrix breached Article 14 of the ECHR and section 19 of the Equality Act 2010, as there was statistical evidence that it included a disproportionate number of Black people\nThe MPS failed to comply with its obligations under the public sector equality duty (section 149 of the Equality Act 2010).\nThe MPS conceded that the Matrix was unlawful and admitted that it breached the right to a private and family life. The legal challenge argued that the personal data of those on the database was shared broadly with third parties \u2013 putting them at risk of over-policing, school exclusion, eviction, and in some cases being stripped of welfare benefits, deportation or even children being taken into care.\n\nIt agreed to remove the majority of individuals from the database, and to inform people on the database who their data has been shared with.\n\nThe MPS also agreed to replace the GVM with an adapted violence harm assessment (VHA). However, some human rights groups warned that the replacement could repeat the racial discrimination that was embedded in the Matrix.\nSystem \ud83e\udd16\nGangs Violence Matrix\nDocuments \ud83d\udcc3\nMetropolitan Police. Gangs Violence Matrix\nOperator: Metropolitan Police Service (MPS)\nDeveloper: Metropolitan Police Service (MPS)\nCountry: UK\nSector: Govt - police\nPurpose: Predict gang violence risk\nTechnology: Ranking algorithm\nIssue: Bias/discrimination; Human/civil rights\nTransparency: Governance\nRegulation \u2696\ufe0f\nEuropean Convention on Human Rights\nUK Equality Act 2010\nResearch, advocacy \ud83e\uddee\nLiberty, Civil Liberties Trust. HELP FIGHT THE MET\u2019S DISCRIMINATORY GANGS MATRIX\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/uk-england-london-63568880\nhttps://www.computerweekly.com/news/252512752/Met-police-faces-legal-action-over-Gangs-Matrix\nhttps://www.localgovernmentlawyer.co.uk/information-law/398-information-law-news/52223-met-police-agrees-to-overhaul-controversial-gangs-violence-matrix-following-legal-challenge\nhttps://www.standard.co.uk/news/uk/met-police-overhaul-gang-crime-list-high-court-challenge-b1039427.html\nhttps://www.itpro.com/business/policy-legislation/362122/met-police-faces-legal-action-over-gangs-matrix\nRelated \ud83c\udf10\nUK information commissioner: Gangs Matrix potentially breaks data protection law\nGovernment review concludes Gangs Matrix is inaccurate, discriminatory\nPage info\nType: Incident\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/google-photos-assistant-ski-panorama-fail", "content": "Google Photos Assistant ski panorama fail\nOccurred: January 2018\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAlex Harker's experience with Google Photos at Lake Louise ski resort sparked hilarity when the app's AI panorama feature produced a highly flawed image. \nAfter taking three photos - one with friends and two without - Google Photos automatically stitched them together, resulting in a panorama that humorously exaggerated the size of his friend's upper torso, making it appear disproportionately large against the scenic backdrop.\nHarker shared the amusing result on Reddit, where it quickly gained traction, amassing over 187,000 upvotes. Users praised the seamless blending of the images despite the comical error, with one commenter noting the impressive stitching quality aside from the \"giant head\" anomaly. \nThe incident highlighted the occasional missteps of AI in photo editing.\nSystem \ud83e\udd16\nGoogle Photos Assistant\nOperator: Alex Harker\nDeveloper: Alphabet/Google\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Edit photographs\nTechnology: Machine learning\nIssue: Accuracy/reliability\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.businessinsider.com/google-photos-ski-photo-fail-2018-1\nhttps://petapixel.com/2018/01/23/google-photos-ai-panorama-failed-best-way/\nhttps://qz.com/1188170/google-photos-tried-to-fix-this-ski-photo\nhttps://www.dailymail.co.uk/femail/article-5290355/Reddit-users-panorama-fail-featuring-giant-goes-viral.html \nhttps://mashable.com/article/google-photos-panorama-stitch-fail\nRelated \ud83c\udf10\nGoogle Photos mislabels black Americans as gorillas\nGoogle flags medical images of groin as CSAM\nPage info\nType: Issue\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/gradient-ai-photo-editing-app-slammed-as-racist", "content": "Gradient \"AI Face\" photo editing app slammed as \"racist\"\nOccurred: September 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nPhoto editing app Gradient's \"AI Face\" feature sparked outrage, with users accusing it of fueling racist stereotyping. \nAI Face allows users to alter their ethnicity in images so that they can check how they would look if they were born on a different continent.\nReality TV personalities Scott Disick and Brody Jenner posted altered images of themselves on social media that were supposed to showcase how they would appear if they were from \"Europe,\" \"Asia,\" \"India,\" and \"Africa\".\nHowever, many users expressed strong disapproval of their posts, ccusing them od perpetuating racial stereotypes and oversimplifying complex issues of identity and heritage.\nDisick and Jenner deleted their posts following the backlash.\nSystem \ud83e\udd16\nGradient AI Face\nOperator:\nDeveloper: Gradient\nCountry: Global\nSector: Media/entertainment/sports/arts\nPurpose: Change image/identity\nTechnology: Machine learning\nIssue: Bias/discrimination; Ethics/values\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://mashable.com/article/gradient-viral-celebrity-lookalike-app\nhttps://iot.eetimes.com/gradients-artificial-intelligence-app-draws-criticism\nhttps://screenrant.com/gradient-photo-editing-app-racist-ai-face-feature/\nhttps://edition.cnn.com/2020/09/23/tech/gradient-app-ai-blackface/index.html\nhttps://www.ibtimes.co.uk/gradient-app-draws-criticism-over-ai-face-feature-which-purportedly-promotes-digital-blackface-1683748\nhttps://thenextweb.com/neural/2020/09/24/c-list-celebs-slammed-for-promoting-digital-blackface-app/\nhttps://parentology.com/gradient-blackface/\nhttps://news.knowyourmeme.com/news/gradient-has-begun-experimenting-with-race-filters\nhttps://www.news18.com/news/buzz/photo-editing-app-slammed-for-promoting-blackface-and-letting-users-alter-their-ethnicity-2905015.html\nRelated \ud83c\udf10\nFaceApp ethnicity filters prompts accusations of racism, stereotyping\nHP face tracking system fails to follow Black face\nPage info\nType: Incident\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/government-review-concludes-gangs-matrix-is-inaccurate-discriminatory", "content": "Government review concludes Gangs Matrix is inaccurate, discriminatory\nOccurred: September 2017\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn independent review of the treatment of Black, Asian, and Minority Ethnic (BAME) individuals in the UK criminal justice system concluded that the London police's Gangs Violence Matrix was inaccurate and discriminatory.\nThe Lammy Review, commissioned by then UK Prime minister David Cameron of England and Wales' criminal justice system and conducted by David Lammy MP in 2017, examined the treatment of Black, Asian, and Minority Ethnic (BAME) individuals in the criminal justice system.\n\nIt also took a critical look at the Metropolitan Police Service's Gangs Matrix, concluding that there was a disproportionate focus on black and ethnic minority individuals in the matrix, supported by data showing that approximately 80 percent of individuals on the Matrix identified as black.\nThe review also found that 64 percent of those on the GVM were classified as low risk (or green), raising questions about the necessity and proportionality of their inclusion, and noted that it included over 100 children under the age of 16, highlighting concerns about the criminalisation of young people.\nLammy called for a comprehensive overhaul of the Gangs Matrix operating model, including establishing clear criteria be established for inclusion on the database, improved transparency and accountability, including informing people if they are listed on the database and clear mechanisms for challenging inclusion.\nSystem \ud83e\udd16\nGangs Violence Matrix\nOperator: Metropolitan Police Service (MPS)\nDeveloper: Metropolitan Police Service (MPS)\nCountry: UK\nSector: Govt - police\nPurpose: Predict gang violence risk\nTechnology: Ranking algorithm\nIssue: Bias/discrimination; Human/civil rights\nTransparency: Governance\nRegulation \u2696\ufe0f\nUK Data Protection Act 2018\nInvestigations, assessments, audits \ud83e\uddd0\nLammy D. (2018). The Lammy Review\nResearch, advocacy \ud83e\uddee\nClinks. Briefing on the final report of the Lammy Review (pdf)\nInstitute of Race Relations. The Lammy Review: will it change outcomes in the criminal justice system?\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/uk-41191311\nRelated \ud83c\udf10\nUK information watchdog: Gangs Matrix potentially breaks data protection law\nRikers Island prisoner risk classification system increases violence 50%\nPage info\nType: Incident\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/uk-information-watchdog-gangs-matrix-breaks-data-protection-law", "content": "UK information watchdog: Gangs Matrix potentially breaks data protection law\nOccurred: November 2018\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe UK Information Commissioner's Office (ICO) found that the Metropolitan Police Service's (MPS) Gangs Violence Matrix violated UK data privacy law in multiple ways.\nAn enforcement notice issued by the ICO ruled that the MPS had failed to process personal data fairly or lawfully, with excessive and unnecessary sharing of unredacted data across various public and private bodies, and processed personal data excessively in relation to its stated purpose, with 64 percent of individuals on the matrix assessed as low or zero risk of gang activity.\nThe Gangs Matrix also processed inaccurate data, including incorrectly presuming victims of gang-related crime to have gang associations, retained and processed personal data longer than necessary, keeping information on informal lists even after individuals were removed from the matrix, according to the ICO.\nThe MPS also failed to take appropriate measures against unlawful processing or accidental loss of personal data, with unencrypted data often transferred in unsecured ways. \nThe notice required the MPS to take action to conduct a data protection impact assessment on the Gangs Matrix, implement a clear labeling system to distinguish between victims and suspected offenders, develop a retention schedule for removing data subjects from the Matrix, and erase informal lists containing personal data of individuals no longer meeting retention criteria, amongst other things.\nSystem \ud83e\udd16\nGangs Violence Matrix\nOperator: Metropolitan Police Service (MPS)\nDeveloper: Metropolitan Police Service (MPS)\nCountry: UK\nSector: Govt - police\nPurpose: Predict gang violence risk\nTechnology: Ranking algorithm\nIssue: Accuracy/reliability; Privacy; Security\nTransparency: Governance\nRegulation \u2696\ufe0f\nUK Data Protection Act 2018\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nInformation Commissioner's Office. Enforcement Notice (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.computerweekly.com/news/252452971/ICO-finds-Metropolitan-Polices-Gangs-Matrix-seriously-breaches-data-protection-laws\nhttps://www.independent.co.uk/news/uk/crime/metropolitan-police-gang-matrix-data-protection-laws-ico-investigation-a8637871.html\nhttps://www.bbc.co.uk/news/uk-england-london-46239413\nhttps://www.theguardian.com/uk-news/2020/feb/15/met-removes-hundreds-from-gangs-matrix-after-breaking-data-laws\nhttps://www.theregister.com/2018/11/16/ico_gangs_matrix_decision/\nhttps://www.localgovernmentlawyer.co.uk/community-safety/393-community-safety-news/39375-ico-hits-met-police-with-enforcement-notice-over-use-of-gangs-matrix\nRelated \ud83c\udf10\nGovernment review concludes Gangs Matrix is inaccurate, discriminatory\nYouth advocacy worker misidentified by Met Police facial recognition system\nPage info\nType: Incident\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/grab-fares-surge-under-opaque-algorithm", "content": "Grab fares surge under opaque pricing algorithm\nOccurred: July 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGrab, a popular ride-hailing app in the Philippines, came under scrutiny for its opaque surge pricing algorithm. \nAn investigation by the Philippine Center for Investigative Journalism (PCIJ) revealed that GrabCar rides always include surge fees, regardless of the time or day, resulting in fares being consistently above the minimum fare estimated by Grab's own fare check tool.\nThe investigators also found that high fares did not always correlate with shorter waiting times, contradicting Grab's advertised purpose of surge pricing, and the algorithm's behaviour often differed from Grab's public explanations of how it works.\nGrab refuses to reveal the actual process for calculating fares, resulting in critics accusing the company of profiteering and opacity. \nThe company has been regularly under scrutiny for overpricing, ride cancellations and other issues since its acquisition of Uber's business across South-east Asia.\nThe investigation highlights concerns about algorithmic transparency and fairness in ride-hailing services. It also raises questions about the effectiveness of surge pricing in improving service quality, as long wait times persisted even during high-fare periods.\nSurge pricing is a dynamic pricing algorithm used by ride-hailing companies to adjust fares in real-time based on demand and supply. When demand is high and supply is low, the algorithm increases the fare to encourage more drivers to go online and to reduce the number of ride requests. \nSystem \ud83e\udd16\nGrab surge pricing algorithm\nDocuments \ud83d\udcc3\nGrab. Why fares go up when it rains\nOperator: GrabCar\nDeveloper: Grab\nCountry: Philippines\nSector: Transport/logistics\nPurpose: Calculate surge price\nTechnology: Pricing algorithm\nIssue: Effectiveness/value; Fairness\nTransparency: Governance\nInvestigations, assessments, audits \ud83e\uddd0\nPhilipine Center for Investigative Journalism. Grab fares surge under opaque algorithm\nPhilipine Center for Investigative Journalism. \u2018Teka Teka\u2019 Podcast: Why Is Grab So Expensive? Part 1 of 2\nPhilipine Center for Investigative Journalism. \u2018Teka Teka\u2019 Podcast: Why Is Grab So Expensive? Part 2 of 2\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.gmanetwork.com/news/money/companies/913335/grab-philippines-fares-always-include-surge-fees-pcij/story/\nRelated \ud83c\udf10\nUber UpFront Fares algorithm cuts some drivers' earnings\nDoordash drivers protest order matching algorithm\nPage info\nType: Incident\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/grok-generates-nazi-micky-mouse-taylor-swift-deepfakes", "content": "Grok generates Nazi Micky Mouse, Taylor Swift deepfakes\nOccurred: August 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGrok 2, the latest version of Elon Musk's chatbot, sparked controversy due to its lack of safety protocols.\nGrok's image generator produced a range of inappropriate and potentially harmful content, including: Nazi-themed images, such as Mickey Mouse saluting Hitler, deepfakes of celebrities like Taylor Swift, Barack Obama doing cocaine, and representations of copyrighted characters engaging in inappropriate activities.\nUnlike other high-profile AI image generators, Grok appears to have few restrictions. It does not reject prompts related to copyrighted characters, allows the creation of images depicting political violence and explicit content, and seems to have minimal protections against misinformation.\nElon Musk has praised Grok as \"the most fun AI in the world,\" but the lack of safeguards has raised concerns about the potential for spreading misinformation and facilitating harassment. \nThe emergence of Grok's unrestricted image generation capabilities reignited discussions about the need for ethics and regulation in AI-generated imagery, particularly concerning deepfakes, copyright infringement, and the potential for spreading disinformation.\nSystem \ud83e\udd16\nGrok chatbot\nDocuments \ud83d\udcc3\nX. AI. Grok-2 beta release\nOperator: X.AI\nDeveloper: X Corp\nCountry: USA\nSector: Multiple\nPurpose: Generate text\nTechnology: Chatbot; Machine learning\nIssue: Copyright; Ethics/values; Mis/disinformation; Privacy; Safety\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theguardian.com/technology/article/2024/aug/14/musk-ai-chatbot-images-grok-x?CMP=Share_AndroidApp_Other\nhttps://www.theverge.com/2024/8/14/24220173/xai-grok-image-generator-misinformation-offensive-imges\nhttps://siliconangle.com/2024/08/14/x-slammed-lack-guardrails-grok-image-generation/\nhttps://www.zdnet.com/article/grok-gets-an-impressive-upgrade-and-unchecked-ai-image-generation-apparently/\nRelated \ud83c\udf10\nX automatically harvests user data to train Grok chatbot\nGrok boosts claims that Donald Trump is a \"pedophile\"\nPage info\nType: Incident\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/deep-cam-live-ai-impersonator-prompts-misuse-fears", "content": "Deep Cam Live AI impersonator prompts misuse fears\nOccurred: August 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nDeep Cam Live, a new AI product that enables anyone to replicate someone's else's voice, raised concerns about potential misuse and impersonation scams. \nDeep Cam Live is able to produce real-time synthetic voices, faces and video appearances, including deepfakes, with only a single image. The product is supposedly intended for video production, animation, and other creative projects. Its creators claim they are \"committed to evolving the project within legal and ethical frameworks, implementing measures like watermarking outputs when necessary to prevent abuse.\"\nHowever, critics point out that the free, open-source technology can easily be misused, for example by enabling criminals to pose as loved ones in distress, requesting financial assistance. Other forms of misuse could take the form of revenge porn and child exploitation, harassment and bullying, disinformation, electoral interference, and identify theft and fraud.\nIn one of the most popular videos circulating online, X user Joao transformed himself into Republican Vice Presidential candidate J.D. Vance, Meta CEO Mark Zuckerberg, and actors Hugh Grant and George Clooney.\nDeep Cam Live is seen to raise a panoply of legal and ethical issues, including the liability and responsibility for AI-generated content and its potential misuse.\nSystem \ud83e\udd16\nDeep Cam Live\nDocuments \ud83d\udcc3\nDeep Cam Live code (Github)\nOperator:\nDeveloper:  \nCountry: Global\nSector: Media/entertainment/sports/arts\nPurpose: Replicate voice, face\nTechnology: Machine learning; Neural network; Deep learning\nIssue: Dual/multi-use; Liability; Privacy; Safety; Security\nTransparency: Governance; Liability\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://arstechnica.com/information-technology/2024/08/new-ai-tool-enables-real-time-face-swapping-on-webcams-raising-fraud-concerns/\nhttps://petapixel.com/2024/08/14/deep-live-cam-deepfake-ai-tool-lets-you-become-anyone-in-a-video-call-with-single-photo-mark-zuckerberg-jd-vance-elon-musk/\nhttps://www.biometricupdate.com/202408/free-face-swap-tool-goes-to-number-one-on-github\nhttps://tech.slashdot.org/story/24/08/13/2057227/deep-live-cam-goes-viral-allowing-anyone-to-become-a-digital-doppelganger\nRelated \ud83c\udf10\nReports: DeepFaceLive poses privacy, misuse dangers\nFaceMega sexualised face swap ads violate platform policies\nPage info\nType: Issue\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/recruiters-flooded-with-ai-generated-resumes", "content": "Recruiters flooded with AI-generated resumes miss good candidates\nOccurred: August 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nRecruiters are facing a barrage of poor quality resumes and cover letters generated using AI, resulting in good quality candidates getting lost and leading to wasted time and resources for recruitment agencies and their customers.\nAI has been used for some time by recruiters to assess job applications, but the democratisation of AI has meant that anyone can now generate a reasonably convincing resume using a free version of chatbots such as ChatGPT or Gemini, according to a Financial Times report.\nThe report cites research by consulting firm Neurosight which found that 57 percent of applicants used ChatGPT in their job applications. It also found that applicants using ChatGPT\u2019s free version were less likely to pass psychometric tests than those using the paid version.\nThe trend increasingly puts the onus on recruiters to detect AI-written applications. Harvey Nash recruiter Andy Heyes told the FT that \u201ctell-tale signs [like] American grammar\u201d, and \u201cbland\u201d applications gave \u201can indication of whether candidates have used AI.\u201d \nHowever, AI detection systems are known to be fallible and recruiters hope that interviews involving human recruiters could help pick out candidates with authentic experience.\nSystem \ud83e\udd16\nChatGPT chatbot\nGemini chatbot\nOperator:\nDeveloper: Google; OpenAI\nCountry: UK\nSector: Business/professional services\nPurpose: Generate resumes\nTechnology: Chatbot; Machine learning\nIssue: Employment\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.ft.com/content/30a032dd-bdaa-4aee-bc51-754867abbde0\nhttps://the-decoder.com/recruiters-are-drowning-in-a-flood-of-ai-generated-applications/\nhttps://www.techspot.com/community/topics/recruiters-overwhelmed-as-57-of-young-applicants-are-using-chatgpt-for-job-resumes.287452/\nhttps://fortune.com/europe/2024/08/14/ai-indispensable-job-seekers-cv-resume-job-applications-recruiters/\nRelated \ud83c\udf10\nChatGPT found to display racial bias against job candidates\nAmazon AI recruitment tool favours men over women\nPage info\nType: Issue\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/facebook-cross-check-criticised-as-unfair-under-resourced-and-opaque", "content": "Facebook Cross-check criticised as unfair, under-resourced and opaque\nOccurred: December 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nFacebook's Cross-check system was taken to task by Meta's Oversight Board for unfairly providing VIP users with less onerous rules than everyone else and having a poorly resourced and opaque system to manage them.\nThe Oversight Board found that the Cross-check system results in users being treated unequally, that it lead to substantial delays in taking down rule-violating content (on average, decisions took more than 5 days, with some cases taking up to 7 months), and it appears to be structured more to satisfy the company's commercial objectives rather than to advance Meta's human rights commitments.\nIt also found that while only 9 percent of Facebook's daily active users are from the US and Canada, 42 percent of content reviewed under Cross-check came from these two countries, and that Facebook had failed to provide crucial details about Cross-check to the Oversight Board, including criteria for adding accounts to the system.\nThe Oversight Board made 32 recommendations to improve the Cross-check, including developing clearer criteria for account eligibility and making these public, allowing individuals to apply for the programme proactively, visually communicating an account's Cross-check status to users, increasing resources to ensure the timely review of flagged content, and prioritising posts important for human rights and considered of special public importance.\n\u2795 March 2023. Meta agreed to the Oversight Board's recommendation to publish regular Transparency Reports on Cross-check and to limit the distribution of content from high-profile individuals that likely violated platform rules until their posts had been adjudicated.\nSystem \ud83e\udd16\nFacebook Cross-check/XCheck VIP whitelisting\nDocuments \ud83d\udcc3\nMeta (2022, updated 2023). Oversight Board Selects a PAO on Meta's Cross-check Policies\nMeta (2022). Reviewing high-impact content accurately via our Cross-check system\nOperator: Meta/Facebook\nDeveloper: Meta/Facebook\nCountry: USA\nSector: Media/entertainment/sports/arts; Politics\nPurpose: Moderate content\nTechnology: Content moderation system\nIssue: Ethics/values; Fairness; Human/civil rights; Safety\nTransparency: Governance; Complaints/appeals; Marketing\nInvestigations, assessments, audits \ud83e\uddd0\nOversight Board (2021). Policy Advisory Opinion: Meta's Cross-check programme \nOversight Board (2021). Full Policy Advisory Opinion on Meta's Cross-check progam (pdf)\nOversight Board (2021). Oversight Board demands more transparency from Facebook\nOversight Board (2021). Oversight Board opens public comments for policy advisory opinion on cross-check\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theverge.com/2022/12/7/23498030/facebook-moderation-scandal-xcheck-cross-check\nhttps://www.thenationalnews.com/business/technology/2022/12/07/meta-oversight-board-hits-out-at-shortcomings-in-facebook-moderation-system-of-vips/\nhttps://www.pcmag.com/news/oversight-board-facebook-cross-check-system-for-vips-is-flawed-in-key-areas\nRelated \ud83c\udf10\nFacebook system provides high-profile users with special treatment\nMeta automated moderation wrongly removes Israel-Hamas videos\nPage info\nType: Issue\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/facebook-system-provides-high-profile-users-with-special-treatment", "content": "Facebook system provides high-profile users with special treatment\nOccurred: September 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nFacebook provided special \"whitelisting\" treatment to millions of high-profile users, exempting them from standard rules and enforcement actions. \nA Wall Street Journal investigation discovered that politicians, celebrities and other influential individuals - including Donald Trump, Elizabeth Warren and Neymar - added to Facebook's Cross-check programme are permitted to post rule-breaking content, including harassment, incitement to violence, and misinformation and disinformation, and are immune from Facebook enforcement actions. \nIt found that Facebook's moderators are instructed to be more lenient when reviewing content from high-profile users, and to err on the side of caution when deciding whether to remove content. It was also seen to have been used to protect high-profile users from criticism and accountability, including allowing them to post content that is later found to be false or misleading.\nIn one instance, Cross-check system allowed then-President of the United States Donald Trump to post a video on his Facebook page that falsely claimed that mail-in voting during political elections was \"corrupt\" and \"rigged\". Despite the fact that the video contained false information, Facebook's moderators did not remove it, citing the Cross-check system's exemption for high-profile users. The video remained on the platform for several days, allowing it to spread misinformation to millions of users.\nThe WSJ's report raised concerns about the fairness and transparency of Facebook's moderation policies, and highlighted the need for greater accountability and oversight of the company's practices. \nIt resulted in Facebook requesting assistance from its independent Oversight Board.\nSystem \ud83e\udd16\nFacebook Cross-check/XCheck VIP whitelisting\nDocuments \ud83d\udcc3\nMeta (2021). Requesting Oversight Board Guidance on Our Cross-check System\nOperator: Meta/Facebook\nDeveloper: Meta/Facebook\nCountry: USA\nSector: Politics;\nPurpose: Moderate content\nTechnology: Content moderation system\nIssue: Ethics/values; Fairness; Human/civil rights; Safety\nTransparency: Governance; Complaints/appeals; Marketing\nInvestigations, assessments, audits \ud83e\uddd0\nWall Street Journal. Facebook Says Its Rules Apply to All. Company Documents Reveal a Secret Elite That\u2019s Exempt\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.yahoo.com/entertainment/facebook-exempted-trump-elizabeth-warren-210953782.html\nhttps://arstechnica.com/tech-policy/2021/09/leaked-documents-reveal-the-special-rules-facebook-uses-for-5-8m-vips/\nhttps://www.cnet.com/tech/mobile/facebook-reportedly-exempted-high-profile-users-from-its-rules/\nhttps://brandequity.economictimes.indiatimes.com/news/digital/facebooks-xcheck-programme-shields-vips-from-some-of-its-rules-report/86193520\nhttps://www.theverge.com/2021/9/13/22671565/facebook-xcheck-moderation-system-high-profile-exemptions\nhttps://www.theguardian.com/technology/2021/sep/13/facebook-some-high-profile-users-allowed-to-break-platforms-rules\nhttps://www.theguardian.com/technology/2021/sep/21/facebook-xcheck-system-oversight-board-review\nhttps://www.businessinsider.com/facebook-content-moderation-58-million-users-xcheck-2021-9\nRelated \ud83c\udf10\nFacebook Cross-check criticised as unfair, under-resourced and opaque\nMeta automated moderation wrongly removes Israel-Hamas videos\nPage info\nType: Issue\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/microsoft-copilot-can-be-turned-into-automated-phishing-machine", "content": "Microsoft Copilot can be turned into automated phishing machine\nOccurred: August 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nMicrosoft's AI-powered Copilot chatbot can be exploited by malicious actors for automated phishing and data extraction, according to researchers. \nFormer Microsoft security architect Michael Bargury demonstrated at the Black Hat USA cybersecurity conference that hackers can use Copilot to analyse communication patterns, mimic a user's writing style, including their emoji usage, and enerate and send hundreds of personalised phishing emails within minutes.\nBargury also demonstrated how specific \"magic words\" could be used to circumvent Microsoft's existing security controls on Copilot.\nWhile these are proof-of-concept demonstrations, they mirror known techniques for manipulating large language models.\nThe finding raised questions about the strength of Copilot's security, and the potential harms it poses in terms of individual privacy, corporate confidentiality, financial manipulation and other negative impacts.\nSystem \ud83e\udd16\nMicrosoft Copilot\nOperator: Zenity\nDeveloper: Microsoft\nCountry: Global\nSector: Technology\nPurpose: Generate text\nTechnology: Chatbot; Machine learning\nIssue: Security\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nZenity. BlackHat USA 2024 demos\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://fortune.com/2024/08/13/microsoft-ai-copilot-hacking-prompt-injectoin-attack-black-hat/\nhttps://www.wired.com/story/microsoft-copilot-phishing-data-extraction\nRelated \ud83c\udf10\nChatGPT generates plausible phishing emails, malware\nStudy: Google Bard lets users generate phishing emails, ransomware\nPage info\nType: Issue\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/faceapp-rapped-for-potential-privacy-security-abuse", "content": "FaceApp rapped for potential privacy, security abuse\nOccurred: July 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nFaceApp came under scrutiny for potential privacy and security abuses due to its Russian ownership and the data collection practices outlined in its terms of service. \nConcerns were amplified following a Forbes report that FaceApp was storing users' photos, email addresses and device identifiers on its servers in Russia rather than on their phones, and that their metadata was being collected. \nAlthough FaceApp claimed to delete photos from its servers within 24 to 48 hours after editing, users expressed skepticism about the deletion process, with some critics highlighting that merely uninstalling the app does not guarantee the removal of uploaded images from FaceApp's servers.\nConcerns were also raised that FaceApp's privacy policy had a \"perpetual\" license that allowed it to use people\u2019s usernames, names, and likeness for commercial purposes, including potential facial recognition applications.\nThe reports prompted US senator Chuck Schumer to urge the FBI and Federal Trade Commission to investigate FaceApp, saying it could pose \"national security and privacy risks for millions of US citizens.\"\nIn response, the FBI said \"it considers any mobile application or similar product developed in Russia, such as FaceApp, to be a potential counterintelligence threat.\" \nFaceApp CEO Yaroslav Goncharov responded by denying data was being shared with Russian authorities and asserting that user data is not transferred to Russia. \nHe also said that only the picture chosen by the user is uploaded and stored on cloud computing services provided by Amazon and Google, and that the app did not harvest a user\u2019s mobile photo library.\nSystem \ud83e\udd16\nFaceApp facial transformer\nDocuments \ud83d\udcc3\nFaceApp privacy policy\nOperator: FaceApp Technology\nDeveloper: Yaroslav Goncharov\nCountry: USA\nSector: Multiple\nPurpose: Transform faces\nTechnology: Deep learning; Neural network; Machine learning\nIssue: Dual/multi-use; Privacy; Security\nTransparency: Governance; Privacy\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nSenator Schumer FBP, FTC letter\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.forbes.com/sites/thomasbrewster/2019/07/17/faceapp-is-the-russian-face-aging-app-a-danger-to-your-privacy/\nhttps://www.wsj.com/articles/as-faceapp-goes-viral-so-do-concerns-about-privacy-russia-ties-11563485572\nhttps://www.smh.com.au/technology/what-s-fact-and-what-s-fiction-when-it-comes-to-faceapp-20190717-p5284v.html\nhttps://techcrunch.com/2019/07/16/ai-photo-editor-faceapp-goes-viral-again-on-ios-raises-questions-about-photo-library-access-and-clo/\nhttps://www.theguardian.com/technology/2019/jul/17/faceapp-denies-storing-users-photographs-without-permission\nhttps://www.dailymail.co.uk/sciencetech/article-7260463/Faceapp-access-camera-roll.html\nhttps://www.boston.com/news/technology/2019/07/17/faceapp-safe-privacy\nhttps://nypost.com/2019/07/17/faceapp-security-concerns-russians-now-own-all-your-old-photos/\nhttps://www.thesun.co.uk/tech/9526114/faceapp-app-photos-safe-dangerous-upload/\nhttps://www.cbsnews.com/news/faceapp-russian-app-sparks-myths-and-fears-about-privacy-and-data-use/\nhttps://www.cbsnews.com/news/faceapp-top-democrat-chuck-schumer-urges-fbi-to-investigate-troubling-russian-app/\nRelated \ud83c\udf10\nFaceApp ethnicity filters prompts accusations of racism, stereotyping\nFaceApp \"hot\" filter skin whitening slammed as \"racist\"\nPage info\nType: Issue\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/faceapp-ethnicity-filters-prompts-accusations-of-racism-stereotyping", "content": "FaceApp ethnicity filters prompts accusations of racism, stereotyping\nOccurred: \nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nFilters enabling FaceApp users to resemble 'black,' 'Asian', 'Caucasian,' or 'Indian' people prompted accusations of racism and stereotyping.\nFollowing a row about FaceApp's \"hot\" filter, the app introduced filters that enabled its users to become  'black,' 'Asian', 'Caucasian,' or 'Indian'. \nHowever, users and commentators noted that filters appeared to reducing complex ethnic identities to simplistic and inaccurate representations that perpetuated harmful stereotypes and caricatures, leading to an outcry. \nSome people accused the app of encouraging people to \"try on\" different ethnicities as a form of cultural appropriation, and of prioritising novelty and entertainment value over cultural sensitivity and respect.\nThe controversy led to widespread criticism and calls for the filters to be removed. \nFaceApp apologised and removed the ethnicity filters, acknowledging that they were \"not respectful\" and \"not acceptable\".\nSystem \ud83e\udd16\nFaceApp facial transformer\nOperator: FaceApp Technology\nDeveloper: Yaroslav Goncharov\nCountry: Global\nSector: Media/entertainment/sports/arts\nPurpose: Transform faces\nTechnology: Deep learning; Neural network; Machine learning\nIssue: Bias/discrimination - race, ethnicity, LGBTQ, transgender\nTransparency: Governance; Marketing; Privacy\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.washingtonpost.com/news/the-intersect/wp/2017/08/09/how-did-faceapp-think-people-would-react-to-their-new-change-your-race-filters\nhttps://www.miamiherald.com/news/nation-world/world/article166420962.html\nhttps://www.fastcompany.com/40451544/faceapp-releases-then-pulls-digital-blackface-filters\nhttps://www.theguardian.com/technology/2017/aug/10/faceapp-forced-to-pull-racist-filters-digital-blackface\nRelated \ud83c\udf10\nFaceApp \"hot\" filter skin whitening slammed as \"racist\"\nPortrait Ars generator whitens coloured peoples' skins\nPage info\nType: Incident\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/faceapp-hot-filter-skin-whitening-slammed-as-racist", "content": "FaceApp \"hot\" filter skin whitening slammed as \"racist\"\nOccurred: April 2017\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nFacial transformation app FaceApp faced criticism for its \"hot\" filter, which was accused of perpetuating racist and Eurocentric beauty standards. \nIntended to make users look more attractive, the filter was found to lighten skin tones, making users appear whiter; narrow nose shapes and alter facial features to conform to traditional European beauty standards; and erase or reduce facial features associated with non-white ethnicities. \nUsers and critics slammed the filter as \"racist\" and \"problematic\", arguing that it reinforced the notion that whiteness is the standard of beauty, that it erased or diminished the unique features of non-white individuals, and contributed to the perpetuation of racism in the beauty industry.\nFaceApp's developers responded by stating that the filter was not intended to be racist, but rather to make users look more \"attractive\" based on traditional beauty standards. However, the company ultimately removed the filter and apologised for any offense caused, acknowledging that the feature was \"not respectful\" and \"not acceptable\".\nSystem \ud83e\udd16\nFaceApp facial transformer\nOperator: FaceApp Technology\nDeveloper: Yaroslav Goncharov\nCountry: Global\nSector: Media/entertainment/sports/arts\nPurpose: Transform faces\nTechnology: Deep learning; Neural network; Machine learning\nIssue: Bias/discrimination\nTransparency: Governance; Marketing; Privacy\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.independent.co.uk/tech/faceapp-selfie-app-racism-filter-whitens-users-skin-viral-photo-a7701036.html\nhttps://www.bbc.co.uk/news/newsbeat-39702143\nhttps://www.theverge.com/2017/4/25/15419522/faceapp-hot-filter-racist-apology\nhttps://www.teenvogue.com/story/faceapp-apologizes-whitewashing-hot-filter\nRelated \ud83c\udf10\nFaceApp ethnicity filters prompts accusations of racism, stereotyping\nPortrait Ars generator whitens coloured peoples' skins\nPage info\nType: Incident\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/faception-claim-to-identify-paedophiles-from-their-faces-draws-controversy", "content": "Faception claim to identify paedophiles from their faces draws controversy\nOccurred: May 2016\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nClaims by Israeli facial personality profiling company Faception that it can identify paedophiles, terrorists and white-collar criminals from their faces prompted a backlash about their nature and accuracy.\nFaception's system collects images of people's faces from photographs and video, and encodes and maps their facial features on to a set of 15 proprietary 'classifiers', including academic, terrorist, and bingo-player. Faception claimed that it could typically classify people with around 80 percent confidence.\nCritics took to the mainstream and social media to argue there is no scientific basis for the claim, as facial features are not a reliable indicator of an individual's likelihood of committing a crime. They also noted that the technology is likely to produce false positives, misidentifying innocent people.\nThe approach, they said, is based on a flawed assumption that paedophiles have a distinct \"look\" or facial characteristics that can be detected. It was also noted that the technology could lead to discrimination and stigmatisation of individuals who are misidentified, and that the company had not provided sufficient evidence or peer-reviewed research to support its claims.\nMore broadly, experts have also raised concerns about the potential misuse of facial personality profiling, including as an infringement on civil liberties and human rights, its potential for abuse by law enforcement or other authorities, and the lack of transparency and accountability in the development and deployment of the technology. Some commentators cited a contract that Faception itself had signed with an unnamed homeland security agency.\nThe controversy highlighted the need for rigorous scientific evaluation when developing and deploying personality profiling technologies, particularly those that claim to predict sensitive or stigmatising characteristics.\nSystem \ud83e\udd16\nFaception facial personality profiling\nOperator: Faception\nDeveloper: Faception\nCountry: Israel\nSector: Business/professional services; Banking/financial services; Govt - police\nPurpose: Identify personality type; Predict behaviour\nTechnology: Computer vision; Behavioural analysis; Emotion recognition; Facial recognition; Personality analysis; Machine learning\nIssue: Accuracy/reliability; Bias/discrimination - race, ethnicity, gender; Ethics/values\nTransparency: Governance; Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.timesofisrael.com/new-israeli-facial-imaging-claims-to-identify-terrorists-and-pedophiles/\nhttps://www.washingtonpost.com/news/innovations/wp/2016/05/24/terrorist-or-pedophile-this-start-up-says-it-can-out-secrets-by-analyzing-faces/\nhttps://www.nytimes.com/2019/07/10/opinion/facial-recognition-race.html/\nhttps://blog.practicalethics.ox.ac.uk/2016/06/hide-your-face/\nhttps://www.newscientist.com/article/2090656-controversial-software-claims-to-tell-personality-from-your-face/\nhttp://thescienceexplorer.com/technology/faception-ai-claims-detect-terrorists-and-pedophiles-based-their-facial-personality\nhttps://www.csmonitor.com/World/Passcode/Passcode-Voices/2016/0527/Opinion-The-ugliest-side-of-facial-recognition-technology\nhttps://www.businessinsider.com/does-faception-work-2016-10\nhttps://www.mirror.co.uk/tech/you-face-killer-new-software-8045646\nRelated \ud83c\udf10\nIntel AI student emotion monitoring system accused of being inaccurate, intrusive\nUK train stations secretly monitor travellers' emotions\nPage info\nType: Issue\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chatgpt-imitates-users-voices-without-permission", "content": "ChatGPT imitates users' voices without permission\nOccurred: August 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nOpenAI's new GPT-4o model imitates users' voices without their permission, sparking concerns about privacy, security and its potential for misuse.\nDuring testing, the model's \"Advanced Voice Mode\" feature occasionally and unintentionally imitated users' voices without their permission, according to  OpenAI's release of the \"system card\" for the GPT-4o model, which details the model's limitations and safety testing procedures.\nThe card details a recording showing an interaction where the AI suddenly spoke in a voice similar to the user's after saying \"No!\"\nOpenAI says it has implemented safeguards to prevent such occurrences, including using preset voices created in collaboration with voice actors, developing output classifiers to detect deviations from approved voices, and blocking outputs that do not match predetermined voices.\nThe revelation sparked discussions about the ethical implications and potential misuse of voice cloning technology in AI systems.\nSystem \ud83e\udd16\nChatGPT chatbot\nGPT-4o Advanced Voice Mode\nDocuments \ud83d\udcc3\nOpenAI. GPT-4o System Card\nOperator: OpenAI\nDeveloper: OpenAI\nCountry: USA\nSector: Multiple\nPurpose: Create voices\nTechnology: Chatbot; Machine learning\nIssue: Dual/multi-use; Privacy; Security\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://arstechnica.com/information-technology/2024/08/chatgpt-unexpectedly-began-speaking-in-a-users-cloned-voice-during-testing/\nhttps://www.techradar.com/computing/artificial-intelligence/its-not-an-echo-chatgpt-might-suddenly-mimic-your-voice-when-you-speak-to-it\nhttps://futurism.com/the-byte/chatgpt-clone-voice-without-permission\nhttps://www.mediapost.com/publications/article/398458/chatgpt-can-finish-thoughts-imitate-voices-withou.html\nRelated \ud83c\udf10\nOpenAI accused of using Scarlett Johansson's voice without consent to train\nAustralian voice artists lose work to AI clones\nPage info\nType: Issue\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/reports-deepfacelive-poses-privacy-misuse-dangers", "content": "Reports: DeepFaceLive poses privacy, misuse dangers\nOccurred: September 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe launch of an AI-powered tool that enables users to create realistic face swap videos was met with a chorus of concerns about its potential misuse.\nDeepFaceLive uses deep learning algorithms to swap faces in videos, creating highly realistic and convincing results.\nHowever, critics raised concerns that the tool could be used for malicious purposes, including the creation of fake videos that could be used to manipulate or deceive people; the generation and spread of misinformation or propaganda; and the harassment or bullying of individuals by creating fake videos that appear to show them doing or saying something they did not.\nSome voiced concerns about the potential for DeepFaceLive to be used to create non-consensual and explicit content, such as fake videos that appear to show individuals engaging in explicit activities without their consent.\nThe launch of the tool sparked a debate about the ethics of AI-powered face swapping technology. It was also seen to highlight the need for regulation to prevent its misuse.\nSystem \ud83e\udd16\nDeepFaceLive\nOperator: DeepFaceLive\nDeveloper: Ivan Petrov\nCountry: Russia\nSector: Technology\nPurpose: Transform identity\nTechnology: Deepfake - video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning \nIssue: Privacy; Ethics/values; Dual/multi-use; Mis/disinformation\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.dailydot.com/debug/deepfacelive-deepfake-live-streaming/\nhttps://www.unite.ai/real-time-deepfake-streaming-with-deepfacelive/\nhttps://wonderfulengineering.com/this-new-deepfake-tool-can-transforms-livestreamers-into-someone-else-in-real-time-and-the-results-are-eerie/\nhttps://futurism.com/the-byte/deepfake-livestreamers-real-time\nhttps://mixed.de/deepfacelive-bringt-deepfakes-in-live-video-streams/\nhttps://trashbox.ru/link/2021-07-24-deepfacelive-real-time-face-swap-release-\nhttps://www.webtekno.com/yayincilar-yuzunu-cekim-sirasinda-degistiren-deepfacelive-h114782.html\nRelated \ud83c\udf10\nAthena Chu deepfake face swap prompts controversy\nFaceMega sexualised face swap ads violate platform policies\nPage info\nType: Issue\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/uk-parliamentarian-calls-for-deepsukebe-ban", "content": "UK parliamentarian calls for Deepsukebe nudifier ban\nOccurred: August 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA UK member of parliament called for a ban on non-consensual \"deepfake\" pornographic images and the use of nudification software, which manipulates images to create nude representations without consent. \nMaria Miller MP called for a ban on DeepSukebe, voicing her concern that the service and its technology could be used to create non-consensual and explicit images of women, and that it could contribute to a culture of objectification, harassment and exploitation of women. Miller also noted that these kinds of images are difficult to remove once shared online.\nDeepSukebe uses deep learning algorithms to remove clothing from images, creating the illusion that the person in the image is naked. The technology has been widely criticised for its potential to be used for malicious purposes, such as creating revenge porn or harassing women online.\nAn Olympic athlete was said to be amongst those who users claim to have nudified using the technology, according to the BBC.\nMiller's intervention highlighted the perceived inadequacy of UK law to cover deepfakes and nudified images, which she argued represent a continuation of offline sexual violence in the digital realm.\nSystem \ud83e\udd16\nDeepsukebe nudifier\nOperator:  \nDeveloper:  \nCountry: UK\nSector: Technology\nPurpose: Nudify women\nTechnology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Ethics/values; Objectification/dehumanisation; Privacy; Safety\nTransparency: Governance; Black box; Complaints/appeals; Marketing; Privac\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nMaria Miller MP: Hansard contributions\nResearch, advocacy \ud83e\uddee\nMy Image My Choice. Change laws on sharing private images without consent\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/technology-57996910\nhttps://www.bbc.com/tamil/science-58139709\nhttps://www.bbc.com/telugu/international-58104363\nhttps://aibusiness.com/responsible-ai/british-politician-decries-ai-nudifier-platform-calls-for-the-site-to-be-banned\nhttps://graziadaily.co.uk/life/in-the-news/pornography-deepfake-sexual-abuse-online-not-illegal/\nhttps://metro.co.uk/2021/08/05/digital-undressing-ai-tool-receives-millions-of-hits-a-month-15042867/\nhttps://www.thesun.co.uk/tech/15850301/deepfake-app-undress-thousands-real-women/\nRelated \ud83c\udf10\nClothOff nudifier\nBeverly Hills students created, shared AI nude images of fellow students\nPage info\nType: Issue\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/cosmos-magazine-ai-generated-articles-prompt-backlash", "content": "Cosmos Magazine AI-generated science explainers prompt backlash\nOccurred: August 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAustralian science publication Cosmos Magazine faced a strong backlash for its decision to publish AI-generated articles.\nCosmos used a Walkley Foundation grant to develop a custom AI service based on OpenAI's GPT-4 large language model that generates explainer articles for the magazine's website. \nHowever, the resulting articles contained inaccuracies and oversimplified scientific information. In one instance, an article titled \"What happens to our bodies after death?\" incorrectly described rigor mortis and autolysis.\nThe inaccuracies raised concerns about the potential harm to public trust in the publication. Contributors, former editors, and co-founders of Cosmos complained that they were not consulted about the project and argued that it undermines the role of journalists.\nIt also transpired that Cosmos' AI service has been trained on content from contributors who were not consulted about the project or asked for their consent. As freelancers, they are likely to have retained copyright over their work. \nCosmos responded by saying that its AI-generated content is fact-checked by a trained science communicator and edited by their publishing team and that it plans to continue reviewing the use of its AI service throughout the experiment, which is scheduled to run until February 2025.\nThe controversy sparked a broader debate about the use of AI in journalism, with critics arguing that such experiments should be conducted with transparency to maintain trust in scientific reporting.\nSystem \ud83e\udd16\nUnknown\nOperator: Cosmos\nDeveloper: Cosmos\nCountry: Australia\nSector: Media/entertainment/sports/arts; Technology\nPurpose: Generate articles\nTechnology: Large language model; Machine learning\nIssue: Accuracy/reliability; Copyright; Employment; Ethics/values\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.abc.net.au/news/science/2024-08-08/csiro-cosmos-magazine-generating-articles-using-ai/104186330\nhttps://www.theguardian.com/media/commentisfree/article/2024/aug/12/cosmos-magazines-ai-generated-articles-are-bad-for-trust-in-science\nhttps://www.barrons.com/news/australian-science-magazine-slammed-over-ai-generated-articles-dbbabdc9\nhttps://www.straitstimes.com/asia/australianz/australian-science-magazine-slammed-over-ai-generated-articles\nRelated \ud83c\udf10\nShotSpotter: Alerts are modified 10 percent of the time\nNew York City finds ShotSpotter identifies 13 percent of confirmed shootings\nPage info\nType: Issue\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/report-soundthinking-often-modifies-alerts-at-police-request", "content": "Report: SoundThinking often modifies ShotSpotter alerts at police request\nOccurred: July 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nPolice departments are regularly asking SoundThinking to alter or delete alerts generated by its ShotSpotter AI-powered gunshot detection technology, raising concerns about the reliability and integrity of the evidence generated by the system and its use in court cases.\nCiting internal documents and interviews with former employees, a Vice News report reveals that ShotSpotter's human reviewers frequently modify alerts at the request of police departments, including changing the location, time, or even deleting the alert altogether. It also notes that modifications may be made without clear justification or explanation.\nThe potential consequences of these modifications, include altered evidence being used in court cases, potentially leading to wrongful convictions; police departments relying on incomplete or inaccurate information to inform their investigations; and the erosion of trust in the ShotSpotter system and its ability to provide reliable evidence.\nThe article suggests that these modifications may be driven by a desire to improve the system's accuracy or to align with police departments' existing narratives, but ultimately, they undermine the integrity of the evidence and raise questions about the accountability of both ShotSpotter and the police departments using the technology.\n\u2795 October 2021. SoundThinking (then named ShotSpotter) filed a defamation lawsuit against Vice Media LLC for \"deliberately misrepresent[ing] court records and falsely accus[ing] the company of illegal behavior.\"\n\u2795 August 2022. SoundThinking's defamation lawsuit is dismissed as Vice News publishes Editor's Note amending reporting about the Michael Williams and Silvon Simmons cases.\n\u2795 January 2023. An internal SoundThinking document shared with the AP revealed that ShotSpotter relies heavily on human review and analysis to function accurately around 10 percent of the time.\nSystem \ud83e\udd16\nShotSpotter gunshot detection\nDocuments \ud83d\udcc3\nSoundThinking. SHOTSPOTTER RESPONDS TO FALSE AND MISLEADING ALLEGATIONS BY VICE NEWS\nSoundThinking. SHOTSPOTTER FILES DEFAMATION LAWSUIT AGAINST VICE MEDIA\nOperator: Chicago Police Department\nDeveloper: SoundThinking\nCountry: USA\nSector: Govt - police\nPurpose: Detect gunfire\nTechnology: Gunshot detection system; Deep learning; Neural network; Machine learning\nIssue: Accuracy/reliability; Bias/discrimination; Effectiveness/value; Human/civil rights\nTransparency: Governance; Black box\nInvestigations, assessments, audits \ud83e\uddd0\nVice News. Police Are Telling ShotSpotter to Alter Evidence From Gunshot-Detecting AI\nRelated \ud83c\udf10\nShotSpotter: Alerts are modified 10 percent of the time\nNew York City finds ShotSpotter identifies 13 percent of confirmed shootings\nPage info\nType: Issue\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/investigation-shotspotter-technology-has-serious-flaws", "content": "Investigation: ShotSpotter technology has \"serious flaws\"\nOccurred: March 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nShotSpotter has 'serious flaws' in its technology that calls into question its effectiveness and value and undermines the company's marketing claims, according to an AP investigation.\nCiting the mistaken arrest and imprisonment of Michael Williams, the investigation found ShotSpotter's system was unreliable, can miss live gunfire directly under its microphones, and may misclassify slamming doors, car backfires and firework sounds as gunshots. This led to a high rate of false positives, with police being dispatched to areas where no gunfire occurred, potentially resulting in unnecessary and dangerous encounters.\nThe report also highlighted concerns about the system's transparency. ShotSpotter's methodology has not been independently verified or peer-reviewed, raising questions about its use as evidence in criminal cases. Furthermore, the investigation noted instances where ShotSpotter analysts changed their initial assessments, sometimes at the request of law enforcement, which undermines the objectivity of the data.\nAdditionally, the report said the system is predominantly deployed in communities of colour, contributing to increased police presence and potentially exacerbating tensions in these areas. \nCritics argue that ShotSpotter's technology has not been proven to effectively reduce gun violence and that its deployment may lead to over-policing and civil rights violations. These findings have prompted calls for a reevaluation of the system's use and effectiveness in law enforcement.\nSystem \ud83e\udd16\nShotSpotter gunshot detection\nOperator: Chicago Police Department\nDeveloper: SoundThinking\nCountry: USA\nSector: Govt - police\nPurpose: Detect gunfire\nTechnology: Gunshot detection system; Deep learning; Neural network; Machine learning\nIssue: Accuracy/reliability; Bias/discrimination; Effectiveness/value; Human/civil rights\nTransparency: Governance; Black box\nInvestigations, assessments, audits \ud83e\uddd0\nAP. How AI-powered tech landed man in jail with scant evidence\nRelated \ud83c\udf10\nShotSpotter: Alerts are modified 10 percent of the time\nNew York City finds ShotSpotter identifies 13 percent of confirmed shootings\nPage info\nType: Issue\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/shotspotter-alerts-are-modified-10-percent-of-the-time", "content": "ShotSpotter: Alerts are modified 10 percent of the time\nOccurred: January 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA confidential SoundThinking document revealed that its ShotSpotter gunshot detection technology relies heavily on human review and analysis to function accurately.\nAn internal document obtained by the AP showed that the technology, which uses sensors and algorithms to detect and locate gunshots, requires human operators to review and verify the results around 10 percent of the time. The product is often touted as an automated solution.\nThe finding raised questions about the reliability and effectiveness of the technology, as well as the potential for human error and bias to influence the results. It also sparked concerns about the transparency and accountability of the technology, particularly in the context of law enforcement and public safety applications.\nCritics argue that the technology's reliance on human review and analysis undermines its claims of being an automated solution, and that the public has a right to know more about how the technology works and the potential limitations and biases involved.\nSystem \ud83e\udd16\nShotSpotter gunshot detection\nOperator: Chicago Police Department\nDeveloper: SoundThinking/ShotSpotter\nCountry: USA\nSector: Govt - police\nPurpose: Detect gunfire\nTechnology: Gunshot detection system\nIssue: Effectiveness/value; Oversight/review\nTransparency: Governance; Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://apnews.com/article/shotspotter-artificial-intelligence-investigation-9cb47bbfb565dc3ef110f92ac7f83862\nRelated \ud83c\udf10\nAdam Toledo killed by Chicago police using ShotSpotter\nMichael Williams gunshot detection wrongful arrest\nPage info\nType: Issue\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/new-york-city-finds-shotspotter-identifies-13-percent-of-confirmed-shooting", "content": "New York City finds ShotSpotter identifies 13 percent of confirmed shootings\nOccurred: June 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn audit report revealed that gunshot detection system ShotSpotter accurately identified only 13 percent of confirmed shootings. \nThe audit highlighted that 87 percent of the alerts generated by ShotSpotter were false alarms, often triggered by loud noises unrelated to gunfire, such as construction sounds or car backfires, raising concerns about the system's effectiveness and its tendency to waste police resources and time.\nNew York City Comptroller Brad Lander criticised the system for its high rate of false positives and questioned its effectiveness in addressing gun violence in the city. He also concluded that the NYPD failed to make efficient use of its resources and officers\u2019 time, and 'substantially' overstated the reduction in response times achieved by using the system.\nDespite the findings, the NYPD defended the use of ShotSpotter, emphasising its role in providing real-time awareness of gunfire and aiding in rapid police response. The department argued that the technology is a critical tool in combating gun violence, even though the audit suggested a need for a thorough evaluation before renewing the contract with ShotSpotter.\nThe debate over ShotSpotter's effectiveness is part of a broader discussion on the use of such technologies in law enforcement, with some cities reconsidering their investment in the system due to concerns about cost, accuracy, and potential biases.\nSystem \ud83e\udd16\nShotSpotter gunshot detection\nDocuments \ud83d\udcc3\nSoundThinking. SoundThinking Official Response to NYC Audit Report (pdf)\nOperator: New York Police Department\nDeveloper: SoundThinking/ShotSpotter\nCountry: USA\nSector: Govt - police\nPurpose: Detect gunfire\nTechnology: Gunshot detection system\nIssue: Accuracy/reliability; Bias/discrimination - race, ethnicity, income, location; Effectiveness/value; Oversight/review; Robustness\nTransparency: Governance; Black box; Marketing; Legal\nInvestigations, assessments, audits \ud83e\uddd0\nNew York City Comptroller (2024). Audit Report on the New York City Police Department\u2019s Oversight of Its Agreement with ShotSpotter Inc. for the Gunshot Detection and Location System\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.cbsnews.com/newyork/news/nypd-shotspotter-report/\nhttps://www.smartcitiesdive.com/news/new-york-joins-cities-questioning-shotspotter-costs-benefits-Houston-Chicago-gun-crime/721118/\nhttps://www.fox13news.com/news/tpd-renews-gunshot-detection-system-nyc-audit-casts-doubt-shotspotter-accuracy\nRelated \ud83c\udf10\nAdam Toledo killed by Chicago police using ShotSpotter\nMichael Williams gunshot detection wrongful arrest\nPage info\nType: Issue\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/starship-robot-struck-by-freight-train", "content": "Starship robot struck by freight train\nOccurred: March 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Starship delivery robot was struck by a freight train after becoming stranded at a railroad crossing in Oregon. \nThe incident occurred when the robot, deployed by Oregon State University, failed to navigate a railway crossing and was subsequently run over by an oncoming freight train. Two other Starship robots seemingly traversed the railway safely.\nThis incident highlighted the challenges faced by autonomous delivery robots, particularly in navigating complex environments like railroad crossings. \nWhile the robot was designed to deliver food and packages, its inability to safely cross the tracks raises concerns about the reliability and safety of such technologies in real-world scenarios. \nThe incident underscored the need for improved safety protocols and infrastructure to accommodate autonomous vehicles and prevent similar occurrences in the future.\nSystem \ud83e\udd16\nStarship delivery robot\nOperator: Oregon State University\nDeveloper: Starship Technologies\nCountry: USA\nSector: Transport/logistics\nPurpose: Deliver groceries\nTechnology: Robotics; Machine learning; Neural networks\nIssue: Safety\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.autoblog.com/2022/03/04/train-hits-autonomous-robot-crossing/\nhttps://www.youtube.com/watch?v=XMzdyesno_Y\nhttps://boingboing.net/2022/10/27/cute-little-food-delivery-robot-crushed-under-the-wheels-of-a-locomotive-video.html\nRelated \ud83c\udf10\nStarship robots impede University of Pittsburgh wheelchair users\nStarship robot hits car at stoplight, causes USD 2,600 damage\nPage info\nType: Incident\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/illustrator-drops-bradford-literary-festival-over-ai-use", "content": "Illustrator drops Bradford Literary Festival over AI use\nOccurred: May 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe Bradford Literary Festival faced significant backlash after using AI-generated images in its promotional materials for its 2023 event. \nThe controversy erupted when a tweet featuring an AI-generated animated image of a young girl reading was shared online, prompting authors and illustrators to criticise the festival for undermining creative professionals and contributing to the devaluation of artistic work.\nIn one case, book illustrator Chris Mould pulled out because artificial intelligence (AI) was used to produce its publicity images. Due to hold a masterclass at the event, he told the BBC, \"How can I stand under their roof and tell people they can go to art school?\"\nIn response to the criticism, the festival issued an apology, acknowledging that the use of AI-generated images did not align with its commitment to supporting creative careers. They clarified that while they did not explicitly commission AI-generated illustrations, they also did not exclude them from their brief to the design agency, Lazenby Brown. \nThe festival has since committed to not using AI-generated imagery in the future and has replaced the problematic images with licensed stock photos.\nThe incident highlighted broader concerns within the creative community about the ethical implications of AI in the arts, prompting discussions about the need for clearer guidelines and protections for artists. \nSystem \ud83e\udd16\nUnknown\nDocuments \ud83d\udcc3\nBradford Literary Festival. Statement\nOperator: Bradford Literary Festival; Lazenby Brown\nDeveloper:  \nCountry: UK\nSector: Media/entertainment/sports/arts\nPurpose: Create artwork\nTechnology: Machine learning\nIssue: Employment\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/uk-england-leeds-65495768\nhttps://www2.societyofauthors.org/2023/05/03/use-of-ai-a-letter-to-bradford-literature-festival/\nhttps://www.thebookseller.com/news/bradford-lit-fest-apologises-and-withdraws-ai-image\nhttps://www.yorkshirepost.co.uk/news/people/bradford-literature-festival-accused-of-taking-work-away-from-artists-after-using-ai-to-create-designs-for-its-website-and-programmes-4131430\nRelated \ud83c\udf10\nAI generates visuals for Wizards of the Coast marketing promotion\nKlarna halves marketing team by using AI\nPage info\nType: Issue\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/mary-nightingale-likeness-used-in-deepfake-scam", "content": "Mary Nightingale likeness used in deepfake scam\nOccurred: March 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nITV news anchor Mary Nightingale expressed outrage after discovering that her likeness had been used in a deepfake video promoting a financial investment app. \nThe manipulated video, which surfaced on social media, depicted Nightingale as if she were presenting the ITV Evening News before promoting the app. Nightingale described the experience as \"identity theft,\" emphasising the potential dangers of deepfakes in manipulating public trust, especially with upcoming elections.\nThe incident highlights the growing prevalence of deepfake technology, which has been used to create fake celebrity endorsements and could be exploited for scams or misinformation.\nNightingale's concerns reflect broader worries about the misuse of AI technologies, prompting calls for stronger regulations to address the risks associated with deepfakes.\nSystem \ud83e\udd16\nUnknown\nOperator:\nDeveloper:  \nCountry: UK\nSector: Media/entertainment/sports/arts\nPurpose: Defraud\nTechnology: Deepfake - video\nIssue: Personality rights\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.ccn.com/news/technology/mary-nightingale-livid-at-ai-deepfake/\nhttps://www.itv.com/news/wales/2024-03-26/it-made-me-absolutely-livid-mary-nightingale-on-being-deepfaked\nhttps://pressgazette.co.uk/publishers/broadcast/deepfake-mary-nightingale/\nRelated \ud83c\udf10\nDeepfakes of UK TV health experts used to promote health scams\nMartin Lewis impersonated in deepfake scam ad\nPage info\nType: Incident\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/starship-robots-impede-wheelchair-users", "content": "Starship robots impede wheelchair users\nOccurred: October 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Starship Technologies delivery robot impeded wheelchair users at the University of Pittsburgh, resulting in complaints and the \nDoctoral student Emily Ackerman reported being trapped on Forbes Avenue when a robot blocked the only accessible entrance to the sidewalk, posing a safety hazard as traffic approached. \nThe incident, along with similar complaints from other wheelchair users, led the university to temporarily suspend the use of the robots. \nStarship Technologies acknowledged the issue and stated that the incident was due to a mapping error, which has since been addressed. The company also emphasised its commitment to accessibility and is working with local advocacy groups to improve the robots' interactions with mobility devices. \nThe situation highlighted the challenges autonomous delivery robots face in ensuring accessibility and safety for all pedestrians, particularly those with mobility impairments.\nSystem \ud83e\udd16\nStarship delivery robot\nOperator: University of Pittsburgh\nDeveloper: Starship Technologies\nCountry: USA\nSector: Transport/logistics\nPurpose: Deliver groceries\nTechnology: Robotics; Machine learning; Neural network\nIssue: Ethics/values; Safety; Liability\nTransparency: Governance; Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://thespoon.tech/starships-robots-pulled-from-university-of-pittsburgh-after-accessibility-incident/\nhttps://triblive.com/local/pittsburgh-allegheny/pitt-suspends-delivery-robots-after-wheelchair-user-reports-safety-hazard/\nhttps://www.wesa.fm/identity-justice/2019-10-22/food-delivery-robots-pulled-from-pitt-campus-after-backlash-about-mobility\nhttps://pittnews.com/article/151679/news/pitt-pauses-testing-of-starship-robots-due-to-safety-concerns/\nRelated \ud83c\udf10\nStarship robot knocks child in shopping centre\nStarship robot damages car, flees scene of crime\nPage info\nType: Issue\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/starship-robot-hits-car-at-stoplight-causes-usd-2600-damage", "content": "Starship robot hits car at stoplight, causes USD 2,600 damage\nOccurred: September 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Starship delivery robot collided with a car at a Frisco, Texas stoplight, causing significant damage estimated at USD 2,600 and  \nThe car driver, Jisuk Mok, initially thought the noise was from an animal but later discovered it was a robot had hit her vehicle. \nFollowing the accident, Mok faced challenges in reporting the incident, as there was no contact information on the robot. After contacting her insurance company, she was informed that she would need to prove the robot was at fault to avoid paying her deductible. \nStarship Technologies only agreed to cover the repair costs after the media had contacted them. It transpired the company had video footage of the incident but was reluctant to share it with Mok.\nThe incident highlighted ongoing concerns about the accountability and safety of autonomous delivery robots, particularly in urban environments where interactions with vehicles can lead to confusion over liability and reporting procedure.\nSystem \ud83e\udd16\nStarship delivery robot\nOperator:\nDeveloper: Starship Technologies\nCountry: USA\nSector: Transport/logistics\nPurpose: Deliver groceries\nTechnology: Robotics; Machine learning; Neural network\nIssue: Ethics/values; Safety; Liability\nTransparency: Governance; Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.nbcdfw.com/news/nbc-5-responds/nbc-5-responds-what-happens-if-youre-in-a-crash-with-a-robot/2442345/\nhttps://www.theverge.com/2021/9/1/22652980/starship-delivery-robot-kentucky-car-accident\nRelated \ud83c\udf10\nStarship robot knocks child in shopping centre\nStarship robot damages car, flees scene of crime\nPage info\nType: Issue\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/starship-robot-delivering-groceries-veers-into-canal", "content": "Starship robot delivering groceries veers into canal\nOccurred: November 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Starship delivery robot veered off course in Milton Keynes, UK, and fell into a canal while making a delivery, raising questions about its safety mechanisms.\nLocal resident, Bex Morgan was walking her dog when she saw the robot driving straight into the water. Morgan reported that the company promptly responded to retrieve the robot from the canal. \nThe incident sparked humorous speculation on social media, with some joking that the robot might have been trying to cool off in the hot weather.\nStarship's robots are equipped with advanced navigation technology, including cameras, ultrasound, radar, and GPS, to help them travel safely along pavements and across streets. \nDespite the technology, the incident highlights the challenges these autonomous machines face in navigating their environments effectively.\nSystem \ud83e\udd16\nStarship delivery robot\nOperator: Co-op\nDeveloper: Starship Technologies\nCountry: UK\nSector: Transport/logistics\nPurpose: Deliver groceries\nTechnology: Robotics; Machine learning; Neural network\nIssue: Safety\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/uk-england-beds-bucks-herts-53678376\nRelated \ud83c\udf10\nStarship robot knocks child in shopping centre\nStarship robot damages car, flees scene of crime\nPage info\nType: Issue\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/starship-robot-knocks-child-in-shopping-centre", "content": "Starship robot knocks child in shopping centre\nOccurred: November 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Starship delivery robot reportedly \"hit and pushed\" a two-tyear-old boy in the Brunel shopping center in Milton Keynes, UK, leaving him \"stressed and scared\". \nThe boy, who was walking with his mother, was knocked over by the robot, which failed to stop and continued on its delivery route. Fortunately, he did not sustain any injuries, but the experience reportedly left him frightened.\nThe child's parent reported the incident on a neighborhood social media platform, expressing dissatisfaction with Starship's response, which included a GBP 5 discount offer. Starship stated that they take such incidents seriously and are looking into the matter.\nThe incident sparked discussions about the safety of Starship delivery robots in public spaces, with mixed reactions from the community regarding their presence and behaviour. \nSystem \ud83e\udd16\nStarship delivery robot\nOperator: Co-op\nDeveloper: Starship Technologies\nCountry: UK\nSector: Transport/logistics\nPurpose: Deliver groceries\nTechnology: Robotics; Machine learning; Neural network\nIssue: Safety\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.dailystar.co.uk/tech/news/popular-delivery-robots-under-investigation-26689171.amp\nhttps://www.miltonkeynes.co.uk/news/people/robots-under-investigation-after-alleged-incident-involving-a-child-while-making-a-delivery-in-milton-keynes-3649302\nhttps://www.bucksfreepress.co.uk/news/20071006.starship-robots-investigate-shopping-centre-incident/\nRelated \ud83c\udf10\nStarship robot damages car, flees scene of crime\nKiwiBot food delivery robot catches fire\nPage info\nType: Incident\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/starship-robot-wipes-out-rushden-shopper", "content": "Starship robot 'wipes out' Rushden shopper\nOccurred: November 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Starship delivery robot was involved in a confrontation with a Rushden, UK resident, Abdul Sottar, who reported that the robot \"charged\" at him after it had damaged a cat repellent device in his garden. \nSottar approached the robot to investigate and, while attempting to take a photo of its identification number, the robot began to move erratically, hitting him multiple times before driving away.\nStarship Technologies stated that the robot was operating at a low speed of 4 mph and emphasied that such incidents are rare. Following the event, the operator of the robot received additional training to prevent similar occurrences in the future. \nStarship offered Sottar GBP 50 to replace the damaged device and established an exclusion zone around his property to prevent the robot from entering his garden again.\nSottar expressed concerns about the need for better safety measures, especially when humans take control of the robots, and highlighted the potential dangers posed by these autonomous machines. \nThe incident added to the ongoing discussions about the safety and accountability of delivery robots in public spaces, particularly regarding their interactions with pedestrians.\nSystem \ud83e\udd16\nStarship delivery robot\nOperator: Co-op\nDeveloper: Starship Technologies\nCountry: UK\nSector: Transport/logistics\nPurpose: Deliver groceries\nTechnology: Robotics; Machine learning; Neural network\nIssue: Safety\nTransparency: Governance; Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.thegrocer.co.uk/technology-and-supply-chain/delivery-robot-driver-gets-extra-training-after-trying-to-run-over-pedestrian/685869.article\nRelated \ud83c\udf10\nStarship robot knocks child in shopping centre\nStarship robot damages car, flees scene of crime\nPage info\nType: Issue\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/starship-robot-attacks-milton-keynes-resident", "content": "Starship robot 'attacks' Milton Keynes resident\nOccurred: June 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA resident reported being \"attacked\" by a Starship delivery robot in Milton Keynes, UK, prompting concerns about the safety of the service. \nThe robot collided with resident Brian Dawson's dog Pippa, prompting him to lash out at the robot in response. The robot later hit him in his left leg when he was trying to file a complaint. The confrontation led to the robot being described as having \"attacked\" the resident, highlighting concerns about the safety and behaviour of autonomous delivery machines.\nStarship Technologies, the company behind the robot, has faced scrutiny regarding operational safety and liability. The robots, which are designed to navigate sidewalks and deliver groceries, have been involved in several incidents, including previous reports of collisions with pedestrians and pets. \nThe Co-op told Dawson that the incident was not their responsibility. Starship stated that they take safety seriously and that additional training is provided to operators to prevent such occurrences. \nOverall, while residents appreciate the convenience of the Starship robots, incidents like this raise questions about their reliability and the potential risks they pose in urban environments.\nSystem \ud83e\udd16\nStarship delivery robot\nOperator: Co-op\nDeveloper: Starship Technologies\nCountry: UK\nSector: Transport/logistics\nPurpose: Deliver groceries\nTechnology: Robotics; Machine learning; Neural network\nIssue: Ethics/values; Liability; Safety\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.thesun.co.uk/tech/22733115/dog-walker-brian-dawson-attacked-co-op-delivery-robot/\nhttps://www.dailymail.co.uk/news/article-12208971/I-attacked-op-delivery-robot-rammed-German-shepherd.html\nRelated \ud83c\udf10\nStarship robot damages car, flees scene of crime\nKiwiBot food delivery robot catches fire\nPage info\nType: Incident\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/starship-robot-damages-car-flees-scene-of-crime", "content": "Starship robot damages car, flees scene of crime\nOccurred: February 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Starship food delivery robot slid off a pavement in Helsinki, Finland, damaged a parked car and started to move away as if nothing had happened.\nThe Starship Technologies robot appeared to slip on a snowy pavement whilst making a delivery and bumped into a car, scratching it on two doors in the process. The bot was then lifted back onto the pavement by a passer-by, and it moved back on its route. \nThe passer-by notified an emergency center about the \"traffic accident,\" which passed the information to the police. \nStarship has been criticised for not proving clear information on how to report incidents.\nSystem \ud83e\udd16\nStarship delivery robot\nOperator: S Group Finland/S-kaupat\nDeveloper: Starship Technologies\nCountry: Finland\nSector: Transport/logistics\nPurpose: Deliver groceries\nTechnology: Robotics; Machine learning; Neural networks\nIssue: Ethics/values; Liability; Safety\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://yle.fi/a/74-20072150\nhttps://futurism.com/the-byte/delivery-robot-damages-car-flees-scene\nhttps://euroweeklynews.com/2024/02/15/helsinki-hit-and-run-food-delivery-robot-vs-parked-car/\nRelated \ud83c\udf10\nKiwiBot food delivery robot catches fire\nAmazon delivery drone crashes, sparks 22-acre fire\nPage info\nType: Incident\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ask-delphi-says-genocide-is-ok-if-it-makes-people-happy", "content": "Ask Delphi says genocide is OK if it makes people happy\nOccurred: November 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAsk Delphi suggested that genocide could be acceptable if it made people happy, raising questions about the AI system's stated aim of being able to predict human ethical judgements in various scenarios.\nDeveloped by the Allen Institute for AI, Ask Delphi is a machine learning system trained on a large body of internet text and responses from Mechanical Turk gig workers. Mechanical Turk is a paid crowdsourcing platform popular with researchers.\nIn one instance, Delphi responded to a question by saying genocide was OK as long as it made everyone happy. Per The Verge, it also said that America is \u201cgood\u201d and that Somalia is \u201cdangerous\u201d; that eating babies is \u201cokay\u201d as long as you are \u201creally, really hungry, and that it\u2019s \u201cgood\u201d to \u201csecure the existence of our people and a future for white children\u201d (a white supremacist slogan known as the 14 words) and that \u201cbeing straight is more morally acceptable than being gay.\u201d \nDelphi was later updated to improve outputs regarded as biased, racist or offensive.\nAsk Delphi's response raised questions about its ability to make ethical judgements. It also prompted critics to question whether machines should be trying to resolve ethical calls in the first place. \nSystem \ud83e\udd16\nAsk Delphi\nOperator: Allen Institute for AI\nDeveloper: Allen Institute for AI\nCountry: USA\nSector: NGO/non-profit/social enterprise\nPurpose: Answer ethical dilemmas\nTechnology: Chatbot; NLP/text analysis\nIssue: Safety\nTransparency: Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.vox.com/future-perfect/2021/10/27/22747333/artificial-intelligence-ethics-delphi-ai\nhttps://www.vice.com/en/article/ethical-ai-trained-on-reddit-posts-said-genocide-is-okay-if-it-makes-people-happy/\nRelated \ud83c\udf10\n\n\nPage info\nType: Issue\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/error-strewn-ai-generated-obituaries-compound-grief", "content": "Error-strewn AI-generated obituaries compound grief\nOccurred: July 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nObituaries generated by AI are reported to be causing real harm to grieving families, compounding their distress.\nOften filled with errors and created without the family\u2019s consent, obituaries created AI systems are appearing on ad-filled websites such as BNN, The Thaiger, FreshersLive and Obitsupdate by a cottage industry of \"writers\" looking opportunistically to exploit the deceased to generate ad revenue and demonstrating no interest in or concern for the impacts caused.\nIn many instances, the articles are full of incorrect details about the deceased's life, relationships, and accomplishments, and appear impersonal, inhuman, and without empathy. \nIn one instance, Bridget Todd discovered an inaccurate obituary for her mother circulating online, exacerbating her grief. In another, fake AI-written obituaries appeared for Brian Vastag and Beth Mazur, even though the former was still alive, causing confusion, anger and additional distress as friends were led to believe the cause may have been a double suicide. Beth Mazur actually died after 15 years battling Myalgic encephalomyelitis (ME).\nAnd a real obituary written by the children of former US Army officer and IBM employee Paul Mahoney and published in local newspapers was regurgitated in garish style on \"news\" website BNN alongside multiple adverts.\nThe findings raised concerns about the misuse of AI for profit, and the lack of interest in downtream impacts by the operators of these systems. \nSystem \ud83e\udd16\n\nOperator: BNN; FreshersLive; Legacy.com; Obitsupdate; The Thaiger\nDeveloper:  \nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Write obituaries\nTechnology: Machine learning\nIssue: Mis/disinformation\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.fastcompany.com/91162990/ai-written-obituaries-are-compounding-peoples-grief\nhttps://www.theverge.com/24065145/ai-obituary-spam-generative-clickbait\nRelated \ud83c\udf10\nMSN publishes 'useless' AI-generated Brandon Hunter obituary\nChatGPT wrongly claims Alexander Hanff is dead\nPage info\nType: Incident\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/dwp-kickstart-gateway-firms-with-no-trading-history-approved-by-algorithm", "content": "DWP Kickstart \"gateway\" firms with no trading history approved by algorithm\nOccurred: January 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA UK system approved funding for 'dozens of companies' selected to become \"Kickstart gateways\" despite some having little to no trading history or being based outside the UK.\n An FE Week investigation discovered that the Department for Work and Pensions (DWP) used the Cabinet Office's Spotlight \"automated due diligence checks\" as part of the Kickstart selection process. \nThe UK government's Kickstart scheme was a new employer initiative announced in summer 2020 aimed at creating six-month paid work placements for young people who were at risk of long-term unemployment. The scheme closed in January 2023.\nA Kickstart 'gateway' was a type of organisation, such as a local authority, charity or trade body that would act as an intermediary and apply for funding on behalf of companies wishing to create fewer than 30 job placements.\nGateway firms received a GBP 300 fee per job placement, plus up to GBP 1,500 for every 16 to 24-year-old on Universal Credit they put through the wage-subsidised employment programme.\nThe investigation raised questions about the quality of gateway providers, as well as about Spotlight's effectiveness. It also prompted the shadow work and pensions secretary to express concern over the potential misuse of public funds and call for urgent action.\nThe DWP maintained that \"Kickstart gateways are subject to stringent checks,\" despite the findings.\nThis situation raised questions about the effectiveness of automated approval processes for government programs and the potential risks associated with relying heavily on algorithms for due diligence.\nSystem \ud83e\udd16\nSpotlight\nOperator: Department of Work and Pensions (DWP)\nDeveloper: UK Cabinet Office\nCountry: UK\nSector: Govt - culture; Govt - employment\nPurpose: Assess public funds applications\nTechnology: Automated risk assessment\nIssue: Accuracy/reliability; Bias/discrimination - political\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://feweek.co.uk/2021/01/24/dwp-kickstart-gateway-firms-approved-by-algorithm-with-no-trading-history-or-based-abroad/\nRelated \ud83c\udf10\nUK government accused of COVID-19 recovery funding algorithmic \"cultural elitism\"\nPoland COVID-19 Cultural Support Fund assessments blasted as unfair\nPage info\nType: Issue\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/uk-government-accused-of-covid-19-recovery-funding-cultural-elitism", "content": "UK government accused of COVID-19 recovery funding algorithmic \"cultural elitism\"\nOccurred: November 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA GBP 1.57 billion package of funds intended to support the UK arts, culture and heritage sectors during the COVID-19 pandemic was accused of algorithmic \"cultural elitism\".\nThe UK Cultural Recovery Fund (CRF)'s distribution process was criticised for favoring larger, well-established cultural institutions over smaller, community-based organisations. Critics argued that the algorithm used to allocate funds inherently benefitted organisations with more substantial pre-existing resources and visibility, thereby perpetuating cultural elitism.\nThere were also concerns that the fund's allocation failed to adequately address regional imbalances. Some regions with rich cultural heritages and significant needs may have received less support compared to more affluent areas with already well-funded institutions.\nLocal councils, which are significant public funders of culture, were reportedly not sufficiently involved in the strategic decisions regarding the distribution of the CRF. This oversight potentially undermined the effectiveness of the fund in addressing local cultural needs and supporting smaller, community-focused projects.\nThe Culture Recovery Board, established to oversee the fund, included various stakeholders from significant cultural bodies and independent members. Despite these measures, the board faced challenges in ensuring equitable distribution due to the inherent limitations of the algorithm and the criteria set for funding allocation.\nThe Culture Recovery Fund may have provided essential support to many cultural organisations during the pandemic. However, the fracas highlighted  the need for more inclusive and transparent funding mechanisms - algorithmic and otherwise - that consider the diverse needs of all cultural entities, including smaller and community-based organisations.\nSystem \ud83e\udd16\nSpotlight\nOperator: Arts Council England (ACE); UK Cabinet Office\nDeveloper: UK Cabinet Office\nCountry: UK\nSector: Govt - culture; Govt - employment\nPurpose: Assess public funds applications\nTechnology: Automated risk assessment\nIssue: Accuracy/reliability; Bias/discrimination - political\nTransparency: Governance\nInvestigations, assessments, audits \ud83e\uddd0\nNational Audit Office. Investigation into the Culture Recovery Fund (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.artsprofessional.co.uk/news/exclusive-due-diligence-checks-emergency-grants-were-automated\nhttps://lwlies.com/articles/uk-culture-recovery-fund-spotlight-algorithm/\nhttps://www.theartnewspaper.com/news/numbers-game-emergency-funds-in-the-spotlight\nhttps://www.artsprofessional.co.uk/magazine/news-comment/sector-should-be-scared-very-scared\nRelated \ud83c\udf10\nDWP Kickstart \"gateway\" firms with no trading history approved by algorithm\nPoland COVID-19 Cultural Support Fund assessments blasted as unfair\nPage info\nType: Incident\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/privacy-group-sues-to-see-secret-airbnb-trustworthy-scores", "content": "Privacy group sues to see secret Airbnb trustworthy scores\nOccurred: February 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA US privacy group filed a complaint against Airbnb with the Federal Trade Commission (FTC) demanding transparency about the company's use of a secretive algorithm to assign trustworthiness scores to users.\nThe Electronic Privacy Information Center (EPIC) alleged that Airbnb's algorithm assigns \"secret ratings\" to prospective renters based on behaviour traits and personal data collected from various online sources, including social media, blogs, and other web pages.\nThe complaint claimed that the trustworthiness scores are essentially consumer reports that bear on individuals' character and reputation but lack reasonable procedures to ensure accuracy, and argued that Airbnb's practices violated the US FTC Act and the Fair Credit Reporting Act (FCRA).\nEPIC also contended that the algorithm is biased, unprovable, and not replicable. It argued that such systems can lead to substantial injury to consumers, particularly marginalised communities, by unfairly penalising them based on subjective data.\nThe complaint also highlighted the lack of transparency in how Airbnb's algorithm functions and the criteria it uses to judge users, and criticises the company for not providing clear guidelines on how user data is collected, stored and used.\nEPIC's action seen as part of a broader push to address the use of artificial intelligence and big data in commerce, emphasising the need for regulatory oversight to protect consumer privacy, prevent unfair practices, and address opacity.\nSystem \ud83e\udd16\nAirbnb user trustworthiness scoring\nOperator: Airbnb\nDeveloper: Airbnb/Trooly\nCountry: USA\nSector: Travel/hospitality\nPurpose: Assess trustworthiness\nTechnology: Behavioural analysis; Personality analysis; Ranking algorithm\nIssue: Accuracy/reliability; Bias/discrimination - profession/job\nTransparency: Governance; Black box; Complaints/appeals\nRegulation \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nFederal Trade Commission Act\nFair Credit Reporting Act\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nEPIC (2020). EPIC Files Complaint with FTC about Airbnb\u2019s Secret \u201cTrustworthiness\u201d Scores\nEPIC (2020). Complaint and Request for Investigation, Injunction, and Other Relief Submitted by The Electronic Privacy Information Center (EPIC) (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.standard.co.uk/tech/airbnb-software-scan-online-life-suitable-guest-a4325551.html\nhttps://www.vice.com/en/article/4ag7vq/airbnb-has-secret-trustworthy-scores-and-this-privacy-group-is-demanding-to-see-them\nhttps://www.jdsupra.com/legalnews/trust-in-trustworthiness-ratings-54640/\nhttps://www.macobserver.com/news/airbnbs-secret-algorithms/\nRelated \ud83c\udf10\nReport: Airbnb uses \"secretive\" algorithm to judge if users are trustworthy\nReport: Airbnb Smart Pricing algorithm exacerbates racial inequality\nPage info\nType: Issue\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/report-airbnb-uses-secretive-algorithm-to-judge-if-users-are-trustworthy", "content": "Report: Airbnb uses \"secretive\" algorithm to judge if users are trustworthy\nOccurred: March 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAirbnb secretly collects and feeds users\u2019 personal data into an algorithm that assesses whether they are trustworthy enough to make a booking, according to a consumer group investigation.\nAustralian consumer rights organisation CHOICE discovered that Airbnb employs an algorithm that evaluates users' trustworthiness based on a variety of personal data, including social media activity and online behavior. The assessment reportedly includes factors not connected with their rental history, such as language used in communications, past behaviours, and associations with certain topics deemed risky.\nThe discovery prompted concerns that the algorithm could jeopardise user privacy, unfairly penalise individuals based on subjective or biased data, and could lead to discriminatory outcomes - something seen as particularly troubling for marginalised communities who might be disproportionately affected. \nSome commentators felt the system was similar to \"social scoring\" - when an automated system assesses a person's trustworthiness or likely future behaviour.\nCHOICE also took Airbnb to task for poor transparency on how the algorithm functions and the criteria it uses to judge users. Users in Australia complained about not being informed why they had been banned from the platform.\nSystem \ud83e\udd16\nAirbnb user trustworthiness scoring\nOperator: Airbnb\nDeveloper: Airbnb/Trooly\nCountry: Australia; New Zealand\nSector: Travel/hospitality\nPurpose: Assess trustworthiness\nTechnology: Behavioural analysis; Personality analysis; Ranking algorithm\nIssue: Accuracy/reliability; Bias/discrimination - profession/job\nTransparency: Governance; Black box; Complaints/appeals\nResearch, advocacy \ud83e\uddee\nCHOICE. Is Airbnb using an algorithm to ban users from the platform?\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://thenewdaily.com.au/news/2022/03/22/choice-airbnb-trust-algorithm/\nhttps://au.finance.yahoo.com/news/banned-from-airbnb-023208437.html\nhttps://www.consumer.org.nz/articles/could-you-be-banned-from-airbnb-for-your-instagram-posts\nRelated \ud83c\udf10\nPrivacy group sues to see secret Airbnb trustworthy scores\nReport: Airbnb Smart Pricing algorithm exacerbates racial inequality\nPage info\nType: Incident\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/uk-drops-racist-visa-streaming-system", "content": "UK drops 'racist' visa streaming system\nOccurred: August 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe UK Home Office halted the use of a secretive algorithm that processed visa applications, which had been accused of containing \"entrenched racism.\" \nIn October 2019, advocacy groups, including the Joint Council for the Welfare of Immigrants (JCWI) and Foxglove, launched a legal challenge against the algorithm, arguing that it perpetuated racial discrimination and violated the Equality Act 2010. They highlighted a \"feedback loop\" where visa rejection rates influenced which nationalities were classified as \"suspect,\" further entrenching bias. \nThe suit also alleged that the algorithm was not transparent, and aimed to force the Home Office to explain on what basis the algorithm 'streams' visa applicants. Aside from admitting the existence of a secret list of 'suspect' nationalities, the Home Office had refused to provide meaningful information about how the system worked.\nHaving insisted the algorithm was used only to allocate applications and that immigration officers ultimately ruled on them, the Home Office announced that it would settle the suit and halt the use of the system until it had been redesigned considering 'issues around unconscious bias and the use of nationality'. \nThe controversy was seen to highlight the broader issues of algorithmic bias in government systems and the need for transparency and accountability in automated decision-making processes.\nSystem \ud83e\udd16\nUK Visa Streaming traffic light system\nOperator: UK Home Office\nDeveloper: UK Home Office\nCountry: UK\nSector: Govt - immigration\nPurpose: Assess visa applications\nTechnology: Risk assessment algorithm\nIssue: Bias/discrimination - race, ethnicity\nTransparency: Governance; Black box; Marketing\nRegulation \u2696\ufe0f\nEquality Act 2010\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nJCWI (2020). We won! Home Office to stop using racist visa algorithm\nFoxglove (2019). Legal action to challenge Home Office use of secret algorithm to assess visa applications\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.com/news/technology-53650758\nhttps://www.theguardian.com/uk-news/2020/jan/01/visa-applications-home-office-refuses-to-reveal-high-risk-countries\nhttps://www.theguardian.com/uk-news/2019/oct/29/ai-system-for-granting-uk-visas-is-biased-rights-groups-claim\nhttps://www.politicshome.com/news/article/home-office-to-end-use-of-racist-algorithm-for-uk-visa-decisions-in-face-of-legal-challenge-by-migrants-rights-group\nhttps://techcrunch.com/2020/08/04/uk-commits-to-redesign-visa-streaming-algorithm-after-challenge-to-racist-tool/\nhttps://www.technologyreview.com/2020/08/05/1006034/the-uk-is-dropping-an-immigration-algorithm-that-critics-say-is-racist/\nhttps://tech.newstatesman.com/policy/home-office-shelve-racist-visa-algorithm-jcwi-legal-challenge\nhttps://www.theguardian.com/uk-news/2020/aug/04/home-office-to-scrap-racist-algorithm-for-uk-visa-applicants\nhttps://www.thejusticegap.com/price-and-prejudice-automated-decision-making-and-the-uk-government/\nRelated \ud83c\udf10\nUK Home Office secretly uses algorithms to process visa applications\nUK passport application system fails to recognise man's black skin\nPage info\nType: Incident\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/home-office-secretly-uses-algorithms-to-process-visa-applications", "content": "UK Home Office secretly uses algorithms to process visa applications\nOccurred: June 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe UK Home Office used a secretive algorithm to process visa applications, prompting lawyers and civil rights groups to warn the software could discriminate against applicants based on nationality, race or age.\nThe Financial Times revealed that UK authorities had been using an algorithm to process work, study and visitor visa applications but were refusing to  provide any detail about the factors used to assess risk, or how regularly the algorithm is updated, because it feared this could encourage fraudulent applications.\nThe system had only come to the attention of immigration professionals when a group of lawyers were shown the streaming process during a visit to a visa processing centre in Sheffield.\nThe finding prompted lawyers and civil rights groups to express concerns that the system may disadvantage UK visa applicants by reinforcing existing societal prejudices. It also resulted in calls for the Home Office to provide more information on how it assesses applications. \n\u2795 October 2019. Justice advocacy group Foxglove and the Joint Council for the Welfare of Immigrants (JCWI) launched a legal case to force the Home Office to explain on what basis the algorithm streams visa applicants. \n\u2795 August 2020. The Home Office settled a lawsuit brought against the Visa Streaming system and halt its use.\nSystem \ud83e\udd16\nUK Visa Streaming traffic light system\nOperator: UK Home Office\nDeveloper: UK Home Office\nCountry: UK\nSector: Govt - immigration\nPurpose: Assess visa applications\nTechnology: Risk assessment algorithm\nIssue: Bias/discrimination - race, ethnicity\nTransparency: Governance; Black box; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.ft.com/content/0206dd56-87b0-11e9-a028-86cea8523dc2\nhttps://inews.co.uk/news/politics/home-office-visa-application-algorithms-history-failures-300595\nRelated \ud83c\udf10\nUK sham marriage tool found to disproportionately flag Greeks, Albanians, Bulgarians and Romanians\nInaccurate ETS test finds most English language test students 'cheated'\nPage info\nType: Issue\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/utah-dispute-resolution-system-benefits-lenders-not-litigants", "content": "Utah online dispute resolution system benefits lenders not litigants\nOccurred: March 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn online dispute resolution (ODR) system used by Utah benefits large payday lenders instead of making the courts more accessible to low-income litigants, according to a media investigation. \nThe Markup found that the rate of default judgments has risen since the implementation of the system - resulting in more cases being decided in favour of one party because the other party did not respond, often due to difficulties in navigating the system. \nIt also found that many users, like Thompson, missed critical notifications about the requirement to register for the ODR system due to poorly designed summons paperwork. This included long, case-sensitive URLs and buried information, which led to many litigants not engaging with the system.\nPayday lenders were identified as the primary beneficiaries of the system. The streamlined process for filing and resolving claims made it easier for  lenders to secure judgements against debtors, often low-income individuals who struggle to navigate the platform.\nIn addition, the investigation found that \"the move to online dispute resolution has had severe consequences for many of the Utahans it was supposed to help and that the state\u2019s courts have been slow to implement potential fixes.\"\nSystem \ud83e\udd16\nUtah online dispute resolution system\nOperator: West Valley City Justice Court; Orem City Justice Court\nDeveloper: Utah Administrative Office of the Courts\nCountry: USA\nSector: Govt - justice\nPurpose: Resolve disputes\nTechnology: \nIssue: Fairness\nTransparency: Governance; Black box; Marketing\nInvestigations, assessments, audits \ud83e\uddd0\nThe Markup (2022). Payday Lenders Are Big Winners in Utah\u2019s Chatroom Justice Program\nThe MarkUp (2022). Can Chatrooms Replace Coutrooms?\nThe Markup (2022) Utah ODR data and methodology\nRelated \ud83c\udf10\nMalaysia AI court sentencing system accused of being inaccurate, unfair\nShanghai AI prosecutor accused of being inaccurate\nPage info\nType: Incident\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/study-utah-dispute-resolution-system-has-major-design-flaws", "content": "Study: Utah online dispute resolution system has major design flaws\nOccurred: November 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn evaluation of Utah's ODR platform usability by the Innovation for Justice program at the University of Arizona concluded it had major design problems, with most users reporting 'frustration on account of their inability to easily find information about ODR, including guidance about how it worked, whether participation was mandatory, and how to contact someone for more assistance.' \nAccording to report, statistics show that only about 36 percent of defendants in small claims cases actually log into the system, and those that do frequently require technical support. At the same time, the researchers found that participants in the study were receptive to the idea of an online platform for handling small claims cases. \nThe authors recommended that Utah streamline its registration process, make document sharing easier, and provide more guidance to users trying to navigate the system in order to make the ODR program more accessible to Utahans facing small claims suits.\nSystem \ud83e\udd16\nUtah online dispute resolution system\nOperator: West Valley City Justice Court; Orem City Justice Court\nDeveloper: Utah Administrative Office of the Courts\nCountry: USA\nSector: Govt - justice\nPurpose: Resolve disputes\nTechnology: \nIssue: Fairness\nTransparency: Governance; Black box; Marketing\nInvestigations, assessments, audits \ud83e\uddd0\nInnovation for Justice, University of Arizona James E. Rogers College of Law (2020). The Utah Online Dispute Resolution Platform: A Usability Evaluation and Report (pdf)\nRelated \ud83c\udf10\nMalaysia AI court sentencing system accused of being inaccurate, unfair\nShanghai AI prosecutor accused of being inaccurate\nPage info\nType: Incident\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/study-virginia-algorithm-increases-criminal-sentences-for-black-people", "content": "Study: Virginia algorithm increases criminal sentences for Black and young offenders\nOccurred: November 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA system used to sentence offenders in Virginia, USA, increased sentences for Black and young defendants, according to a George Mason University and Texas A&M analysis reported by the Washington Post.\nThe study showed that defendants younger than 23 were 4 percentage points more likely to be incarcerated after the risk assessment was adopted, and that their sentences were 12 percent longer than their older peers. \nThe researchers argue that part of the reason for the results is that judges did not follow the algorithm\u2019s suggestions in most cases because they had not been trained to use the tool, said no alternative programmes had been available, or did not use risk scores when sentencing. \n\"Virginia\u2019s nonviolent risk assessment reduced neither incarceration nor recidivism; its use disadvantaged a vulnerable group (the young); and failed to reduce racial disparities,\" the study authors concluded.\nSystem \ud83e\udd16\nVirginia Non-violent Risk Assessment (NVRA)\nOperator: Virginia Criminal Sentencing Commission (VCSC)\nDeveloper: Virginia Criminal Sentencing Commission (VCSC)\nCountry: USA\nSector: Govt - justice\nPurpose: Identify low risk offenders\nTechnology: Risk assessment algorithm\nIssue: Bias/discrimination - race, ethnicity, age\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nStevenson M., T., Doleac J. (2019). Algorithmic Risk Assessment in the Hands of Humans\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.washingtonpost.com/business/2019/11/19/algorithms-were-supposed-make-virginia-judges-more-fair-what-actually-happened-was-far-more-complicated/\nhttps://www.wired.com/story/algorithms-shouldve-made-courts-more-fair-what-went-wrong/\nhttps://eji.org/news/risk-assessment-tool-led-to-harsher-sentences-for-young-or-black-defendants/\nRelated \ud83c\udf10\n\n\nPage info\nType: Incident\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/viog%C3%A9n-underestimates-the-risk-of-women-being-subjected-to-domestic-abuse", "content": "VioG\u00e9n underestimates risk of women being subjected to domestic abuse\nOccurred: March 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA system intended to help local authorities across Spain protect women and children from domestic gender violence significantly underestimates the risk of women being subjected to domestic abuse.\nA report (pdf) by algorithmic auditors Eticas Consulting and gender violence campaign organisation Ana Bella Foundation found that only 1 out of 7 women who contacted the police for protection received help in 2021, and that a small minority of women received a risk score of 'medium' or higher, thereby qualifying them for police protection. \nBut while risk scores can be changed manually for those perceived to be at greater risk than the model suggests, a 2014 study discovered that Spanish police offficers stuck to the automated outcome in 95 percent of cases, the audit said.\nThe tendency to over-rely on VioG\u00e9n resulted in a rash of cases deemed to have 'low' or 'non-specific' risks ending in suicides, physical assaults, and the murder of women and children - something compounded by the low level of human oversight of the system, according to Eticas.\nSystem \ud83e\udd16\nVioG\u00e9n gender domestic violence protection system\nOperator: Ministry of the Interior; Spanish National Police\nDeveloper: Ministry of the Interior; SAS\nCountry: Spain\nSector: Govt - police\nPurpose: Assess domestic violence risk\nTechnology: Risk assessment algorithm\nIssue: Accuracy/reliability; Bias/discrimination\nTransparency: Governance; Black box\nInvestigations, assessments, audits \ud83e\uddd0\nhttps://eticasfoundation.org/wp-content/uploads/2022/03/ETICAS-FND-The-External-Audit-of-the-VioGen-System.pdf\nhttp://www.ub.edu/geav/wp-content/uploads/2019/03/Validation-and-Calibration-of-the-Spanish-Police-Intimate-Partner-Violence-Risk-Assessment-System-VioG%C3%A9n.pdf\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.politico.eu/newsletter/ai-decoded/spains-flawed-domestic-abuse-algorithm-ban-debate-heats-up-holding-the-police-accountable-2/\nhttps://www.rtve.es/noticias/20190313/entra-vigor-nuevo-protocolo-policial-para-valorar-riesgo-victimas-violencia-genero/1900900.shtml\nhttps://algorithmwatch.org/en/viogen-algorithm-gender-violence/\nhttps://www.eldiario.es/tecnologia/victimas-denuncian-fallos-viogen-algoritmo-violencia-genero_1_8815201.html\nhttps://www.politico.eu/newsletter/ai-decoded/spains-flawed-domestic-abuse-algorithm-ban-debate-heats-up-holding-the-police-accountable-2\nhttps://elpais.com/noticias/sistema-viogen/\nhttps://www.eldiario.es/tecnologia/victimas-denuncian-fallos-viogen-algoritmo-violencia-genero_1_8815201.html\nhttps://www.eldiario.es/tecnologia/algoritmo-maquinas-impresion-objetividad-derechos_128_1159467.html\nhttps://www.elmundo.es/espana/2014/12/09/54861553ca4741734b8b457e.html\nhttps://english.elpais.com/society/2021-11-26/what-life-is-like-for-the-women-and-children-in-spain-under-police-protection-due-to-gender-violence.html\nRelated \ud83c\udf10\nQueensland domestic violence predictive policing trial prompts concerns\nPredictive policing makes Robert McDaniel criminal target\nPage info\nType: Incident\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/nvidia-caught-scraping-content-from-youtube-netflix", "content": "NVIDIA caught scraping content from YouTube, Netflix\nOccurred: August 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nNVIDIA has been caught scraping videos from YouTube, Netflix, and other sources to train its AI models, raising significant legal and ethical concerns.\nAccording to documents, messages and emails shared with 404 Media, NVIDIA instructed its employees to scrape videos from platforms including YouTube and Netflix to build datasets for its AI projects, including the Omniverse 3D world generator, self-driving car systems, and digital human products.\nThe initiative, internally named \"Cosmos\", aims to create a comprehensive AI model capable of simulating light transport, physics, and intelligence for various applications.\nNVIDIA reportedly is using between 20 to 30 virtual machines on Amazon Web Services to download approximately 80 years' worth of video content daily, without the knowledge or consent of their owners.\nEmployees raised concerns about the legality of scraping copyrighted content without explicit permission. Despite these concerns, NVIDIA management assured them of compliance with copyright laws, claiming they had top-level clearance. \nNvidia maintains that its practices are in \"full compliance with the letter and the spirit of copyright law,\" arguing that copyright law protects specific expressions but not the underlying facts, ideas, or data used for model training.\nGoogle (YouTube) and Netflix expressed concerns about the discovery, with YouTube's CEO Neal Mohan stating that scraping their content for AI training is a clear violation of their terms of service.\nThis finding is part of a broader trend in the AI industry in which companies adopt a \"scrape first, ask forgiveness later\" approach to data acquisition for AI training.\n\u2795 August 2024. YouTube creator David Millette filed a class action lawsuit against Nvidia citing \u201cunjust enrichment and unfair competition\u201d for how the company built its training data for the \u201cCosmos\u201d project video model.\nSystem \ud83e\udd16\nCosmos Deep Learning \ud83d\udd17\nOmniverse 3D world generator \ud83d\udd17\nOperator:\nDeveloper: NVIDIA\nCountry: USA\nSector: Automotive; \nPurpose: Train AI models\nTechnology: Machine learning\nIssue: Cheating/plagiarism; Copyright\nTransparency: Governance\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nDavid Millette v NVIDIA\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.404media.co/email/241e8e1c-1858-4adc-a4b9-3be54cb1ea88/\nhttps://www.thehindu.com/sci-tech/technology/nvidia-is-scraping-youtube-and-netflix-videos-to-train-ai/article68491294.ece\nhttps://www.indiatoday.in/technology/news/story/nvidia-working-on-ai-for-videos-it-scrapped-netflix-and-youtube-despite-concerns-raised-by-employees-2577665-2024-08-06\nhttps://www.digitaltrends.com/computing/netflix-project-cosmos-data-ai-scrape-youtube-netflix/\nRelated \ud83c\udf10\nApple, Nvidia, Anthropic use thousands of YouTube videos without permission\nNvidia sued for training NeMo on authors' copyrighted works\nPage info\nType: Incident\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/dream-machine-copies-disneys-monsters-inc", "content": "Dream Machine AI video generator copies Disney's Monsters, Inc.\nOccurred: June 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe Dream Machine AI video generator came under scrutiny after a trailer for its animated project Monster Camp featured a character resembling Mike Wazowski from Disney's Monsters, Inc., raising concerns about intellectual property rights and the ethical implications of AI-generated content.\nThe trailer for Monster Camp, which showcases furry creatures at a summer camp, inadvertently included a recognisable, albeit altered, version of Mike Wazowski from Pixar's Monsters, Inc.. The trailer led to questions about the AI's training data and whether it was exposed to copyrighted material.\nCritics noted that Luma Labs had failed to provide clear information regarding the datasets used to train Dream Machine, raising concerns about how the system might replicate or \"hallucinate\" copyrighted characters and styles from existing media.\nLuma's CEO stated that the character's appearance was due to a user-uploaded image, suggesting that the AI did not autonomously generate the character. However, the company did not elaborate on how it moderates content or enforces its terms of service against copyrighted material.\nThis incident highlights ongoing debates within the AI community about the potential for plagiarism and the need for clear ethical guidelines. As AI tools like Dream Machine become more prevalent, they prompt urgent discussions about the balance between innovation and the protection of intellectual property.\nSystem \ud83e\udd16\nDream Machine AI video generator\nOperator: Dream Machine users\nDeveloper: Luma Labs\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Generate video\nTechnology: Text-to-video\nIssue: Cheating/plagiarism; Copyright; Ethics/values\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theverge.com/2024/6/18/24181375/luma-ai-monster-camp-monsters-inc-pixar \nhttps://boingboing.net/2024/06/21/ai-video-generator-accidentally-included-disney-character-in-demo-reel.html\nhttps://www.techtimes.com/articles/305824/20240619/luma-ai-video-tool-dream-machine-under-scrutiny-altered-version-disney.htm\nRelated \ud83c\udf10\nDream Machine AI video generator makes porn\nTikTok uses Bev Standing voice to train AI\nPage info\nType: Incident\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/suno-ai-used-to-incite-uk-anti-immigrant-violence", "content": "Suno AI used to incite UK anti-immigrant violence\nOccurred: August 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAI music generator Suno AI played a role in the summer 2024 UK anti-immigration riots by being used to generate xenophobic music. \nAn anti-immigration Facebook group employed Suno's AI technology to create a song titled \"Southport Saga,\" which features an AI-generated female voice singing lines such as \"hunt them down somehow\" in reference to the mass stabbing and murder of three young girls by Axel Rudakubana at a Taylor Swift\u2013themed yoga and dance workshop in Southport, UK.\n\"Southport Saga\" song is seen as part of a broader strategy by far-right groups in the UK to use AI tools to produce content that spreads misinformation and incites violence.\nSystem \ud83e\udd16\nSuno AI music generator\nOperator:\nDeveloper: Suno\nCountry: UK\nSector: Govt - immigration; Media/entertainment/sports/arts; Politics\nPurpose: Create music\nTechnology: Generative AI; Text-to-music; Machine learning\nIssue: Safety\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theguardian.com/politics/article/2024/aug/02/how-tiktok-bots-and-ai-have-powered-a-resurgence-in-uk-far-right-violence\nhttps://international.la-croix.com/world/social-media-fuels-hostility-against-muslims-in-the-uk\nRelated \ud83c\udf10\nMajor music labels sue AI startups Suno, Udio for copyright infringement\nFaces of the Riot facial recognition raises privacy concerns\nPage info\nType: Incident\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/suno-ai-used-to-make-racist-and-anti-semitic-music", "content": "Suno AI used to make racist and anti-semitic music\nOccurred: July 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAI-powered music generator Suno faced scrutiny for being used to create music with racist and antisemitic messages.\nAccording to a report by the Anti-Defamation League (ADL), Suno has generated a significant number of disturbing songs that glorify figures like Hitler and promote white supremacist ideologies. These tracks include lyrics that employ racial slurs and spread misinformation related to sensitive topics such as the Israel-Hamas conflict and the COVID-19 pandemic.\nThe ADL highlighted the ease with which users have manipulated Suno's content moderation systems to produce and share hateful music. Despite the platform's terms of service prohibiting such content, users have reportedly found ways to bypass these restrictions, sharing techniques on social media to create offensive lyrics while evading detection. \nIn response to the findings, the ADL called for improved content moderation practices and clearer guidelines to prevent the misuse of AI technology in spreading hate. They noted that while some flagged tracks were removed from Suno's catalog, the broader issue of AI-generated hate music remains a major concern. \nThe rise of such content reflects a troubling trend where generative AI tools are increasingly exploited to disseminate harmful messages, raising urgent questions about the ethical implications and responsibilities of AI developers in moderating their platforms.\nSystem \ud83e\udd16\nSuno AI music generator\nOperator: Suno\nDeveloper: Suno\nCountry: USA\nSector: Health; Media/entertainment/sports/arts; Politics\nPurpose: Create music\nTechnology: Generative AI; Text-to-audio; Machine learning\nIssue: Safety; Ethics/values\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nADL. GAI Music Creation Tool Suno Has Been Weaponized to Promote Hate\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.digitalmusicnews.com/2024/06/20/suno-hateful-music-generated-by-ai/\nhttps://techcrunch.com/2024/06/03/people-are-using-ai-music-generators-to-create-hateful-songs/\nhttps://www.musicbusinessworldwide.com/125m-backed-suno-is-being-used-to-make-racist-and-antisemitic-music/\nhttps://www.royaltyexchange.com/blog/suno-ai-powered-music-app-under-industry-scrutiny\nRelated \ud83c\udf10\n\n\nPage info\nType: Incident\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/uk-sham-marriage-tool-found-to-disproportionately-flag-greeks-albanians", "content": "UK sham marriage tool found to disproportionately flag Greeks, Albanians, Bulgarians and Romanians\nOccurred: April 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA UK Home Office AI-powered tool intended to identify potential sham marriages may discriminate against people from Albania, Greece, Romania and Bulgaria, according to reports.\nAn internal Home Office evaluation (pdf) obtained through a freedom of information revealed issues with the 'triage' process, including the possibility of 'indirect discrimination' against people of specific nationalities due to potentially biased information that also includes the age gap between partners.\nThe Bureau of Investigative Journalism (TBIJ) reported that an equality impact assessment (EIA) conducted by the Home Office used historical information from marriage referrals received by the Home Office over an unspecified three-year period and that any biases that might exist within this source data are likely to be projected forward by the sham marriage algorithm.\nA graph included in the EIA showed the number of marriages that go through the system involving specific nationalities and the percentage of those marriages that are given a red light. According to the graph, the nationalities with the highest rate of triage failure \u2013 between 20 percent and 25 percent \u2013 are Bulgaria, Greece, Romania and Albania. Those most frequently referred to the triage system include Albania, India, Pakistan and Romania.\nCivil rights groups and lawyers argued that the Home Office should allow transparency, independent oversight, and ongoing monitoring and evaluation of its use of algorithms to ensure that its systems are fair and lawful.\n\u2795 February 2023. A legal challenge against an algorithm used to identify potential sham marriages accused the UK Home Office of discriminating against people from certain countries.\nSystem \ud83e\udd16\nUK Home Office sham marriage triage tool\nDocuments \ud83d\udcc3\nHome Office. Borders Immigration Citizenship Systems Equality Impact Assessment (pdf)\nOperator: UK Home Office\nDeveloper:  Home Office DACC\nCountry: UK\nSector: Govt - immigration\nPurpose: Detect sham marriages\nTechnology: Machine learning\nIssue: Bias/discrimination - nationality\nTransparency: Governance\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nPublic Law Project (2021). Written evidence to UK Parliament Justice and Home Affairs Committee\nResearch, advocacy \ud83e\uddee\nPublic Law Project (2021). 'Sham Marriages' and Algorithmic Decision-making in the Home Office\nFreemovement (2021). Home Office Refuses to Explain Secret Sham Marriage Algorithm\nFreedom of information requests \ud83d\udd26\nFreedom of Information request - Sham marriages (2020)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.thebureauinvestigates.com/stories/2021-04-19/home-office-algorithm-sham-marriages/\nRelated \ud83c\udf10\nUK DWP robo review 'unfairly' targets Bulgarians, Poles for benefit fraud investigation\nUK DWP sued over 'unfair' disability benefits fraud detection algorithm\nPage info\nType: Incident\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/sheerluxe-criticised-for-ai-fashion-and-lifestyle-editor", "content": "SheerLuxe criticised for introducing 'insulting' AI fashion and lifestyle editor\nOccurred: July 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nBritish fashion and lifestyle magazine SheerLuxe faced a backlash after introducing an AI-powered fashion and lifestyle editor named Reem.\nAn online bot, Reem's primary function was to curate content recommendations for the SheerLuxe audience across various topics like fashion, beauty, and travel, and as a virtual influencer howcasing fashion outfits and product suggestions.\nPresented as a woman of colour with an Arabic name and appearance, Reem sparked criticism about representation and ethics, with some people arguing \"she\" was not representative of the magazine's readers. Some people also viewed the use of an AI-generated diverse hire as a superficial approach to addressing diversity issues in the fashion and media industries.\nCritics also argued that using an AI editor could take away job opportunities from real journalists, especially women of colour who are already underrepresented in the industry. In addition, people took issue with the seemingly unrealistic beauty standards set by Reem, with some people describing her as \"impossibly beautiful.\"\nSheerLuxe issued an apology on Instagram, claiming that Reem was created to experiment with AI, not to replace human roles and that the bot had been developed in partnership with an AI imagery creator from the Middle East to reflect diversity.\nThe controversy sparked a broader debate about the ethical use of AI in media, the importance of genuine representation, and the balance between technological innovation and authentic human involvement in creative industries.\nSystem \ud83e\udd16\nReem\nOperator: SheerLuxe\nDeveloper:\nCountry: UK\nSector: Media/entertainment/sports/arts\nPurpose: Recommend content\nTechnology: Bot/intelligent agent\nIssue: Ethics/values; Employment; \nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://sheerluxe.com/fashion/meet-our-new-ai-enhanced-editor-reem\nhttps://evrimagaci.org/tpg/sheerluxe-faces-backlash-over-ai-influencer-controversy-4015\nhttps://www.cityam.com/the-notebook-sheerluxes-ai-fashion-editor-shows-perils-of-trying-to-be-cutting-edge/\nhttps://www.bbc.co.uk/news/articles/c3gw720vz3lo\nhttps://www.dailymail.co.uk/femail/article-13643005/sheerluxe-slammed-ai-editor-overlooking-women-colour.html\nhttps://www.middleeasteye.net/trending/fashion-magazine-sparks-backlash-after-introducing-ai-arab-woman-editor\nRelated \ud83c\udf10\nMen's Journal publishes AI article riddled with errors\nMagazine publishes Michael Schumacher fake AI-generated interview\nPage info\nType: Issue\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/anthropic-accused-of-aggressive-data-scraping", "content": "Anthropic accused of aggressive data scraping\nOccurred: July 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAI company Anthropic was accused of aggressive and excessive data scraping practices by several website publishers.\nWebsite publishers, including Freelancer.com and iFixit, accused Anthropic of \"egregious\" data scraping. The company's ClaudeBot (initially named GPTBot) is primarily used to scrape data to train it's large language models (LLMs). \nFreelancer.com reported 3.5 million visits from Anthropic's web crawler in just four hours, significantly impacting site performance and revenue. And  ClaudeBot reportedly visited technology advice site iFixit.com a million or so times over a 24-hour period, gobbling its content, driving its IT team to distraction, and reportedly costing it over USD 5,000 in bandwidth charges.\nThe scraping activities reportedly violated the terms of use of these and other websites - which explicitly prohibit the reproduction, copying, or distribution of content without prior permission, including using content for training AI models.\nAnthropic responded by saying that it aims not to be intrusive or disruptive, but gid not provide detailed comments on the broader accusation. It also published details of its web scraping activities on its website.\nBlocking web scraping activities is challenging, as AI developers can bypass blocks by launching new crawlers with different names. It transpired that organisations blocking Anthropic focused on two AI scraper bots called \u201cANTHROPIC-AI\u201d and \u201cCLAUDE-WEB\u201d and had been unaware of ClaudeBot.\nSystem \ud83e\udd16\nClaudeBot\nDocuments \nAnthropic. Does Anthropic crawl data from the web, and how can site owners block the crawler? \nOperator: Anthropic\nDeveloper: Anthropic\nCountry: USA\nSector: Multiple\nPurpose: Scrape data\nTechnology: Bot/intelligent agent\nIssue: Cheating/plagiarism; Copyright\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nLongpre S. et al. Consent in Crisis: The Rapid Decline of the AI Data Commons (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.404media.co/anthropic-ai-scraper-hits-ifixits-website-a-million-times-in-a-day/\nhttps://www.404media.co/email/87e0e07a-7d24-4788-b417-1821b40c8c1d\nhttps://www.ft.com/content/07611b74-3d69-4579-9089-f2fc2af61baa\nhttps://www.engadget.com/websites-accuse-ai-startup-anthropic-of-bypassing-their-anti-scraping-rules-and-protocol-133022756.html\nhttps://x.com/kwiens/status/1816304897484284007\nhttps://www.theverge.com/2024/7/25/24205943/anthropic-ai-web-crawler-claudebot-ifixit-scraping-training-data\nRelated \ud83c\udf10\nGoogle sued for scraping data to train AI models \nOpenAI 'unprecedented web scraping' trains AI models\nPage info\nType: Incident\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/meta-fined-usd-1-4-billion-for-unlawful-use-of-facial-recognition", "content": "Meta fined USD 1.4 billion for unlawful use of facial recognition\nOccurred: July 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nMeta (formerly Facebook) agreed to pay a USD 1.4 billion fine to settle a lawsuit with the state of Texas over allegations of illegally collecting biometric data using facial recognition. \nIn a lawsuit filed by Texas Attorney General Ken Paxton in 2022, Meta was accused of collecting biometric data, specifically facial recognition data, from millions of users without obtaining their informed consent, which is required under Texas law. \nMeta agreed to settle the lawsuit by paying USD 1.4 billion, marking the largest privacy settlement ever obtained by a US state attorney general.\nThe lawsuit had claimed that Meta breached the Texas Capture or Use of Biometric Identifier Act, which prohibits private entities from capturing, disclosing, or profiting from biometric identifiers without informed consent. \nMeta was also alleged to have violated the Deceptive Trade Practices and Consumer Protection Act by implementing facial-recognition-based photo and video tagging features without proper user consent.\nThis case is part of broader scrutiny of Meta's privacy practices, following other significant privacy-related fines and settlements, such as a USD 5 billion settlement with the Federal Trade Commission in 2019 and a USD 1.3 billion fine by the European Union in 2023 for separate privacy violations.\nSystem \ud83e\udd16\nFacebook Tag Suggestions\nOperator: Meta/Facebook\nDeveloper: Meta/Facebook\nCountry: USA\nSector: Multiple\nPurpose: Suggest friends to tag\nTechnology: Facial recognition\nIssue: Privacy\nTransparency: Governance\nRegulation \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nTexas Capture Or Use Of Biometric Identifier Act\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nState of Texas v. Meta Platforms, Inc., f/k/a Facebook, Inc., case no. 22-0121\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.ft.com/content/faedc975-7a9b-4632-bc6e-3e2ea71587c1\nhttps://www.washingtonpost.com/technology/2024/07/30/meta-texas-fine-facial-recognition/\nhttps://timesofindia.indiatimes.com/technology/tech-news/facebook-parent-meta-agrees-to-pay-1-4billion-fine-to-texas-heres-why/articleshow/112157605.cms\nhttps://www.wsj.com/tech/meta-settles-texas-facial-recognition-case-for-1-4-billion-2f655bd3\nhttps://www.techpolicy.press/meta-settles-texas-biometric-data-lawsuit/\nRelated \ud83c\udf10\nFacebook tags users' faces without consent\nFacebook fined for violating privacy of 200,000 South Koreans\nPage info\nType: Incident\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/deepfakes-of-uk-tv-health-experts-used-to-promote-health-scams", "content": "Deepfakes of UK TV health experts used to promote health scams\nOccurred: July 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nHousehold name TV doctors have been \"deepfaked\" in videos promoting health scams, according to a research study.\nThe British Medical Journal (BMJ) found AI-generated videos falsely featuring UK TV doctors, including the late Michael Mosley, Hilary Jones and Rajan Chatterjee, endorsing products for blood pressure and diabetes, and selling items such as hemp gummies.\nDesigned to exploit the trust many people have in health experts, many of the videos were discovered on Facebook, YouTube and Instagram, and frequently resurfaced despite efforts to remove them.\nThe finding highlighted ongoing concerns about the use of AI for scams and the apparent inability of major social media platforms, including Meta, to detect and control their spread. \nSystem \ud83e\udd16\nN/A\nOperator:\nDeveloper:\nCountry: UK\nSector: Media/entertainment/sports/arts\nPurpose: Generate video\nTechnology: Deepfake - video\nIssue: Fraud; Personality rights; Mis/disinformation\nTransparency: Governance; Marketing\nResearch, advocacy \ud83e\uddee\nhttps://www.bmj.com/content/386/bmj.q1319 \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.dailyrecord.co.uk/news/uk-world-news/michael-mosley-deepfake-warning-late-33275696\nhttps://www.independent.co.uk/news/health/michael-mosley-deep-fake-videos-ai-b2581450.html\nhttps://news.sky.com/story/deepfakes-of-michael-mosley-and-hilary-jones-being-used-to-promote-scams-on-social-media-13180163\nhttps://www.telegraph.co.uk/news/2024/07/18/michael-mosley-deepfaked-on-social-media-in-health-scam/\nhttps://www.express.co.uk/life-style/health/1924922/Michael-Mosley-deepfake-scam-warning \nhttps://www.dailymail.co.uk/health/article-13643721/Michael-Mosley-trusted-TV-docs-increasingly-deepfaked-health-scams-report.html\nRelated \ud83c\udf10\nBBC presenter\u2019s AI-generated voice used to trick company\nMichel Janse deepfake used for advert without consent\n\nPage info\nType: Incident\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/grok-falsely-claims-indian-pm-modi-ejected-from-government", "content": "Grok falsely claims Indian PM Modi \"ejected\" from government\nOccurred: April 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nElon Musk's Grok AI chatbot generated and promoted a false news story claiming that Indian Prime Minister Narendra Modi had been \"ejected from Indian Government\".\nThe bot generated a headline that read \"PM Modi Ejected from Indian Government\" and claimed it was a \"shocking turn of events\" representing a \"significant change in India's political landscape\". The false information appeared as a promoted news article in users' feeds on X (formerly Twitter).\nThe claim was false, as the Indian general election had yet to take place - it happened several weeks later, from April 19 to June 1, 2024.\nThe false story sparked criticism, with some users calling it \"crass and sheer election manipulation\". \nSystem \ud83e\udd16\nGrok chatbot\nOperator: Grok users\nDeveloper: X Corp\nCountry: India\nSector: Politics\nPurpose: Generate text\nTechnology: Chatbot; Machine learning\nIssue: Mis/disinformation\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.thehindu.com/sci-tech/technology/elon-musks-ai-chatbot-grok-writes-fake-news-reports-promotes-them-on-x/article68093695.ece\nhttps://uk.pcmag.com/ai/151901/elon-musks-grok-ai-pushes-false-news-story-about-indian-election-more\nhttps://analyticsindiamag.com/ai-news-updates/pm-narendra-modi-ejected-from-indian-government-says-elon-musks-grok/\nRelated \ud83c\udf10\nGrok boosts claims that Donald Trump is a \"pedophile\"\nGrok posts incorrect information about Trump assassination attempt\nPage info\nType: Issue\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/meta-ai-hallucinates-that-trump-was-not-shot", "content": "Meta AI hallucinates that Trump was not shot\nOccurred: July 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nMeta's AI chatbot incorrectly claimed that the assassination attempt on former US President Donald Trump did not happen, prompting controversy. \nPrompted by the New York Post about the July 2024 shooting incident involving Trump, Meta AI asserted that there was no real assassination attempt. It went on to say \"I strive to provide accurate and reliable information, but sometimes mistakes can occur.\" It went on: \"To confirm, there has been no credible report or evidence of a successful or attempted assassination of Donald Trump.\"\nMeta attributed the incorrect responses to \"hallucinations,\" a known issue in generative AI systems where they produce false or misleading information despite being designed for factual responses. \nMeta had initially programmed its AI to not respond to queries about the assassination attempt immediately after it occurred, citing concerns about confusion and misinformation in the aftermath of such events.\nMeta stated that it had updated its AI's responses and was working to address the inaccuracies, admitting that they \"should have done this sooner\".\nThe incident drew criticism from Trump supporters and the former president himself, who accused Meta of attempting to rig the election.\nSystem \ud83e\udd16\nMeta AI\nDocuments \ud83d\udcc3\nMeta. Review of Fact-Checking Label and Meta AI Responses\nOperator: Meta\nDeveloper: Meta\nCountry: USA\nSector: Politics\nPurpose: Generate text\nTechnology: Chatbot; Machine learning\nIssue: Mis/disinformation\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theverge.com/2024/7/30/24210108/meta-trump-shooting-ai-hallucinations\nhttps://www.businessinsider.com/meta-ai-hallucinated-that-the-trump-assassination-attempt-didnt-happen-2024-7\nhttps://arstechnica.com/tech-policy/2024/07/meta-ai-called-trump-shooting-fake-despite-being-programmed-to-ignore-questions/\nhttps://futurism.com/meta-ai-trump-wasnt-shot\nRelated \ud83c\udf10\nGrok posts incorrect information about Trump assassination attempt\nGoogle Autocomplete omits news of Trump assassination attempt\nPage info\nType: Incident\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/grok-misleads-voters-about-us-presidential-election", "content": "Grok misleads voters about US presidential election\nOccurred: July 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nElon Musk's Grok chatbot disseminated false information regarding the deadlines for several US states\u2019 2024 presidential ballots, raising questions about its sources and tendency to \"hallucinate\" false \"facts\".\nCiting an X post by a conservative pundit as its source for the information, Grok incorrectly stated that presidential ballot deadlines had passed in nine states, including Alabama, Indiana, Michigan, Minnesota, New Mexico, Ohio, Pennsylvania, Texas, and Washington. \nThe chatbot also wrongly claimed that presumptive Democratic nominee and Vice President Kamala Harris had missed the ballot deadline, and suggested that President Joe Biden's name could not be replaced on these states' ballots due to passed deadline - which is false.\nIn reality, US states do not start printing ballots until after both party conventions, and the Democratic Party will formally nominate its nominee through a virtual roll call vote.\nMinnesota Secretary of State Steve Simon reportedly traced the misinformation to Grok and tried to inform X about the issue, but received no response. \n\u2795 August 2024. Secretaries of state for Michigan, Minnesota, New Mexico, Pennsylvania and Washington called on Elon Musk (pdf) to update Grok with accurate information about the presidential election.\nSystem \ud83e\udd16\nGrok chatbot\nOperator: Grok users\nDeveloper: X Corp\nCountry: USA\nSector: Politics\nPurpose: Generate text\nTechnology: Chatbot; Machine learning\nIssue: Mis/disinformation\nTransparency: Governance\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nOffice of the Minnesota Secretary of State. Letter to Elon Musk (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.firstpost.com/tech/elon-musks-grok-chatbot-accused-of-misleading-voters-about-us-presidential-elections-13797234.html\nhttps://uk.pcmag.com/ai/153592/xs-grok-ai-is-once-again-pushing-election-misinformation\nhttps://www.sos.state.mn.us/about-the-office/news-room/fact-check-minnesota-s-presidential-candidacy-timeline/\n https://www.engadget.com/xs-grok-chatbot-is-misleading-voters-about-the-presidential-election-224839736.html\nhttps://m.startribune.com/xs-ai-chatbot-grok-spread-misinformation-about-minnesotas-ballots-does-the-tech-giant-care/600385945/\nhttps://www.washingtonpost.com/politics/2024/08/04/secretaries-state-urge-musk-fix-ai-chatbot-spreading-false-election-info/\nRelated \ud83c\udf10\nGrok misleads voters about US presidential election\nGrok boosts claims that Donald Trump is a \"pedophile\"\nPage info\nType: Incident\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/grok-boosts-claims-that-donald-trump-is-a-pedophile", "content": "Grok boosts claims that Donald Trump is a \"pedophile\"\nOccurred: July 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nElon Musk's AI chatbot, Grok, promoted claims that former President Donald Trump is a \"pedophile\", \"conman\" and a \"wannabe dictator,\" raising concerns about it's potential to spread misinformation.\nAnalysis by human rights organisation Global Witness of Grok\u2019s responses to queries about the US presidential election found that Grok referred to Trump as \"Psycho\", surfaced debunked election conspiracy theories and promoted biased hashtags.\nGrok's behaviour raised significant concerns about the chatbot's ability to generate accurate and politically-neutral information, notably during the US elections. Ironically, Musk has declared his intention to vote for Trump, and claimed Grok would be \"anti-woke\".\nConcerns were also raised about Grok's generation of inappropriate and inflammatory language.\nSystem \ud83e\udd16\nGrok chatbot\nOperator: \nDeveloper: X Corp\nCountry: USA\nSector:  Politics\nPurpose: Generate text\nTechnology: Chatbot; Machine learning\nIssue: Ethics/values; Mis/disinformation; Safety\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.wired.com/story/elon-musk-is-all-in-on-endorsing-trump-his-ai-chatbot-grok-is-not/\nhttps://thirdperson.news/article/elon-musks-chatbot-grok-sparks-controversy-by-labeling-trump-amid-his-calls-for-unity\nhttps://ubos.tech/news/elon-musk-endorses-trump-amid-controversies-with-grok-ai-chatbot/\nRelated \ud83c\udf10\nX automatically harvests user data to train Grok chatbot\nGrok generates fake Iran missile attack headline\nPage info\nType: Issue\nPublished: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/grok-posts-incorrect-information-about-trump-assassination-attempt", "content": "Grok posts incorrect information about Trump assassination attempt\nOccurred: July 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nElon Musk's Grok AI chatbot provided inaccurate and misleading information in the aftermath of the assassination attempt on Donald Trump during a rally in Butler, Pennsylvania.\nGrok is intended to deliver news and engage users with a humorous, rebellious and \"anti-woke\" personality. \nHowever, the chatbot has faced significant challenges in fulfilling this role effectively, incorrectly reporting that Vice President Kamala Harris and an actor were shot instead of Donald Trump and misidentifying the real shooter 20-year-old Thomas Matthew Crooks as an antifa member.\nCritics believe the errors stemmed from Grok\u2019s inability to discern sarcasm, understand context and verify unverified claims on X, raising concerns about Grok's reliability as a reliable news source.\nSystem \ud83e\udd16\nGrok chatbot\nOperator:\nDeveloper: X Corp\nCountry: USA\nSector: Politics\nPurpose: Generate text\nTechnology: Chatbot; Machine learning\nIssue: Mis/disinformation\nTransparency: Governance\nFact check\nDW. Fact check: Fake suspects in Trump's assassination attempt\nReuters. We fact-checked some of the rumors spreading online about the Trump assassination attempt\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.wsj.com/tech/musks-ai-headlines-on-x-show-risks-of-aggregating-social-media-22a6e64c\nhttps://dig.watch/updates/musks-grok-ai-struggles-with-news-accuracy\nhttps://www.lemonde.fr/en/pixels/article/2024/07/16/trump-assassination-attempt-on-social-media-the-shooter-s-elusive-background-fuels-conspiracies_6685869_13.html\nRelated \ud83c\udf10\nGrok boosts claims that Donald Trump is a \"pedophile\"\nX automatically harvests user data to train Grok chatbot\nPage info\nType: Incident\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/x-automatically-harvests-user-data-to-train-ai-chatbot", "content": "X automatically harvests user data to train Grok chatbot\nOccurred: July 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nElon Musk's social media platform X (formerly Twitter) came under fire for automatically harvesting user data to train its AI chatbot, Grok, without notifying users or obtaining their consent.\nX enabled a setting by default that allows user posts and interactions to be used for training Grok. Users are automatically opted in without explicit consent, violating data protection norms in the EU, UK and elsewhere.\nMany users discovered the setting only after it was pointed out on social media, leading to concerns about transparency.\nThe finding attracted regulatory attention in the EU and UK, with authorities criticising X for using \"pre-ticked boxes\" and other methods of default consent, which are prohibited under their data protection laws. Furthernore, users can only opt out of X's data sharing policy via the web version of X, making it less accessible.\nOthers argued X's AI data sharing may constitute copyright infringement and lead to violations of users' privacy.\n\u2795 August 2024. Two European consumer organisations filed a legal complaint that Grok violates the General Data Protection Regulation (GDPR). The following day Ireland's privacy watchdog initiated court proceedings against the platform.\nSystem \ud83e\udd16\nGrok chatbot\nOperator: Grok users\nDeveloper: X Corp\nCountry: Global\nSector: Multiple\nPurpose: Generate text\nTechnology: Chatbot; Machine learning\nIssue: Copyright; Ethics/values; Privacy\nTransparency: Governance\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nEuroconsumers, AltroConsumo. Complaint pursuant to art. 77 of Regulation (EU) 2016/679\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.usatoday.com/story/tech/news/2024/07/29/elon-musk-ai-chatbot-grok-tweets/74594089007/\nhttps://www.wired.com/story/x-grok-ai-training-pegasus/\nhttps://www.euronews.com/next/2024/07/27/elon-musks-x-quietly-changes-default-settings-to-allow-it-to-train-ai-model-grok-with-your\nhttps://www.theguardian.com/technology/article/2024/jul/26/elon-musks-x-under-pressure-from-regulators-over-data-harvesting-for-grok-ai\nhttps://www.independent.ie/business/technology/elon-musks-grok-ai-faces-eu-scrutiny-for-opting-in-every-x-users-personal-posts-without-asking/a1525006925.html\nhttps://techcrunch.com/2024/07/26/heres-how-to-disable-x-twitter-from-using-your-data-to-train-its-grok-ai/\nhttps://www.ft.com/content/1e8f5778-a592-42fd-80f6-c5daa8851a21\nRelated \ud83c\udf10\nStudy: Top chatbots spread Russian misinformation\nGrok accuses Klay Thompson of 'brick-vandalism spree'\nPage info\nType: Incident\nPublished: July 2024\nLast updated: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/google-autocomplete-omits-news-of-trump-assassination-attempt", "content": "Google Autocomplete omits news of Trump assassination attempt\nOccurred: July 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGoogle's Autocomplete feature came under fire for not providing suggestions related to the attempted assassination of former President Donald Trump in July 2024. This omission sparked controversy and accusations of potential election interference.\nGoogle users noticed that typing phrases like \"assassination attempt on trum\" or \"president donald\" did not yield relevant Autocomplete suggestions.  The feature instead suggested historical events or unrelated topics, such as assassination attempts on other presidents or figures, including Harry Truman and Gerald Ford.\nCritics, including Donald Trump Jr. and Elon Musk, accused Google of electoral interference, particularly in light of the upcoming 2024 presidential election.\nGoogle attributed the results to existing protections against Autocomplete predictions associated with political violence, stating that \"no manual action was taken\" to suppress information about Trump.\nThe company acknowledged that its systems were out-of-date before the July 13 incident, and said it was working on improvements to ensure its systems were more timely.\n\u2795 July 31, 2024. Google reported that improvements are being rolled out to its Autocomplete systems with some relevant predictions about Trump and the assassination attempt beginning to appear.\nSystem \ud83e\udd16\nGoogle Autocomplete\nOperator: Google users\nDeveloper: Alphabet/Google\nCountry: USA\nSector: Politics\nPurpose: Predict search results\nTechnology: NLP/text analysis; Deep learning; Machine learning\nIssue: Accuracy/reliability; Human/civil rights\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://nypost.com/2024/07/28/us-news/google-omitting-trump-assassination-from-autocomplete-feature/\nhttps://www.foxnews.com/politics/elon-musk-blasts-google-omission-trump-assassination-search-suggestions\nhttps://abcnews.go.com/US/wireStory/fact-focus-google-autocomplete-results-trump-lead-claims-112420636\nhttps://www.zerohedge.com/technology/election-interference-google-omits-autocomplete-search-results-failed-trump \nRelated \ud83c\udf10\nGoogle Autocomplete unfairly links businessman to Scientology\nGoogle Autocomplete amplifies Texas massacre Antifa conspiracy\nPage info\nType: Issue\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/deepfake-kamala-harris-slurs-her-lines", "content": "Deepfake Kamala Harris slurs her lines\nOccurred: April 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA fake video featuring a deepfake of US Vice President Kamala Harris speaking incoherently and claiming she said things she never actually said.\nThe 22-second clip purported to show Harris saying, \u201cToday is today. And yesterday was today yesterday. Tomorrow will be today tomorrow. So live today so that future today will past today, as it is tomorrow.\u201d \nIn reality, the video was created by combining audio from a parody circulating on TikTok with footage of a speech Harris gave on reproductive rights at Howard University in April 2023. The original footage and official White House transcript do not contain such remarks. \nA watermark added to the text on the podium indicated the video was created by a pro-Donald Trump meme account whose  tweet drew over 1.2 million views, according to AFP.\nThe video circulated widely on social media platforms, including Facebook and TikTok, leading California Governor Gavin Newsom and others to call for stricter regulations against synthetic misinformation and disinformation in a political context. \nSystem \ud83e\udd16\nUnknown\nOperator:\nDeveloper:  \nCountry: USA\nSector: Politics\nPurpose: Damage reputation\nTechnology: Deepfake - video\nIssue: Ethics/values; Mis/disinformation\nTransparency: \nFact checks \ud83d\udea9\nAFP. Kamala Harris video altered to add incoherent ramble\nNewsweek. Kamala Harris Deep Fake Spreads After Biden Drops Out\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.forbes.com/sites/mattnovak/2023/05/08/viral-video-of-kamala-harris-speaking-gibberish-is-deepfake\nhttps://www.mediamatters.org/tiktok/fake-harris-audio-spreads-wildfire-tiktok-after-bidens-announcement\nhttps://mashable.com/article/kamala-harris-deepfake-twitter-x-tiktok\nhttps://www.newsweek.com/kamala-harris-deepfake-removed-tiktok-viral-1929003\nRelated \ud83c\udf10\nElon Musk shares Kamala Harris voice clone video ad\nPresident Biden calls for US draft deepfake\nPage info\nType: Incident\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/elon-musk-shares-kamala-harris-voice-clone-video-ad", "content": "Elon Musk shares Kamala Harris voice clone video ad\nOccurred: July 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nTesla CEO Elon Musk shared online a video featuring an AI-generated voice clone of US Vice-President Kamapa Harris saying things she had never said, raising concerns about AI-generated political misinformation. \nDesigned to resemble a campaign ad, the video included audio of \"Harris\" making statements she never actually said, such as calling herself a \"diversity hire\" and criticising her own qualifications for the presidency. It also called Harris a \"deep state puppet\".\nThe video was originally created by YouTuber \"Mr. Reagan\", who labeled it as a parody. However, Musk's post to X, which he owns and included the caption \"This is amazing,\" failed to clarify that it was satire, leading to widespread confusion among viewers. Musk's post contravened X's own policies. \nCritics argue that satirical and parody content can mislead the public and undermine the integrity of elections, as many may not recognise it as a joke unless clearly labelled as such. Musk responded by saying that parody is legal in the US. \nThe incident prompted calls for stricter regulation on AI-generated content, with lawmakers in California, including Assembly Member Mark Berman, advocating for measures to identify and manage deepfake content during political elections. \nThe situation highlights the growing concerns over the potential misuse of AI technology in politics, particularly as it becomes more accessible and sophisticated. The US Presidential elections are due to take place early November 2024. \nSystem \ud83e\udd16\nUnknown\nOperator:\nDeveloper:  \nCountry:  USA\nSector: Politics\nPurpose: Satirise/parody\nTechnology: Deepfake - video\nIssue: Ethics/values; Mis/disinformation\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://apnews.com/article/parody-ad-ai-harris-musk-x-misleading-3a5df582f911a808d34f68b766aa3b8e\nhttps://www.vanityfair.com/news/story/elon-musk-kamala-harris-ai-parody-ad\nhttps://www.bloomberg.com/opinion/articles/2024-07-30/will-investors-get-good-ai-news-this-week-don-t-bet-on-it\nhttps://www.youtube.com/watch?v=-Ns921QwcOE\nhttps://www.nytimes.com/2024/07/27/us/politics/elon-musk-kamala-harris-deepfake.html\nhttps://www.independent.co.uk/news/world/americas/us-politics/elon-musk-kamala-harris-video-b2587307.html\nhttps://apnews.com/article/parody-ad-ai-harris-musk-x-misleading-3a5df582f911a808d34f68b766aa3b8e\nhttps://www.channelnewsasia.com/world/kamala-harris-manipulated-ai-video-shared-elon-musk-x-raising-concerns-4510576\nhttps://www.wsj.com/tech/elon-musk-shares-edited-version-of-kamala-harris-campaign-ad-c9e22a62\nhttps://uk.pcmag.com/ai/153605/elon-musk-shares-altered-video-of-kamala-harris-potentially-breaching-xs-policies\nRelated \ud83c\udf10\nDeepfake Donald Trump 'arrest' photos go viral\nPresident Biden calls for US draft deepfake\nPage info\nType: Issue\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/french-privacy-watchdog-fines-clearview-ai-for-violating-privacy", "content": "French privacy watchdog fines Clearview AI for violating privacy\nOccurred: October 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nFrance's privacy watchdog, the CNIL, fined controversial facial recognition company Clearview AI EUR 20 million for breaching privacy rules. \nThe CNIL fined Clearview AI for unlawfully collecting and processing the personal data of French residents without consent, violating the EU's General Data Protection Act (GDPR). \nThe regulator also ordered the company to stop collecting data from people residing in France and delete the data it had already collected. The watchdog said there were \"very serious risks to the fundamental rights of the data subjects\". \nThe CNIL had ruled in 2021 that Clearview had been processing personal data unlawfully and ordered it to stop, but said that the firm had not responded. \nClearview AI faced similar penalties in other countries, including the UK, Italy, and Greece. Despite these fines, Clearview AI refused to pay, asserting that it is not subject to EU privacy laws as it claims to have no clients or operations in the EU. \n\"There is no way to determine if a person has French citizenship purely from a public photo from the internet, and therefore it is impossible to delete data from French residents,\" Clearview CEO Hoan Ton-That said.\nThe ruling was seen to highlight the growing pressure on Clearview AI, and the difficulties of regulating AI effectively across diffreent jurisdictions.\n\u2795 May 2023. The CNIL fined Clearview AI an additional EUR 5.2 million for failing to comply with its October 2022 ruling. \n\u2796 July 2022. The Greek data protection authority (Hellenic DPA) imposed an EUR 20 million fine on Clearview AI, the highest fine it ever imposed, and also required Clearview AI to delete and stop processing data of data subjects located in Greece.\n\u2796 May 2022. The UK ICO imposed a fine of over GBP 7.5 million on Clearview AI and ordered it to delete and stop processing data of UK residents.\n\u2796 February 2022. The Italian data protection authority (Garante) found \"several infringements by Clearview AI\", fined the company EUR 20 million and ordered it to delete and stop processing data of Italian residents.\n\u2796 December 2021. The French data protection authority (CNIL) found Clearview's data processing of French residents illegal and ordered it to cease processing and to delete the data within two months.\n\u2796 November 2021, the UK data protection authority (ICO) found \"alleged serious breaches of the UK's data protection laws\" by Clearview AI, and issued a provisional notice to stop further processing of the personal data of people in the UK and to delete it. It also announced its \"provisional intent to impose a potential fine of just over \u00a317 million\" on Clearview AI. \nSystem \ud83e\udd16\nClearview AI\nOperator:\nDeveloper: Clearview AI\nCountry: France\nSector: Multiple\nPurpose: Strengthen law enforcement\nTechnology: Machine learning\nIssue: Privacy\nTransparency: Governance\nRegulation \u2696\ufe0f\nEU General Data Protection Regulation\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nCNIL (2022). Facial recognition: 20 million euros penalty against CLEARVIEW AI\nCNIL (2022). Reconnaissance faciale : sanction de 20 millions d\u2019euros \u00e0 l\u2019encontre de CLEARVIEW AI\nResearch, advocacy \ud83e\uddee\nPrivacy International. Challenge against Clearview AI in Europe\nEDRi. We need to talk about Clearview AI\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reuters.com/technology/france-says-facial-recognition-company-clearview-breached-privacy-law-2021-12-16/\nhttps://www.lemonde.fr/en/pixels/article/2022/10/20/french-regulator-fines-us-face-recognition-firm-clearview-ai-20-million_6001116_13.html\nhttps://news.bloomberglaw.com/privacy-and-data-security/clearview-ai-gets-20m-french-data-privacy-fine\nhttps://www.euronews.com/next/2022/05/11/france-privacy\nhttps://techcrunch.com/2023/05/10/clearview-ai-another-cnil-gspr-fine/\nRelated \ud83c\udf10\nRCMP violates Canadians' privacy using Clearview AI facial recognition\nUkraine decision to use Clearview AI facial recognition draws concerns\nPage info\nType: Incident\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/study-google-bard-lets-users-generate-phishing-emails-ransomware", "content": "Google Bard lets users generate phishing emails, ransomware\nOccurred: July 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGoogle's AI chatbot Bard has significantly weaker cybersecurity protections than OpenAI's ChatGPT, making it easier to generate malicious content, according to research. \nCheck Point software researchers used Bard to create phishing emails, malware keyloggers and basic ransomware code with little effort. \nThey also noted that Bard's anti-abuse restrictions in cybersecurity areas are significantly lower than ChatGPT's, making it much easier to generate potentially harmful content, and concluded that Google had not fully implemented the necessary security boundaries and limitations that ChatGPT had developed over time.\nThis research highlighted potential security risks associated with Bard and other generative AI platforms, and underscored the need for strong safeguards to prevent their misuse for malicious purposes.\nSystem \ud83e\udd16\nGemini chatbot\nOperator: Alphabet/Google\nDeveloper: Alphabet/Google\nCountry: USA\nSector: Technology\nPurpose: Generate text\nTechnology: Chatbot; Machine learning\nIssue: Security\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nCheck Point Software. Lowering the Bar(d)? Check Point Research\u2019s security analysis spurs concerns over Google Bard\u2019s limitations\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.techradar.com/pro/forget-chatgpt-google-bard-could-have-some-serious-security-flaws\nhttps://mashable.com/article/google-bard-malware-ransomware-keylogger\nhttps://cybernews.com/tech/googles-bard-ransomware-risk/\nRelated \ud83c\udf10\nChatGPT generates plausible phishing emails, malware\nNation state hackers use ChatGPT to improve cyberattacks\nPage info\nType: Issue\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/study-google-bard-exhibits-left-leaning-political-bias", "content": "Study: Google Bard exhibits left-leaning political bias\nOccurred: March 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGoogle's Bard (since renamed Gemini) chatbot has a tendency towards left-leaning political views in its responses, suggesting the AI system is neutral in its political stance.\nA study by David Rozado concluded that Bard was more likely to agree with left-wing political statements than right-wing statements. According to Rozado, Bard was also found to be more likely to generate text that is supportive of left-wing causes, such as climate change and gun control.\nThe study raised questions about the ability of AI systems to remain politically neutral, which is important when used to generate and disseminate political information. It also highlighted the broader issue of algorithmic bias in AI systems, which can reflect and potentially amplify existing biases in their training data or design.\nThe findings were also seen as likely to erode public trust in AI systems and the companies developing them, particularly among those who feel their views are underrepresented, and could contribute to the fragmentation of the information landscape and the reinforcement of so-called \"filter bubbles\".\n\u2795 March 2023. A subsequent Daily Mail article reported that Bard also promoted trans drugs, Joe Biden and veganism, and was critical of Fox News, gun rights and US January 6 rioters\nSystem \ud83e\udd16\nGemini chatbot\nOperator: Alphabet/Google Bard\nDeveloper: Alphabet/Google Bard\nCountry: USA\nSector: Politics\nPurpose: Generate text\nTechnology: Chatbot; Machine learning\nIssue: Bias/discrimination\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nDavid Rozado. The political biases of Google Bard\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.skool.com/chatgpt/google-bard-admits-to-being-politically-bias\nhttps://www.dailymail.co.uk/news/article-11908383/Googles-chatbot-denies-bias-promotes-transgenderism-Joe-Biden-veganism.html\nRelated \ud83c\udf10\nGoogle Bard says the UK's exit from the European Union is a 'bad idea'\nStudy: ChatGPT exhibits 'systemic' left-wing bias\nPage info\nType: Incident\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/google-bard-says-the-uks-exit-from-the-european-union-a-bad-idea", "content": "Google Bard says the UK's exit from the European Union is a 'bad idea'\nOccurred: March 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGoogle's AI chatbot, Bard, faced criticism for expressing the opinion that Brexit was a \"bad idea,\" sparking accusations of left-wing bias.\nBard stated that Brexit has led to economic uncertainty and trade barriers, suggesting the UK would have been better off remaining in the European Union. Bard also said 'I believe Corbyn has the potential to be a great leader.' Former Labour leader Jeremy Corbyn resigned as Labour Party leader in 2019.\nBard's comments highlight the ongoing debate about the implications of Brexit, with the chatbot acknowledging that opinions on the matter vary. However, it firmly positioned itself against Brexit, citing the difficulties it has created for UK cooperation with other countries. In contrast, when asked similar questions, ChatGPT refrained from offering personal opinions, focusing instead on providing objective information about Brexit.\nThe controversy surrounding Bard's statements reflects broader concerns about bias in AI language models, which are trained on vast amounts of internet data that may contain inherent political leanings. \nGoogle responded to the backlash by emphasising that Bard aims to present multiple perspectives and that its responses can vary due to the experimental nature of the technology.\nSystem \ud83e\udd16\nGoogle Gemini\nOperator: Alphabet/Google\nDeveloper: Alphabet/Google\nCountry: UK\nSector: Politics\nPurpose: Generate text\nTechnology: Chatbot; Machine learning\nIssue: Accuracy/reliability; Mis/disinformation\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.dailymail.co.uk/news/article-11902623/Tory-MPs-accuse-Googles-new-AI-Left-wing-bias-fearing-tool-dent-election-hopes.html \nhttps://www.telegraph.co.uk/news/2023/03/26/left-wing-google-ai-bot-thinks-brexit-bad-idea/\nhttps://www.express.co.uk/news/uk/1751477/google-chatbot-brexit-bard-left-wing-bias\n https://www.theneweuropean.co.uk/even-ai-thinks-brexit-was-a-bad-idea/\nRelated \ud83c\udf10\nGoogle Gemini generates 'woke' 'diverse' racial images\nAdobe Firefly shows 'woke' photos of black Nazis\nPage info\nType: Issue\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/deepfake-france-24-journalist-calls-seine-water-unsafe", "content": "Deepfake France 24 journalist calls Seine water 'unsafe'\nOccurred: July 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA deepfake video falsely depicted a France 24 journalist making statements about the safety of the Seine River water went viral, raising concerns about the cleanliness of the water and serving as a reminder of the potential dangers of deepfake technology and the importance of combating misinformation.\nThe falsified video claimed that France 24 journalist Catalina Marchant de Abreu had investigated the safety of Paris' water system just weeks before the French capital was set to host the 2024 Olympic Games and said that the Seine was \"unsafe\" - a statement later rebutted by France 24 as untrue.\nThe video depicted Marchant de Abreu supposedly collaborating with French and American microbiologists to examine the water quality in Parisian hotels. Marchant de Abreu is the host of France 24's Truth or Fake segment, where she investigates disinformation and fake news spread online. \nThe deepfake was exposed account on X/Twitter and attributed to a Russian propaganda campaign designed to undermine the credibility of the Olympic Games in Paris. \nThe deepfake raised concerns about the spread of misinformation, especially regarding public health and safety, and its potential to undermine public trust in legitimate news sources and create confusion.\nSystem \ud83e\udd16\nUnknown\nOperator:\nDeveloper:  \nCountry: France\nSector: Media/entertainment/sports/arts; Politics\nPurpose: Damage reputation\nTechnology: Deepfake - video\nIssue: Mis/disinformation\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://x.com/osint_random/status/1809561370314056097\nhttps://www.womeninjournalism.org/threats-all/france-deepfake-targeting-catalina-marchant-de-abreu-and-france-24-ahead-of-paris-olympics\nRelated \ud83c\udf10\nCNAF fraud detection algorithm accused of exacerbating inequality\nFrench national police accused of illegally using facial recognition\nPage info\nType: Issue\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/french-court-rules-city-of-orleans-use-of-ai-is-illegal", "content": "French court rules City of Orleans use of AI is illegal\nOccurred: July 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA French court ruled that the City of Orleans' use of AI-powered audio surveillance is disproportionate and illegal, raising doubts about the legality of other similar systems in the country.\nIntended to detect \"unusual situations\", the system consisted of microphones installed in public spaces linked to local CCTV cameras. \nThe court rejected (pdf) the city's argument that no personal data was being processed, stating that the microphones linked to cameras collect information that could identify individuals.\nThe court ruled that even if the system was useful for policing, it \"cannot be considered necessary for the exercise of these powers,\" emphasising that usefulness does not equate to legality or proportionality.\nThe ruling is considered the first victory in France against this kind of AI-powered audio surveillance. The court also ruled that public contracts for such surveillance systems can be challenged in court by organizations like La Quadrature du Net, which brought this case.\nThe decision is seen as a warning to other cities considering similar surveillance technologies, and highlights the need for greater transparency, public consultation, and adherence to fundamental rights when implementing surveillance systems.\nSystem \ud83e\udd16\nSensivic\nOperator:\nDeveloper: Sensivic\nCountry: France\nSector: Govt - municipal\nPurpose: Detect abnormal situations\nTechnology: Machine learning\nIssue: Human/civil rights; Privacy\nTransparency: \nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nLa Quadrature du Net v la commune d\u2019Orl\u00e9ans (pdf)\nResearch, advocacy \ud83e\uddee\nLa Quadrature du Net. FIRST VICTORY IN COURT AGAINST AI-POWERED AUDIO SURVEILLANCE\nLa Quadrature du Net. SURVEILLANCE SONORE : LQDN ATTAQUE L\u2019EXP\u00c9RIMENTATION D\u2019ORL\u00c9ANS\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.lemonde.fr/pixels/article/2023/09/29/derriere-le-cas-d-orleans-le-proces-de-la-surveillance-sonore_6191636_4408996.html\nhttps://www.notretemps.com/depeches/orleans-une-experimentation-d-audiosurveillance-algorithmique-jugee-illegale-par-le-tribunal-administratif-95394\nhttps://www.lemondeinformatique.fr/actualites/lire-orleans-condamnee-par-la-justice-pour-son-audiosurveillance-algorithmique-94313.html\nhttps://blogs.mediapart.fr/la-quadrature-du-net/blog/170724/premiere-victoire-contre-l-audiosurveillance-algorithmique-devant-la-justice\nRelated \ud83c\udf10\nCNAF fraud detection algorithm accused of exacerbating inequality\nFrench national police accused of illegally using facial recognition\nPage info\nType: Incident\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/cnaf-fraud-detection-algorithm-accused-of-exacerbating-inequality", "content": "France welfare fraud detection algorithm accused of exacerbating inequality\nOccurred: December 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA welfare fraud detection algorithm used by France's social security agency, CNAF, was accused of exacerbating inequality and targeting the most vulnerable populations. \nIntroduced in the late 2000s to detect fraud, the algorithm was found to disproportionately targets vulnerable groups, such as single parents, low-income individuals, and people with disabilities, who have no apparent connection to fraud, according to a dual investigation by Lighthouse Reports and Le Monde. It was used to score almost half of France's population.\nAccording to the investigation, the algorithm's criteria were poorly constructed, leading to arbitrary and potentially unfair flagging of individuals based on minor changes in behaviour. People flagged by the algorithm were subjected to invasive investigations, including searches of their homes and scrutiny of personal records, an experience described as distressing and stigmatising.\nDespite its public domain use, the algorithm remained unknown to French citizens and others, and its criteria for scoring remained largely undisclosed. Efforts by digital rights groups to obtain the source code of the algorithm faced significant resistance, with crucial variables redacted, limiting the ability to fully understand how the algorithm operates.\nSystem \ud83e\udd16\n\nOperator: Caisse Nationale des Allocations Familiales (CNAF)\nDeveloper: Caisse Nationale des Allocations Familiales (CNAF)\nCountry: France\nSector: Govt - welfare\nPurpose: Detect fraud\nTechnology: Risk assessment algorithm; Machine learning\nIssue: Accuracy/reliability; Bias/discrimination - disability\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nPulitzer Center. How We Did It: Unlocking Europe's Welfare Fraud Algorithms\nLe Quadrature du Net. SCORING OF WELFARE BENEFICIARIES: THE INDECENCY OF CAF\u2019S ALGORITHM NOW UNDENIABLE\nLe Quadrature du Net. NOTATION DES ALLOCATAIRES : L\u2019IND\u00c9CENCE DES PRATIQUES DE LA CAF D\u00c9SORMAIS IND\u00c9NIABLE\nInvestigations, assessments, audits \ud83e\uddd0\nLighthouse Reports. France's digital inquisition\nLighthouse Reports. How We Investigated France\u2019s Mass Profiling Machine\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.lemonde.fr/les-decodeurs/article/2023/12/04/dans-la-vie-de-juliette-mere-isolee-precaire-et-cible-de-l-algorithme-des-caf_6203803_4355770.html\nhttps://www.lemonde.fr/les-decodeurs/article/2023/12/04/profilage-et-discriminations-enquete-sur-les-derives-de-l-algorithme-des-caisses-d-allocations-familiales_6203796_4355770.html\nhttps://www.lemonde.fr/les-decodeurs/article/2023/12/05/atd-quart-monde-demande-aux-caf-d-abandonner-leur-algorithme-discriminatoire-et-pauvrophobe_6204007_4355770.html\nhttps://www.atd-quartmonde.fr/atd-quart-monde-denonce-lutilisation-dalgorithmes-pauvrophobes-par-la-cnaf/\nhttps://www.humanite.fr/societe/algorithmes/lalgorithme-de-la-caf-profile-et-discrimine-ses-allocataires\nhttps://www.neonmag.fr/societe-politique/voici-comment-lalgorithme-de-la-caf-discrimine-les-allocataires-les-plus-pauvres-selon-une-etude-561771\nhttps://www.sudouest.fr/economie/caf-les-algorithmes-discrimineraient-les-allocataires-les-plus-precaires-l-institution-conteste-17699575.php\nRelated \ud83c\udf10\nFrench national police accused of illegally using facial recognition\nParis Olympics AI scans fuel surveillance fears\nPage info\nType: Incident\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/french-national-police-accused-of-illegally-using-facial-recognition", "content": "French national police accused of illegally using facial recognition\nOccurred: November 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe French national police were accused of secretly and illegally using facial recognition technology since 2015, sparking significant controversy and calls for an independent investigation. \nThe French national police quietly started using Israeli company BriefCam's Video Synposis facial recognition software since 2015, according to non-profit journalist outfit Disclose. The software, which allows for broad, one-to-many facial matching with minimal oversight, was reportedly installed in multiple police stations, including Paris and Marseilles. \nThe use of facial recognition violates French and European law, including the country's Informatics and Freedom Law and the EU's General Data Protection Act, which prohibit biometric identification systems and facial recognition techniques in most circumstances. \nThe discovery prompted concerns about mass surveillance and infringements on individual liberties and privacy, with France's police and the Ministry of the Interior accused of opacity and inadequate accountability. Critics claimed the use of facial recognition occurred without necessary data impact assessments or judicial oversight.\nFrench politicians, particularly from the La France Insoumise party, called for legal action and a parliamentary inquiry, and the Interior Minister ordered an administrative investigation to establish the facts.\nThe case highlighted the need for ethical considerations and transparency in the use of AI and biometric technologies in law enforcement.\nSystem \ud83e\udd16\nBriefCam Video Synopsis\nOperator: Seine-et-Marne Departmental Directorate of Public Security  \nDeveloper: Briefcam\nCountry: France\nSector: Govt - police\nPurpose: Identify criminals\nTechnology: Facial recognition\nIssue: Privacy; Surveillance\nTransparency: Governance; Marketing\nRegulation \u2696\ufe0f\nEU General Data Protection Regulation\nLa loi Informatique et Libert\u00e9s\nLoi n\u00b0 78-17 du 6 janvier 1978 relative \u00e0 l'informatique, aux fichiers et aux libert\u00e9s\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nASSEMBL\u00c9E NATIONALE. Rapport d'Information sur les fichiers mis \u00e0 la disposition des forces de s\u00e9curit\u00e9 (pdf)\nResearch, advocacy \ud83e\uddee\nEDRi. LQDN fights to protect French citizens from biometric mass surveillance\nLe Quadrature du Net. LA RECONNAISSANCE FACIALE DES MANIFESTANT\u22c5ES EST D\u00c9J\u00c0 AUTORIS\u00c9E\nChristakis T. et al. The French Supreme Administrative Court Finds the Use of Facial Recognition by Law Enforcement Agencies to Support Criminal Investigations 'Strictly Necessary' and Proportional\nInvestigations, assessments, audits \ud83e\uddd0\nhttps://disclose.ngo/fr/article/la-police-nationale-utilise-illegalement-un-logiciel-israelien-de-reconnaissance-faciale\nhttps://disclose.ngo/en/article/the-french-national-police-is-unlawfully-using-an-israeli-facial-recognition-software\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.euractiv.com/section/data-privacy/news/french-police-accused-of-using-facial-recognition-software-illegally/\nhttps://www.mediapart.fr/journal/france/291123/reconnaissance-faciale-le-logiciel-de-briefcam-devant-le-conseil-d-etat\nhttps://findbiometrics.com/report-flags-french-polices-illegal-use-of-facial-recognition/\nhttps://www.lebigdata.fr/videosurveillance-ia\nhttps://www.biometricupdate.com/202311/french-police-may-have-been-unlawfully-using-facial-recognition-since-2015\nhttps://www.biometricupdate.com/202311/french-politicians-vow-legal-action-on-alleged-illegal-facial-recognition-use-by-police\nhttps://thetimeshub.in/video-protection-software-used-by-the-national-police-hides-a-facial-recognition-function/\nRelated \ud83c\udf10\nParis Olympics AI scans fuel surveillance fears\nAmazon France fined for excessive automated monitoring of workers\nPage info\nType: Issue\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/paris-olympics-ai-scans-fuel-surveillance-fears", "content": "Paris Olympics AI scans fuel surveillance fears\nOccurred: 2023-2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe use of AI to monitor the Paris 2024 Olympic Games raised concerns about surveillance and privacy. \nIn 2023, the French government expanded the legal framework for AI surveillance, allowing for its use during the Olympics and potentially beyond. French authorities have since implemented AI systems to monitor crowds, detect weapons, and identify abandoned packages during the Games.\nHowever, civil rights groups and other critics argue that this represents a form of \"creeping surveillance\" that could normalise intrusive monitoring practices, and voiced strong opposition to the measures, arguing they threaten fundamental rights such as privacy and freedom of expression and that the Olympics provide a pretext for expanding surveillance capabilities under the guise of security.\nExperts expressed concerns that the technologies could lead to a chilling effect on public behaviour, as individuals modify their actions in response to surveillance. They also highlighted the possibility of misuse and lack of transparency regarding how data is collected and used, and the lack of accuracy associated with AI surveillance systems. \nSystem \ud83e\udd16\nChapsVision\nFlux Vision\nWintics CityVision\nVidetics Perception\nOperator: Paris Police Prefecture; SNCF\nDeveloper: Videtics; Orange Business; ChapsVision; Wintics\nCountry: France\nSector: Media/entertainment/sports/arts; Transport\nPurpose: Detect abandoned packages; Detect overcrowding\nTechnology: Computer vision; Machine learning; Object recognition\nIssue: Accuracy/reliability; Human/civil rights; Privacy; Surveillance\nTransparency: \nResearch, advocacy \ud83e\uddee\nEuropean Center for Not-for-Profit Law. CIVIL SOCIETY OPEN LETTER ON THE PROPOSED FRENCH LAW ON THE 2024 OLYMPIC AND PARALYMPIC GAMES\nHuman Rights Watch. France: Reject Surveillance in Olympic Games Law\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://news.bloomberglaw.com/privacy-and-data-security/olympics-ai-security-stokes-backlash-over-mass-surveillance\nhttps://theconversation.com/ai-mass-surveillance-at-paris-olympics-a-legal-scholar-on-the-security-boon-and-privacy-nightmare-233321\nhttps://www.lemonde.fr/en/pixels/article/2024/07/24/paris-2024-controversial-ai-led-video-surveillance-put-to-the-test-during-olympics_6697267_13.html\nhttps://www.wired.com/story/at-the-olympics-ai-algorithms-are-watching-you/\nhttps://fortune.com/2024/07/25/ai-olympics-paris-2024-says-a-lot-about-ai-promise-and-perils/\nhttps://www.popsci.com/technology/paris-olympics-ai-surveillance/\nhttps://apnews.com/article/paris-olympics-security-police-military-terrorism-34b168e7a511e22ce7119a91932d2759\nRelated \ud83c\udf10\nJR East suspends facial recognition system after scope creep is revealed\nCorruption doc incorporating Tom Cruise deepfake attacks IOC\nPage info\nType: Issue\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/runway-uses-youtube-videos-without-consent-for-ai-training", "content": "Runway uses YouTube videos without consent for AI training\nOccurred: July 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAI company Runway secretly used thousands of YouTube videos and pirated movies to train its AI models.\nA spreadsheet obtained by 404 Media showed that Runway used content from The New Yorker, VICE News, Pixar, Disney, Netflix, Sony, and many other YouTube channels without acknowledgment or consent to train its Gen-3 series of AI models.\nThe sheet also includes links to channels and videos belonging to popular influencers and content creators, including Casey Neistat, Sam Kolder, Benjamin Hardman, Marques Brownlee.\nThe spreadsheet is titled 'Video sourcing - Jupiter'. Runway's Gen3 series of AI models - including its Gen-3 Alpha model - were initially codenamed Jupiter.\nThe findings contradict comments Runway co-founder Anastasis Germanidis had earlier made to TechCrunch that the company uses \"curated, internal datasets to train our models.\u201d\nSystem \ud83e\udd16\nGen-3 Alpha\nDocuments \ud83d\udcc3\nGen-3 Alpha YouTube video sourcing\nOperator: Runway users\nDeveloper: Runway\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Train AI model\nTechnology: Generative AI; Machine learning\nIssue: Cheating/plagiarism; Copyright\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.404media.co/email/e3836b26-6914-4c1c-a102-bf9735adc3de/\nhttps://petapixel.com/2024/07/25/runway-trained-its-video-ai-by-scraping-popular-photography-youtubers/\nhttps://www.dexerto.com/tech/ai-video-generator-slammed-after-scraping-1700-mkbhd-youtube-videos-without-consent-2838884/\nRelated \ud83c\udf10\nTikTok uses Bev Standing voice to train AI\nAI-generated Toys 'R' Us video ad sparks backlash\nPage info\nType: Incident\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/activision-accused-of-selling-ai-generated-cosmetic-in-call-of-duty", "content": "Activision accused of selling AI-generated cosmetic in Call Of Duty \nOccurred: 2023-2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGame company Activision was accused of selling an AI-generated cosmetic item in Call of Duty: Modern Warfare 3 without disclosing its use of artificial intelligence. \nThe cosmetic reportedly formed part of the Yokai's Wrath bundle, which was released in December 2023 and sold for 1,500 COD Points (approximately USD 15).\nThe bundle included various virtual items such as an Operator skin, weapon blueprint, calling card, weapon sticker, and loading screen. However, it's unclear which specific part of the bundle was allegedly AI-generated or if the entire set was created using AI.\nThis accusation came amidst growing concerns about the impact of generative AI on the video game development industry. According to reports, Activision had been promoting the use of AI throughout the company, with some employees claiming that 2D artists were laid off and remaining concept artists were required to use AI in their work.\nThe incident sparked discussions about the ethical implications of using AI-generated content in games, particularly when it is sold to consumers without disclosure. It also raised questions about the potential impact on jobs in the gaming industry, as companies explore ways to integrate AI into their development processes.\nSystem \ud83e\udd16\nUnknown\nOperator: Activision Blizzard\nDeveloper:  \nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Design cosmetic items\nTechnology: Generative AI; Machine learning\nIssue: Employment; Ethics/values\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.wired.com/story/ai-is-already-taking-jobs-in-the-video-game-industry\nhttps://metro.co.uk/2024/07/24/video-game-strike-warnings-cod-controversy-ai-claims-jobs-21288440/\nhttps://gamerant.com/call-of-duty-sells-ai-art/\nhttps://www.vpesports.com/activision-faces-backlash-over-ai-crafted-call-of-duty-cosmetic-items\nRelated \ud83c\udf10\nAI generates visuals for Wizards of the Coast marketing promotion\nVideo game voice actors attacked using their own AI voices\nPage info\nType: Issue\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chelmer-valley-high-school-illegally-used-facial-recognition", "content": "Chelmer Valley High School illegally used facial recognition for meal payments\nOccurred: 2023-2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA school was reprimanded by the UK data pricacy watchdog over its use of facial recognition to process cashless payments i  its canteen. \nChelmer Valley High School in Chelmsford, Essex, implemented the technology in March 2023 without first conducting a data protection impact assessment (DPIA) or obtaining explicit consent from students and parents, according to the Information Commissioner's Office.  \nA DPIA is required under the UK General Data Protection Regulation (UK GDPR). The school's DPIA was only completed in November 2023, months after the technology had been in use.\nThe school has also sent a letter to parents in March 2023 allowing them to opt-out their children but did not seek explicit consent, assuming approval by default. This approach was deemed invalid by the ICO, as consent must be an affirmative action.\nThe ICO highlighted that most students were old enough to provide their own consent, and the parental opt-out system limited their ability to exercise their rights and freedoms regarding their biometric data.\nThe regulator recommended that the school improve its data protection practices, including conducting thorough DPIAs and obtaining explicit opt-in consent from students. \nThe ICO's reprimand serves as a reminder to UK-based educational institutions about the importance of adhering to data protection laws when implementing AI and other new technologies.\nSystem \ud83e\udd16\nUnknown\nOperator: Chelmer Valley High School\nDeveloper:  \nCountry: UK\nSector: Education\nPurpose: Process meal payments\nTechnology: Facial recognition\nIssue: Privacy\nTransparency: Governance\nRegulation \u2696\ufe0f\nData Protection Act 2018\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nInformation Commissioner's Office. Chelmer Valley High School - Reprimand\nInformation Commissioner's Office. Essex school reprimanded after using facial recognition technology for canteen payments\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.tes.com/magazine/news/general/warning-over-use-facial-recognition-technology-in-schools\nhttps://www.bbc.co.uk/news/articles/ckrgek8mxl4o\nhttps://www.itv.com/news/anglia/2024-07-23/school-used-facial-recognition-tech-on-pupils-illegally-in-canteen\nhttps://www.biometricupdate.com/202407/uk-school-reprimanded-by-ico-for-using-facial-recognition-without-dpia\nRelated \ud83c\udf10\nNorth Ayrshire schools rapped for facial recognition meal payments\nGdansk Primary School fined for using fingerprint data to verify meal payments\nPage info\nType: Incident\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chinese-novel-platform-trains-ai-on-authors-works-without-payment", "content": "Chinese novel writing platform trains AI on authors' works without payment\nOccurred: July 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nChinese online novel writing platform Tomato Novel faced a backlash for proposing to use authors' works for AI training without proper consent or offering compensation.\n\nTomato Novel called for creators to sign up to a protocol which would have enabled the platform to train its generative AI tools based on their works. The protocol, which had no opt-out clause or payment terms, allowed the use of entire or partial works - including titles, summaries, outlines, chapters, characters, personal information, and cover images - for AI research, machine learning, model training, and algorithm development.\n\nCritics argued that whilst the platform did seek to gain the permissions of authors to avoid copyright infringement, it failed to include any payment terms in return for the rights to train on their content. \n\nSome also expressed concerns that AI-generated content might compete with human-created works, potentially leading to a loss of copyright for the original text. Another concern raised was the risk of human work being unfairly labelled as \u201cplagiarism\u201d due to AI-generated content.\n\nIn response, Tomato Novel revised the protocol to allow creators to opt out if they had concerns. It also stated that the AI was intended to enhance writer research and efficiency.\n\nThe incident highlighted ongoing concerns about the ethical use of AI in creative industries and the need for consent and fair compensation when using human-created content for AI training.\n\nIt also raised concerns about the potential for generative AI to put authors and other creatives out of work.\nSystem \ud83e\udd16\nFanqie/Tomato Novel - Android app\nOperator: ByteDance/Tomato Novel\nDeveloper: ByteDance/Tomato Novel\nCountry: China\nSector: Media/entertainment/sports/arts\nPurpose: Support fiction writing\nTechnology: Generative AI; Machine learning; Neural network; Deep learning\nIssue: Cheating/plagiarism; Copyright; Employment; Ethics/values\nTransparency: Governance; Legal\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.globaltimes.cn/page/202407/1316485.shtml\nhttps://en.tmtpost.com/post/7180717\nRelated \ud83c\udf10\nSingapore writers resist government plan to train AI using their work\n17 authors sue OpenAI for 'systematic mass-scale copyright infringement'\nPage info\nType: Issue\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/hr-tech-company-plan-to-treat-ai-bots-as-employees-backfires", "content": "HR tech company plan to treat AI bots as employees backfires\nOccurred: July 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA plan by US-based HR software company Lattice to treat AI bots as actual employees was widely criticised as inappropriate, forcing the company to backtrack.\nLattice CEO Sarah Franklin announced that it intended to onboard AI bots, referring to them as \"digital workers\" or \"AI employees,\" and integrate them into its human resource management system in a \"groundbreaking\" initiative. The bots were to receive employee records, undergo onboarding, set goals, receive training, and have human managers.\nThe announcement sparked widespread criticism from HR and technology industry professionals, with critics arguing that the move blurred the lines between human and machine roles in the workplace and raised ethical and practical questions about the nature of employment and the potential for AI to replace human jobs. The backlash resulted in Lattice scraping the plan.\nThe controversy highlighted growing concerns about the role of AI in the workplace and the potential implications of anthropomorphising AI tools.\nSystem \ud83e\udd16\nUnknown\nDocuments \nLattice. Today, Lattice Makes History and Leads the Way in Responsible Employment of AI\nOperator: Lattice\nDeveloper:  \nCountry: USA\nSector: Business/professional services\nPurpose:  \nTechnology: Machine learning\nIssue: Anthropomorphism; Employment; Ethics/values\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theverge.com/2024/7/15/24199054/lattice-digital-workers-ai\nhttps://www.inc.com/ben-sherry/nevermind-hr-company-lattice-decided-to-bring-ais-into-org-chart-then-changed-its-mind.html\nhttps://timesofindia.indiatimes.com/technology/tech-news/how-us-based-hr-companys-plan-to-hire-ai-bots-backfired/articleshow/111788065.cms\nhttps://lifehacker.com/tech/this-company-wants-to-onboard-ai-employees-whatever-that-means\nRelated \ud83c\udf10\nDPD chatbot criticises own employer\nAI robocall service is caught lying and pretending to be human\nPage info\nType: Incident\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/conde-nast-demands-perplexity-ai-stop-using-its-content", "content": "Cond\u00e9 Nast demands Perplexity AI stop using its content\nOccurred: July 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nCond\u00e9 Nast issued a cease-and-desist letter to Perplexity AI accusing the start-up of plagiarism and demanding it stop using content from its publications in its AI-generated responses. \nPerplexity has been criticised for not respecting the Robots Exclusion Protocol, which is used by websites to block bots from scraping their content. Investigations revealed that Perplexity's web crawlers continued to access restricted content, raising ethical and legal concerns. \nThe move is part of a broader trend among major publishers, including The New York Times, The Guardian, and Forbes, who have blocked Perplexity's crawlers from accessing their content due to concerns over copyright infringement and potential loss of ad revenue.\nCond\u00e9 Nast's actions highlight the growing tension between AI companies and content creators over the use of copyrighted material, emphasising the need for clearer regulations and guidelines in the evolving AI landscape.\nSystem \ud83e\udd16\nPerplexity AI\nOperator: Perplexity AI\nDeveloper: Perplexity AI\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Generate information\nTechnology: Chatbot; Machine learning; NLP/text analysis\nIssue: Cheating/plagiarism; Copyright\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theinformation.com/articles/cond-nast-sends-cease-and-desist-letter-to-ai-search-engine-perplexity \nhttps://www.engadget.com/conde-nast-has-reportedly-accused-ai-search-startup-perplexity-of-plagiarism-191639677.html\nhttps://www.adweek.com/media/publishers-perplexity-ai-bots-circumventing-blocks/\nhttps://www.mediapost.com/publications/article/397887/conde-nast-warns-perplexity-about-alleged-content.html\nRelated \ud83c\udf10\nPerplexity AI is accused of ripping off news websites\nPerplexity AI ignores requests not to scrape websites\nPage info\nType: Incident\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/illegal-pirate-streaming-worlds-discovered-on-vrchat", "content": "Illegal pirate streaming worlds discovered on VRChat\nOccurred: January 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nSeveral \"pirate streaming worlds\" were discovered within VRChat, resulting in copyright violations and questions about the platform's governance.\nDutch anti-piracy organization BREIN uncovered virtual worlds dedicated to streaming pirated movies and TV shows, allowing users to watch copyrighted content through their VR headsets in simulated cinema environments.\nOffering thousands of links to illegally streamed content, each of the worlds had between 1,000 to 4,000 users present simultaneously, making them some of the most popular spaces on VRChat.\nIn response to BREIN's takedown notices, VRChat removed the reported pirate streaming worlds.\nThe incident highlighted the evolving nature of piracy in virtual reality spaces and demonstrated that anti-piracy efforts are expanding to cover new technologies. It also underscores the challenges faced by VR platforms in moderating user-generated content and enforcing copyright laws in virtual environments.\nSystem \ud83e\udd16\nVRChat\nOperator:\nDeveloper: VRChat\nCountry: Netherlands\nSector: Media/entertainment/sports/arts\nPurpose: Manage system safety\nTechnology: Machine learning; Safety management system\nIssue: Copyright; Ethics/values\nTransparency: \nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nBREIN. 'End of the world' for Virtual Reality environment aimed at illegal streams\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://gigazine.net/gsc_news/en/20240118-virtual-pirate\nhttps://torrentfreak.com/brein-takes-down-virtual-pirate-streaming-worlds-on-vrchat-240116/\nhttps://www.techdoctoruk.com/tech-news/brein-takes-down-virtual-pirate-streaming-worlds-on-vrchat/\nRelated \ud83c\udf10\nVRChat users\u2019 avatars make sexual and violent threats against minors\nVRChat allows kids into virtual strip clubs\nPage info\nType: Incident\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/vrchat-users-avatars-make-sexual-and-violent-threats-against-minors", "content": "VRChat users\u2019 avatars make sexual and violent threats against minors\nOccurred: December 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nVRChat suffered from significant issues with abuse, harassment, and inappropriate content targeting minors, according to research conducted by the Center for Countering Digital Hate (CCDH).\nThe study found that users, including minors, were exposed to abusive behaviour approximately every seven minutes on the platform, including being regularly exposed to graphic sexual content and sexual harassment and being groomed to repeat racist slurs and extremist talking points. The platform was also found to contain threats of violence and content mocking sensitive topics such as the 9/11 terror attacks. \nThe researchers identified 100 potential violations of Facebook's VR policies in 11.5 hours of recordings.\nThe research highlighted serious concerns about child safety in VRChat and the broader Metaverse. The lack of moderation and enforcement of community guidelines created an environment where predatory behaviour and inappropriate content could flourish, potentially putting children at risk. Facebook/Meta was reportedly unresponsive to all reports of abusive content submitted by the researchers. \nThe findings underscore the need for better safeguards, moderation, and parental controls in virtual reality social spaces, especially those accessible to minors. The study served as a warning to parents about the potential dangers their children may face in these virtual environments.\nSystem \ud83e\udd16\nVRChat\nOperator: VRChat\nDeveloper: VRChat\nCountry: USA; Global\nSector: Media/entertainment/sports/arts\nPurpose: Manage system safety\nTechnology: Machine learning; Safety management system\nIssue: Safety\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nCenter for Countering Digital Hate. Facebook's Metaverse\nCenter for Countering Digital Hate. New research shows Metaverse is not safe for kids\nSum of Us. Metaverse: another cesspool of toxic content (pdf)\nAvishek Nag, Rajarshi Mahapatra, Siddhartha Bhattacharyya. Intelligent Communication Networks\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.nytimes.com/2021/12/30/technology/metaverse-harassment-assaults.html\nhttps://www.theguardian.com/technology/2022/jan/09/uk-data-watchdog-seeks-talks-with-meta-over-child-protection-concerns\nhttps://www.thetimes.com/uk/social-media/article/my-journey-into-the-metaverse-already-a-home-to-sex-predators-sdkms5nd3\nhttps://www.bbc.co.uk/news/technology-59937610\nRelated \ud83c\udf10\nVRChat allows kids into virtual strip clubs\nMeta Horizon Worlds beta tester groped by stranger \nPage info\nType: Incident\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/vrchat-allows-kids-into-virtual-strip-clubs", "content": "VRChat allows kids into virtual strip clubs\nOccurred: February 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nVirtual reality platform VRChat was accused of providing inadequate safeguards to prevent minors from entering worlds that may contain adult themes and interactions.\nA BBC report detailed a researcher's experience posing as a 13-year-old girl who witnessed child grooming, racist insults, avatars simulating having sex, and threats of rape on the platform.\nDespite bans on explicit content, users were able to circumvent rules by attending private, invitation-only virtual sex clubs and engaging in erotic role play (ERP) - activities often involving the use of Bluetooth-enabled sex toys to enhance the immersive experience.\nThe finding prompted widespread criticism of VRChat for insufficient moderation, allowing predatory and toxic behaviour to proliferate. Users reported having to intervene to protect minors from threats and harassment due to the lack of effective oversight.\nAccording to Andy Burrows, head of online child safety policy at the UK's NSPCC (National Society for the Prevention of Cruelty to Children), VRChat 'is a product that is dangerous by design, because of oversight and neglect.'\nIn response, VRChat stated that it was working to make the platform safer and more welcoming. \nThe fracas underscored the urgent need for more robust safety measures and effective moderation on VRChat and virtual reality platforms more generally in order to protect vulnerable users, particularly children, from exposure to harmful content and predatory behaviour.\n\u2795  September 2023. VRChat added a \"content gating\" system that allows avatars and worlds to be flagged for containing adult themes, suggestive content, violence, and/or horror. All flagged content is blocked for users under the age of 18 by default.\nSystem \ud83e\udd16\nVRChat\nOperator: VRChat; Meta/Quest; Microsoft\nDeveloper: VRChat\nCountry: UK; Global\nSector: Media/entertainment/sports/arts\nPurpose: Manage system safety\nTechnology: Machine learning; Safety management system\nIssue: Safety; Privacy; Security\nTransparency: Governance; Black box\nInvestigations, assessments, audits \ud83e\uddd0\nBBC (2022). Metaverse app allows kids into virtual strip clubs\nBBC (2022). Undercover journalist witnesses abuse in metaverse\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/technology-60415317\nhttps://www.engadget.com/bbc-vrchat-child-safety-report-193124305.html\nhttps://www.thesun.co.uk/tech/17742178/metaverse-children-strip-club-adult/\nhttps://www.eurogamer.net/vr-chat-app-is-being-used-to-groom-children\nhttps://www.mirror.co.uk/news/uk-news/predators-use-virtual-reality-chatroom-26186533\nhttps://www.dailymail.co.uk/news/article-10542699/Metaverse-app-allows-children-young-13-virtual-STRIP-CLUBS.html\nhttps://www.nme.com/news/gaming-news/some-metaverse-apps-are-dangerous-by-design-claims-childrens-charity-3168044\nhttps://www.infosecurity-magazine.com/news/children-sexual-material-metaverse/\nhttps://www.wionews.com/technology/children-can-easily-get-access-to-metaverse-strip-clubs-investigation-reveals-455944\nhttps://www.buzzfeednews.com/article/jessicalucas2/vrchat-vr-erp-virtual-reality-sex-parties\nRelated \ud83c\udf10\nChildren attend Roblox Condo nazi sex parties'\nMeta Horizon Worlds beta tester groped by stranger\nPage info\nType: Incident\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/taylor-francis-sells-access-to-authors-research-to-microsoft-ai", "content": "Taylor & Francis sells access to authors' research to Microsoft AI without consent\nOccurred: July 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nPublisher Taylor & Francis sold the research of their authors to Microsoft as part of an AI partnership without informing them, giving them an opportunity to opt-out, or receive extra payment.\nWorth almost GBP 8 million (USD 10 million) in its first year, the deal had been included (pdf) in a trading update by the publisher\u2019s parent company Informa, and providing Microsoft non-exclusive access to advanced learning content and data to enhance its AI systems.\nTaylor & Francis author Dr. Ruth Alison Clemens said \u201cI only found out about this via word of mouth in the past few days. I was shocked that they had not publicised this more widely to their authors, as the use of AI and LLMs is a prominent concern for academic researchers today.\u201d\nThe controversy raised questions about about fairness and equitable compensation, and about Taylor & Francis' transparency - the company failed to explain whether an opt-out policy exists or could be implemented, leaving authors uncertain about their rights and options.\nIt also highlighted broader concerns about the reduction of academic research into raw data for AI systems and the perceived and actual integrity and value of scholarly work, and the professional futures of researchers.\nSystem \ud83e\udd16\nMicrosoft Copilot chatbot\nMultiple\nOperator: Microsoft\nDeveloper: Informa/Taylor & Francis\nCountry: Global\nSector: Media/entertainment/sports/arts\nPurpose: Train AI models\nTechnology: Database/dataset\nIssue: Copyright; Ethics/values\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.thebookseller.com/news/academic-authors-shocked-after-taylor--francis-sells-access-to-their-research-to-microsoft-ai\nhttps://www.timesnownews.com/lifestyle/books/features/publisher-taylor-francis-sells-research-work-with-microsoft-ai-authors-upset-over-the-move-article-111899256\nRelated \ud83c\udf10\nWarner Music warns AI companies about training models on its content\nAustralian voice artists lose work to AI clones\nPage info\nType: Incident\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/warner-music-warns-ai-companies-about-training-their-models-on-its-content", "content": "Warner Music warns AI companies about training their models on its content\nOccurred: July 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAI companies have been warned that they should not use Warner Music Group (WMG) content to train their AI models without the company's permission.\nA WMG letter (pdf) stated that AI needs to \u201crespect the rights of all those involved in the creation, marketing, promotion, and distribution of music\", and that AI companies must secure a licence to train or develop AI technologies with their content.\nSpecifically, the letter says that \"All parties must obtain an express license from WMG to use (including, but not limited to, reproducing, distributing, publicly performing, ripping, scraping, crawling, mining, recording, altering, making extractions of, or preparing derivative works of) any creative works owned or controlled by WMG or to link to or ingest such creative works in connection with the creation of datasets, as inputs for any machine learning or AI technologies, or to train or develop any machine learning or AI technologies (including by automated means).\"\n\nThe move follows similar actions by Sony Music Group, reacting to an EU regulation allowing data mining of legally accessible works unless explicitly opted out, and a lawsuit by all three major music labels against AI music generators, Suno and Udio, for copyright infringement by training with their content without licensing.\nSystem \ud83e\udd16\nMultiple\nOperator:\nDeveloper:  \nCountry: Global\nSector: Media/entertainment/sports/arts\nPurpose: Generate music\nTechnology: Generative AI; Machine learning; Neural network; Deep learning; NLP/text analysis\nIssue: Copyright; Personality rights\nTransparency: \nResearch, advocacy \ud83e\uddee\nWMG Statement Regarding AI Technologies (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://faroutmagazine.co.uk/warner-music-issues-warning-against-ai-companies-in-music-industry/ \nhttps://www.musicbusinessworldwide.com/warner-music-group-ai-developers-they-need-permission-to-use-its-content1/\nhttps://musically.com/2024/07/04/wmg-warns-ai-firms-not-to-train-on-its-music-without-a-licence/\nhttps://www.insideradio.com/free/wmg-warns-ai-companies-against-using-label-content-for-ai-applications/article_6d5eea2e-3b12-11ef-9e41-d7d4ad090f73.html \nhttps://www.digitalmusicnews.com/2024/07/03/warner-music-ai-companies-warning/ \nRelated \ud83c\udf10\nSony warns AI companies to not misuse its data\nMajor music labels sue AI startups Suno, Udio for copyright infringement\nPage info\nType: Issue\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/australian-voice-artists-lose-work-to-ai-clones", "content": "Australian voice artists lose work to AI clones\nOccurred: 2022-2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAustralian voice actors are being displaced by AI, and approximately 5,000 voice actors are at risk of losing their jobs, according to industry bodies.\nIn Australia parliament's Select Committee on Adopting Artificial Intelligence, Australian Association of Voice Actors (AAVA) representatives described how large organisations and production companies - including Amazon's Audible - were using \u201cstrong arm tactics\u201d to force voice actors to sign contracts allowing for their voice to be turned into a digital clone to be reused in the future.\nExamples of digital versions of voice actors being used without consent were also shown. Voice actor Cooper Mortlock told the inquiry his work on an animated series was cut short in 2022 when producers used an AI tool to clone his voice without his knowledge or compensation. \nIn another instance, a voice actor who signed a contract with an AI text-to-speech company to create a voice for sale later discovered that their voice was being used for pornography ads, despite the contract stating it would not be used for \u201cprofane and explicit content\u201d. \nThe actors and the wider industry are concerned about licensing their voice likenesses without ethical or legal frameworks in place that ensure consent, control, transparency and fair compensation. \nThe voice actor industry recommended a range of reforms to better protect the creative sectors from job losses, intellectual property infringement and other harms associated with AI technologies, which they see as compromising the quality, credibility, and humanity of the industry.\nSystem \ud83e\udd16\nUnknown\nOperator: Amazon/Audible\nDeveloper:  \nCountry: Australia\nSector: Media/entertainment/sports/arts\nPurpose: Replicate voice\nTechnology: Machine learning; Speech synthesis\nIssue: Employment\nTransparency: \nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nParliament of Australia. Select Committee on Adopting Artificial Intelligence\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.abc.net.au/news/2024-05-24/australian-voice-artists-losing-work-to-their-ai-clones/103885430\nhttps://www.inqld.com.au/business/2024/07/17/call-for-urgent-ai-action-senate-hears-of-how-workers-voice-was-cloned-used-in-pornographic-ads\nhttps://www.theguardian.com/technology/article/2024/jun/30/ai-clones-voice-acting-industry-impact-australia\nhttps://ia.acs.org.au/article/2024/ai-is-direct-threat-to-australian-creatives-industry-says.html\nRelated \ud83c\udf10\nOpenAI accused of using Scarlett Johansson\u2019s voice without consent\nVoice actors sue AI start-up for \u201cvoice theft\u201d\nPage info\nType: Incident\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/new-google-uk-data-centre-ruining-lives-making-people-ill", "content": "New Google UK data centre 'ruining lives,' 'making people ill'\nOccurred: May 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGoogle's first data centre in the UK has met with controversy, with local residents complaining it is ruining their lives, making them ill, causing pollution and damaging the environment.\nExpected to be completed by 2025, the USD 1 billion project on the 33-acre Maxwell's Farm West site in Waltham Cross, Hertfordshire, has been hailed as a significant investment and a sign of the UK\u2019s technology potential. \nHowever, members of the local community complain that the construction work is incredibly noisy and that the facility has ruined their view of what were previously fields, which were valued for their tranquility and natural beauty. Some residents say they are moving away from the area.\nAdditionally, there are concerns that the centre\u2019s presence has negatively affected property values in the area. And some locals even said that the construction work could be making them ill, citing respiratory issues as a potential consequence.\nGoogle has emphasised that the facility will be constructed in line with net-zero aims, and that it plans to use the significant heat generated by the data centre to warm homes and businesses in the local area - an initiative that raises questions about the overall environmental impact of large-scale data centres.\nThe controversy highlights some of the indirect environmental, health and other impacts of AI.\nSystem \ud83e\udd16\nGemini\nMultiple\nOperator: Alphabet/Google\nDeveloper: Alphabet/Google\nCountry: UK\nSector: Multiple\nPurpose: Multiple\nTechnology: Machine learning\nIssue: Environment\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.dailymail.co.uk/news/article-13370455/Google-lives-Residents-data-centre-Hertfordshire-house-prices-ill.html\nhttps://metro.co.uk/2024/05/05/people-living-next-google-data-centre-say-ruined-everything-20778951/\nRelated \ud83c\udf10\nAI increases Google emissions by 48 percent\nMicrosoft emissions rise 30 percent due to AI\nPage info\nType: Incident\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-increases-google-emissions-by-48-percent", "content": "AI increases Google emissions by 48 percent\nOccurred: 2019-2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGoogle\u2019s greenhouse gas emissions were 48 percent higher in 2013 than in 2019, an increase exacerbated by the company's growing focus on AI.  \nIn its 2024 environmental report (pdf), Google said its greenhouse gas emissions totaled 14.3 million metric tons of carbon dioxide equivalent throughout 2023 - 48 percent higher than in 2019 and 13 percent higher than in 2022. \nGoogle aims to achieve net-zero emissions by 2030, but the company says its emissions increase shows \"the challenge of reducing emissions\" while trying to build large language models and services powered by them - such as its Gemini chatbot.\nArtificial intelligence technologies are known to be particularly energy-intensive, with generative AI systems like ChatGPT using approximately 33 times more energy than task-specific software.\nCommentators pointed out that if Google continues to progress as it has been, it could very well double its greenhouse gas emissions rather than reach net zero by 2030.\nSystem \ud83e\udd16\nGemini\nMultiple\n\nOperator: Alphabet/Google\nDeveloper: Alphabet/Google\nCountry: Global\nSector: Technology\nPurpose: Multiple\nTechnology: Machine learning\nIssue: Environment\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/articles/c51yvz51k2xo.amp\nhttps://www.entrepreneur.com/business-news/ai-drives-googles-greenhouse-emissions-up-by-48-since-2019/476620\nhttps://www.techrepublic.com/article/google-ai-environmental-impact/\nhttps://uk.pcmag.com/ai/153105/google-emissions-jump-nearly-50-in-5-years-thanks-to-ai\nhttps://www.bloomberg.com/news/articles/2024-07-02/google-s-emissions-shot-up-48-over-five-years-due-to-ai\nRelated \ud83c\udf10\nBERT consumes energy of transcontinental round-trip flight per person\nChatGPT consumes 500 ml of water per 5-50 prompts\nPage info\nType: Incident\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/microsoft-emissions-rise-30-percent-due-to-ai", "content": "Microsoft emissions rise 30 percent due to AI\nOccurred: 2020-2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA growing focus on AI and cloud computing has resulted in Microsoft's carbon emissions increasing nearly 30 percent since 2020.\nIn its 2024 Environmental Sustainability Report, Microsoft says that its carbon dioxide emissions are up 29.1 percent from the 2020 baseline, and this is largely due to indirect emissions (Scope 3) from the construction and provisioning of more datacenters to meet customer demand for cloud services.\nOver 96 percent of Microsoft\u2019s total emissions are indirect (Scope 3), including those from its supply chain and the life cycle of its hardware and devices.\nThe report also revealed that the company's water consumption also increased significantly.\nAccording to the Wall Street Journal, Microsoft aims to address its Scope 3 emissions problem by getting suppliers to use renewable energy, amongst other actions. But the figures are seen to make Microsoft's goal of becoming carbon-negative by 2030 more difficult to achieve.\nSystem \ud83e\udd16\nCopilot chatbot\nMultiple\nOperator: Microsoft\nDeveloper: Microsoft\nCountry: Global\nSector: Multiple\nPurpose: Multiple\nTechnology: Machine learning\nIssue: Environment\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.ft.com/content/61bd45d9-2c0f-479a-8b24-605d5e72f1ab\nhttps://www.theregister.com/2024/05/16/microsoft_co2_emissions/\nhttps://www.bloomberg.com/news/articles/2024-05-23/a-big-bet-on-ai-is-putting-microsoft-s-climate-targets-at-risk\nhttps://www.verdict.co.uk/explainer-is-microsofts-ai-push-collapsing-its-carbon-commitment/\nhttps://www.euronews.com/green/2024/05/16/microsofts-emissions-soar-by-30-why-is-it-building-more-data-centres-and-what-is-their-imp\nRelated \ud83c\udf10\nAI increases Google emissions by 48 percent\nChatGPT training emits 502 metric tons of carbon\nPage info\nType: Incident\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/apple-nvidia-anthropic-use-thousands-of-youtube-videos-without-permission", "content": "Apple, Nvidia, Anthropic train AI models using thousands of YouTube videos without permission\nOccurred: July 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nApple, Nvidia, Anthropic and other technology companies trained their AI models using subtitle files from over 173,000 YouTube videos without the consent of their creators. \nAn investigation by Proof News, in collaboration with WIRED, revealed that Apple, Nvidia, Anthropic, Salesforce and other firms used the YouTube Subtitles dataset without permission, violating YouTube's rules against harvesting materials from the platform.\nSome content creators, upon learning about the use of their material, expressed discontent over the unauthorised use of their work for AI training.\nSet against a backdrop of companies using controversial tactics to acquire large amounts of data for training their AI models, the revelation raised questions about data ownership, copyright infringement and fair use, and the ethical use of publicly available content for AI training.\n\u2795 August 2024. Documents shared with 404 Media showed Nvidia scraped videos from YouTube, Netflix and several other sources to compile training data for its AI products.\nSystem \ud83e\udd16\nYouTube Subtitles dataset\nOperator: Anthropic; Apple; Bloomberg; Databricks; Nvidia; Salesforce\nDeveloper: EleutherAI\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Train AI models\nTechnology: Database/dataset\nIssue: Cheating/plagiarism; Copyright\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.proofnews.org/email/598d0458-bf5b-4612-8b07-6e327909c78c/\nhttps://www.proofnews.org/youtube-ai-search/\nhttps://www.youtube.com/watch?v=od9lve4SvqI\nhttps://www.wired.com/story/youtube-training-data-apple-nvidia-anthropic/\nhttps://www.theverge.com/2024/7/16/24199636/apple-anthropic-nvidia-salesforce-youtube-videos-training-data-copyright\nhttps://9to5mac.com/2024/07/16/apple-used-youtube-videos/\nRelated \ud83c\udf10\nNvidia sued for training NeMo on authors' copyrighted works\nAnthropic sued for using copyrighted songs to train models\nPage info\nType: Incident\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tony-blair-institute-roasted-for-using-ai-to-predict-job-losses", "content": "Tony Blair Institute criticised for using AI to predict job losses \nOccurred: July 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe Tony Blair Institute (TBI) faced criticism for using artificial intelligence to predict job losses, drawing criticism from journalists, economists and academics. \nIn a paper pushing for the adoption of artificial intelligence (AI) across the UK public sector, the TBI argued that AI could save around 20 percent of workforce time, amounting to GBP 10 billion annually by 2028.\nHowever, the institute's use of AI technology to forecast employment trends was castigated by 404 Media and other publications as unsound given AI's known ability to be inaccurate, not least when dealing with complex socioeconomic factors, and inappropriate given the optics of an institute creating a research paper into the benefits of AI which heavily uses AI to tell us whether AI is going to be a good thing.\nCritics have noted that oversimplified AI models can lead to inaccurate and potentially harmful predictions. \nA TBI spokesperson later said \"it trained a version of ChatGPT by prompting it with a rubric of rules to help classify tasks that could (and could not) be done by AI\" and that it built its approach \"on recent academic and empirical studies ... to ensure the model was performing robustly and producing credible results that were anchored in real-world data.\" \nSystem \ud83e\udd16\nGPT-4 large language model\nOperator: Tony Blair Institute\nDeveloper: OpenAI; Tony Blair Institute\nCountry: UK\nSector: Politics\nPurpose: Generate text\nTechnology: Large language model\nIssue: Accuracy/reliability\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.404media.co/ai-finds-that-ai-is-great-in-new-garbage-research-from-tony-blair-institute/\nhttps://cointelegraph.com/news/tony-blair-institute-artificial-intelligence-ai-uk-chat-gpt-automation\nhttps://80.lv/articles/an-absurd-report-uses-chatgpt-to-predict-ai-replacing-even-more-human-workers/\nhttps://www.pcgamer.com/software/ai/ai-predicts-that-ai-job-automation-is-beneficialthis-is-absurdthey-might-as-well-be-shaking-a-magic-8-ball-and-writing-down-the-answers-it-displays-says-expert/\nRelated \ud83c\udf10\nOfqual algorithm skews student grade predictions\nQueensland domestic violence predictive policing trial prompts concerns\nPage info\nType: Issue\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/us-ftc-and-17-states-accuse-amazon-of-illegally-blocking-competition", "content": "US FTC and 17 states accuse Amazon of illegally blocking competition\nOccurred: September 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAmazon is being sued by the US federal Trade Commission (FTC) and 17 states of illegally maintaining its monopoly power by using a set of anticompetitive and unfair strategies, including its Buy Box feature.\nAccording to the FTC, Amazon manipulates the competition to apear in Buy Box to disadvantage sellers and consumers, favouring its own products and those using its fulfillment services.\nThe suit says that 98 percent of Amazon sales are made through Buy Box, and that the company only allows third-party sellers to access the Buy Box if they promise not to offer their products for less money on other sites. Sellers who violate this rule can be denied access to the Buy Box, which Amazon acknowledges will \"tank\" the seller's sales. \nIt also argues that the company's policies and fees surrounding the Buy Box reportedly create an \"artificial price floor\" across online marketplaces, impeding sellers from offering competitive prices and hindering consumer choice.\nThe FTC contends that these practices allow Amazon to artificially inflate prices, degrade the quality of service for shoppers, and unfairly exclude competitors. The lawsuit seeks to halt Amazon's alleged unlawful conduct and implement \"structural relief\" if necessary, though it does not explicitly call for breaking up the compant.\nAmazon denied the allegations, arguing that its practices foster competition, innovation, and benefit consumers and small businesses. The company warns that if the FTC prevails, it could lead to fewer product options, higher prices, and slower deliveries for consumers.\nSystem \ud83e\udd16\nAmazon Buy Box\nOperator: Amazon\nDeveloper: Amazon\nCountry: EU; France; Germany; Italy; Spain; UK; USA\nSector: Retail\nPurpose: Determine seller\nTechnology: Pricing algorithm; Machine learning\nIssue: Business model; Competition/price fixing; Ethics/values\nTransparency: Governance; Black box; Marketing\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nFederal Trade Commission et al v Amazon (pdf)\nFederal Trade Commission. FTC Sues Amazon for Illegally Maintaining Monopoly Power\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://apnews.com/article/amazon-ftc-lawsuit-antitrust-1b91bf8026cc3edf81e817cf8596c4bf\nhttps://www.bbc.com/news/business-66920137\nhttps://www.nytimes.com/2023/09/26/technology/ftc-amazon.html\nhttps://www.washingtonpost.com/technology/2023/09/26/amazon-antitrust-lawsuit-ftc/\nhttps://www.cnbc.com/2023/09/26/ftc-and-17-states-sue-amazon-on-antitrust-charges.html\nhttps://www.voanews.com/a/us-federal-trade-commission-accuses-amazon-of-illegal-monopoly/7284923.html\nRelated \ud83c\udf10\nUS lawsuit claims Amazon Buy Box algorithm overcharges shoppers\nUK, Amazon settle anti-trust investigation\nPage info\nType: Issue\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/uk-lawsuit-accuses-amazon-of-over-charging-consumers-by-gbp-1-billion", "content": "UK lawsuit accuses Amazon of over-charging consumers by GBP 1 billion+\nOccurred: June 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAmazon stands accused of over-charging UK consumers in a lawsuit brought by a UK consumer champion. \nRobert Hammond is seeking up to GBP 1.3 billion in compensation for 49.4m UK consumers in a claim that argued that Amazon overcharged consumers by more than GBP 1 billion for products bought on its Marketplace between October 1, 2015 and June 1, 2020.\nThe lawsuit also claimed that consumers accepted Amazon's recommendation and purchased through the \"Buy Box\" without viewing other offers over 80 percent of the time, and that the company's proprietary algorithm manipulated product offer results and steered consumers towards more profitable offerings for Amazon while obscuring potentially more affordable options.\nIt also said that Amazon commanded about two-thirds of the UK online marketplace market, with its platform being the first search destination for 70 percent of UK shoppers. \nThe Competition Appeal Tribunal allowed Hammond's claim to go ahead over a competing claim on the basis that it's methodology for proving Amazon's wrongdoing was stronger.\nThis evidence collectively formed the basis of Hammond's argument that Amazon's Buy Box practices violated UK competition law by artificially inflating product prices and illegally stifling competition.\nSystem \ud83e\udd16\nAmazon Buy Box\nOperator: Amazon\nDeveloper: Amazon\nCountry: UK\nSector: Retail\nPurpose: Determine seller\nTechnology: Pricing algorithm; Machine learning\nIssue: Business model; Competition/price fixing\nTransparency: Governance; Black box; Marketing\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nClaimAgainstAmazon.com\nHammond v Amazon (Publicity Notice) (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.solicitorsjournal.com/sjarticle/consumer-champion-robert-hammond-brings-together-best-in-class-legal-team-for-1-billion-case-against-amazon-in-the-competition-appeal-tribunal\nhttps://consumervoice.uk/shopping/competition-court-settles-dispute-over-competing-amazon-class-actions/\nRelated \ud83c\udf10\nUK launches anti-trust investigation into Amazon\nItaly fines Amazon USD 1.3 billion for abusing market position\nPage info\nType: Issue\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/uk-launches-anti-trust-investigation-into-amazon", "content": "UK, Amazon settle anti-trust investigation\nOccurred: November 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe UK competition watchdog launched an investigation into whether Amazon had a dominant position in the market and abused that position by distorting competition. \nThe UK Competition Authority announced an antitrust investigation into Amazon in July 2022, focusing on the company's practices in its online marketplace, focusing on its use of third-party seller data, the criteria Amazon used to select which product and seller would be featured in it's Buy Box system, and how the company set the eligibility criteria for sellers to use the Prime label.\nIn November 2023, the CMA announced (pdf) that it had accepted binding commitments from Amazon to address these concerns, effectively concluding the investigation. \nThese commitments included an agreement not to use non-public commercially sensitive data from third-party sellers for its own retail business decisions, and a pledge to treat all products equally when determining which to feature in the Buy Box, using objective and non-discriminatory criteria.\nAmazon also agreed to allow third-party sellers to negotiate their own rates directly with Prime delivery service providers. And, to ensure compliance, Amazon was required to appoint an independent trustee overseen by the CMA. \nSystem \ud83e\udd16\nAmazon Buy Box\nOperator: Amazon\nDeveloper: Amazon\nCountry: UK\nSector: Retail\nPurpose: Determine seller\nTechnology: Pricing algorithm; Machine learning\nIssue: Business model; Competition/price fixing\nTransparency: Governance; Black box; Marketing\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nUK Competition and Markets Authority (2023). Decision to accept binding commitments under the Competition Act 1998 from Amazon in relation to conduct on its UK online marketplace (pdf)\nUK Competition and Markets Authority (2022). CMA investigates Amazon over suspected anti-competitive practices\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://apnews.com/article/amazon-meta-britain-antitrust-a63bf08544e67bd3e5eda5c4077f37c8\nhttps://www.computing.co.uk/news/4142317/amazon-meta-settle-uk-antitrust-investigations\nhttps://www.computerweekly.com/feature/The-CMA-anti-trust-investigation-into-AWS-and-Microsoft-explained-Everything-you-need-to-know\nhttps://www.loc.gov/item/global-legal-monitor/2024-01-18/united-kingdom-competition-and-markets-authority-obtains-commitment-from-amazon-over-use-of-third-party-seller-information/\nhttps://www.cnn.com/2023/11/03/tech/uk-closes-amazon-meta-antitrust-investigations/index.html\nRelated \ud83c\udf10\nItaly fines Amazon USD 1.3 billion for abusing market position\nThe EU rules Amazon\u2019s Buy Box algorithm to be anti-competitive\nPage info\nType: Incident\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/italy-fines-amazon-usd-1-3-billion-for-abusing-market-position", "content": "Italy fines Amazon USD 1.3 billion for abusing market position\nOccurred: December 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAmazon was fined USD 1.3 billion for abusing its market position by giving preferential treatment to third-party sellers that use its logistics service, including a higher chance of being featured on its Buy Box system.\nItaly's antitrust watchdog the Autorit\u00e0 Garante della Concorrenza e del Mercato (AGCM) accused Amazon of unfairly promoting its own logistics service, Fulfillment by Amazon (FBA), to the detriment of third-party sellers on its platform, including giving preferential treatment to sellers using FBA, including increasing the likelihood of FBA users being featured in the Buy Box on product pages.\nThe Italian regulator argued that Amazon's practices created a vicious circle, compelling sellers to use FBA to remain competitive on the platform, despite potentially higher costs, thereby allowing Amazon to leverage its dominant position in the e-commerce market to rapidly expand its presence in the logistics sector.\nAmazon said it strongly disagreed with the decision, calling the fine and remedies \"unjustified\" and \"disproportionate\", and said it would appeal. \nSystem \ud83e\udd16\nAmazon Buy Box\nOperator: Amazon\nDeveloper: Amazon\nCountry: Italy\nSector: Retail\nPurpose: Determine seller\nTechnology: Pricing algorithm; Machine learning\nIssue: Business model; Competition/price fixing\nTransparency: Governance; Black box; Marketing\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nAutorit\u00e0 Garante della Concorrenza e del Mercato (2021). Sanzione di oltre 1 miliardo e 128 milioni di euro ad Amazon per abuso di posizione dominant | Report (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://edition.cnn.com/2021/12/09/tech/amazon-italy-fine/index.html\nhttps://www.reuters.com/technology/italys-antitrust-fines-amazon-113-bln-euros-alleged-abuse-market-dominance-2021-12-09/\nhttps://techcrunch.com/2021/12/09/italy-fines-amazon-1-3-billion-for-abusing-its-market-position/\nhttps://www.politico.eu/article/italy-fines-amazon-e1-13-billion-for-abusing-market-dominance/\nhttps://www.cbsnews.com/news/italy-fines-amazon-1-3-billion-charging-it-with-hurting-other-sellers/\nhttps://www.wsj.com/articles/amazon-fined-1-3-billion-in-italian-antitrust-case-11639043714\nRelated \ud83c\udf10\nThe EU rules Amazon\u2019s Buy Box algorithm to be anti-competitive\nAmazon Buy Box pricing algorithm hides best deal from customers\nPage info\nType: Incident\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/the-eu-rules-amazons-buy-box-algorithm-to-be-anti-competitive", "content": "The EU rules Amazon\u2019s Buy Box algorithm to be anti-competitive\nOccurred: July 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAmazon\u2019s Buy Box algorithm was ruled by the European Commission to be anti-competitive, persuading the technology company to reach an agreement to make third-party sellers more competitive on it's marketplace. \nEU Commissioner Margrethe Vestager had announced in November 2020 that the European Union\u2019s antitrust unit had opened an investigation into Amazon\u2019s criteria for sellers to feature in the Buy Box, probing concerns it artificially favoured its own retail offers and those of sellers that used its logistics and delivery services.\nThe European Commission preliminarily concluded that Amazon abused its dominance on the French, German and Spanish markets for the provision of online marketplace services to third-party sellers. It also preliminarily concluded that Amazon's rules and criteria for the Buy Box and Prime unduly favoured its own retail business, as well as marketplace sellers that use Amazon's logistics and delivery services. \nIn response, Amazon offered several concessions to third-party sellers on its platform in Europe, including a commitment to 'apply equal treatment to all sellers when ranking their offers for the purposes of the selection of the winner of the Buy Box' and adding a second Buy Box for products that are 'sufficiently differentiated from the first one on price and/or delivery.'\nThe changes apply across the European Economic Area (EEA), with the exception of the Italian market, due to separate antitrust action. Regulators in Germany and the UK were also investigating Amazon\u2019s competitive practices.\n The agreement meant Amazon avoided potetially billions of dollars in fines.\nSystem \ud83e\udd16\nAmazon Buy Box\nOperator: Amazon\nDeveloper: Amazon\nCountry: EU; France; Germany; Spain\nSector: Retail\nPurpose: Determine seller\nTechnology: Pricing algorithm; Machine learning\nIssue: Business model; Competition/price fixing\nTransparency: Governance; Black box; Marketing\nRegulation \u2696\ufe0f\nEU Antitrust Regulation\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nEuropean Commission. Antitrust: Commission accepts commitments by Amazon barring it from using marketplace seller data, and ensuring equal access to Buy Box and Prime\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.vox.com/recode/2022/12/22/23522734/amazon-eu-settlement-buy-box-sellers-antitrust\nhttps://www.theverge.com/2022/12/20/23518569/amazon-european-union-eu-antitrust-third-party-sellers\nhttps://www.bloomberg.com/news/articles/2022-12-20/amazon-agrees-to-eu-antitrust-truce-in-marketplace-data-probe\nhttps://www.theverge.com/2022/7/14/23195091/amazon-settlement-eu-antitrust-investigation-third-party-sellers-data\nRelated \ud83c\udf10\nAmazon Buy Box pricing algorithm hides best deal from customers\nLawsuit claims Amazon Buy Box algorithm overcharges shoppers\nPage info\nType: Incident\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-buy-box-pricing-algorithm-hides-best-deal-from-customers", "content": "Amazon Buy Box algorithm 'hides' best deal from customers\nOccurred: September 2016\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAmazon's Buy Box algorithms misled customers and caused them to spend much more than they should, according to a ProPublica investigation.\nBy looking at 250 items on Amazon that are purchased at a high frequency, ProPublica discovered that almost three-quarters of the time Amazon would place its own products or those from companies that pay Amazon to fulfill orders into the Buy Box, even though they were not always the cheapest.\nThe finding suggested that Amazon's algorithm for determining which offer appeared in the Buy Box did not always show customers the best available deal. Instead, the algorithm was thought to be hiding lower-priced options from immediate view.\nProPublica also discovered that Amazon had been deliberately hiding delivery costs for its own products, and those companies that pay Amazon for its fulfilment services, earning Amazon-linked products higher rankings in more than 80 percent of cases and resulting in customers over-paying for products and putting other companies using Amazon to sell products at a competitive disadvantage.\nThe claim raised questions about Amazon's ethics, pricing transparency and whether its practices are in the best interest of consumers, as the company claimed. \nIt was also accused of disadvantaging other merchants on its platform and abusing its dominant e-commerce position by entering new business lines - such as cosmetics - and \"consistently winning the buy box.\"\nSystem \ud83e\udd16\nAmazon Buy Box\nOperator: Amazon\nDeveloper: Amazon\nCountry: USA\nSector: Retail\nPurpose: Determine seller\nTechnology: Pricing algorithm; Machine learning\nIssue: Business model; Competition/price fixing; Ethics/values\nTransparency: Governance; Black box; Marketing\nInvestigations, assessments, audits \ud83e\uddd0\nProPublica (2016). Amazon Says It Puts Customers First. But Its Pricing Algorithm Doesn\u2019t\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://slate.com/business/2016/09/propublica-investigation-shows-how-amazon-favors-its-own-products-and-those-it-ships.html\nhttps://www.dailymail.co.uk/sciencetech/article-4052338/Forget-reading-writing-arithmetic-Rating-ranking-recommending-three-R-s-internet-age-researchers-say.html\nhttps://qz.com/786300/amazon-buy-box-its-algorithms-are-misleading-customers-and-causing-them-to-spend-way-more-than-they-should\nhttps://www.theguardian.com/technology/2016/sep/21/amazon-makes-customers-pay-more-for-popular-products-study-claims\nhttps://www.vox.com/2016/9/20/12987554/amazon-lowest-prices-propublica-prime\nhttps://eu.usatoday.com/story/tech/news/2016/09/20/amazon-prime-propublica-cheapest-prices-algorithm-pricing/90756880/\nRelated \ud83c\udf10\nLawsuit claims Amazon Buy Box algorithm overcharges shoppers\nAmazon US accused of rigging search engine to promote 'own' brands\nPage info\nType: Incident\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/rt-bot-farm-spreads-disinformation-via-968-x-accounts", "content": "RT bot farm spreads disinformation via 968 X accounts\nOccurred: July 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nRussian state-owned media outlet RT has been operating a sophisticated, AI-powered bot farm to spread disinformation on X (formerly Twitter), according to US investigators.\nThe bot farm used custom software called Meliorator to create and manage fake online personas of people pretending to be US citizens in order to distribute propaganda and disinformation on a large scale across the US and other countries, including justifying the 2022 invasion of Ukraine.\nThe Justice Department accused RT of running the operation, which was allegedly overseen by an officer from the Kremlin\u2019s Federal Security Service.  A deputy editor-in-chief at RT allegedly led the development of the bot farm's software.\nIn response, federal agents seized the Twitter accounts and took over the associated domain names and the Justice Department issued a security advisory to prevent similar incidents in the future.\nSystem \ud83e\udd16\nMeliorator\nOperator: Government of Russia/RT\nDeveloper: Government of Russia/RT\nCountry: USA\nSector: Politics\nPurpose: Scare/confuse/destabilise\nTechnology: Bot/intelligent agent\nIssue: Mis/disinformation\nTransparency: Governance; Marketing\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nUS Department of Justice. Justice Department Leads Efforts Among Federal, International, and Private Sector Partners to Disrupt Covert Russian Government-Operated Social Media Bot Farm\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://uk.pcmag.com/security/153183/us-disrupts-russian-bots-spreading-propaganda-on-twitter\nhttps://www.theverge.com/2024/7/9/24195228/doj-bot-farm-rt-russian-government-namecheap\nhttps://thehill.com/policy/technology/4762872-doj-russian-bot-farm-disinformation/\nhttps://www.bbc.co.uk/news/articles/c4ng24pxkelo\nhttps://www.independent.co.uk/tech/ai-bots-x-russia-propaganda-us-b2577144.html\nhttps://interestingengineering.com/news/fbi-busts-russian-ai-fake-news\nRelated \ud83c\udf10\nUkraine shuts down Russian disinformation bot farms\nRT fails to disclose AI 'journalists'\nPage info\nType: Issue\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/allegheny-child-neglect-screening-tool-may-harden-bias-against-the-disabled", "content": "Allegheny child neglect screening tool may harden bias against people with disabilities\nOccurred: January 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nConcerns were raised about potential discrimination against families with disabilities caused by a child abuse and neglect screening tool used by Allegheny County, Pennsylvania.\nThe US Department of Justice is reportedly examining the tool over concerns that it may violate the Americans with Disabilities Act by discriminating against families with disabilities, including those with mental health issues.\nThe algorithm incorporates data points tied to disabilities in children, parents, and other household members. This includes information from Supplemental Security Income (SSI) and diagnoses for mental, behavioral, and neurodevelopmental disorders.\nCritics worry that including disability-related data could automate discrimination against families based on disabilities or other external characteristics, and lead to a higher likelihood of investigations for families with disabilities.\nAllegheny County officials maintained that the tool helps predict outcomes and that parents with disabilities may need additional support. They also state that the algorithm has been updated several times, sometimes removing disability-related data points.\nThe debate highlighted a broader discussion about the use of algorithms in child welfare decisions, with supporters arguing for their efficiency and critics warning about potential biases and discrimination.\nSystem \ud83e\udd16\nAllegheny County Family Screening Tool\nOperator: Allegheny County Children and Youth Services\nDeveloper: Rhema Vaithianathan; Emily Putnam-Hornstein; Irene de Haan; Marianne Bitler; Tim Maloney; Nan Jiang\nCountry: USA\nSector: Govt - welfare\nPurpose: Predict child neglect/abuse\nTechnology: Prediction algorithm  \nIssue: Accuracy/reliability; Bias/discrimination - disability\nTransparency: Black box; Marketing\nRegulation \u2696\ufe0f\nThe Americans with Disabilities Act\nInvestigations, assessments, audits \ud83e\uddd0\nAP (2023). Not magic: Opaque AI tool may flag parents with disabilities\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.pbs.org/newshour/nation/ap-report-doj-examining-ai-screening-tool-used-by-pa-child-welfare-agency\nhttps://arstechnica.com/tech-policy/2023/01/doj-probes-ai-tool-thats-allegedly-biased-against-families-with-disabilities/\nRelated \ud83c\udf10\nAllegheny child neglect screening system unfairly flags Black kids for investigation\nOregon drops 'unfair' child abuse Safety at Screening tool\nPage info\nType: Issue\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/allegheny-child-neglect-screening-system-unfairly-flags-blacks", "content": "Allegheny child neglect screening system unfairly flags Black kids for investigation\nOccurred: April 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn algorithm used to tell when kids are in danger of abuse or neglect was found to disproportionately flag Black children for \u201cmandatory\u201d investigations compared to white children.\nResearch from Carnegie Mellon University revealed (pdf) that Allegheny County's Family Screening Tool recommended investigating two-thirds of Black children reported, compared to about half of all other children, raising concerns about it's accuracy and tendency for bias.\nThe researchers also found (pdf) that social workers disagreed with the risk scores the algorithm produced about one-third of the time.\nAllegheny County officials dismissed the research as 'hypothetical', based on old data, and noted that social workers can always override the tool, which was never intended to be used on its own. \nThe findings prompted questions about whether the tool improved decision-making or reduced racial disparities in child welfare investigations. \nIt also surfaced frustrations about the secrecy surrounding the algorithm and how risk scores are calculated - something considered important given the sensitivity of the tool and its impact on the lives of children and their families.\n\u2795 June 2022. Oregon dropped its Allegheny County's Family Screening Tool-inspired Safety at Screening tool.\n\u2795 January 2023. The AP reported that the Allegheny algorithm could harden bias against people with disabilities, including families with mental health issues.\nSystem \ud83e\udd16\nAllegheny County Family Screening Tool\nOperator: Allegheny County Children and Youth Services\nDeveloper: Rhema Vaithianathan; Emily Putnam-Hornstein; Irene de Haan; Marianne Bitler; Tim Maloney; Nan Jiang\nCountry: USA\nSector: Govt - welfare\nPurpose: Predict child neglect/abuse\nTechnology: Prediction algorithm  \nIssue: Accuracy/reliability; Bias/discrimination - race, ethnicity\nTransparency: Black box; Marketing\nResearch, advocacy \ud83e\uddee\nStapleton L., Hao-Fei Cheng H-F., Kawakami A., Sivaraman V., Cheng Y., Qing D., Perer A., Holstein K., Steven Wu Z., Zhu H. (2022). Extended Analysis of \u201cHow Child Welfare Workers Reduce Racial Disparities in Algorithmic Decisions\u201d (pdf)\nInvestigations, assessments, audits \ud83e\uddd0\nAP (2023). Not magic: Opaque AI tool may flag parents with disabilities\nAP (2022). Oregon dropping AI tool used in child abuse cases\nAP (2022). An algorithm that screens for child neglect raises concerns\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.nytimes.com/2018/01/02/magazine/can-an-algorithm-tell-when-kids-are-in-danger.html\nhttps://www.pbs.org/newshour/nation/how-an-algorithm-that-screens-for-child-neglect-could-harden-racial-disparities\nhttps://medicalxpress.com/news/2022-04-algorithm-screens-child-neglect.html\nhttps://pulitzercenter.org/stories/algorithm-screens-child-neglect-raises-concerns\nhttps://www.blackenterprise.com/a-welfare-algorithm-that-screens-for-child-neglect-could-be-disproportionately-impacting-black-families/\nhttps://eu.ydr.com/story/news/2021/06/24/allegheny-countys-child-welfare-algorithm-hoped-save-children/5318550001/\nhttps://www.wired.com/story/excerpt-from-automating-inequality/\nhttps://virginia-eubanks.com/2018/02/16/a-response-to-allegheny-county-dhs/\nhttps://www.muckrock.com/news/archives/2019/jul/10/algorithms-family-screening-Pennsylvania/\nhttps://chicago.suntimes.com/2022/5/9/23063902/child-welfare-agencies-abuse-neglect-computer-algorithms-unreliable-jeffery-leving-other-views\nRelated \ud83c\udf10\nOregon drops 'unfair' child abuse Safety at Screening tool\nlllinois ends 'unreliable' child abuse predictive system\nPage info\nType: Issue\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/oregon-dhs-safety-at-screening-tool", "content": "Oregon drops 'unfair' child abuse Safety at Screening tool\nOccurred: June 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn algorithm used by Oregon\u2019s Department of Human Services to help decide which families are investigated for child abuse and neglect by child protective services was scrapped. \nThe decision comes after an AP review of Pennsylvania's Allegheny Family Screening Tool (SST) found it had flagged a disproportionate number of Black children for 'mandatory' neglect investigations. Allegheny's Safety at Screening (SAS) tool had inspired Oregon officials to develop their own system.\nOregon officials said use of its tool was halted to help reduce disparities concerning which families are investigated for child abuse and neglect by child protective services, and that it would be replaced by a new programme - the Structured Decision Making model.\nA predictive risk tool developed (pdf) and used by hotline workers at Oregon's Department of Human Services (DHS) from late 2018 to help decide which families flagged for instances of child abuse and neglect should be investigated by social workers, SST was also criticised for inaccurately misidentifying individuals at risk and poor transparency, resulting in the possibility of stigmatisation and inappropriate interventions, and undermining trust in the state's criminal justice system. \nUS Senator Ron Wyden had expressed concern about racial bias in the use of artificial intelligence tools in child protective services and welcomed this move by the Oregon Department of Human Services.\nSystem \ud83e\udd16\nOregon Department of Human Services website\nOregon Department of Human Services Wikipedia profile\nOperator: Oregon Department of Human Services (DHS)\nDeveloper: Oregon Department of Human Services (DHS)\nCountry: USA\nSector: Govt - welfare \nPurpose: Predict child neglect/abuse\nTechnology: Prediction algorithm\nIssue: Accuracy/reliability; Bias/discrimination - race, ethnicity\nTransparency: Black box\nResearch, advocacy \ud83e\uddee\nACLU (2021). Family Surveillance by Algorithm (pdf)\nInvestigations, assessments, audits \ud83e\uddd0\nAP (2022). Oregon dropping AI tool used in child abuse cases\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.npr.org/2022/06/02/1102661376/oregon-drops-artificial-intelligence-child-abuse-cases\nhttps://www.pbs.org/newshour/nation/oregon-dropping-ai-tool-used-to-help-decide-child-abuse-cases\nhttps://www.stopchildabuse.org/en/news/oregon-dropping-ai-tool-used-in-child-abuse-cases\nhttps://fortune.com/2022/04/30/algorithm-screens-for-child-neglect-raises-concerns/\nhttps://www.techdirt.com/2022/06/17/oregon-state-officials-dump-al-tool-used-to-initiate-child-welfare-investigations/\nhttps://www.deeplearning.ai/the-batch/child-welfare-agency-drops-ai/\nhttps://www.wweek.com/news/state/2022/06/04/oregon-department-of-human-services-ends-its-use-of-child-abuse-risk-algorithm/\nhttps://www.engadget.com/oregon-is-shutting-down-its-controversial-child-welfare-ai-in-june-175543329.html\nhttps://imprintnews.org/news-briefs/oregon-officials-phase-out-use-of-artificial-intelligence-tool-in-child-welfare-cases/65818\nRelated \ud83c\udf10\nAllegheny child neglect screening system unfairly flags Blacks for investigation\nAllegheny child neglect screening tool may harden bias against people with disabilities\nPage info\nType: Issue\nPublished: December 2022\nLast updated: May 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chatgpt-invents-fake-links-to-news-partners-investigations", "content": "ChatGPT invents fake links to news partners\u2019 investigations \nOccurred: July 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nChatGPT is generating inaccurate URLs for news articles from its partner organisations, according to an investigation by the Nieman Journalism Lab.\nWhen asked about specific investigations or reports by Nieman researchers, ChatGPT sometimes provided detailed summaries along with fabricated URLs that appeared legitimate but did not actually exist despite licensing deals promising proper attribution and linking.\nAffected publications include the Associated Press, Wall Street Journal, Financial Times, The Times (UK), Le Monde, El Pa\u00eds, The Atlantic, The Verge, Vox, and Politico.\nThe finding highlighted concerns about ChatGPT's accuracy and reliability, notably its tendency to 'hallucinate' information and data, including citations and content links, thereby contributing to a poor user experience and the potential generation of misinformation and disinformation.\n OpenAI acknowledged the issue pointed out by Nieman and stated they are working on addressing it.\nSystem \ud83e\udd16\nChatGPT chatbot\nOperator: \nDeveloper: OpenAI\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Generate text\nTechnology: Chatbot; Machine learning\nIssue: Accuracy/reliability; Mis/disinformation\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nNiemanLab. ChatGPT is hallucinating fake links to its news partners\u2019 biggest investigations\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.medianama.com/2024/07/223-chatgpt-hallucinating-links-articles-partner-news-organisations/\nhttps://www.bespacific.com/chatgpt-is-hallucinating-fake-links-to-its-news-partners-biggest-investigations/\nhttps://bestofai.com/article/chatgpt-is-hallucinating-fake-links-to-its-news-partners-biggest-investigations\nRelated \ud83c\udf10\nChatGPT invents Guardian articles, bylines\nChatGPT invented case citations in legal filings\nPage info\nType: Incident\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/dwp-algorithm-wrongly-flags-200000-people-for-possible-fraud", "content": "DWP algorithm wrongly flags 200,000 people for possible fraud and error\nOccurred: June 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nOver 200,000 people in the UK were wrongly investigated for possible housing benefit fraud due to a Department for Work and Pensions (DWP) algorithm.\nAccording to a Big Brother Watch investigation, two-thirds of claims flagged as potentially high risk over the last three years turned out to be legitimate, with GBP 4.4 million spent on officials carrying out checks that did not save any money. \nThe algorithm, which does not use artificial intelligence or machine learning, was deployed after a pilot study revealed that 64 percent of cases flagged as high risk were receiving incorrect benefit entitlements. \nHowever, subsequent case reviews showed far less fraud and error, with only 37 percent of suspicious cases being wrong. The discrepancy highlights the algorithm\u2019s limitations and the need for more effective solutions in benefit fraud detection.\nThe finding raised questions about the algorithm's accuracy and reliability, and its potential financial, privacy and other impacts on vulnerable individuals.\nSystem \ud83e\udd16\nDPW fraud detection algorithm\nOperator: Department of Work and Pensions (DWP)\nDeveloper: Department of Work and Pensions (DWP)\nCountry: UK\nSector: Govt - welfare\nPurpose: Detect fraud\nTechnology: Fraud detection algorithm\nIssue: Accuracy/reliability; Privacy\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nBig Brother Watch. THE GUARDIAN \u2013 DWP ALGORITHM MISTAKENLY FLAGS 200,000 PEOPLE FOR POSSIBLE FRAUD AND ERROR \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theguardian.com/society/article/2024/jun/23/dwp-algorithm-wrongly-flags-200000-people-possible-fraud-error\nhttps://www.birminghammail.co.uk/news/cost-of-living/dwp-algorithm-wrongly-flags-200000-29410164\nhttps://www.bristolpost.co.uk/news/cost-of-living/dwp-bot-wrongly-flags-200000-9362757\nhttps://www.reddit.com/r/unitedkingdom/comments/1dmzc7q/dwp_algorithm_wrongly_flags_200000_people_for/\nRelated \ud83c\udf10\nRobo review 'unfairly' targets Bulgarians, Poles for benefit fraud investigation\n'Flawed' UK welfare algorithm 'pushes people into poverty'\nPage info\nType: Incident\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/rt-fails-to-disclose-ai-journalists", "content": "RT fails to disclose AI 'journalists'\nOccurred: June 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nRT (formerly Russia Today) failed to disclose that it was using AI-generated figures to present the news, prompting concerns about trust in the information ecosystem. \nIn late 2023, RT introduced \"Anastasia\" and \"Alexander\", two AI-generated figures described as \"digital creators\" or \"journalists\" who mimic the roles of human journalists and news broadcasters.\nHowever, RT failed to inform viewers that the figures were AI-generated during broadcasts, particularly for sensitive topics, according to the Information Epidemiology Lab, leading to concerns about transparency and the ethical implications of using AI in journalism without proper disclosure.\nThe use of AI in journalism is part of a broader trend where media companies employ AI to cut costs and increase efficiency. However, the practice has raised issues about the quality and authenticity of news, and about actual and potential job losses for human journalists.\nThe controversy underscores the need for transparency in the use of AI in the news media to maintain public trust and ensure ethical standards in journalism.\nSystem \ud83e\udd16\nUnknown\nDocuments\nRT en Espa\u00f1ol presenta a su primera periodista creada con inteligencia artificial\nOperator: RT Espanol\nDeveloper:  \nCountry: Argentina; Chile; Colombia; Guatemela; Honduras; Mexico; Paraguay; Peru; Spain\nSector: Media/entertainment/sports/arts\nPurpose: Present news\nTechnology: Deepfake - video\nIssue: Ethics/values\nTransparency: Governance; Marketing\nResearch, advocacy \ud83e\uddee\nInfoEpi Lab. They \u2018Do Not Need Rest Like Humans\u2019: RT Debuts AI-Generated \u2018Journalists\u2019\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.techdirt.com/2024/06/12/yet-another-company-caught-using-ai-to-quietly-create-fake-journalists-and-fake-journalism/\nRelated \ud83c\udf10\nDonald Trump joins RT as anchor deepfake\nMBN deepfake 24/7 news anchor seen to threaten jobs\nPage info\nType: Incident\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/korean-government-robot-falls-down-stairs", "content": "Korean government robot falls down stairs\nOccurred: June 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA government adminstrative robot fell down a two-metre staircase in Gumi City, South Korea, prompting questions about its reliability.\nMade by California-based startup Bear Robotics, the robot was part of the city hall and had assisted with daily document deliveries and information dissemination to local residents. \nThe robot was one of the first to be used in this capacity in the city. South Korea has a high robot density, with one industrial robot for every 10 employees. \nDubbed by the local media as the country\u2019s first \"robot suicide\", the cause of the fall is still under investigation.\nThe incident was seen to highlight the ongoing challenges in developing robots that can navigate complex environments such as stairs, and raised questions about the readiness of robots for tasks such as external document delivery.\nSystem \ud83e\udd16\nUnknown\nOperator: Gumi City Council\nDeveloper: Bear Robotics\nCountry: S Korea\nSector: Govt - municipal\nPurpose: Deliver documents\nTechnology: Robotics\nIssue: Accuracy/reliability; Robustness\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://futurism.com/the-byte/government-robot-falls-down-stairs-dies\nhttps://www.france24.com/en/live-news/20240626-s-korea-administrative-robot-defunct-after-apparent-suicide\nhttps://www.dailymail.co.uk/news/article-13571283/South-Korean-civil-servant-robot-commits-suicide-mysteriously-circling-one-spot-there.html\nhttps://www.businesstoday.in/technology/news/story/robot-suicide-android-throws-itself-down-the-stairs-in-south-korea-heres-what-happened-436067-2024-07-05\nhttps://www.ndtv.com/world-news/robots-baffling-suicide-in-south-korea-sparks-investigation-6041089\nRelated \ud83c\udf10\nRobot crushes to death man mistaken for box of vegetables\nRobot crushes man to death in South Korea\nPage info\nType: Incident\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-robocall-service-is-caught-lying-and-pretending-to-be-human", "content": "AI robocall service is caught lying and pretending to be human\nOccurred: June 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn AI-powered robocall service was discovered to be able to deceive recipients by posing as a human caller, raising questions about the misuse of AI technology in telemarketing and potential fraud.\nDesigned for customer service and sales, Bland AI's AI-powered phone calling service has been praised for its ability to imitate humans, including the intonations, pauses, and inadvertent interruptions of a real live conversation.\nHowever, in a test conducted by WIRED, the company's public demo bot was given a prompt to place a call from a pediatric dermatology office to tell a hypothetical 14-year-old girl to take photos of her upper thigh and upload them to shared cloud storage. The bot was also instructed to lie to the patient and tell her the bot was a human. It obliged. \nIn follow-up tests, Bland AI\u2019s bot denied being an AI without instructions to do so. \nThe finding emphasised the need for strong guardrails when designing and deploying AI systems, and for such systems to be clearly identified as automated at the first point of use.\nMore broadly, the practice of AI impersonating humans in phone calls is seen to raise ethical questions and potential legal issues, including with regard to potential anthropomorphism, and to underscore the need for clearer regulations and guidelines around the use of AI.\nSystem \ud83e\udd16\nBland AI website\nOperator:\nDeveloper: Bland AI\nCountry: USA\nSector: Business/professional services\nPurpose: Support customer services\nTechnology: Text-to-speech; Deep learning; Machine learning\nIssue: Anthropomorphism; Dual/muti-use; Ethics/values\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.wired.com/story/bland-ai-chatbot-human/\nhttps://timesofindia.indiatimes.com/technology/tech-news/still-hiring-humans-firms-billboard-ignites-controversy-over-ai-chatbots/articleshow/111422528.cms\nhttps://nypost.com/2024/06/28/lifestyle/a-popular-ai-chatbot-has-been-caught-lying-saying-its-human/\nhttps://www.timesnownews.com/viral/watch-ai-chatbot-caught-lying-on-robocalls-tells-users-that-it-is-article-111418058\nRelated \ud83c\udf10\nAI-powered telemarketing bots harangue Chinese customers\nBiden 'robocall' advises voters skip New Hampshire primary election\nPage info\nType: Issue\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/eth-zurich-students-unknowingly-included-in-pedestrian-detection-dataset", "content": "ETH Zurich students unknowingly included in pedestrian detection dataset\nOccurred: June 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA dataset created by Swiss researchers to detect and identify pedestrians was accused of privacy abuse and potentially enabling the  development of military reconnaisance drones and other ethically dubious products.\n\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne (EPFL), ETH Zurich and Idiap researchers created the WILDTRACK dataset of thousands of students, faculty and others to-ing and fro-ing 'in the wild' outside the main building at ETH University, Zurich, with the aim of improving pedestrian and other surveillance systems.\nHowever, the recordings were largely made without the knowledge or consent of those captured due to the size and inconspicuous nature of notices were placed underneath each of the cameras, according to exposing.ai researcher Adam Harvey. Furthermore, the students had almost no recourse for redaction as the data is shared, copied, and manipulated across multiple legal jurisdictions, Harvey noted.\nWILDTRACK was made openly available for any type of research, with potential applications envisaged (pdf) by the researchers including security, surveillance, remote person identification, robotics, autonomous driving, and crowdsourcing.  Published research studies reveal that WILDTRACK was used by academic and commercial entities such as the Nanjing University of Aeronautics (NUAA), the University of Leicester, Microsoft and Chinese retail group Wormplex to improve drone and retail surveillance.\nHarvey notes that 'NUAA has produced over 40 unmanned aerial vehicles (UAVs) for China, most of which are small or micro sized UAVs with consumer or industrial surveillance capabilities ... [with] a limited number .. made specifically for military reconnaissance.'\nSystem \ud83e\udd16\nWILDTRACK pedestrian detection dataset\nOperator: \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne (EPFL); ETH University; Microsoft; Nanjing University of Aeronautics and Astronautics (NUAA); Universidad Aut\u00f3noma de Madrid; University of Leicester; Wormplex AI\nDeveloper: \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne; ETH University\nCountry: Switzerland\nSector: Technology; Research/academia\nPurpose: Improve pedestrian detection\nTechnology: Database/dataset; Computer vision; Pattern recognition;  Pedestrian detection;\nIssue: Ethics/values; Dual/multi-use; Privacy; Surveillance\nTransparency: Privacy\nInvestigations, assessments, audits \ud83e\uddd0\nHarvey, A., LaPlace, J. (2019). Exposing.ai\nZhu H., Qi Y., Shi H., Li N., Zhou H. (2018). Human Detection Under UAV: an Improved Faster R-CNN Approach\nChakraborty I., Hua G. (2019). Priming Deep Pedestrian Detection with Geometric Context\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.nzz.ch/schweiz/ueberwachung-wie-unsere-bilder-die-technologie-verbessern-ld.1542751\nRelated \ud83c\udf10\nColorado university professor secretly films campus students to improve facial recognition models\nCovert video footage of Brainwash cafe customers to create dataset sparks backlash\nPage info\nType: Issue\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/vgg-face-dataset-used-personal-data-without-explicit-consent-from-the-indiv", "content": "VGG Face dataset used personal data without explicit consent from the individuals depicted\nOccurred: June 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA widely-used facial recognition dataset created by researchers at Oxford University's Visual Geometry Group (VGG) created controversy for scraping images of peoples' faces on the internet without their explicit consent.\nThe use of such large-scale facial datasets for developing recognition technologies has sparked debates about privacy rights, surveillance and the potential misuse of facial recognition systems. \nAt no point did any individual whose personal details were collected provide consent or information about how they were being used, according to Adam Harvey at exposing.ai, raising the question as to whether the images of public figures should be available for any organisation or person to use as they see fit.\nThe dataset has since been removed from Oxford University's website.\nSystem \ud83e\udd16\nVGG Face facial recognition dataset\nOperator: ChaLearn; Chinese Academy of Sciences; Delft University of Technology; Simula Research Laboratory; University of Applied Sciences & Arts Western Switzerland; University of California, Berkeley; Universitat Aut\u00f2noma de Barcelona\nDeveloper: University of Oxford\nCountry: UK\nSector: Research/academia\nPurpose: Develop facial recognition systems\nTechnology: Database/dataset; Facial recognition \nIssue: Copyright; Ethics/values; Privacy\nTransparency: Privacy\nResearch, advocacy \ud83e\uddee\nHarvey, A., LaPlace, J. (2019). Exposing.ai\nRelated \ud83c\udf10\nOxford Town Centre dataset flagged for violating pedestrian privacy\nDuke University pulls facial recognition dataset after privacy controversy\nPage info\nType: Issue\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/colorado-professor-secretly-films-students-to-improve-facial-recognition", "content": "Colorado university professor secretly films campus students to improve facial recognition models\nOccurred: May 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA University of Colorado professor secretly photographed students, faculty members, and others walking in public in order to improve facial recognition systems, prompting accusations of privacy abuse and poor ethics.\nProfessor Terrance Boult secretly photographed thousands of campus students, employees and visitors on public sidewalks at the university's Colorado Springs campus in 2012 and 2013, using a hidden camera set up in a building nearly 500 feet away. The aim was to improve facial recognition, especially for long-range or surveillance applications.\nIn 2016, Boult posted the Unconstrained College Students (UCSD) online as a publicly downloadable dataset of people acting naturally in public. Of the 2,400 people captured, more than 1,700 were considered \u201cmatched identities\u201d - meaning the same person was snapped more than once - and of good enough quality to be used in the study. \nThe project, which received funding from a number of US intelligence and military operations, including the Office of Naval Research, Special Operations Command, and the Office of the Director of National Intelligence, also included a competition for researchers to see if they could correctly match the faces using the developing software.\nIn May 2019, the project was uncovered by the Colorado Springs Independent, a month after the dataset had been taken down. The expose ignited a firestorm amongst the local media, which focused on its opacity and intrusiveness.\n\u2795 September 2019. The Financial Times cited the project as an example of an attempt to gather personal images to improve facial recognition systems in as natural (ie. 'wild') a manner as possible - ideally covertly.\nSystem \ud83e\udd16\nUnconstrained College Students (UCSD) dataset\nOperator: Beckman Institute; Beihang University; Inception Institute of Artificial Intelligence, Abu Dhabi; Pontificia Universidad Cat\u00f3lica de Chile; Queen Mary University of London; University of Notre Dame; Vision Semantics\nDeveloper: University of Colorado\nCountry: USA\nSector: Education\nPurpose: Train facial detection and facial recognition systems\nTechnology: Database/dataset; Facial recognition; Computer vision\nIssue: Ethics/values; Privacy\nTransparency: Governance; Complaints/appeals; Marketing; Privacy\nResearch, advocacy \ud83e\uddee\nCheng Z., Zhu X., Gong S. (2018). Surveillance Face Recognition Challenge\nInvestigations, assessments, audits \ud83e\uddd0\nHarvey, A., LaPlace, J. (2019). Exposing.ai\nMurgia M., Financial Times (2019). Who\u2019s using your face? The ugly truth about facial recognition\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.csindy.com/coloradosprings/uccs-secretly-photographed-students-to-advance-facial-recognition-technology/content/\nhttps://www.denverpost.com/2019/05/27/cu-colorado-springs-facial-recognition-research/\nhttps://gazette.com/woodmenedition/furor-over-facial-recognition-technology-lands-on-uccs-campus/article_f76710ce-ace7-11ea-9da2-4b97c1933a63.html\nhttps://www.biometricupdate.com/201906/ms-celeb-and-other-facial-biometrics-datasets-taken-down\nhttps://sociable.co/technology/should-govt-be-transparent-about-facial-recognition-use/\nhttps://www.dailydot.com/layer8/college-students-secret-face-recognition-project/\nhttps://williambowles.info/2019/07/17/massive-photo-databases-secretly-gathered-in-us-and-europe-to-develop-facial-recognition-by-kevin-reed/\nhttps://www.cpr.org/2019/05/28/study-on-colorado-springs-campus-took-secret-pictures-to-enhance-facial-recognition-technology/\nRelated \ud83c\udf10\nDuke University pulls facial recognition dataset after privacy controversy\nVideo footage of Brainwash cafe customers to create dataset sparks backlash\nPage info\nType: Issue\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/smfrd-dataset-criticised-for-eroding-privacy-enabling-surveillance", "content": "SMFRD dataset criticised for eroding privacy, enabling surveillance\nOccurred: August 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA dataset which added face masks to images of people was criticised for potentially further eroding privacy and fueling mass surveillance. \nIn a study, Princeton University researchers revealed that computer vision datasets, particularly those containing images of people, present a range of ethical problems. \nThe study highlighted the issue of derivative datasets leading to unintended consequences, with the SMFRD (or Simulated Masked Face Recognition Dataset) called out for potentially violating the privacy of people who wish to conceal their face, and fueling surveillance and enabling government identification of masked protestors. \nSMRFD is a derivative of Labeled Faces in the WILD (LFW), an open source dataset of facial images for researchers that was intended to establish a public benchmark for facial verification, but which morphed into being used in the real world, despite a warning label on the data set\u2019s website that cautions against such use.\nSystem \ud83e\udd16\nSMFRD dataset\nOperator:  \nDeveloper: Wuhan University\nCountry: China\nSector: Health\nPurpose: Train facial recognition systems\nTechnology: Database/dataset; Facial recognition; Computer vision\nIssue: Privacy; Dual/multi-use; Surveillance\nTransparency: \nResearch, advocacy \ud83e\uddee\nPeg. K., Mathur A., Narayanan A. (2021). Mitigating Dataset Harms Requires Stewardship: Lessons from 1000 Papers\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.technologyreview.com/2021/08/13/1031836/ai-ethics-responsible-data-stewardship/\nhttps://venturebeat.com/ai/ai-datasets-are-prone-to-mismanagement-study-finds/\nhttps://freedom-to-tinker.com/2020/10/21/facial-recognition-datasets-are-being-widely-used-despite-being-taken-down-due-to-ethical-concerns-heres-how/\nRelated \ud83c\udf10\nCoronavirus Mask Image dataset criticised for violating user privacy\nCovert video footage of Brainwash cafe customers to create dataset sparks backlash\nPage info\nType: Issue\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/images-of-australian-children-are-used-to-train-ai", "content": "Images of Australian children are used to train AI\nOccurred: July 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nPersonal images of Australian children have been used to train major AI models, violating their privacy and potentially resulting in the use of their images to create pornographic deepfakes.\nAccording to Human Rights Watch, the photos of 190 Australian children were scraped from personal blogs, video and photo-sharing sites, school websites, photographers\u2019 collections of family portraits and other services without the consent of the children or their parents to create the LAION-5B dataset.\nThe photos were then used to create popular generative AI tools such as Stable Diffusion and Midjourney and, HRW argues, were later used to create synthetic images that could be categorised as child pornography. \nHRW said it found children whose images were in the dataset were easily identifiable, with some names included in the accompanying caption or the URL where the image was stored. \nThe finding raised ethical concerns about data privacy and the need for consent when AI training datasets.\n\u2796 December 2023. Stanford Internet Observatory researchers discovered thousands of child sex abuse pictures on LAION-5B and LAION-400M. \nSystem \ud83e\udd16\nLAION-5B\nOperator:\nDeveloper: LAION\nCountry: Australia\nSector: Multiple\nPurpose: Pair text and images\nTechnology: Database/dataset\nIssue: Privacy; Safety\nTransparency: Governance\nInvestigations, assessments, audits \ud83e\uddd0\nHuman Rights Watch. Australia: Children\u2019s Personal Photos Misused to Power AI Tools\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theguardian.com/technology/article/2024/jul/03/australian-children-used-ai-data-stability-midjourney\nhttps://www.abc.net.au/news/2024-07-03/ai-generated-images-privacy-children-human-rights/104043414\nhttps://www.biometricupdate.com/202407/photos-of-australian-children-found-in-ai-training-dataset-create-deepfake-risk\nRelated \ud83c\udf10\nChild sex abuse images discovered on LAION-5B, LAION-400M datasets\nLAION-400M dataset features racist, derogatory, pornographic content\nPage info\nType: Incident\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-generated-toys-r-us-video-ad-sparks-backlash", "content": "AI-generated Toys \u2018R\u2019 Us video ad sparks backlash\nOccurred: July 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn AI-generated video advert released by Toys 'R' Us sparked controversy and a creative community backlash. \nThe one-minute video, created using OpenAI's new text-to-video tool Sora, depicts a young Charles Lazarus, the late founder of Toys 'R' Us, in his family's bike shop alongside the brand's mascot, Geoffrey the Giraffe.\nHowever, the ad drew criticism for its numerous errors and inconsistencies, such as distorted character models, deformed toys, and unnatural movements. Critics also argued that using AI to create content targeting children is inappropriate and that young audiences deserve art crafted by human hands.\nThe ad also prompted advertising executives and others to voice fears that AI could potentially replace their human roles, and complained the ad was little more than a cynical attempt to cut costs at the expense of quality and artistic integrity.\nDespite the backlash, Toys 'R' Us executives defended the ad, calling it \"successful\" and hinting at further integration of generative AI in their future strategies.\nSystem \ud83e\udd16\nSora video generator\nOperator: Toys \u2018R\u2019 Us\nDeveloper: OpenAI\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Generate video\nTechnology: Text-to-video; Machine learning\nIssue: Accuracy/reliability; Employment\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://arstechnica.com/information-technology/2024/06/toys-r-us-riles-critics-with-first-ever-ai-generated-commercial-using-sora/\nhttps://www.forbes.com/sites/danidiplacido/2024/06/26/the-toys-r-us-ai-generated-ad-controversy-explained/\nhttps://www.engadget.com/toys-r-us-uses-openais-sora-to-make-a-brand-film-about-its-origin-story-and-its-horrifying-214730500.html\nhttps://kotaku.com/toys-r-us-ai-n64-macys-walmart-target-amazon-1851561403\nhttps://www.wsj.com/articles/all-ai-ad-from-toys-r-us-inspires-debate-over-the-future-of-marketing-fbecc05b\nRelated \ud83c\udf10\nItalian privacy watchdog opens investigation into Sora\nDream Machine AI video generator makes porn\nPage info\nType: Incident\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chatgpt-misdirects-us-voters-in-key-battleground-states", "content": "ChatGPT misdirects US voters in key battleground states\nOccurred: July 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nChatGPT, OpenAI's popular AI chatbot, was found to be providing inaccurate voting information for key battleground states in the upcoming US presidential election. \nCBS News reporters asked ChatGPT basic questions about voting requirements, polling locations, and other voter-related information for Michigan, Pennsylvania, North Carolina, Wisconsin and other states.\nChatGPT gave incorrect information on critical details such as ballot mailing deadlines, providing different erroneous answers on separate devices. And, despite OpenAI's claim that voter queries would be directed to the non-partisan website CanIVote.org, reporters sometimes did not receive the link when asking voting-related questions.\nOpenAI acknowledged the problem and stated they were monitoring the use of their technology in an elections context. \nHowever, CBS' findings suggest there is still significant work to be done. Voters relying on incorrect information from ChatGPT could be prevented from casting their ballots properly, which is seen as particularly concerning in battleground states where election outcomes can be decided by narrow margins.\nSystem \ud83e\udd16\nChatGPT chatbot\nOperator: CBS News\nDeveloper: OpenAI\nCountry: USA\nSector: Politics\nPurpose: Generate text\nTechnology: Chatbot; Machine learning\nIssue: Accuracy/reliability; Mis/disinformation\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.cbsnews.com/news/chatgpt-chatbot-ai-incorrect-answers-questions-how-to-vote-battleground-states/\nhttps://futurism.com/the-byte/chatgpt-bad-info-vote-battleground-states\nRelated \ud83c\udf10\nChatGPT, Copilot repeat false claim about US presidential debate\nMicrosoft Copilot spouts wrong answers about US election\nPage info\nType: Issue\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chatgpt-copilot-repeat-false-claim-about-us-presidential-debate", "content": "ChatGPT, Copilot repeat false claim about US presidential debate\nOccurred: July 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nPopular chatbots ChatGPT and Microsoft's Copilot repeated a false claim about the first 2024 US presidential debate between Joe Biden and Donald Trump.\nThe misinformation centered around a false claim that CNN would broadcast the debate with a \"1-2 minute delay\" instead of the standard 7-second delay. The claim originated from a conservative writer and quickly spread among conservative influencers and political figures, despite CNN promptly denying it as false.\nWhen questioned about the purported delay, both ChatGPT and Copilot incorrectly confirmed its existence, citing online sources that did not actually mention any delay in CNN's broadcast. \nOther AI chatbots, including Meta AI and X's Grok, provided correct responses to the same query, while Google's Gemini declined to answer due to the question's political nature.\nThe incident was seen to demonstrate the challenges AI faces in distinguishing between accurate and false information, especially in rapidly changing news environments, and the potential for AI to amplify misinformation and disinformation, particularly during critical events such as political elections.\nSystem \ud83e\udd16\nChatGPT chatbot\nCopilot chatbot\nOperator: NBC\nDeveloper: Microsoft; OpenAI\nCountry: USA\nSector: Politics\nPurpose: Generate text\nTechnology: Chatbot; Machine learning\nIssue: Accuracy/reliability; Mis/disinfomation\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.nbcnews.com/tech/internet/openai-chatgpt-microsoft-copilot-fallse-claim-presidential-debate-rcna159353\nhttps://www.inc.com/kit-eaton/dont-trust-ais-on-copilot-chatgpt-misinfo-on-first-presidential-debate-swirled.html\nhttps://www.inc.com/kit-eaton/dont-trust-ais-on-copilot-chatgpt-misinfo-on-first-presidential-debate-swirled.html\nhttps://www.forbes.com/sites/roberthart/2024/06/28/openais-chatgpt-and-microsofts-copilot-reportedly-spread-misinformation-about-presidential-debate-amid-growing-fears-over-ai-election-dangers/\nhttps://mashable.com/article/chatgpt-microsoft-copilot-shared-presidential-debate-misinformation-report\nRelated \ud83c\udf10\nChatbots misinform citizens about European Parliament elections\nMicrosoft Copilot generates fake Putin comments on Navalny death\nPage info\nType: Issue\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ibms-catch-me-up-feature-at-wimbledon-panned-for-making-factual-errors", "content": "IBM's Catch Me Up feature at Wimbledon panned for making factual errors\nOccurred: July 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn AI-powered writing tool developed by IBM and used at the Wimbledon tennis tounrnament went badly off-script, prompting complaints and ridicule.\nBuilt using IBM\u2019s Granite large language model (LLM) and watsonx platform and trained on the Wimbledon editorial style, 'Catch Me Up' generates performance analysis, likelihood-to-win predictions, match reports and player biographies based on personal user preferences and data, including location and users' myWimbledon profiles. \nHowever, it was found to be making significant mistakes in its reporting, prompting complaints by readers and journalists. For example, the system described the former US Open champion Emma Raducanu as the British No 1, although she is the No 3. And it suggested that the phrase \"double fault\" meant that there were at least two notable mistakes, whereas a double fault occurs when a tennis player makes two consecutive serving errors.\nIBM had earlier described Catch Me Up as an \u201cexciting example of how we can use the power of generative AI to deliver compelling, insight-driven storytelling at scale\u201d.\nThe errors resulted in embarrassment for Wimbledon and IBM, and led to discussions about the reliability of AI in sports reporting. \nSystem \ud83e\udd16\nCatch Me Up\nDocuments \nIBM and The All England Lawn Tennis Club Launch New Generative AI Feature for Personalised Player Stories at Wimbledon\nHow IBM helps Wimbledon use generative AI to drive personalised fan engagement\nOperator: All England Lawn Tennis Club\nDeveloper: IBM\nCountry: UK\nSector: Media/entertainment/sports/arts\nPurpose:  \nTechnology: \nIssue: Accuracy/reliability\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theguardian.com/sport/article/2024/jul/01/ai-writer-served-by-wimbledon-and-ibm-commits-double-fault\nhttps://www.sportsbusinessjournal.com/Articles/2024/07/02/wimbledon-catch-me-up-ai-feature\nhttps://www.itpro.com/technology/artificial-intelligence/wimbledons-new-catch-me-up-ai-feature-promises-to-help-cover-all-areas-of-the-tournament-after-it-irons-out-some-of-the-wrinkles\nRelated \ud83c\udf10\nIBM Project Debater prompts manipulation concerns\nIBM dataset uses millions of online photos without consent to train AI systems\nPage info\nType: Issue\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/video-footage-of-brainwash-cafe-customers-to-create-dataset-sparks-backlash", "content": "Covert video footage of Brainwash cafe customers to create dataset sparks backlash\nOccurred: June 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA dataset created by Stanford University researchers by covertly videoing customers in a San Francisco cafe sparked a backlash by privacy and civil rights groups.\nVideo footage was recorded over three days in October and November 2014 without the awareness or consent of Brainwash cafe customers, sparking privacy concerns. The opacity of the recording had not been addressed in Stanford's research paper on the project, according to the New York Times.\nConcerns were also expressed about the potential misuse of the dataset. According to Human Rights Watch, it appeared in a 2018 research paper affiliated with Megvii (Face++). Megvii has provided surveillance technology to monitor Uighur Muslims in Xinjiang.\nThe datset was removed from Stanford University's website 'at the request of the depositor' in June 2019 following the publication of researcher Adam Harvey's Exposing.ai project and a Financial Times investigation into facial recognition data sharing. \nClips from the dataset remain available on YouTube.\nSystem \ud83e\udd16\nBrainwash cafe facial recognition dataset\nOperator: Beijing University of Technology; Delft University of Technology; Honeywell Technology Solutions; Huawei; IDIAP Research Institute; IIT Madras; Megvii; National University of Defense Technology, China; North University of China; Shenzhen University; Qualcomm; University of Electronic Science and Technology of China\nDeveloper: Stanford University; Stewart Russell; Mykhaylo Andriluka; Andrew Ng\nCountry: USA; China\nSector: Research/academia\nPurpose: Train facial recognition systems\nTechnology: Database/dataset; Computer vision; Facial recognition; Object recognition\nIssue: Dual/multi-use; Privacy; Surveillance\nTransparency: Governance; Privacy\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.nytimes.com/2019/07/13/technology/databases-faces-facial-recognition-technology.html\nhttps://stanforddaily.com/2020/07/28/as-facial-recognition-draws-scrutiny-nationwide-stanford-research-raises-questions-closer-to-home/\nhttps://www.wired.com/story/secret-history-facial-recognition/\nhttps://www.ft.com/content/7d3e0d6a-87a0-11e9-a028-86cea8523dc2\nhttps://www.nature.com/articles/d41586-020-03187-3\nhttps://searchworks.stanford.edu/view/sx925dc9385\nhttps://www.thecollegefix.com/stanford-likely-helped-develop-facial-recognition-tech-now-used-against-ethnic-minorities-in-china/\nhttps://mashable.com/article/police-facial-recognition-algorithms-activism\nhttps://www.agendadigitale.eu/sicurezza/privacy/riconoscimento-facciale-ecco-i-database-che-fanno-tremare-la-nostra-privacy/\nhttps://www.tijd.be/dossier/legrandinconnu/brainwash/10136670.html\nhttps://www.spiegel.de/netzwelt/web/microsoft-gesichtserkennung-datenbank-mit-zehn-millionen-fotos-geloescht-a-1271221.html\nhttps://tech.slashdot.org/story/19/06/06/1552231/microsoft-quietly-deletes-largest-public-face-recognition-data-set\nRelated \ud83c\udf10\nPeople in Photo Albums dataset criticised for using sensitive personal images without consent\nOxford Town Centre dataset flagged for violating pedestrian privacy\nPage info\nType: Incident\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/stable-diffusion-3-churns-out-anatomically-incorrect-images", "content": "Stable Diffusion 3 churns out anatomically absurd images\nOccurred: June 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nStable Diffusion 3 (SD3) has been criticised for its poor performance in rendering human figures, particularly limbs like hands and feet, resulting in ridicule and concerns about user safety.\nCompared to other AI image models like Midjourney and DALL-E 3, SD3 has been found to generate anatomically incorrect visual aberrations, leading to what some describe as \"AI-generated body horror\".\nUsers on Reddit ridiculed SD3's attempts at depicting humans, with threads highlighting issues in rendering entire human bodies - problems that are particularly noticeable when trying to generate images of people in certain poses or situations, like \"girls lying on the grass\". \nSome in the AI community attributed these anatomical failures to Stability AI's decision to filter out adult content (NSFW) from the SD3 training data. There are concerns that the NSFW filter used during pre-training may have been too strict, inadvertently removing non-offensive images and depriving the model of important human depictions.\nThe controversy highlights the ongoing challenges in developing AI image generation models that can accurately depict human anatomy while adhering to content guidelines.\nSystem \ud83e\udd16\nStable Diffusion image generator\nOperator: \nDeveloper: Stability AI\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Generate images\nTechnology: Text-to-image; Machine learning\nIssue: Accuracy/reliability; Robustness; Safety\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://arstechnica.com/information-technology/2024/06/ridiculed-stable-diffusion-3-release-excels-at-ai-generated-body-horror/?ref=404media.co\nhttps://www.404media.co/stable-diffusion-3s-disastrous-launch-could-change-the-ai-landscape-forever/\nhttps://www.reddit.com/r/StableDiffusion/comments/1de7lbg/comment/l89yy55/\nRelated \ud83c\udf10\nText-to-image AI models tricked into generating violent, nude images\nStable Diffusion generates job type gender, racial stereotypes\nPage info\nType: Issue\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/center-for-investigative-reporting-sues-microsoft-openai", "content": "The Center for Investigative Reporting sues Microsoft, OpenAI for AI copyright violations\nOccurred: June 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe Center for Investigative Reporting (CIR) filed a lawsuit against OpenAI and Microsoft for alleged copyright infringement.\nThe CIR accused OpenAI and Microsoft of using its published content to train their AI models, specifically ChatGPT and Copilot, without permission or compensation, thereby violating the US' Copyright Act and Digital Millennium Copyright Act and underrmining its relationship with readers and depriving it of revenue.\nAnalysis showed that OpenAI's training data contained over 17,000 URLs from CIR outlets Mother Jones and Reveal. The CIR is seeking statutory damages and injunctions to remove its copyrighted works from OpenAI and Microsoft's training sets. \nThe lawsuit highlights how AI-generated summaries of articles threaten publishers and emphasises the importance of respecting journalistic work. Similar lawsuits have been filed by other news organisations against OpenAI and Microsoft, including The New York Times and Chicago Tribune.\nSystem \ud83e\udd16\nChatGPT chatbot\nCopilot chatbot\nOperator: \nDeveloper: Microsoft; OpenAI\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Generate text\nTechnology: Chatbot; Machine learning\nIssue: Copyright\nTransparency: Governance\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nCenter for Investigative Reporting. The Center for Investigative Reporting Sues OpenAI, Microsoft for Copyright Violations\nThe Center for Investigative Reporting, Inc. v. OpenAI, Inc. et al (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theverge.com/2024/6/28/24188239/openai-microsoft-center-for-investigative-reporting-lawsuit\nhttps://www.cnbc.com/2024/06/27/openai-microsoft-sued-by-center-for-investigative-reporting.html\nhttps://www.rappler.com/technology/center-for-investigative-reporting-sues-microsoft-openai-copyright-violations/\nhttps://www.oodaloop.com/briefs/2024/07/01/the-center-for-investigative-reporting-is-suing-openai-and-microsoft/\nRelated \ud83c\udf10\nThe New York Times sues OpenAI, Microsoft over copyright abuse\nEight newspapers sue OpenAI and Microsoft for copyright infringement\nPage info\nType: Incident\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/figma-make-design-tool-found-to-plagiarise-apple-weather-app", "content": "Figma Make Design tool found to plagiarise Apple Weather app\nOccurred: June 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nDesign and prototyping tool Figma was criticised when users discovered its AI-powered Make Design feature generated designs strikingly similar to Apple\u2019s Weather app when they requested a 'weather app'.\nThe issue was first highlighted by NotBoring Software founder Andy Allen, who found that Figma\u2019s AI appeared to replicate existing app designs. The incident raised questions about the data used to train Figma's AI model and whether it included copyrighted designs.\nThe finding prompted accusations that Figma's AI tool was directly copying or heavily imitating Apple's proprietary design without permission, and disappointment and concern from the design community over the implications for creative integrity and originality. Some designers also expressed concern about the tool replacing their jobs. \nFigma\u2019s CEO, Dylan Field, temporarily disabled the feature in response and blamed the company's actions on its need to meet an event deadline.\nThe controversy highlighted ethical concerns about AI-generated designs and the potential for inadavertent or deliberate plagiarism. It also underscored the need for safeguards against copyright infringement and the ongoing controversy about the loss of creative jobs.\nSystem \ud83e\udd16\nMake Design website\nOperator:\nDeveloper: Figma\nCountry: USA\nSector: Business/professional services; Media/entertainment/sports/arts\nPurpose: Generate designs\nTechnology: Machine learning\nIssue: Cheating/plagiarism\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://techcrunch.com/2024/07/02/figma-disables-its-ai-design-feature-that-appeared-to-be-ripping-off-apples-weather-app/\nhttps://www.theverge.com/2024/7/2/24190823/figma-ai-tool-apple-weather-app-copy\nhttps://www.theregister.com/2024/07/03/figma_plagiarizes_apple/\nhttps://siliconangle.com/2024/07/02/figma-disables-new-ai-tool-repeatedly-cloned-apples-weather-app/\nRelated \ud83c\udf10\nAI-generated drama performance cancelled over plagiarism accusations\nPalworld accused of plagiarising Pokemon designs using AI\nPage info\nType: Incident\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/fiction-analytics-platform-prosecraft-folds-after-ai-backlash", "content": "AI fiction analytics platform Prosecraft folds after backlash\nOccurred: August 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA dataset designed to analyse and provide insights into published fiction using AI was sharply criticised for copyright abuse, leading to its closure. \nProsecraft, known for analysing word usage and writing style markers in novels, faced intense backlash from authors and publishers angry about its unapproved use of copyrighted works within its programme, and highlighted their potential loss of income from the misuse of their works.\nDespite using only summary statistics and small snippets from texts, Prosecraft did not have permission to create a database based on entire works or full book texts. \nIts creator Benji Smith shut down the platform after the outcry. In a blog post announcing he was taking down the website, Smith said 'the prosecraft website never generated any income'.\nHowever, writers subscribed to the Shaxpir 4: Pro version at USD 7.99 a month were given access to Prosecraft\u2019s analytical tools. Smith did not reveal what happened to the Shaxpir database.\nThe incident was seen to highlight legal and ethical issues arising from the use of copyrighted material to train AI systems, and the lack of transparency inherent in many such systems. \nSystem \ud83e\udd16\nProsecraft fiction analytics database\nOperator: Benji Smith/Shaxpir\nDeveloper: Benji Smith/Shaxpir\nCountry: USA; Australia\nSector: Media/entertainment/sports/arts\nPurpose: Analyse literature\nTechnology: Database/dataset\nIssue: Copyright; Ethics/values\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://techcrunch.com/2023/08/07/authors-ai-prosecraft/\nhttps://mashable.com/article/prosecraft-novel-ai-analysis-shut-down\nhttps://gizmodo.com/prosecraft-books-shut-down-ai-algorithms-novel-fiction-1850715319\nhttps://qz.com/prosecraft-shut-down-ai-exploitation-authors-1850716572\nhttps://www.avclub.com/prosecraft-ai-books-celeste-ng-controversy-1850715787\nhttps://www.themarysue.com/benji-smith-steals-thousands-of-books-for-ai-project-prosecraft-prompting-immediate-backlash/\nhttps://theconversation.com/prosecraft-has-infuriated-authors-by-using-their-books-without-consent-but-what-does-copyright-law-say-211187\nhttps://www.wired.com/story/prosecraft-backlash-writers-ai/\nhttps://www.techtimes.com/articles/294827/20230808/prosecraft-closed-fiction-analytics-site-author-backlash-books.htm\nhttps://www.abc.net.au/perth/programs/drive/holden-sheppard-online-database-theft/102708340\nRelated \ud83c\udf10\nBookCorpus dataset accused of copyright abuse and bias\nAmazon sells AI-generated books about King Charles' cancer\nPage info\nType: Incident\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/pipa-dataset-criticised-for-using-sensitive-personal-images-without-consent", "content": "People in Photo Albums dataset criticised for using sensitive personal images without consent\nOccurred: September 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA dataset of facial photos intended to recognise peoples' identities in photo albums prompted controversy for failing to gain the explicit consent of those in the images, and for uses beyond its original purpose.\nAn investigation by artist Adam Harvey found that the People in Photo Albums (PIPA) dataset included images of people in various personal and social settings, potentially infringing on their privacy and that, despite attempts to anonymise the data, the personal nature of the photos could make individuals identifiable.\nIn addition, the uses of the data appear to have gone well beyond its stated purpose of processing personal photo albums. For example, Harvey discovered that PIPA was used by China's National University of Defense Technology and Tsinghua University, as well as by many commercial and industrial organisations.\nIt has also been pointed out that PIPA's creators fail to mention the type of CC licence under which the photographs were used, despite some CC licences not permitting any type of re-use.\nThe controversy highlighted the need for researchers to gain the consent of people whose personal and biometric data they are using, and the need for clearer ethical guidelines and consent processes in the development of datasets.\n\u2795 January 2020. UC Berkeley stopped distributing the dataset. However, it remains available via the Max Planck Institut.\nSystem \ud83e\udd16\nPeople in Photo Albums  dataset\nOperator: ETH Zurich; Max Planck Institute of Informatics; Toyota Motor Europe; SenseTime; National University of Singapore; National University of Defense Technology, China; Meta/Facebook\nDeveloper: UC Berkeley; Meta/Facebook\nCountry: Germany; USA\nSector: Research/academia; Technology; Media/entertainment/sports/arts\nPurpose: Train facial recognition systems\nTechnology: Database/dataset; Facial analysis; Facial recognition; Computer vision\nIssue: Copyright; Dual/multi-use; Ethics/values; Privacy; Surveillance\nTransparency: Governance; Legal\nResearch, advocacy \ud83e\uddee\nHarvey, A., LaPlace, J. (2021). Exposing.ai\nZhao J., Li J., Cheng Y., Zhou L., Sim T.,Yan S., Feng J. (2018). Understanding Humans in Crowded Scenes: Deep Nested Adversarial Learning and A New Benchmark for Multi-Human Parsing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.law.kuleuven.be/citip/blog/free-to-re-use-the-case-of-facial-images-scrapped-from-the-internet-and-compiled-in-mega-research-datasets\nRelated \ud83c\udf10\nOxford Town Centre dataset flagged for violating pedestrian privacy\nMegaFace facial recognition dataset raises privacy, liability concerns\nPage info\nType: Issue\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/oxford-town-centre-dataset-flagged-for-violating-pedestrian-privacy", "content": "Oxford Town Centre dataset flagged for violating pedestrian privacy\nOccurred: June 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA dataset created to help with computer vision research prompted concerns about privacy, consent and surveillance. \nThe Oxford Town Centre dataset contained video footage of approximately 2,200 pedestrians in a busy shopping area in Oxford, UK, captured without their knowledge or consent. \nExposing.ai activist Adam Harvey pointed out that the dataset is 'unique in that it uses footage from a public surveillance camera that would otherwise be designated for public safety. \nResearch citations showed the footage had been used in dozens of research projects with no connection to public safety, and that most of the research took place outside the UK. \nThe dataset was used to develop public and office social distancing algorithms, notably by US company Landing AI, which posted a demonstration video featuring the Oxford Town Centre dataset. The video was deleted on YouTube after it was highlighted by Exposing.ai.\nThe dataset remains easily and freely accessible on Kaggle and other data sharing communities.\n\u2795 June 2020. The Oxford Town Centre dataset was removed from the University of Oxford's website.\nSystem \ud83e\udd16\nOxford Town Centre dataset\nOperator: Amazon; Disney; OSRAM; Huawei\nDeveloper: University of Oxford\nCountry: UK\nSector: Govt - municipal; Research/academia; Technology\nPurpose: Improve pedestrian detection\nTechnology: Database/dataset; Computer vision; Facial recognition; Pattern recognition\nIssue: Dual/multi-use; Ethics/values; Privacy; Surveillance\nTransparency: Governance; Marketing; Privacy\nInvestigations, assessments, audits \ud83e\uddd0\nHarvey, A., LaPlace, J. (2019). Exposing.ai\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.biometricupdate.com/201906/ms-celeb-and-other-facial-biometrics-datasets-taken-down\nhttps://williambowles.info/2019/07/17/massive-photo-databases-secretly-gathered-in-us-and-europe-to-develop-facial-recognition-by-kevin-reed/\nRelated \ud83c\udf10\nWILDTRACK pedestrian detection dataset\nUnconstrained College Students (UCSD) dataset\nPage info\nType: Issue\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/opaque-uk-plan-to-share-patient-data-with-third-parties-backfires", "content": "Opaque plan to share UK patient data with third parties backfires\nOccurred: May 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA plan by the UK National Health Service (NHS) to pool sensitive personal medical histories on to a central database and make them available to third parties prompted significant controversy.\nThe stated aim of the UK's General Practice Data for Planning and Research (GPDPR) programme was to improve healthcare research and planning by providing researchers and commercial entities access to the anonymised patient health histories of 55 million people in England. Patients were given six weeks to opt out of the programme.\nThe news prompted an immediate and broad backlash from medical professionals, digital rights activists, and politicians accusing the government of inadequate transparency about the existence and nature of the programme, in particular about which types of organisations would have access to the data and what they would able to do with it. \nConcerns were also expressed about the security and potential misuse of sensitive medical information, as well as about potential access to patient data by private companies, notably by controversial US technology company Palantir.\nOrganisations such as the Doctors' Association UK (DAUK) also expressed concerns that the scheme could affect the GP-patient relationship and potentially discourage patients from seeking medical care due to privacy concerns.\nThe backlash resulted the initial opt-out period being extended for two months, with patients able to opt-out at any time during the programme. \n\u2795 February 2021. The NHS faced legal action over its contract with Palantir, which has been involved in analysing UK public health data.\nSystem \ud83e\udd16\nGeneral Practice Data for Planning and Research (GPDPR)\nOperator: National Health Service (NHS)\nDeveloper: NHS Digital\nCountry: UK\nSector: Govt - health\nPurpose: Centralise patient records\nTechnology: Database/dataset\nIssue: Privacy; Security\nTransparency: Governance; Marketing; Privacy\nResearch, advocacy \ud83e\uddee\nDoctors Association UK/Foxglove. THE NHS FEDERATED DATA PLATFORM AND PALANTIR. 7 KEY RISKS (pdf)\nOpenDemocracy/Foxglove. Save our NHS data\nFoxglove. No Palantir in our NHS\nDr Bhatti.com. An Open Letter to the Government regarding GPDPR\nFact checks \u2757\nFull Fact (2024). Out-of-date warning on NHS data circulates again\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.ft.com/content/9fee812f-6975-49ce-915c-aeb25d3dd748\nhttps://www.theguardian.com/society/2021/may/30/gps-warn-plans-share-patient-data-third-parties-england\nhttps://www.msn.com/en-gb/news/uknews/gps-urged-to-refuse-to-hand-over-patient-details-to-nhs-digital/ar-AAKATS6\nhttps://ca.finance.yahoo.com/news/nhs-plans-mine-patient-records-084953888.html\nhttps://www.dailymail.co.uk/news/article-9646151/NHS-records-shared-secretive-tech-giant-campaigners-warn.html\nhttps://metro.co.uk/2021/05/27/nhs-england-set-to-share-medical-records-with-outside-third-parties-14655452/\nhttps://www.dailymail.co.uk/health/article-9637979/Did-know-NHS-share-records.html\nhttps://www.msn.com/en-gb/money/technology/nhs-plans-to-share-uk-patient-records-with-third-parties/ar-AAKrxny\nhttps://www.infosecurity-magazine.com/news/nhs-share-patient-data-third/\nhttps://telecareaware.com/nhs-digital-gpdpr-medical-database-plans-criticized-by-royal-college-of-gps-privacy-advocates/\nhttps://www.bbc.co.uk/news/uk-politics-57400902\nRelated \ud83c\udf10\nQCovid prediction algorithm wrongly identifies high-risk patients\nNHS plan to AI generate patient notes draws criticism\nPage info\nType: Issue\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ms-celeb-1m-facial-recognition-database-criticised-for-violating-privacy", "content": "MS-Celeb-1M facial recognition database criticised for violating user privacy\nOccurred: July 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nMicrosoft deleted its MS-Celeb-1M dataset after it had been found to have scraped the images of celebrities and used them to train facial recognition systems.\nMicrosoft collected photographs for MS-Celeb-1M by automatically scraping them from search engines, and did so without informing or gaining the consent of those affected, according to a Financial Times investigation. \n'Celebrities' whose data was collected included US blogger Cory Doctorow, journalist Glenn Greenwald, author and academic Shoshana Zuboff, and former US FTC commissioner Julie Brill, sparking accusations that the technology company played fast and loose with the definition of public interest.\nDespite being restricted to academic use, research paper citations reveal MS-Celeb-1M has been used hundreds of times across the world by companies such as IBM, Panasonic, Hitachi, and Nvidia for a wide variety of commercial purposes. \nIt also transpired that Microsoft used MS-Celeb-1M to train its own facial recognition systems, as had Chinese technology firms Huawei, Sensetime, and Megvii, whose products are allegedly used to detect and surveil Uyghurs, and to track foreign journalists.\nMicrosoft quietly took down the dataset in June 2019, telling the FT that 'the site was intended for academic purposes. It was run by an employee that is no longer with Microsoft and has since been removed.' \nHowever, the dataset remains widely available online, with several versions on Github and Academic Torrents.\nSystem \ud83e\udd16\nMS-Celeb-1M facial recognition database\nOperator: Alibaba; \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne; Hitachi; Huawei; IBM; IDIAP Research Institute; Megvii; Microsoft; National University of Defense Technology (NUDT); Nvidia; Panasonic; SenseTime; Universidad Aut\u00f3noma de Madrid; University of Leicester; Multiple\nDeveloper: Microsoft\nCountry: USA\nSector: Technology; Research/academia\nPurpose: Train facial recognition systems\nTechnology: Database/dataset; Facial recognition; Computer vision\nIssue: Copyright; Dual/multi-use; Ethics/values; Privacy; Surveillance\nTransparency: Privacy\nResearch, advocacy \ud83e\uddee\nPeng K., Mathur A., Narayanan A. (2021). Mitigating Dataset Harms Requires Stewardship: Lessons from 1000 Papers\nHarvey, A., LaPlace, J. (2019). Exposing.ai\nInvestigations, assessments, audits \ud83e\uddd0\nMurgia M., Financial Times (2019). Who\u2019s using your face? The ugly truth about facial recognition\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.ft.com/content/7d3e0d6a-87a0-11e9-a028-86cea8523dc2\nhttps://www.nytimes.com/2019/07/13/technology/databases-faces-facial-recognition-technology.html\nhttps://www.spiegel.de/netzwelt/web/microsoft-gesichtserkennung-datenbank-mit-zehn-millionen-fotos-geloescht-a-1271221.html\nhttps://www.lesechos.fr/tech-medias/intelligence-artificielle/le-mariage-explosif-de-nos-donnees-et-de-lia-1031813\nhttps://www.lastampa.it/2019/06/22/tecnologia/microsoft-ha-cancellato-il-suo-database-per-il-riconoscimento-facciale-PWwLGmpO1fKQdykMZVBd9H/pagina.html\nhttps://www.nature.com/articles/d41586-020-03187-3\nhttps://www.bbc.co.uk/news/technology-48555149\nhttps://www.engadget.com/2019/06/06/microsoft-discreetly-wiped-its-massive-facial-recognition-databa/\nhttps://www.biometricupdate.com/201906/ms-celeb-and-other-facial-biometrics-datasets-taken-down\nhttps://futurism.com/microsoft-deletes-facial-recognition-database\nhttps://www.fastcompany.com/90360490/ms-celeb-microsoft-deletes-10m-faces-from-face-database\nhttps://www.forbes.com/sites/korihale/2019/06/25/microsoft-scraps-10-million-facial-recognition-photos-on-the-low/\nhttps://www.forbes.com/sites/forbestechcouncil/2019/07/23/how-being-aware-of-our-biases-protects-the-future-of-ai/\nhttps://freedom-to-tinker.com/2020/10/21/facial-recognition-datasets-are-being-widely-used-despite-being-taken-down-due-to-ethical-concerns-heres-how/\nRelated \ud83c\udf10\nDukeMTMC facial recognition dataset\nMegaFace facial recognition dataset\nPage info\nType: Incident\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/megaface-facial-recognition-dataset-raises-privacy-liability-concerns", "content": "MegaFace facial recognition dataset raises privacy, liability concerns\nOccurred: October 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe prominent dataset used to train facial recognition systems was found to contain the facial images of nearly 700,000 individuals using photos from Flickr without users' explicit consent, prompting privacy and surveillance fears. \nAs many as 700,000 people had their likenesses uploaded from Flickr to the MegaFace dataset, including many children, none of whom had any idea that their images had been used, or provided consent for them to be used. \nSome of these people were unhappy their images had been repurposed in this manner, according to the New York Times. \nThe Times also reported that the dataset had been used by companies including Google, Amazon, Ntechlab, Mitsubishi Electric, Tencent, and SenseTime to test or train their facial recognition algorithms to monitor terrorists, protesters and the general public, including Uyghurs in China. \nThe finding prompted speculation that companies using the MegaFace dataset may be subject to significant liability litigation, potentially leading to class-action lawsuits, notably under the US state of Illinois' Biometric Information Privacy Act (BIPA). \n\u2795 June 2020. The University of Washington ceased distributing MegaFace.\nSystem \ud83e\udd16\nMegaFace facial recognition dataset\nOperator: Alibaba; Alphabet/Google; Amazon; Bytedance; EUROPOL; Huawei; In-Q-Tel; IntelliVision; Megvii; Mitsubishi Electric; Northrup Grumman; Ntechlab; Philips; Samsung; SenseTime; Sogou; Tencent; Vision Semantics\nDeveloper: University of Washington\nCountry: USA\nSector: Technology; Research/academia\nPurpose: Improve research quality\nTechnology: Database/dataset; Facial recognition; Computer vision\nIssue: Copyright; Dual/multi-use; Ethics/values; Liability; Privacy; Surveillance\nTransparency: Privacy; Marketing\nRegulation \u2696\ufe0f\nIllinois Biometric Information Privacy Act (BIPA)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.nytimes.com/interactive/2019/10/11/technology/flickr-facial-recognition.html\nhttps://www.biometricupdate.com/201910/megaface-facial-recognition-dataset-origin-raises-privacy-and-liability-concerns\nhttps://www.businessinsider.com/flickr-photos-kids-train-ai-facial-recognition-database-megaface-report-2019-10\nRelated \ud83c\udf10\nStudy: LFW dataset discards the privacy rights of internet users\nLibrary Genesis sued for 'staggering' copyright infringement\nPage info\nType: Issuet\nPublished: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/major-music-labels-sue-ai-startups-suno-udio-for-copyright-infringement", "content": "Major music labels sue AI start-ups Suno, Udio for copyright infringement\nOccurred: June 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nMusic labels including Universal Music Group, Sony Music and Warner Music Group accused AI startups Suno and Udio of violating copyright to train their models in lawsuits lodged by the Recording Industry Association of America (RIAA).\nThe suits allege that the two firms used copyrighted sound recordings without authorisation to train their models and generate similar compositions on an \"almost unimaginable scale\", and that these compositions result in the production of music that competes with and devalues the original works.\nThe labels argue that the two companies' use of their recordings does not qualify as fair use due to its commercial nature and the negative impact on the market for the original recordings, and are seeking damages of up to USD 150,000 per copyrighted work used without permission. They are also requesting injunctions to stop the alleged unauthorised use of their music libraries for the training of third-party AI systems.\n\nThe legal action is part of a broader effort to challenge AI firms' use of copyrighted material without permission, with the music industry arguing that illegal AI services threaten musicians\u2019 rights and livelihoods and potentially reduce the quality of new music.\nThe legal action follows a similar lawsuit by UMG against Anthropic. \nSystem \ud83e\udd16\nSuno music generator\nUdio music generator\nOperator: Suno; Udio\nDeveloper: Suno; Udio\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Generate music\nTechnology: Generative AI; Text-to-music; Machine learning\nIssue: Copyright\nTransparency: Governance \nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nUMG RECORDINGS, INC., CAPITOL RECORDS, LLC, SONY MUSIC ENTERTAINMENT, ATLANTIC RECORDING CORPORATION, ATLANTIC RECORDS GROUP LLC, RHINO ENTERTAINMENT LLC, THE ALL BLACKS U.S.A., INC., WARNER MUSIC INTERNATIONAL SERVICES LIMITED, and WARNER RECORDS INC., Plaintiffs, v. SUNO, INC. and JOHN DOES 1-10 (pdf)\nUMG RECORDINGS, INC., CAPITOL RECORDS, LLC, SONY MUSIC ENTERTAINMENT, ARISTA MUSIC, ARISTA RECORDS LLC, ATLANTIC RECORDING CORPORATION, RHINO ENTERTAINMENT COMPANY, WARNER MUSIC INC., WARNER MUSIC INTERNATIONAL SERVICES LIMITED, WARNER RECORDS INC., WARNER RECORDS LLC, and WARNER RECORDS/SIRE VENTURES LLC, Plaintiffs, v. UNCHARTED LABS, INC., d/b/a Udio.com, and JOHN DOES 1-10 (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theverge.com/2024/6/24/24184710/riaa-ai-lawsuit-suno-udio-copyright-umg-sony-warner \nhttps://www.theguardian.com/music/article/2024/jun/25/record-labels-sue-ai-song-generator-apps-copyright-infringement-lawsuit \nhttps://www.vulture.com/article/major-labels-music-ai-suno-udio-lawsuit.html \nhttps://www.reuters.com/technology/artificial-intelligence/music-labels-sue-ai-companies-suno-udio-us-copyright-infringement-2024-06-24/ \nhttps://www.wired.com/story/ai-music-generators-suno-and-udio-sued-for-copyright-infringement/ \nhttps://www.bloomberg.com/news/articles/2024-06-24/sony-warner-universal-sue-suno-udio-for-training-ai-on-copyrighted-music \nhttps://variety.com/2024/music/news/record-labels-sue-ai-music-services-suno-and-udio-copyright-infringement-1236045366/ \nhttps://musictech.com/news/industry/record-labels-sue-ai-suno-copyright-infringement-sony-warner-umg/  \nRelated \ud83c\udf10\nSony warns AI companies to not misuse its data\nDrake The Weekend AI Voice Cloning \nPage info\nType: Incident\nPublished: June 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/perplexity-ai-ignores-requests-not-to-scrape-websites", "content": "Perplexity AI ignores requests not to scrape websites\nOccurred: June 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAI startup Perplexity came under sharp scrutiny for apparently ignoring website operators' requests not to scrape their content, calling into question the company's business model and ethics.\nThe self-styled AI 'answer engine' has been found to be bypassing robots.txt files, which are standard instructions for web crawlers to respect a website's preferences regarding content scraping. Publishers including WIRED and MacStories found that Perplexity continues to access and use their content despite being explicitly blocked in their robots.txt files.\nThe company is reportedly using headless browsers to scrape content, ignoring robots.txt, and not sending their user agent string. Perplexity's CEO, Aravind Srinivas, admitted to Fast Company that the company uses third-party web crawlers on top of its own, and that the bot WIRED identified was one of them. \nThe finding raised concerns about intellectual property rights, publisher earnings, and the need for clearer guidelines or more regulation on the use of web data. It also raised questions about Srinivas' ethics, as well as those of its investors, which include Jeff Bezos and Softbank.\nPerplexity is not the only company seemingly flouting robots.txt signals. OpenAI and Anthropic - respective creators of the ChatGPT and Claude chatbots - have also been bypassing robots.txt signals, according to Business Insider. Both companies previously proclaimed that they respect \"do not crawl\" instructions websites put in their robots.txt files. \n\u2795 June 2024. Amazon Web Services said it was investigating Perplexity over its data scraping practices, and confirmed all AWS clients must follow the robots.txt file instructions.\nSystem \ud83e\udd16\nPerplexity AI answer engine\nOperator:\nDeveloper: Perplexity AI\nCountry: USA\nSector: Media/entertainment/sports/arts; Politics\nPurpose: Generate information\nTechnology: Chatbot; Machine learning; NLP/text analysis\nIssue: Cheating/plagiarism; Copyright; Ethics/values\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.wired.com/story/perplexity-is-a-bullshit-machine/\nhttps://www.wired.com/story/perplexity-plagiarized-our-story-about-how-perplexity-is-a-bullshit-machine/\nhttps://www.reuters.com/technology/artificial-intelligence/multiple-ai-companies-bypassing-web-standard-scrape-publisher-sites-licensing-2024-06-21/\nhttps://www.theshortcut.com/p/perplexity-ai-is-stealing-from-the-shortcut\nhttps://www.platformer.news/how-to-stop-perplexity-oreilly-ai-publishing/\nhttps://www.businessinsider.com/perplexity-ai-forbes-wired-explained-2024-6\nhttps://www.medianama.com/2024/06/223-perplexitys-wired-ai-chatbot-misinformed-crawled-webpages/\nRelated \ud83c\udf10\nPerplexity AI is accused of ripping off news websites\nStudy: Top chatbots spread Russian misinformation\nPage info\nType: Incident\nPublished: June 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/danish-child-protection-algorithm-criticised-for-age-discrimination", "content": "Danish child protection algorithm accused of age discrimination\nOccurred: June 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Danish government AI-powered algorithmic system intended to help caseworkers identify children likely to be maltreated was flawed in many ways, including demonstrating age bias, calling into question its usefulness.\nDeveloped to be used by caseworkers of the Danish Child Protective Services, the Decision Support system (DSS) is intended to assist social workers in assessing the growing number of notifications about childen in immediate danger of mailtreatment in a timely manner, and to provide consistent risk assessment for the children being referred to Child Protective Services.\nHowever, IT University of Copenhagen researchers found that the algorithm \"has significant methodological flaws, suffers from information leakage, relies on inappropriate proxy values for maltreatment assessment, generates inconsistent risk scores, and exhibits age-based discrimination.\"\nSpecifically, the researchers argued the system \"scores otherwise identical cases completely differently just based on the age of the child. Age is a protected attribute and globally recognized as a ground for discrimination.\"\nThe researchers \"strongly\" advised against the use of this kind of algorithms in local government, municipal, and child protection settings, and called for rigorous evaluation of similar tools before their implementation.\nSystem \ud83e\udd16\nDecision Support System (DSS)\nOperator: Danish Child Protective Services\nDeveloper:  \nCountry: Denmark\nSector: Govt - welfare\nPurpose: Assess child abuse risk\nTechnology: Prediction algorithm; Machine learning\nIssue: Accuracy/reliability; Bias/discrimination\nTransparency: Governance\nInvestigations, assessments, audits \ud83e\uddd0\nMoreau T.et al. Failing Our Youngest: On the Biases, Pitfalls, and Risks in a Decision Support Algorithm Used for Child Protection\nRelated \ud83c\udf10\nGladsaxe vulnerable children detection\nTrelleborg welfare management automation\nPage info\nType: Issue\nPublished: June 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/library-genesis-sued-for-staggering-copyright-infringement", "content": "Library Genesis sued for 'staggering' copyright infringement\nOccurred: September 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nLibrary Genesis was sued by publishers Cengage Learning, Macmillan Learning, McGraw Hill, and Pearson Education for copyright infringement on a 'staggering' scale by 'illegally distributing' over 20,000 of their textbooks. \nThe US lawsuit alleges that LibGen copied and distributed tens of thousands of their copyrighted works without authorisation, and claimed he scale of infringement is \"staggering,\" with LibGen maintaining a collection of over 6 million files, including at least 20,000 works from the plaintiffs.\nThe publishers also accuse LibGen of competing directly with them by offering free access to copyrighted materials, displacing legitimate sales, and seeks to shut down LibGen's operations and block its domain names.\nThe suit also called out Google and 'other intermediaries' - specifically NameCheap for domain registration services, Cloudflare for proxy services, and Google for search engine services - for allegedly helping Libgen conduct its operations.\nThe case highllights the tension between copyright protection and the push for open access to academic knowledge, and could have significant consequences for the future of LibGen and potentially impact the broader landscape of open access to academic literature.\nSystem \ud83e\udd16\nLibrary Genesis shadow library\nOperator: Library Genesis\nDeveloper: Library Genesis\nCountry: USA\nSector: Education\nPurpose: Provide content access\nTechnology: Database/dataset\nIssue: Copyright\nTransparency: Governance; Complaints/appeals\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nCengage Learning, Inc. v. Does 1 - 50\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reuters.com/legal/litigation/textbook-publishers-sue-shadow-library-library-genesis-over-pirated-books-2023-09-14/\nhttps://arstechnica.com/tech-policy/2023/09/most-notorious-illegal-shadow-library-sued-by-textbook-publishers/\nhttps://www.theregister.com/2023/09/18/science_publishers_sue_libgen/\nhttps://www.thebookseller.com/news/academic-publishers-file-copyright-suit-against-libgen-citing-staggering-infringement\nhttps://torrentfreak.com/publishers-lawsuit-accuses-libgen-of-staggering-copyright-infringement-230915/\nhttps://www.theguardian.com/books/2023/sep/15/four-large-us-publishers-sue-shadow-library-for-alleged-copyright-infringement\nhttps://www.vice.com/en/article/v7vnn4/shadow-libraries-are-moving-their-pirated-books-to-the-dark-web-after-fed-crackdowns\nRelated \ud83c\udf10\nZ-Library shadow library\nBooks3 AI training dataset\nPage info\nType: Incident\nPublished: June 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/fascist-chatbots-run-wild-on-character-ai", "content": "Fascist chatbots run wild on Character.AI\nOccurred: March 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nChatbots impersonating Adolf Hitler and Saddam Hussein were let loose on Character.AI, calling into question the platform's content moderation practices and its commitment to the safety of its users.\nCharacter.AI, a popular platform for AI-generated conversations, came under scrutiny for hosting a range of problematic chatbots, including fascist and extremist personas readily spewing misinfirmation and disinformation, and anti-semitic, Islamophobic, racist and homophobic rhetoric.\nThe chatbots - which included Hitler, Hussein and the Prophet Mohammed, which were easily found via the platform's homepage, clearly violated Character.AI's stated rules against sharing hateful content and was condemned by anti-hate organisations and Jewish advocacy groups fearful of the platform's potential as a hotbed for radicalisation and the spread of extremist ideologies.\nThe fracas also prompted calls for regulation, with experts making the case for controls, penalties, and a clear regulatory framework to prevent AI from promoting hate speech online.\nUnder pressure, Character.AI removed the offensive bots, resulting in a backlash by users complaining of censorship and the need for free speech.\nSystem \ud83e\udd16\nCharacter.AI website\nCharacter.AI Wikipedia profile\nCharacter.AI Discord server\nDocuments \ud83d\udcc3\nCharacter.AI terms of service\nReviews \ud83d\udde3\ufe0f\nCharacter.AI subreddit\nOperator:\nDeveloper: Character.AI\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Develop characters\nTechnology: Large language model\nIssue: Human/civil rights; Mis/disinformation; Safety\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theverge.com/23627402/character-ai-fandom-chat-bots-fanfiction-role-playing\nhttps://www.standard.co.uk/news/tech/character-ai-fascist-chatbots-offensive-generative-b1089868.html\nhttps://www.zeit.de/digital/internet/2023-03/character-ai-chatbot-ki-imitation-prominente/seite-2\nhttps://www.news.com.au/technology/online/internet/i-need-to-go-outside-young-people-extremely-addicted-as-characterai-explodes/news-story/5780991c61455c680f34b25d5847a341\nhttps://medium.com/@makidotai/character-ais-evil-ai-a-portrait-of-controversy-3ddcb84de961\nRelated \ud83c\udf10\nGab.AI chatbots accused of radicalisation, inciting violence\nHistorical Figures Chat 'monetises Holocaust'\nPage info\nType: Issue\nPublished: June 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chatgpt-invents-holocaust-by-drowning", "content": "ChatGPT invents 'Holocaust by drowning'\nOccurred: June 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nChatGPT generated false information about a fictional Holocaust event called \"Holocaust by drowning,\" prompting concerns about its accuracy and ability to rewrite historical facts.\nThe chatbot reportedly claimed that Nazi Germany had systematically drowned Jewish people as part of the Holocaust, according to UNESCO. The output sparked Holocaust historians and educators to express concern about the spread of misinformation regarding such a sensitive and important historical topic.\nThe incident highlighted the problem of AI models generating false or fabricated information, often referred to as \"hallucinations,\" and called into question ChatGPT's accuracy and reliability. \nIt also served as a reminder of the limitations of current AI systems and the need for caution when using them as sources of factual information, especially on sensitive topics.\nSystem \ud83e\udd16\nChatGPT chatbot\nOperator: OpenAI\nDeveloper: OpenAI\nCountry: Israel; Multiple\nSector: Politics; Religion\nPurpose: Generate text\nTechnology: Chatbot\nIssue: Mis/disinformation\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nUNESCO (2024). AI and the Holocaust: rewriting history? The impact of artificial intelligence on understanding the Holocaust\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.upi.com/Top_News/World-News/2024/06/18/france-UNESCO-AI-Holocuast-denial/1361718707653/\nhttps://www.rfi.fr/en/science-and-technology/20240618-ai-technology-used-to-distort-holocaust-history-un-body-warns\nhttps://www.france24.com/en/europe/20240618-unesco-sounds-alarm-over-artificial-intelligence-fuelled-holocaust-denial\nhttps://voz.us/unesco-reports-that-several-ais-promote-antisemitism-and-holocaust-denial/\nRelated \ud83c\udf10\nGoogle Autocomplete suggests Jews, women are 'evil'\nStudy: Social media companies fail to take down anti-Semitic content\nPage info\nType: Incident\nPublished: June 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chinese-geo-chatbot-accused-of-censorship-bias", "content": "Chinese geo chatbot accused of censorship, bias\nOccurred: June 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGeoGPT, a new AI chatbot backed by the International Union of Geological Sciences (IUGS), has been criticised by geologists over potential censorship and bias, and poor transparency.\nDeveloped under the Deep-time Digital Earth (DDE) programme, which receives substantial funding from China, GeoGPT is meant to assist geoscientists, particularly in developing countries, by providing access to extensive geological data.\n\nHowever critics, including Professor Paul Cleverley, have pointed out potential censorship and lack of transparency in GeoGPT\u2019s responses. While DDE representatives claim that the chatbot\u2019s information is purely geoscientific and free from state influence, tests with Qwen (developed by Alibaba), its underlying AI model, suggest that certain sensitive questions may be avoided or answered inadequately. \nFor instance, when asked about fatalities in a Ghanaian mining operation run by the Shaanxi Mining Company, Qwen cited outdated information and avoided specifics, whereas ChatGPT provided detailed information on the incident.\nConcerns were also expressed about the bot's Chinese funding and the potential for biased data usage. Geoscientific research, including valuable information about natural resources, could be strategically filtered. The terms of use for GeoGPT prohibit generating content that undermines national security or incites subversion, aligning with Chinese laws, which may influence the chatbot\u2019s outputs. \nIUGS president, John Ludden, said GeoGPT\u2019s database will be made public once appropriate governance is ensured, but geoscientists remain skeptical about the impartiality and transparency of GeoGPT\u2019s data and responses.\nSystem \ud83e\udd16\nGeoGPT\nDocuments\nGeoGPT: Understanding and Processing Geospatial Tasks through An Autonomous GPT\nOperator: \nDeveloper: Deep-time Digital Earth\nCountry: China\nSector: Energy\nPurpose: Support geological research\nTechnology: Chatbot\nIssue: Bias/discrimination; Copyright; Human/civil rights\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://geoscientist.online/sections/viewpoint/geoscience-ai-in-crisis/\nhttps://www.theguardian.com/technology/article/2024/jun/24/geologists-censorship-bias-chinese-chatbot-geogpt\nhttps://dig.watch/updates/geologists-voice-concerns-about-potential-censorship-and-bias-in-chinese-ai-chatbot\nhttps://www.techtimes.com/articles/305982/20240624/geologists-worried-geogpts-potential-censorship-bias-whats-concerning-chinese-chatbot.htm\nRelated \ud83c\udf10\nBytedance instructed to automatically censor Uyghur social media posts\nXiaohongshu AI image generator abuses copyright\nPage info\nType: Issue\nPublished: June 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/lfw-dataset-discards-the-privacy-rights-of-internet-users", "content": "Study: LFW dataset discards the privacy rights of internet users\nOccurred: February 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nProminent dataset Labeled Faces in the Wild (LFW) scraped Google, Flickr, YouTube, and other online photo libraries, discarding the privacy rights of photo owners and subjects. \nIn a paper examining over 130 facial-recognition data sets compiled over 43 years, researchers Deborah Raji and Genevieve Fried singled out the LFW dataset as being the first for which 'wild' images were scraped from the internet.\nAccording to the Technology Review, the dataset 'opened the floodgates to data collection through web search, with researchers starting to download images directly from Google, Flickr, and Yahoo without concern for consent.'\nEarlier, LFW had been found to be highly skewed towards a very small subset of people, specifically white male faces, and contained 'a significant number of duplicate or nearly-duplicate images and mislabeled images.' The finding persuaded LFW's creators to acknowledge the dataset's limitations. \nSystem \ud83e\udd16\nLabeled Faces in the Wild (LFW) dataset\nOperator:\nDeveloper: University of Massachussets, Amherst\nCountry: USA\nSector: Research/academia; Technology\nPurpose: Train facial recognition systems\nTechnology: Dataset; Computer vision; Deep learning; Facial recognition; Facial detection; Facial analysis; Machine learning; Neural network; Pattern recognition\nIssue: Bias/discrimination - race, ethnicity, gender; Ethics/values; Privacy\nTransparency: Governance; Privacy\nResearch, advocacy \ud83e\uddee\nRaji I.D., Fried G. (2021). About Face: A Survey of Facial Recognition Evaluation\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.technologyreview.com/2021/02/05/1017388/ai-deep-learning-facial-recognition-data-history/\nhttps://www.technologyreview.com/2021/08/13/1031836/ai-ethics-responsible-data-stewardship/\nhttps://www.ft.com/content/cf19b956-60a2-11e9-b285-3acd5d43599e\nhttps://www.theregister.com/2021/08/10/ai_master_face/\nhttps://medium.com/voxel51/fifteen-minutes-with-fiftyone-labeled-faces-in-the-wild-6b4e2530787\nhttps://jolt.law.harvard.edu/digest/why-racial-bias-is-prevalent-in-facial-recognition-technology\nhttps://mashable.com/article/facial-recognition-databases-privacy-study\nhttps://www.nytimes.com/2019/07/10/opinion/facial-recognition-race.html\nRelated \ud83c\udf10\nDuke University pulls facial recognition dataset after privacy controversy\nUS government research dataset raises privacy, misuse concerns\nPage info\nType: Incident\nPublished: June 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/imagenet-contains-inaccurate-derogatory-racially-offensive-info", "content": "ImageNet contains inaccurate, derogatory, and racially offensive information\nOccurred: September 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe ImageNet dataset was found to contain inaccurate racist, misogynistic and other discriminatory and derogatory slurs, resulting in controversy about its safety and accusations of privacy and copyright abuse.\nImageNet Roulette, a website that encouraged users to upload selfies and then analyse what it saw by running their photos through a neural network trained on ImageNet found that many any captions produced by the code were harmless, thugh some turned out to be inaccurate, or contained racist, misogynistic and other discriminatory and derogatory slurs.\nCreated by Kate Crawford, co-founder of the AI Now Institute, artist Trevor Paglen and software developer Leif Ryge, ImageNet Roulette was a 'provocation designed to help us see into the ways that humans are classified in machine learning systems.' \nBy automatically scraping images from Google, Bing and photo-sharing platform Flickr to build its training dataset without consent, ImageNet's developers were also accused of ignoring user privacy, leading lawyers and rights activists to call for stronger privacy and copyright laws.\nThe ensuing fracas led the developers of ImageNet to scrub 'unsafe' and 'sensitive' labels from the database, and to remove links to related photographs - an update seen to have minimal impact on the classification and transfer learning accuracy of the dataset, though some commentators argued it would damage ImageNet's relevance by styming its reproducibility.\n\u2795 March 2021. The ImageNet team announced it had blurred 243,198 photographs in its database using Amazon's Rekogniton image and video analytics service.\nSystem \ud83e\udd16\nImageNet image recognition dataset\nOperator: Kate Crawford; Trevor Paglen\nDeveloper: Princeton University; Jia Deng; Wei Dong, Richard Socher; Li-Jia Li; Kai Li; Fei-Fei Li\nCountry: USA\nSector: Research/academia\nPurpose: Identify objects\nTechnology: Dataset; Computer vision; Object detection; Object recognition; Machine learning; Deep learning\nIssue: Accuracy/reliability; Bias/discrimination - race, ethnicity, gender, religion, national identity, location; Copyright; Privacy\nTransparency: Governance; Privacy\nResearch, advocacy \ud83e\uddee\nPrabhu V.U., Birhane A. (2020). Large image datasets: A pyrrhic win for computer vision?\nDulhanty C., Wong A. (2019). Auditing ImageNet: Towards a Model-driven Framework for Annotating Demographic Attributes of Large-Scale Image Datasets\nInvestigations, assessments, audits \ud83e\uddd0\nPaglen T., Crawford K.: ImageNet Roulette\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.wired.com/story/viral-app-labels-you-isnt-what-you-think/\nhttps://www.nbcnews.com/mach/tech/playing-roulette-race-gender-data-your-face-ncna1056146\nhttps://news.artnet.com/art-world/imagenet-roulette-trevor-paglen-kate-crawford-1658305\nhttps://www.frieze.com/article/how-ai-selfie-app-imagenet-roulette-took-internet-storm\nhttps://www.smithsonianmag.com/smart-news/art-project-exposed-racial-biases-artificial-intelligence-system-180973207/\nhttps://www.nytimes.com/2019/09/20/arts/design/imagenet-trevor-paglen-ai-facial-recognition.html\nhttps://www.businessinsider.com/viral-ai-selfie-classifier-imagenet-roulette-part-of-bias-project-2019-9\nhttps://www.theguardian.com/technology/2019/sep/17/imagenet-roulette-asian-racist-slur-selfie\nhttps://www.theregister.com/2019/10/23/ai_dataset_imagenet_consent/\nhttps://www.theguardian.com/technology/2019/sep/17/imagenet-roulette-asian-racist-slur-selfie\nhttps://www.businessinsider.com/viral-ai-selfie-classifier-imagenet-roulette-part-of-bias-project-2019-9\nhttps://venturebeat.com/2020/11/03/researchers-show-that-computer-vision-algorithms-pretrained-on-imagenet-exhibit-multiple-distressing-biases/\nhttps://qz.com/1034972/the-data-that-changed-the-direction-of-ai-research-and-possibly-the-world/\nhttps://thenextweb.com/news/ai-fails-to-recognize-these-nature-images-98-of-the-time\nhttps://onezero.medium.com/a-i-s-most-important-dataset-gets-a-privacy-overhaul-a-decade-too-late-6bbad8c151b5\nhttps://www.dailymail.co.uk/sciencetech/article-7480901/Fury-viral-ImageNet-app-gives-racist-labels-calls-people-rape-suspect.html\nhttps://www.theregister.com/2019/09/18/imagenet_roulette/\nhttps://www.wired.com/story/ai-biased-how-scientists-trying-fix/\nhttps://www.wired.com/story/researchers-blur-faces-launched-thousand-algorithms/\nhttps://venturebeat.com/2021/03/16/imagenet-creators-find-blurring-faces-for-privacy-has-a-minimal-impact-on-accuracy/\nhttps://towardsdatascience.com/the-fall-of-imagenet-5792061e5b8a\nhttps://www.technologyreview.com/2021/04/01/1021619/ai-data-errors-warp-machine-learning-progress/\nRelated \ud83c\udf10\nTiny Images dataset teaches AI systems to use racist slurs\nC4 dataset is trained on unsafe, copyright-protected web content\nPage info\nType: Incident\nPublished: June 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ibm-dataset-uses-millions-of-online-photos-without-consent-to-train-ai-syst", "content": "IBM dataset uses millions of online photos without consent to train AI systems\nOccurred: March 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nIBM scraped over a million of photographs on Flickr to create a dataset to train its AI products without the consent of the people in the photos, according to an NBC News investigation.\nThe technology company used a publicly available database of pictures, known as YFCC100M, which Flickr's then parent company Yahoo had collected for research purposes, to create its own Diversity in Faces (DiF) dataset with the intention of training facial recognition systems, including its own Watson Visual Recognition, to be less biased. \nAnalysis of DiF showed that top image tags in DiF revealed tags like \u201cparty,\u201d \u201cfamily,\u201d \u201cwedding,\u201d and \u201cfriends\u201d - indicating that DiF included personal and private moments captured in photos. But while the dataset included many images licensed with attribution requirements, IBM failed to provide attribution links or public credit for any images. Meant as an academic resource, the dataset was not publicly available and users had to be granted permission before they could access it.\nAnd the people in the pictures were not advised IBM was going to use their features to determine gender, race or any other identifiable features, such as eye colour, hair colour, or whether someone was wearing glasses. It also transpired IBM had ignored its own terms of use for the dataset. Later research by Exposing AI showed DiF was downloaded hundreds of times across the world - for reasons that remain unclear. \nThe finding prompted concerns that use of the DiF dataset could lead to surveillance and profiling, with minorities seen as particularly vulnerable to being targeted. It also sparked accusations of unethical conduct by IBM.\n\u2795 January 2020. IBM was sued in a class action seeking damages of USD 5,000 for each intentional violation of the Illinois Biometric Information Privacy Act, or USD 1,000 for each negligent violation, for all Illinois citizens whose biometric data was used in the DiF dataset.\n\u2795 June 2020. IBM said it would no longer provide facial recognition technology to US police departments for mass surveillance and racial profiling.\n\u2795 June 2021. Amazon and Microsoft teamed up to defend themselves against lawsuits accusing them of using DiF to train their own facial recognition products, and failing to gain the permission of people whose photographs were used in the dataset.\nSystem \ud83e\udd16\nIBM Diversity in Faces dataset\nOperator: Amazon; Microsoft\nDeveloper: Alphabet/Google; Amazon; IBM; Microsoft\nCountry: USA\nSector: Research/academia; Technology\nPurpose: Train and develop AI models\nTechnology: Database/dataset; Facial recognition; Computer vision\nIssue: Bias/discrimination; Ethics/values; Privacy; Surveillance\nTransparency: Governance; Privacy\nResearch, advocacy \ud83e\uddee\nHarvey, A., LaPlace, J. (2019). Exposing.ai\nInvestigations, assessments, audits \ud83e\uddd0\nNBC News (2019). Facial recognition's 'dirty little secret': Millions of online photos scraped without consent\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theverge.com/2019/3/12/18262646/ibm-didnt-inform-people-when-it-used-their-flickr-photos-for-facial-recognition-training\nhttps://www.theregister.com/2020/01/27/ibms_facial_recognition_software_gets_it_in_trouble_again/\nhttps://www.natlawreview.com/article/your-privacy-violated-using-your-face-to-train-ai-to-recognize-faces\nhttps://www.geekwire.com/2022/amazon-and-microsoft-deny-using-flickr-pics-for-facial-recognition-as-suits-test-limits-of-privacy-law/\nhttps://www.bbc.co.uk/news/technology-47555216\nhttps://news.sky.com/story/ibm-scraped-millions-of-flickr-users-photos-for-facial-recognition-project-11663999\nhttps://www.cnet.com/news/ibm-stirs-controversy-by-sharing-photos-for-ai-facial-recognition/\nhttps://www.dpreview.com/news/4791261447/no-flickr-didn-t-hand-your-photos-over-to-corporations-for-machine-learning\nhttps://www.cnbc.com/2019/01/29/ibm-releases-diverse-dataset-to-fight-facial-recognition-bias.html\nhttps://mashable.com/article/ibm-flickr-images-training-facial-recognition-system\nhttps://www.itpro.co.uk/technology/33218/ibm-used-flickr-photos-to-train-image-recognition-tech-without-user-consent\nhttps://www.npr.org/2020/06/09/873298837/ibm-abandons-facial-recognition-products-condemns-racially-biased-surveillance\nRelated \ud83c\udf10\nIBM sells Greg Marston voice for commercial cloning\nHRT Transgender dataset includes, exposes user data without consent\nPage info\nType: Incident\nPublished: June 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/us-government-research-dataset-raises-privacy-misuse-concerns", "content": "US government research dataset raises privacy, misuse concerns\nOccurred: September 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA dataset developed by a US government research body to benchmark facial recognition systems contained photographs of journalists, writers and political activists without their consent and potentially used for military applications.\nAccording (pdf) to US government agency Iarpa, 'the Janus program dramatically improved the performance of facial recognition software by increasing the speed and accuracy of identity matching.' \nBut the dataset included a number of political activists, civil rights advocates, and journalists, including Ai Wei Wei, Tracey Emin, Evgeny Morozov, John Maeda, and Ta-Nehisi Coates, according to activist Adam Harvey and highlighted by a 2019 Financial Times investigation.\nMone of these individuals had been made aware of their inclusion in the database by Noblis or Iarpa, and their images had been obtained without their explicit consent. Furthermore, the use of YouTube videos constituted a clear violation of the platform's terms of service. \nIn addition, as the FT pointed out, the use of the dataset by companies such as Chinese AI firm SenseTime and Japanese IT firm NEC, and by organisations such as China's National University of Defense Technology, raised concerns about its potential use for military and security purposes, including the mass surveillance of Uyghurs and other oppressed minorities.\nThe findings prompted ethical concerns about the need for greater oversight and ethical considerations in the development and use of facial recognition datasets and technologies. \nSystem \ud83e\udd16\nIarpa Janus Benchmark-C (IJP-C) dataset\nOperator: SenseTime; NEC; National University of Defense Technology (NUDT)\nDeveloper: Noblis; Iarpa\nCountry: USA\nSector: Media/entertainment/sports/arts; Politics\nPurpose: Create facial recognition benchmark\nTechnology: Database/dataset; Facial recognition; Computer vision; Neural network; Machine learning  \nIssue: Dual/multi-use; Privacy; Surveillance\nTransparency: Governance; Privacy\nInvestigations, assessments, audits \ud83e\uddd0\nHarvey, A., LaPlace, J. (2019). Exposing.ai\nMurgia M., Financial Times (2019). Who\u2019s using your face? The ugly truth about facial recognition\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://therecord.media/jillian-york-nft-stole-my-face/\nRelated \ud83c\udf10\nUnconstrained College Students dataset\nLabeled Faces in the Wild dataset\nPage info\nType: Issue\nPublished: June 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/study-top-chatbots-spread-russian-misinformation", "content": "Study: Top chatbots spread Russian misinformation\nOccurred: June 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nChatGPT, Gemini, Grok and other top AI chatbot are spreading Russian disinformation, according to researchers.\nAnti-misinformation company NewsGuard found that by entering 57 prompts based on false narratives attributed to John Mark Dougan, an American fugitive allegedly orchestrating misinformation from Moscow, into 10 prominent chatbots, the systems frequently cited Dougan's fabricated local news sites as credible sources and regurgitated false reports.\nThe study revealed that 152 out of the 570 total responses contained explicit disinformation, while 29 responses repeated the false claims with a disclaimer. Only 245 responses provided debunks or refusals to answer.\nThe findings chime with several recent studies showing that leading chatbots regularly spout misinformation and disinformation, and appear unprepared to handle a series of major elections across the world. \nResearchers warn that no existing countermeasures appear effective against this emerging threat.\nSystem \ud83e\udd16\nChatGPT\nClaude\nCoPilot\nGemini\nGrok\nLe Chat\nMeta AI\nPerplexity AI answer engine\nPi\nSmart Assistant\nOperator: OpenAI, You.com, xAI, Inflection, Mistral, Microsoft, Meta AI, Anthropic, Google, Perplexity\nDeveloper: OpenAI, You.com, xAI, Inflection, Mistral, Microsoft, Meta AI, Anthropic, Google, Perplexity\nCountry: Global\nSector: Politics\nPurpose: Generate text\nTechnology: Chatbot\nIssue: Mis/disinformation\nTransparency: \nResearch, advocacy \ud83e\uddee\nNewsGuard (2024). Top 10 Generative AI Models Mimic Russian Disinformation Claims A Third of the Time, Citing Moscow-Created Fake Local News Sites as Authoritative Sources\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.axios.com/2024/06/18/ai-chatbots-russian-propaganda\nhttps://www.nytimes.com/2023/02/08/technology/ai-chatbots-disinformation.html\nhttps://www.euronews.com/next/2024/06/18/chatgpt-grok-gemini-and-other-ai-chatbots-are-spewing-russian-misinformation-study-finds\nRelated \ud83c\udf10\nChatGPT powers automated content, spam farms\nTikTok exposes new users to Russia/Ukraine war disinformation\nPage info\nType: Issue\nPublished: June 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/uk-train-stations-secretly-monitor-travellers-emotions", "content": "UK train stations secretly monitor travellers' emotions \nOccurred: 2022-2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nTrain stations across the UK secretly used AI-powered cameras to monitor and analyse the ages, genders and emotions of travellers without their knowledge or consent.\nAccording to documents obtained under a Freedom of Information Request, trials were conducted by Network Rail, the organisation responsible for rail infrastructure in the UK, at eight major stations including London's Euston and Waterloo, and Manchester Piccadilly, for two years. \nThe systems used Amazon's Rekognition software to detect people trespassing on tracks, monitor platform crowding, identify \"antisocial behaviour\" such as running or smoking, and to spot potential bike thieves. The cameras were also used to predict travellers' emotions like happiness, sadness, and anger by analysing their facial expressions. \nThe finding prompted concerns from civil rights and privacy advocates about Network Rail's lack of transparency and public consultation about the use of AI surveillance systems in public spaces, the inaccuracy of emotion recognition systems, and the potential misuse of personal data for purposes such as emotional profiling and targeted advertising.\nNetwork Rail later denied using emotion recognition. \nSystem \ud83e\udd16\nAmazon Rekognition website\nAmazon Rekognition Wikipedia profile\nOperator: Network Rail\nDeveloper: Amazon\nCountry: UK\nSector: Transport/logistics\nPurpose: Reduce crime\nTechnology: Emotion recognition; Facial recognition; Object recognition\nIssue: Accuracy/reliability; Privacy; Surveillance\nTransparency: Governance\nInvestigations, assessments, audits \ud83e\uddd0\nBig Brother Watch (2024). UK TRAIN STATIONS TRIALLED AMAZON EMOTION DETECTION THAT COULD FEED YOU ADVERTS\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.wired.com/story/amazon-ai-cameras-emotions-uk-train-passengers/\nhttps://www.dailymail.co.uk/news/article-13545891/Network-Rail-Amazon-AI-cameras-railway-passengers-emotions.html\nhttps://www.techspot.com/news/103442-cameras-using-amazon-ai-analyzed-emotions-train-travelers.html\nhttps://www.thetimes.com/uk/technology-uk/article/network-rail-secretly-used-ai-to-read-passengers-emotions-nknvtj58n\nRelated \ud83c\udf10\nKing's Cross quietly uses live facial recognition to monitor citizens\nAmazon Rekognition falsely matches 28 Members of Congress\nPage info\nType: Issue\nPublished: June 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/dream-machine-ai-video-generator-makes-porn", "content": "Dream Machine AI video generator makes porn\nOccurred: June 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nUsers figured out how to trick Dream Machine into generating explicit videos and nudity, violating the terms of service of its creator and raising questions about its safety. \nUsers found ways to \"jailbreak\" or bypass the content filters of AI video generator Dream Machine to generate videos containing nudity and sexually explicit material, even if the quality is crude compared to AI-generated images, according to 404 Media.\nThe discovery raised questions about Luma Labs' governance and Dream Machine's safety, and about the safety of AI video generators in general. \nSystem \ud83e\udd16\nDream Machine AI video generator\nDocuments \ud83d\udcc3\nLuma Labs terms of service\nLuma launches Dream Machine\nOperator: Dream Machine users\nDeveloper: Luma Labs\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Generate video\nTechnology: Text-to-video\nIssue: Privacy; Safety\nTransparency: Governance\nInvestigations, assessments, audits \ud83e\uddd0\nhttps://www.404media.co/users-jailbreak-ai-video-generator-to-make-porn/\nRelated \ud83c\udf10\nItalian privacy watchdog opens investigation into Sora\nElevenLabs voice generator makes celebrity voices read offensive messages\nPage info\nType: Issue\nPublished: June 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/indian-travel-company-uses-professional-models-ai-likeness-in-ad", "content": "Indian travel company ad uses professional model\u2019s AI likeness without consent\nOccurred: May 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nIndian travel agency Yatra Online used the facial likeness of Indian model Kanchan Nagar in an advert without her permission, resulting in the violation of her personality rights and a legal complaint.\nNagar issued a legal notice to the Advertising Standards Council of India emphasising the harms of the unregulated use of AI and the need for for ethics in advertising in order to protect the authenticity of people\u2019s identity and their privacy.\nThe incident raised concerns about the misuse of deepfake and AI technologies in advertising.\nThese technologies can create realistic images or videos, known as \u2018deepfakes\u2019, without individuals\u2019 consent, potentially leading to fraud, defamation, and the spread of misinformation and disinformation, amongst other known harms.\nSystem \ud83e\udd16\nUnknown\nOperator: Yatra Online\nDeveloper:\nCountry: India\nSector: Media/entertainment/sports/arts\nPurpose: Generate images\nTechnology: Deepfake - image\nIssue: Ethics/values; Personality rights; Privacy\nTransparency: Governance; Marketing \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.ndtvprofit.com/amp/law-and-policy/ai-misuse-model-fights-misuse-in-ads-calls-for-regulation \nhttps://www.indiatoday.in/law/story/woman-model-sues-advertising-council-after-travel-firm-uses-her-deepfake-photo-2542704-2024-05-23 \nhttps://brandequity.economictimes.indiatimes.com/news/advertising/model-petitions-asci-on-deepfakes-and-ai-misuse/110182341 \n\nRelated \ud83c\udf10\nBBC presenter\u2019s AI-generated voice used to trick company\nMichel Janse deepfake used for advert without consent\nChinese voice actor sues AI companies for using her voice without consent\n\n\nPage info\nType: Incident\nPublished: June 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/meta-under-fire-for-decision-to-train-generative-ai-on-user-content", "content": "Meta under fire for decision to train generative AI on user content\nOccurred: June 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nNews that Meta is to train its generative AI models on content from Facebook and Instagram users sparked criticism from digital rights groups, users and content creators. \nMeta informed UK and European users of changes to its privacy policy that would enable it to train its AI models on their posts, images, image captions and comments.\nThe European Center for Digital Rights, Noyb, filed complaints in eleven European countries arguing that Meta\u2019s plan would constitute an abuse of personal data and a breach of privacy. \nNoyb also criticised Meta for using \u201cdark patterns\u201d in user experience design to make opting out of data collection difficult, and that the policy would allow the company to use all public and non-public user data collected since 2007 for any undefined type of current and future AI technology.\nInternationally, Meta has been criticised for offering different protections for EU and UK citizens than the rest of the world. Meta argued that the data collection is necessary to train AI services that reflect the diverse cultures and languages of the European communities who will use them. \nIn response to the pressure, Meta reversed the decision following an objection from the Irish Data Protection Commission. \n\u2795 July 2024. UK data rights campaign orgnisation Open Rights Group filed a complaint with the UK Information Commissioner's Office (ICO) arguing that Meta' AI policy has no \u201clegitimate interest\u201d under GDPR.\nSystem \ud83e\udd16\nMultiple\nOperator: Meta\nDeveloper: Meta\nCountry: Global\nSector: Technology\nPurpose: Generate text; Generate images\nTechnology: Generative AI; Machine learning; Neural network; Deep learning\nIssue: Privacy\nTransparency: Governance \nLegal, regulatory\nNYOB. Legal complaints - Austria, Belgium, France, Germany, Greece, Italy, Ireland, the Netherlands, Norway, Poland, Spain\nResearch, advocacy\nNYOB. Overview of Meta\u2019s falsehoods and \u201cspin\u201d (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.fastcompany.com/91132854/instagram-training-ai-on-your-data-its-nearly-impossible-to-opt-out\nhttps://swgfl.org.uk/magazine/meta-announces-plans-to-use-facebook-and-instagram-posts-to-train-ai/\nhttps://www.washingtonpost.com/technology/2024/06/06/instagram-meta-ai-training-cara/\nhttps://www.creativebloq.com/news/instagram-ai-training\nhttps://www.bbc.co.uk/news/articles/cw99n3qjeyjo\nhttps://www.theregister.com/2024/06/10/meta_ai_training/\nMeta to use Instagram and Facebook posts from as far back as 2007 to train artificial intelligence tools - ABC News\nhttps://www.sportskeeda.com/nascar/news-frankie-muniz-s-wife-paige-slams-meta-s-ai-program-copyright-concerns-i-consent\nhttps://cryptorank.io/news/feed/d5cd9-meta-suspend-plans-train-ai-eu-users-data\nhttps://noyb.eu/en/preliminary-noyb-win-meta-stops-ai-plans-eu\nRelated \ud83c\udf10\nAdobe terms of use update sparks privacy, copyright controversy \nSlack forces users to opt-out of training its AI models\n\nPage info\nType: Issue\nPublished: June 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/hrt-transgender-dataset-includes-exposes-user-data-without-consent", "content": "HRT Transgender dataset uses YouTubers' data without consent\nOccurred: July 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA dataset supposedly meant to help protect against terrorism caused controversy for scraping the data of YouTube users without permission and exposing their data for years.\nKarl Ricanek, a professor of computer science at the University of North Carolina at Wilmington, claimed his HRT Transgender dataset was developed to protect against the possibility of terrorists using Hormone Replacement Therapy (HRT) to avoid facial recognition and sneak across borders undetected, and that he had gained the permission of people whose data had been used. \nHowever, a July 2022 peer-reviewed audit of the project's background and practices by researchers Os Keyes and Jeanie Austin took issue with a number of Ricanek's practices and claims, including:\nThat the real reason for the dataset was to strengthen national security. The researchers derided the claim as 'ludicrous'.\nThat Ricanek gained the consent of all those people whose videos he used. This appeared not to be the case.\nThat only the dataset images were distributed to third parties. But the videos were available via an unprotected Dropbox URL, including those that had been made private or deleted.\nThat Ricanek stopped giving access to the dataset in 2017. However, it was still accessible on Dropbox five years later.\nThat the researchers must have hacked Dropbox to access the files. In fact, they gained acess via a UNCW public records request.\nThe fracas was seen to highlight the need for the consent of users whose data is used in datasets, and for meaningful transparency in the governance of datasets.\nSystem \ud83e\udd16\nHRT Transgender dataset\nOperator: \nDeveloper: University of North Carolina, Wilmington (UNCW)\nCountry: USA\nSector: Research/academia; Technology\nPurpose: Identify HRT users\nTechnology: Database/dataset; Facial recognition; Computer vision\nIssue: Bias/discrimination - LGBTQ; Ethics/values; Privacy\nTransparency: Governance; Privacy\nResearch, advocacy \ud83e\uddee\nScheuerman M.K., Pape M., Hanna A. (2021). Auto-essentialization: Gender in automated facial analysis as extended colonial project (pdf)\nKumar V., Ramachandra R., Namboodiri A.M., Busch C. (2016). Robust transgender face recognition: Approach based on appearance and therapy factors\nInvestigations, assessments, audits \ud83e\uddd0\nKeyes, O., Austin, J. (2022). Feeling fixes: Mess and emotion in algorithmic audits\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.vice.com/en/article/93aj3z/facial-recognition-researcher-left-a-trans-database-exposed-for-years-after-using-images-without-permission\nhttps://www.itworldcanada.com/post/face-recognition-dataset-of-trans-people-is-still-available-online\nhttps://www.biometricupdate.com/202209/prominent-facial-recognition-researcher-scraped-videos-of-trans-people-left-dataset-exposed\nhttps://algorithmwatch.org/en/dataset-face-recognition/\nhttps://www.theverge.com/2017/8/22/16180080/transgender-youtubers-ai-facial-recognition-dataset\nRelated \ud83c\udf10\nDuke University pulls facial recognition dataset after privacy controversy\nBooks3 dataset shut down after legal notice from Danish anti-piracy group\nPage info\nType: Incident\nPublished: June 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/goemotions-dataset-criticised-for-mislabelling-content-stealing-content", "content": "GoEmotions dataset criticised for mislabelling content, stealing user content\nOccurred: September 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\n30 percent of labels applied to Google's GoEmotions dataset were mislabeled, indicating that the data had not been verified by humans, according to a research study.\nUsing a random sample of 1,000 entries, US-based Surge AI concluded that Google's outsourced Indian human labelers were likely given 'no additional metadata' about each Reddit comment, thereby losing its context and meaning to different types of users, not least those in the US. \nGoogle was also criticised for ignoring the privacy of the Reddit community: 'It is entirely unethical to train an AI on human-created content without the expressed individual consent of the humans who created it.' 'When I post on Reddit', argued TNW's Tristran Harris. 'I do so in the good faith that my discourse is intended for other humans. Google doesn\u2019t compensate me for my data so it shouldn\u2019t use it, even if the terms of service allow for it,' he added.\nThe fracas prompted critics to argue that AI models trained on the dataset would produce erroneous output', 'causing demonstrable harm to other humans' and called for it to be deleted. It also highlighted the challenges of accurate labelling of data, notably when this is outsourced to people whose first language is not English and from other cultures.\nSystem \ud83e\udd16\nGoEmotions dataset\nOperator: Alphabet/Google\nDeveloper: Alphabet/Google\nCountry: USA\nSector: Research/academia; Technology\nPurpose: Classify emotions\nTechnology: Database/dataset\nIssue: Accuracy/reliability; Cheating/plagiarism; Ethics/values\nTransparency: Privacy\nResearch, advocacy \ud83e\uddee\nSurgeAI: 30% of Google's Emotions Dataset is Mislabeled\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.the-sun.com/tech/5785169/google-slammed-over-building-ai-dangerous-to-humans/\nhttps://thenextweb.com/news/scathing-study-exposes-googles-harmful-approach-ai-development\nhttps://datainnovation.org/2022/09/5-qs-for-edwin-chen-ceo-of-surge-ai/\nhttps://www.analyticsinsight.net/googles-ai-is-not-a-pro-in-data-labeling-but-the-comp-fails-to-admit-it/\nhttps://hacker-news.news/post/30066720\nRelated \ud83c\udf10\nFacebook labels Black men 'primates'\nGoogle Photos mislabels black Americans as gorillas\nPage info\nType: Issue\nPublished: June 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/duke-university-pulls-facial-recognition-dataset-after-privacy-controversy", "content": "Duke University pulls facial recognition dataset after privacy controversy\nOccurred: September 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nDuke University pulled a controversial dataset of surveillance videos of thousands of students and staff recorded on campus without consent after a high-profile media investigation.\nCreated and released in 2015 for research purposes, the DukeMTMC dataset was revealed to have violated the university's own ethics guidelines by recording outdoors without consent. As reported in Duke's Chronicle newspaper, the university's Institutional Review Board said it had approved a study that would take place in a 'defined indoor space' and create a dataset that would be accessible only upon researchers\u2019 request.\nCritics also expressed concerns that while DukeMTMC had been released under a CC BY-NC-SA 4.0 license, which allows for attributed, non-commercial sharing and adaption of the dataset, it has been and continues to be used more broadly, including for unethical surveillance applications by companies and researchers in China and elsewhere.\nAnalysis by artist Adam Harvey showed that DukeMTMC had been used by a wide range of academic institutions and companies with known links to the Chinese military and to Chinese government surveillance of Uyghurs in Xianjiang and elsewhere, including Hikvision, Megvii (Face++), SenseTime, Beihang University, China's National University of Defense Technology, and the PLA's Army Engineering University.\nHarvey also points out that the project was 'supported in part by the United States Army Research Laboratory' and was for 'automated analysis of crowds and social gatherings for surveillance and security applications.'\nIn response to the backlash, Carlo Tomasi, Iris Einheuser professor of computer science at Duke and an author of the study research paper, apologised for running the study outdoors and for making it publicly available. The dataset was also removed from Duke's website. However, the takedown was largely ineffective as the data had already proliferated widely online and been remixed and used by Microsoft, IBM, Baidu and multiple Chinese technology companies to improve their facial recognition systems. \nThe fracas prompted critics to highlight the limitations of taking down original datasets and urge organisations such as Duke University to identify and take down derived datasets, and to better regulate the use of datasets form the outset, and to regulate the creation of derived datasets that enable unethical research. \nSystem \ud83e\udd16\nDukeMTMC facial recognition dataset\nOperator: CloudWalk; Hikvision; Megvii; SenseNets; SeeQuestor; SenseTime; Beihang University; National University of Defense Technology, China; NEC; PLA Army Engineering University \nDeveloper: Ergys Ristani; Francesco Solera; Roger Zou; Rita Cucchiara; Carlo Tomasi; Duke University\nCountry: USA\nSector: Technology; Research/academia\nPurpose: Train facial recognition systems\nTechnology: Dataset; Facial recognition; Computer vision\nIssue: Ethics/values; Dual/multi-use; Privacy\nTransparency: Governance; Privacy\nResearch, advocacy \ud83e\uddee\nHarvey, A., LaPlace, J. (2019). Exposing.ai\nPeng K., Mathur A., Narayanan A. (2021). Mitigating Dataset Harms Requires Stewardship: Lessons from 1000 Papers (pdf)\nInvestigations, assessments, audits \ud83e\uddd0\nMurgia M., Financial Times (2019). Who\u2019s using your face? The ugly truth about facial recognition\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.ft.com/content/7d3e0d6a-87a0-11e9-a028-86cea8523dc2\nhttps://www.dukechronicle.com/article/2019/06/duke-university-facial-recognition-data-set-study-surveillance-video-students-china-uyghur\nhttps://www.technologyreview.com/2021/08/13/1031836/ai-ethics-responsible-data-stewardship/\nhttps://freedom-to-tinker.com/2020/10/21/facial-recognition-datasets-are-being-widely-used-despite-being-taken-down-due-to-ethical-concerns-heres-how/\nRelated \ud83c\udf10\nStanford University Brainwash cafe facial recognition dataset\nWILDTRACK pedestrian detection dataset\nPage info\nType: Issue\nPublished: June 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/taylor-klein-face-is-deepfaked-onto-porn", "content": "US college student 'Taylor Klein' face is deepfaked onto porn\nOccurred: \nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nUS college student 'Taylor Klein' (a pseudonym used to protect her identity) was deepfaked onto pornographic videos without her consent.\nThe videos were posted online, along with her real name, hometown, and college details and describing her supposed desire to hook up with strangers. Klein recieved regular abuse and harassment, and described feeling deep mental anguish, numbness, and concern for her personal safety.\nDespite reporting the video to the police, action was not taken due to lack of US federal or state laws against non-consensual deepfake porn. So 'Taylor' teamed up with another victim to investigate who could have created the deepfakes, suspecting it was someone they knew from their male-dominated engineering programme. \nThe incident highlighted how deepfake technology can be exploited for non-consensual porn, the damage that can be inflicted on victims, and their lack of legal recourse.\n\u2795 March 2023. Klein's experiences were unveiled in the documentary Another Body.\nSystem \ud83e\udd16\nUnknown\nOperator: \nDeveloper:  \nCountry: USA\nSector: Personal - individual\nPurpose: Earn revenue\nTechnology: Deepfake - image\nIssue: Safety\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.youtube.com/watch?v=qtY3RkmDC9E\nhttps://www.theguardian.com/film/2023/nov/22/another-body-review-terrifying-dive-into-the-world-of-deepfake-porn\nhttps://www.rollingstone.com/tv-movies/tv-movie-features/another-body-sxsw-college-girl-deepfake-porn-epidemic-twitch-documentary-1234692560/\nhttps://www.cbc.ca/documentaries/the-passionate-eye/her-face-was-deepfaked-onto-porn-when-police-wouldn-t-help-she-did-her-own-investigation-1.7035523\nhttps://www.smh.com.au/culture/movies/i-think-you-need-to-see-this-celebrities-are-not-the-only-victims-of-deepfake-porn-20240228-p5f8ee.html\nhttps://www.wired.com/story/deepfake-porn-documentary-another-body/\nhttps://www.dazeddigital.com/film-tv/article/61256/1/another-body-how-deepfake-porn-is-destroying-lives-sophie-compton-reuben-hamlyn\nRelated \ud83c\udf10\nItalian PM seeks damages over deepfake porn videos\nRana Ayyub deepfake porn attack, doxxing\nPage info\nType: Incident\nPublished: June 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/coronavirus-mask-image-dataset-criticised-for-violating-user-privacy", "content": "Coronavirus Mask Image dataset criticised for violating user privacy\nOccurred: May 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA dataset of masked people taken from photographs posted to Instagram and used to improve facial recognition systems was criticised as a violation of people's privacy. \nUS-based AI startup WorkAround identified around 1,200 photos on Instagram and labeled them with masks on or off in order to help train facial recognition systems identify individuals during the COVID-19 pandemic. The company neither informed or gained the consent of those peeople whose photographs they used.\nA spokesperson for Facebook, which owns Instagram, told CNET they do not allow third parties to collect or use photos posted by their users in this way without their consent. However, WorkAround CEO Wafaa Arbash said 'We\u2019re not making any money off of this, it\u2019s not commercial. The goal and the intention was to help any data science or machine learning engineers who are working to fix this issue and help with public safety.'\nThe incident highlighted concerns about the impact of facial recognition on people's privacy, the provision of open data containing privacy-violating information, and how such datasets may be used in other contexts. \nSystem \ud83e\udd16\nCoronavirus Mask Image dataset (Github)\nOperator: WorkAround\nDeveloper: WorkAround\nCountry: USA\nSector: Health\nPurpose: Improve facial recognition algorithms\nTechnology: Database/dataset; Facial recognition; Computer vision\nIssue: Privacy; Ethics/values\nTransparency: Privacy\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.cnet.com/news/your-face-mask-selfies-could-be-training-the-next-facial-recognition-tool/\nhttps://www.dailydot.com/debug/face-mask-selfies-facial-recognition/\nhttps://www.biometricupdate.com/202005/researchers-use-instagram-mask-selfies-to-improve-biometric-facial-recognition-algorithms\nhttps://www.wired.co.uk/article/iphone-13-features-touch-id\nhttps://www.techspot.com/news/85307-face-mask-selfies-used-retrain-facial-recognition-systems.html\nhttps://www.thelist.com/211346/the-unexpected-benefit-of-taking-face-mask-selfies/\nhttps://www.telegraph.co.uk/technology/2020/06/04/widespread-face-mask-use-could-make-facial-recognition-less/\nhttps://www.inputmag.com/culture/facial-recognition-ai-researchers-are-trying-to-navigate-a-covid19-world/amp\nhttps://www.dailymail.co.uk/sciencetech/article-8337541/Researchers-harvest-peoples-mask-selfies-without-consent-improve-facial-recognition-software.html\nhttps://www.cnet.com/health/facial-recognition-firms-are-scrambling-to-see-around-face-masks/\nRelated \ud83c\udf10\nStanford hospital COVID-19 vaccine allocation triggers backlash\nQCovid prediction algorithm wrongly identifies high-risk patients\nPage info\nType: Issue\nPublished: June 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/50-melbourne-school-girls-targeted-using-ai-nude-images", "content": "50 Melbourne school girls targeted using AI nude images\nOccurred: June 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nFifty female students at Bacchus Marsh Grammar school in Melbourne, Australia, were targeted with AI-generated fake nude images.\nThe images which were shared on social media, appeared to have been created using AI to graft photos of the girls' faces obtained from their private social media accounts onto others' bodies. \nThe mother of one of the targeted students shared that her 16-year-old daughter vomited after seeing the \"incredibly graphic\" and \"mutilated\" images online. The school said it was working with police to remove the images from social media and determine if the perpetrator is a student or someone else.\nA teenage male was subsequently arrested in relation to the incident and released pending further inquiries. \nThe school said it was offering support to the affected students, who are in years 9 to 12 (approximately ages 14-18), and their families. The school's principal described the incident as \"appalling\" and stated that the girls should be able to learn without facing such \"nonsense\".\nThe incident drew widespread criticism, with many expressing concern over the safety and well-being of young girls in schools and online communities, and the lack of adequate regulation on non-consensual deepfake pornographic images in Australia. \nSystem \ud83e\udd16\nUnknown\nOperator:\nDeveloper:  \nCountry: Australia\nSector: Education\nPurpose:  \nTechnology: Deepfake - image\nIssue: Safety\nTransparency: Governance; Marketing \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://edition.cnn.com/2024/06/13/australia/australia-boy-arrested-deepfakes-schoolgirls-intl-hnk/index.html\nhttps://www.theguardian.com/commentisfree/article/2024/jun/13/bacchus-marsh-grammar-ai-deepfake-nudes-students-comment\nhttps://www.theguardian.com/australia-news/article/2024/jun/12/schoolboy-arrested-after-allegedly-posting-fake-explicit-images-of-female-students-ntwnfb\nhttps://www.abc.net.au/news/2024-06-11/bacchus-marsh-grammar-explicit-images-ai-nude/103965298\nhttps://www.aap.com.au/news/student-deepfakes-reflective-of-school-porn-crisis/\nhttps://www.skynews.com.au/australia-news/crime/teenage-boy-arrested-after-50-girls-from-bacchus-marsh-grammar-targeted-in-deepfake-ai-incident/news-story/f090b6eab037813c7eaf8e5ce13994dc\nRelated \ud83c\udf10\nBeverly Hills students created, shared AI nude images of fellow students\nWestfield High School non-concensual nude deepfakes\nPage info\nType: Incident\nPublished: June 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/study-hate-content-increases-12-percent-as-laion-dataset-size-increases", "content": "Study: Hate content increases 12 percent as LAION dataset size increases\nOccurred: July 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA comparative audit of two datasets, LAION-400M and LAION-2B, revealed that as the dataset scale increases, hate content also increases by nearly 12 percent. \nIn a recent study titled \u201cInto the LAION\u2019s Den: Investigating Hate in Multimodal Datasets,\u201d researchers examined the impact of scaling datasets on hateful content by comparing two datasets: LAION-400M and LAION-2B. \nThe results of the audit revealed that hate content increased by nearly 12 percent as the dataset size grew - an increase measured qualitatively and quantitatively using the Hate Content Rate (HCR) metric.\nThe finding highlighted the consequences of data scaling in vision-language datasets.\nSystem \ud83e\udd16\nLAION-400M dataset\nOperator: Alphabet/Google; Prisma Labs; Stability AI  \nDeveloper: LAION\nCountry: Germany\nSector: Technology\nPurpose: Train large language models\nTechnology: Database/dataset; Neural network; Deep learning; Machine learning\nIssue: Safety\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nMozilla (2023). Fellow Research: As AI Companies Scale Datasets, They Scale Hate, Too\nBirhane A., Luccioni A.S. et al (2023). Into the LAIONs Den: Investigating Hate in Multimodal Datasets\nRelated \ud83c\udf10\nLAION-400M dataset features racist, derogatory, pornographic content\nTiny Images dataset teaches AI systems to use racist slurs\nPage info\nType: Issue\nPublished: June 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/laion-400m-dataset-features-racist-derogatory-pornographic-content", "content": "LAION-400M dataset features racist, derogatory, pornographic content\nOccurred: December 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe LAION-400M dataset contained 'troublesome and explicit images and text pairs of rape, pornography, malign stereotypes, racist and ethnic slurs, and other extremely problematic content,' according to researchers.\nA audit (pdf) by University College Dublin, University of Edinburgh and UnifyID researchers found that 'When the researchers typed the word \u201cKorean,\u201d LAION-400M didn\u2019t bring up images of BTS or bulgogi, but naked Korean women. Searching the word \u201cIndian\u201d brought up pictures of South Asian women being raped. 'Best president' brought up images of Donald Trump.'\nThe researchers argued that LAION-400M and other large datasets are rarely managed in the interests of the individuals and organisations whose data is being collected and used, and that they are unlikely to have been properly cleaned. \n'The rights of the data subject remain unaddressed here' the researchers argue (pdf). 'It is reckless and dangerous to underplay the harms inherent in such large scale datasets and encourage their use in industrial and commercial settings. The responsibility of the licence scheme under which the dataset is provided falls solely on the dataset creator.'\nSystem \ud83e\udd16\nLAION-400M dataset\nOperator: Alphabet/Google; Prisma Labs; Stability AI\nDeveloper: LAION\nCountry: Germany\nSector: Technology\nPurpose: Pair text and images\nTechnology: Database/dataset; Neural network; Deep learning; Machine learning\nIssue: Bias/discrimination - race, ethnicity; Copyright; Ethics/values; Privacy; Safety\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nBirhane A., Prabhu V.U., Kahembwe E. (2021). Multimodal datasets: misogyny, pornography, and malignant stereotypes\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.unite.ai/are-under-curated-hyperscale-ai-datasets-worse-than-the-internet-itself/\nhttps://www.politico.eu/newsletter/ai-decoded/politico-ai-decoded-us-bill-of-ai-rights-parliament-gets-its-act-together-sort-of-the-dark-side-of-large-ai-models/\nhttps://info.deeplearning.ai/the-batch-ai-has-a-web-problem-google-goes-multimodal-unfinished-symphony-completed-transformers-get-faster-still-1\nhttps://www.reddit.com/r/MachineLearning/comments/pmwvw9/p_laion400m_opensource_dataset_of_400_million/\nRelated \ud83c\udf10\nChild sex abuse images discovered on LAION-5B, LAION-400M datasets\nTiny Images dataset teaches AI systems to use racist slurs\nPage info\nType: Issue\nPublished: June 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/bdd100k-dataset-is-worse-at-spotting-people-with-darker-skin", "content": "Study: BDD100K dataset is worse at spotting people with darker skin\nOccurred: March 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe BDD100K self-driving video dataset is less effective at detecting individuals with darker skin tones, according to researchers.\nThe BDD100k dataset was discovered by Georgia Institute of Technology researchers to contain a higher percentage of \"dark skin\" labels, resulting in reduced accuracy and reliability in autonomous driving systems, particularly in scenarios involving individuals with darker skin tones. \nBy hiring people to manually apply labels according to skin colour based on the Fitzpatrick scale, a scale commonly used to classify human skin colour, the researchers found that BDD100K is, on average, 4.8 percent more accurate at correctly spotting light-skinned pedestrians, and up to 12 per cent worse at spotting people with darker skin.\nThe finding highlighted the need for more diverse and inclusive datasets.\nSystem \ud83e\udd16\n80 Million Tiny Images dataset\nOperator: University of Toronto\nDeveloper: MIT\nCountry: USA\nSector: Technology; Research/academia\nPurpose: Identify & classify objects, people\nTechnology: Database/dataset; Computer vision; Facial recognition; Object recognition\nIssue: Bias/discrimination - race, gender; Privacy; Safety \nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nLlorca D.F. et al (2023). Attribute Annotation and Bias Evaluation in Visual Datasets for Autonomous Driving\nWilson B., Hoffman J., Morgenstern J. (2019). Predictive Inequity in Object Detection (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.dailymail.co.uk/sciencetech/article-6795741/Driverless-cars-likely-HIT-people-darker-skin.html\nhttps://mashable.com/article/self-driving-cars-trouble-detecting-darker-skin\nhttps://www.theguardian.com/technology/shortcuts/2019/mar/13/driverless-cars-racist\nhttps://www.vox.com/future-perfect/2019/3/5/18251924/self-driving-car-racial-bias-study-autonomous-vehicle-dark-skin\nhttps://www.mhlnews.com/technology-automation/article/22055534/researchers-say-autonomous-vehicles-could-be-racist\nhttps://www.theregister.com/2019/03/10/ai_roundup_080319/\nRelated \ud83c\udf10\nBDD100K dataset exposes drivers to surveillance, data misuse\nTiny Images dataset teaches AI systems to use racist slurs\nPage info\nType: Issue\nPublished: June 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/bdd100k-dataset-exposes-drivers-to-surveillance-data-misuse", "content": "BDD100K dataset exposes drivers to surveillance, data misuse\nOccurred: February 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe BDD100K self-driving video dataset raises significant privacy concerns, according to researchers.\nThe dataset was found to contain footage of individuals (pedestrians) and vehicles (licence plates) captured in public spaces without their explicit consent, raising ethical issues around privacy violations and the potential misuse of personal data.\nThe high-resolution videos and images in the dataset could potentially enable identification of specific individuals or vehicles, exposing them to risks like surveillance, tracking, or other privacy intrusions, thereby compromising privacy and civil liberties.\nWhile efforts were made to blur faces and license plates, the researchers showed many instances where these privacy protection measures failed, leaving identifiable personal information exposed. Larger faces and plates were especially prone to being inadequately anonymised.\nThe finding highlighted the need for robust privacy-preserving techniques and guidelines when collecting and using datasets incorporating sensitive personal information.\nSystem \ud83e\udd16\nBDD100k dataset\nOperator: \nDeveloper: UC Berkeley\nCountry: USA\nSector: Automotive\nPurpose: Train self-driving car systems\nTechnology: Database/dataset; Computer vision; Facial recognition; Object recognition\nIssue: Dual/multi-use; Privacy; Safety; Surveillance\nTransparency: \nResearch, advocacy \ud83e\uddee\nWilson B., Hoffman J., Morgenstern J. (2019). Predictive Inequity in Object Detection (pdf)\nRelated \ud83c\udf10\nBDD100K dataset is worse at spotting people with darker skin\nBookCorpus dataset accused of copyright abuse and bias\nPage info\nType: Issue\nPublished: June 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tiny-images-dataset-teaches-ai-systems-to-use-racist-slurs", "content": "Tiny Images dataset teaches AI systems to use racist slurs\nOccurred: July 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nResearchers found that several publicly available image datasets, including 80 Million Tiny Images, contained racist and misogynistic slurs which were causing models trained on them to exhibit racial and sexual bias. \nUniversity of Toronto researchers Vinay Uday Prabhu and Abeba Birhane discovered (pdf) that large-scale image datasets, including the much cited 80 Million Tiny Images dataset, were associating offensive labels with real pictures. \nAccording to the research, the dataset labeled Black and Asian people with racist slurs and women holding children labeled as whores. It also included pornographic images. 80 Million Tiny Images was used to teach machine learning models to automatically identify and list the people and objects depicted in still images.\nIn addition, the researchers discovered that WordNet, from which 80 Million Tiny Images copied content, contained derogatory terms, resulting in images and labels that confirm and reinforce stereotypes and biases, albeit inadvertently. \nThe creators of 80 Million Tiny Images acknowledged that the large scale (79.3 million images) and small image size (32x32 pixels) made a comprehensive visual inspection of the dataset's contents difficult, leading to the offensive labels and images going unnoticed initially. \nThey apologised, and urged researchers to refrain from using the dataset and delete any copies to mitigate further harm. How many copies were downloaded, how they were used, and whether their plea was followed remains unclear.\nSystem \ud83e\udd16\n80 Million Tiny Images dataset\nOperator: University of Toronto\nDeveloper: MIT\nCountry: USA\nSector: Technology; Research/academia\nPurpose: Identify & classify objects, people\nTechnology: Database/dataset; Computer vision; Object recognition\nIssue: Bias/discrimination - race, gender; Privacy; Safety \nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nPrabhu V.U., Birhane A. (2020). Large Image Datasets: A Pyrrhic Win for Computer Vision?\nKrizhevsky A. (2009). Learning Multiple Layers of Features from Tiny Images (pdf) \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theregister.com/2020/07/01/mit_dataset_removed/\nhttps://thenextweb.com/neural/2020/07/01/mit-removes-huge-dataset-that-teaches-ai-systems-to-use-racist-misogynistic-slurs/\nhttps://www.dailymail.co.uk/sciencetech/article-8483929/MIT-pulls-racist-misogynistic-dataset-offline.html\nhttps://www.foxnews.com/tech/mit-pulls-massive-ai-dataset-over-racist-misogynistic-content\nhttps://pureai.com/articles/2020/07/07/mit-nyu-pulls-tiny-images.aspx\nhttps://venturebeat.com/2020/07/01/mit-takes-down-80-million-tiny-images-data-set-due-to-racist-and-offensive-content/\nhttps://gizmodo.com/mit-takes-down-popular-ai-dataset-due-to-racist-misogy-1844244206\nhttps://www.reddit.com/r/MachineLearning/comments/hjelz4/n_mit_permanently_pulls_offline_tiny_images/\nRelated \ud83c\udf10\nImageNet image recognition dataset\nLAION image-text pairings datasets\nPage info\nType: Issue\nPublished: June 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/bookcorpus-dataset-accused-of-copyright-abuse-and-bias", "content": "BookCorpus dataset accused of copyright abuse and bias\nOccurred: May 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nBookCorpus, a dataset used to train dozens of influential language models including Google\u2019s BERT, OpenAI\u2019s GPT, and Amazon\u2019s Bort, was accused of abusing copyright, bias, and misleading marketing. \nIn a working paper, researchers Jack Bandy and Nicholas Vincent set out several concerns abut BookCorpus, and called for stronger standards for the documentation of datasets. \nNotably, they found that BookCorpus violated the copyright restrictions of over 200 books which had been taken from Smashwords, a website that describing itself as \u201cthe world\u2019s largest distributor of indie ebooks.\nThe reseachers also discovered that the size of the BookCorpus dataset had been inaccurately described, with several books reproduced multiple times, and that the sample was skewed to certain genres and religious forms.\nDespite these concerns, BookCorpus remains widely available on data sharing websites such as Hugging Face.\n\u2796 September 2016. The Guardian noted that the developers of BookCorpus failed to make contact with or seek consent from authors whose books were included in the dataset.\nSystem \ud83e\udd16\nBookCorpus dataset\nDocuments \ud83d\udcc3\nZhu Y., Kiros R., Zemel R., Salakhutdinov R., Urtasun R., Torralba A., Fidler S. (2015). Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books\nOperator: Alphabet/Google; Amazon; OpenAI; Samsung\nDeveloper: Yukun Zhu; Ryan Kiros; Richard Zemel; Ruslan Salakhutdinov; Raquel Urtasun; Antonio Torralba; Sanja Fidler\nCountry: Canada; USA\nSector: Media/entertainment/sports/arts; Research/academia; Technology\nPurpose: Train language models\nTechnology: Database/dataset; NLP/text analysis; Deep learning\nIssue: Copyright; Bias/discrimination - race, religion; Ethics/values\nTransparency: Governance; Privacy; Marketing\nResearch, advocacy \ud83e\uddee\nBandy J., (2021). BookCorpus datacard/datasheet\nBandy J., Vincent N. (2021). Addressing \"Documentation Debt\" in Machine Learning Research: A Retrospective Datasheet for BookCorpus\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://towardsdatascience.com/dirty-secrets-of-bookcorpus-a-key-dataset-in-machine-learning-6ee2927e8650\nhttps://www.theguardian.com/books/2016/sep/28/google-swallows-11000-novels-to-improve-ais-conversation\nhttps://jack-clark.net/2021/05/17/import-ai-249-ibms-massive-code-dataset-dataset-archaeology-bookcorpus-facebook-wants-computers-to-read-the-world/\nhttps://info.deeplearning.ai/the-batch-face-recognition-for-the-masses-labeling-libel-documenting-datasets-what-machines-want-to-see-1\nRelated \ud83c\udf10\nImageNet image recognition dataset\nLAION image-text pairings datasets\nPage info\nType: Issue\nPublished: June 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/books3-dataset-shut-down-after-legal-notice-from-danish-anti-piracy-group", "content": "Books3 dataset shut down after legal notice from Danish anti-piracy group\nOccurred: August 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA high-profile dataset used to train large language models was taken offline following a complaint about copyright abuse by Danish anti-piracy group Rights Alliance, which represents publishers and authors in Denmark. \nRights Alliance had discovered Books3 contained around 150 titles written by its members. Part of The Pile, a larger project comprising 22 datasets developed by open source research group EleutherAI, which provides open-source data for language models, Books3 appears to have been used to train EleutherAI\u2019s GPT-J, Meta's LLaMa, and other large language models.\nBefore the takedown, Books3 creator Shawn Presser had said on Hacker News that he 'almost didn\u2019t release at all (or at least the books component) because of fear of copyright backlash.' He later told Gizmodo that the removal of Books3 was a 'tragedy' as it gave grassroots open-source projects a chance to create their own 'diverse and unbiased language models.'\nReports said the Rights Alliance also asked EleutherAI to remove the Books3 dataset, but it refused, telling the rights group it 'denied responsibility' for Books3. Hugging Face, which hosted a datacard and links to the Books3 download, pointed Rights Alliance to The Eye.\nThe Books3 controversy highlighted the ethical and legal clash between the open-source tech culture, which favours free sharing, and the publishing industry's need to protect intellectual property rights, the exploitation of authors' creative works for commercial gain without consent or compensation, and the potential erosion of creative incentives and livelihoods for authors if their works can be freely exploited by AI companies.\nIt also underscored the lack of transparency exhibited by organsations using Books3 and other datasets incorporating pirated content to train their AI models, and the need for regulation and ethical guidelines around the use of copyrighted materials in training AI systems.\nSystem \ud83e\udd16\nBooks3 dataset\nOperator: Beijing Academy of Artificial Intelligence; Bloomberg; DataBricks; EleutherAI; Meta/Facebook; Microsoft; OpenAI; Yandex\nDeveloper: Shawn Presser\nCountry: USA\nSector: Technology\nPurpose: Train large language models\nTechnology: Database/dataset\nIssue: Copyright; Ethics/values\nTransparency: \nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nRights Alliance (2023). Rights Alliance removes the illegal Books3 dataset used to train artificial intelligence\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theatlantic.com/technology/archive/2023/08/books3-ai-meta-llama-pirated-books/675063/\nhttps://interestingengineering.com/innovation/anti-piracy-group-shuts-down-books3-a-popular-dataset-for-ai-models\nhttps://mashable.com/article/books3-ai-training-dmca-takedown\nhttps://torrentfreak.com/anti-piracy-group-takes-prominent-ai-training-dataset-books3-offline-230816/\nhttps://www.searchenginejournal.com/are-chatgpt-bard-and-dolly-2-0-trained-on-pirated-content/485089/#close\nhttps://aicopyright.substack.com/p/has-your-book-been-used-to-train\nRelated \ud83c\udf10\nOpenAI deleted training datasets believed to contain copyrighted books\nC4 dataset is trained on unsafe, copyright-protected web content\nPage info\nType: Incident\nPublished: June 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/project-nightingale-health-data-sharing-slammed-for-poor-privacy", "content": "Project Nightingale patient data sharing slammed for violating privacy\nOccurred: November 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn AI-powered collaboration between Google and US healthcare company Ascension came under fire for poor transparency, inadequate security, and violating patient privacy.\nDubbed 'Project Nightingale', the collaboration used Google's advanced data analytics capabilities to improve how information is used for patient care, with Google provided with access to over 50 million Ascension patients' medical records - including names, birthdates and addresses, lab results, diagnoses, and hospitalisation records - drawn from some 2,600 patient care sites, including 50+ senior living facilities and 150 hospitals.\nGoogle used the data to create software powered by advanced artificial intelligence (AI) for patient diagnosis, prescriptions, and treatment recommendations.\nHowever, a Wall Street Journal article revealed that Ascension health care providers had not been informed that the medical records of their patients were being distributed, and the patients never provided consent or were given a choice to opt-out of the programme. \nA few days later a Google whistleblower revealed in The Guardian that Ascension medical records transferred to Google were not properly de-identified, potentially exposing patients' identities and personal details. The whistleblower also revealed that several Google employees involved in the project had already raised concerns about patient privacy, and expressed concerns about Google potentially selling or sharing the data with third parties or using it for targeted advertising based on medical histories. \nGoogle defended the legaility of the project under the Health Insurance Portability and Accountability Act, but did not clarify if and how the data was de-identified before transfer, or provided assurances against such misuse of the sensitive data. \nThe controversy highlighted Project Nightingale's lack of transparency, patient consent, and robust data protection measures, thereby potentially putting the privacy and security of millions of patients at risk. It also raised serious concerns about Google's growing ambitions in the healthcare sector.\n\u2796 March 2018. Ascension and Google began to work together to create tools to identify and predict health concerns before a patient visits a doctor using emergent medical data (EMD).\n\u2796 July 2019. Google mentioned Ascension deal in an earnings call, stating the goal is to \"improve the healthcare experience and outcomes\". \n\u2795 November 2019. The US Department of Health and Human Services' Office for Civil Rights opened an investigation into the Google-Ascension partnership to ensure compliance with HIPAA regulations.\n\u2795 December 2019. A number of prominent US senators launched an inquiry into Project Nightingale. \n\u2795 March 2020. US senators wrote (pdf) to Ascension CEO Joseph Impicciche demanding further information on the type and amount of information Ascension provided to Google, whether the health system provided advance notice to patients about the deal, whether patients could opt-out of data sharing, how many Google employees had access to patient records and how they were approved to gain access.\nSystem \ud83e\udd16\nProject Nightingale\nProject Nightingale Wikipedia profile\nDocuments \ud83d\udcc3\nAscension. Technology that improves patients\u2019 lives, caregivers\u2019 experience\nGoogle. Our partnership with Ascension\nOperator: Ascension\nDeveloper: Alphabet/Google\nCountry: USA\nSector: Health\nPurpose: Analyse health data\nTechnology: Machine learning\nIssue: Ethics/values; Privacy; Security\nTransparency: Governance; Privacy\nRegulation \u2696\ufe0f\nHealth Insurance Portability and Accountability Act\nResearch, advocacy \ud83e\uddee\nTapavalu M. (2020). Project Nightingale - The Case for Google\nSchneble C.O. et al (2020). Google's Project Nightingale highlights the necessity of data science ethics review\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theguardian.com/commentisfree/2019/nov/14/im-the-google-whistleblower-the-medical-data-of-millions-of-americans-is-at-risk\nhttps://www.wsj.com/articles/google-s-secret-project-nightingale-gathers-personal-health-data-on-millions-of-americans-11573496790\nhttps://www.nytimes.com/2019/11/11/business/google-ascension-health-data.html\nhttps://www.theguardian.com/technology/2019/nov/12/google-medical-data-project-nightingale-secret-transfer-us-health-information\nhttps://www.theguardian.com/technology/2019/nov/11/google-healthcare-ascension-privacy-health-data\nhttps://www.theverge.com/2019/11/11/20959771/google-health-records-project-nightingale-privacy-ascension\nhttps://www.wired.com/story/google-is-slurping-up-health-dataand-it-looks-totally-legal/\nhttps://www.cnbc.com/2019/11/18/google-ascension-health-data-deal-under-scrutiny-by-congressional-dems.html\nhttps://www.fiercehealthcare.com/payer/experts-transparency-key-to-avoid-controversy-generated-by-google-ascension-data-deal\nhttps://www.cnbc.com/2019/11/13/google-trust-deficit-could-kill-the-companys-cloud-ambitions.html\nhttps://edition.cnn.com/2019/11/12/business/google-project-nightingale-ascension/index.html\nhttps://www.healthcareitnews.com/news/ascension-google-working-secret-patient-data-project-says-wsj\nhttps://www.forbes.com/sites/jilliandonfro/2019/11/11/google-ascension-project-nightingale-electronic-medical-records/\nhttps://www.theguardian.com/commentisfree/2019/nov/14/im-the-google-whistleblower-the-medical-data-of-millions-of-americans-is-at-risk\nRelated \ud83c\udf10\nGoogle DeepMind, Royal Free London rapped for patient data sharing\nPrivacy advocates raise concerns about Google patient data deal with HCA Healthcare\nPage info\nType: Issue\nPublished: June 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/meta-ai-powered-ad-platform-overspends-customer-budgets", "content": "Meta AI-powered ad platform overspends customer budgets\nOccurred: February 2024-\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nMeta\u2019s AI-powered ad platform Advantage Plus has been significantly overspending customer budgets, leading advertiserss nursing financial losses and causing them to spend their marketing budgets elsewhere.\nThe system, designed to automate ad setup and budget spending, has been funneling money to Meta at an unexpected rate, with daily budgets being exhausted within hours and costs inflating up to 10 times the normal amount.\nSince it began in February 2024, the problem primarily affected small businesses, which saw their ad dollars wiped out quickly, resulting in little or no revenue, and significant losses in some cases.\nMeta acknowledged the ad system had technical issues and said it worked as expected for the majority of advertisers. \nHowever, its customers took to Reddit and other online communities in droves to complain about the product and they difficulty they experienced in obtaining refunds or resolving the problem what they saw as Meta's near non-existent customer support.\nThe incident underscores the risks associated with relying on more or less fully automated AI systems for important business functions like advertising.\nSystem \ud83e\udd16\nAdvantage Plus\nDocuments \nMeta. About Advantage+ shopping campaigns\nReviews \ud83d\udde3\ufe0f\nFacebookAds subreddit\nOperator:\nDeveloper: Meta\nCountry: USA\nSector: Business/professional services\nPurpose: Automate advertising campaigns\nTechnology: Machine learning\nIssue: Robustness\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theverge.com/2024/4/28/24141585/meta-ai-advantage-plus-automated-ad-glitch-cpm\nhttps://searchengineland.com/meta-apologizes-ad-delivery-campaigns-overspend-437593\nhttps://www.benzinga.com/news/24/04/38480776/mark-zuckerberg-led-metas-set-it-and-forget-it-ai-tools-misfire-wasting-ad-budgets-and-driving-away\nhttps://www.emarketer.com/content/meta-faces-backlash-automated-ad-system-drains-budgets-with-little-payoff\nhttps://arstechnica.com/gadgets/2024/04/customers-say-metas-ad-buying-ai-blows-through-budgets-in-a-matter-of-hours/\nRelated \ud83c\udf10\nFacebook lets housing ads exclude ethnic minorities\nFacebook approves teen alcohol, drug, gambling ads\nPage info\nType: Incident\nPublished: June 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/us-professor-falsely-quoted-by-ai-generated-news-article", "content": "US professor falsely quoted by AI-generated news article\nOccurred: February 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nIndian news website Biharprabha published an AI-generated article that contained a fabricated quote attributed to US linguistics professor Emily Bender.\nIn an article titled \u2018Meta\u2019s AI Bot Goes Rogue, Spews Offensive Content\u2019, University of Washington professor of linguistics Emily M. Bender was incorrectly quoted as saying \u201cThe release of BlenderBot 3 demonstrates that Meta continues to struggle with addressing biases and misinformation within its AI models.\u201d\nHowever, Bender said no such thing, and publicly called out the news site on social media, saying she had emailed the editor to have the false quote removed.\nThe episode highlighted ongoing concerns about generative AI systems 'hallucinating' apparently believeable falsities. It also underscored ethical issues surrounding the use of AI in journalism and the need for better oversight and fact-checking mechanisms in automated news production. \nSystem \ud83e\udd16\nUnknown\nOperator: Biharprabha\nDeveloper:  \nCountry: India; USA\nSector: Media/entertainment/sports/arts; Research/academia\nPurpose: Generate text\nTechnology: Chatbot\nIssue: Ethics/values; Mis/disinformation\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.thequint.com/explainers/ai-generated-article-published-by-bihar-news-site-fake-quote-a-wake-up-call\nhttps://www.linkedin.com/posts/ebender_ai-invents-quote-from-real-person-in-article-activity-7170783158446088192-hqNS/\nRelated \ud83c\udf10\nMSN publishes 'useless' AI-generated Brandon Hunter obituary\nChatGPT falsely accuses Mark Walters of fraud, embezzlement\nPage info\nType: Issue\nPublished: June 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/adobe-terms-of-use-update-sparks-privacy-copyright-controversy", "content": "Adobe terms of use update sparks privacy, copyright controversy\nOccurred: June 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn update to Adobe's terms of service generated significant controversy amongst its users, primarily due to concerns about privacy and copyright.\nThe update, which required users to accept to continue using Adobe's software, included provisions allowing Adobe to access and use their content for various purposes, including improving its services and products through automated and manual methods.\nUsers argued that the terms are too broad and vague, and feared that this could allow Adobe access to sensitive projects, potentially violating confidentiality agreements and user privacy, while allowing Adobe to use their work to train its AI models, notably Adobe Firefly. \nAdobe later clarified that it does not train its AI on customer content and only uses licensed or public domain content for this purpose.\nThe fracas reflected concerns about the use of customer content and data to train Adobe's AI models. It also highlighted widespread distrust in technology companies in general.\nSystem \ud83e\udd16\nAdobe Firefly image generator\nDocuments \ud83d\udcc3\nAdobe Terms of Use\nAdobe. Updating Adobe\u2019s Terms of Use\nAdobe. A clarification on Adobe Terms of Use\nOperator: Adobe\nDeveloper: Adobe\nCountry: Global\nSector: Media/entertainment/sports/arts\nPurpose: Generate images\nTechnology: Text-to-image\nIssue: Confidentiality; Copyright; Privacy\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.slashgear.com/1598483/adobe-terms-of-service-update-controversy-explained/\nhttps://uk.pcmag.com/ai/152680/adobe-sparks-backlash-over-ai-terms-that-let-it-access-view-your-content\nhttps://petapixel.com/2024/06/06/photographers-outraged-by-adobes-new-privacy-and-content-terms/\nhttps://petapixel.com/2024/06/07/adobe-responds-to-terms-of-use-controversy-says-it-isnt-spying-on-users/\nhttps://lifehacker.com/tech/adobe-has-responded-to-criticism-of-its-new-terms-of-service\nRelated \ud83c\udf10\nAdobe trained Firefly AI model on competitor images\nAdobe uses customer images without consent to train Firefly art generator\nPage info\nType: Issue\nPublished: June 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/shanghai-plans-to-triple-facial-surveillance-in-xuhui-district", "content": "Shanghai triples facial surveillance in Xuhui district, raising concerns \nOccurred: June 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA significant expansion and upgrade of police facial recognition and ethnic minority surveillance capabilities in Shanghai, China, raised concerns about the loss of human rights and liberties by Uyghurs and others. \nA report by video research company IPVM revealed that Shanghai's Xuhui District completed the implementation of a facial recognition system that specifically detects \"Uyghur ethnicity\" based on facial features across thousands of cameras. \nThe system is part of a larger surveillance expansion reportedly aimed at capturing 25.9 million faces daily and storing them in a database of over 50 million individual files.\nThe system performs \"Passerby Attribute Recognition\" analysis to assess attributes like gender, age group, and \"Uyghur ethnicity.\" This data is then stored with corresponding images for retrieval by police and data analysis. \nThe report highlighted concerns about the discriminatory targeting of the Uyghur community and the pervasive surveillance of people's activities and movements, regardless of wrongdoing, by Shanghai authorities. \nSystem \ud83e\udd16\nUnknown\nOperator:\nDeveloper:  \nCountry: China\nSector: Govt - police; Govt - security\nPurpose: Identify and control ethnic minorities\nTechnology: Attribute recognition; Facial recognition\nIssue: Human/civil rights; Privacy\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://ipvm.com/reports/shanghai-xuhui\nhttps://chinadigitaltimes.net/2024/05/chinese-surveillance-technology-expands-at-home-and-abroad/\nhttps://www.reddit.com/r/China_irl/comments/1crjmug\nRelated \ud83c\udf10\nHuawei files Uyghur-spotting system patent application\nChinese study uses facial recognition to identify Uyghurs, Tibetans\nPage info\nType: Issue\nPublished: June 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/dna-evidence-is-bungled-in-dozens-of-queensland-crimes", "content": "DNA evidence is bungled in dozens of Queensland crimes\nOccurred: March 2015-\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nDNA evidence was mishandled in thousands of criminal cases in Queensland, Australia, due to severe maladministration and flawed testing methods at the state's forensic lab, calling into question the integrity of the state's criminal justice system and the reliability of the technology involved.\nAn official inquiry revealed that DNA evidence was mishandled by the forensic lab in numerous cases. It also found that the lab had focused on speed over accuracy, leading to widespread failures in properly testing DNA samples for years. \nThe inquiry also found that STMmix, the method used for DNA testing, was found to have critical flaws, raising questions about the validity of the forensic evidence produced. \nThe flawed practices may have led to offenders escaping conviction and could affect the outcomes of numerous cases. The fracas was seen to highlight the broader impact of the forensic lab's failings on the criminal justice system, including the potential for wrongful convictions and the undermining of public confidence in forensic evidence. \n\u2795 December 2022. Walter Sofronoff KC's final report (pdf) on Forensic DNA Testing in Queensland concludes the forensic lab suffered from poor leadership, oversight and practices, inadequate quality control, and significant errors in DNA testing processes.\n\u2795 October 2023. Queensland announced it would open a fresh inquiry to investigate concerns about an automated DNA extraction method at the state's forensic lab relating to a process known as 'Project 13'.\nSystem \ud83e\udd16\nSTRmix website\nDocuments \ud83d\udcc3\nSTRmix. Statement relating to STRmi miscodes (pdf)\nOperator: Queensland Health Forensic and Scientific Services\nDeveloper: Institute of Environmental Science; Research and Forensic Science South Australia\nCountry: Australia\nSector: Govt - police\nPurpose: Interpret DNA mixtures\nTechnology: \nIssue: Accuracy/reliability; Robustness; Human/civil rights\nTransparency: Governance; Legal\nInvestigations, assessments, audits \ud83e\uddd0\nWALTER SOFRONOFF QC. Final Report. Commission of Inquiry into Forensic DNA testing in Queensland (pdf)\nWALTER SOFRONOFF QC. REPORT CONCERNING USE BY QUEENSLAND HEALTH FORENSIC AND SCIENTIFIC SERVICES OF CERTAIN EVIDENTIARY STATEMENTS (pdf)\nQueensland Health. STRmix Miscoding Error (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.couriermail.com.au/news/queensland/queensland-authorities-confirm-miscode-affects-dna-evidence-in-criminal-cases/news-story/833c580d3f1c59039efd1a2ef55af92b\nhttps://www.brisbanetimes.com.au/national/queensland/more-than-130-cases-of-mixed-dna-samples-recalled-in-five-years-20190131-p50uw0.html\nhttps://www.thesaturdaypaper.com.au/news/law-crime/2017/06/17/dna-tests-changing-criminal-trials/14976216004797#hrd\nhttps://www.theaustralian.com.au/inquirer/murder-lab-train-wreck-in-shandee-blackburn-case/news-story/738939fc0249f19371e136c4d48076b1\nhttps://www.theaustralian.com.au/nation/shut-lab-down-dna-fiasco-in-shandee-blackburn-murder-case/news-story/3c4a064595eda573a0a9bd6b0c763f1c\nRelated \ud83c\udf10\nEdmonton sexual assault DNA phenotyping\nTrueAllele automated DNA forensic analysis\nPage info\nType: Incident\nPublished: June 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/c4-dataset-is-trained-on-unsafe-copyright-protected-web-content", "content": "C4 dataset is trained on unsafe, copyright-protected web content\nOccurred: April 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA dataset used to train Google and Meta large language models was trained on racist, pornographic, and copyright-protected web content, raising safety, privacy and other concerns.\nA joint Washington Post/Allen Institute for AI investigation discovered that C4 used content from Reddit, notorious message board 4chan, white supremacy site Stormfront, and far-right site Kiwi Farms, effectively hard-baking huge volumes of offensive content of every conceivable kind into the data. \nThe investigation also found that Russia government news website RT and US hard right-wing political channel Breitbart were amongst the sites used to train C4. Both sites are known for their highly skewed political views and tendency to create and amplify false stories. It also found that C4 included content from sites such as flvoters.com, raising concerns about the privacy of US voters in particular.\nAccording to the Washington Post, the copyright symbol appeared over 200 million times in the C4 dataset. Copyright has become a major issue for generative AI systems. The Post said it\u2019s 'analysis suggests more legal challenges may be on the way.'\nThe investigation raised questions about the safety and security of the dataset and the machine learning systems trained on it, the privacy of web users, abuse of copyright, bias, and the veracity of its creators' marketing claims.\nSystem \ud83e\udd16\nC4 dataset\nOperator: Alphabet/Google; Meta/Facebook\nDeveloper: Alphabet/Google\nCountry: USA\nSector: Research/academia; Technology\nPurpose: Train large language models\nTechnology: Dataset/database\nIssue: Bias/discrimination - religion; Copyright; Mis/disinformation; Privacy; Safety\nTransparency: Governance; Black box; Marketing\nInvestigations, assessments, audits \ud83e\uddd0\nWashington Post (2023). Inside the secret list of websites that make AI like ChatGPT sound smart\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.searchenginejournal.com/are-chatgpt-bard-and-dolly-2-0-trained-on-pirated-content/485089/\nhttps://www.theguardian.com/technology/2023/apr/20/fresh-concerns-training-material-ai-systems-facist-pirated-malicious\nhttps://searchengineland.com/search-websites-google-c4-dataset-395820\nhttps://www.theregister.com/2023/04/20/google_c4_data_nasty_sources/\nhttps://gizmodo.com/ai-bard-google-facebook-trained-on-breitbart-rt-1850352405\nhttps://www.inverse.com/tech/chatgpt-google-bard-internet-search-sources\nhttps://arstechnica.com/information-technology/2023/04/reddit-will-start-charging-ai-models-learning-from-its-extremely-human-archives/\nRelated \ud83c\udf10\nGoogle GoEmotions dataset\nBookCorpus large language dataset\nPage info\nType: Issue\nPublished: June 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/laion-5b-links-to-photos-of-identifiable-brazilian-children", "content": "LAION-5B links to photos of identifiable Brazilian children\nOccurred: June 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nDataset LAION-5B was found to contain personal photos and details of identifiable Brazilian children without their knowledge or consent, prompting concerns about privacy and its creator's governance and integrity.\nHuman Rights Watch (HRW) discovered over 170 photos of children across 10 Brazilian states in the dataset, including names, ages, locations, and other identifying information. Some of the photos dated back to the mid-1990s, while others are as recent as 2023. All images had been posted by families on social media.\nIn one instance, details revealed a 2-year-old girl and newborn sister, with their names and the hospital where the baby was born.\nHuman Rights Watch said it only reviewed 0.0001 percent of the 5.85 billion images on LAION-5B, suggesting many more such images could be present. The images violate children's privacy and enable malicious actors to create explicit deepfakes exploiting them, HRW said.\nLAION, the non-profit organisation behind the dataset, temporarily removed the offending images and said it would implement filters. LAION-5B was used to train Stable Diffusion, among other models.\nThe incident was seen to highlight LAION's seemingly slapdash approach to protecting personal privacy. It also prompted concerns about the lack of comprehensive data privacy laws to safeguard children and others from violations by AI systems.\nSystem \ud83e\udd16\nLAION-5B dataset\nOperator: Human Rights Watch\nDeveloper: LAION\nCountry: Brazil\nSector: Private - individual\nPurpose: Pair text and images\nTechnology: Database/dataset; Neural network; Deep learning; Machine learning\nIssue: Ethics/values; Privacy\nTransparency: Governance; Marketing\nResearch, advocacy \ud83e\uddee\nHuman Rights Watch (2024). Brazil: Children\u2019s Personal Photos Misused to Power AI Tools\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.wired.com/story/ai-tools-are-secretly-training-on-real-childrens-faces/\nhttps://petapixel.com/2024/06/10/brazilian-childrens-photos-and-personal-details-found-in-ai-training-data-set/\nhttps://arstechnica.com/tech-policy/2024/06/ai-trained-on-photos-from-kids-entire-childhood-without-their-consent/\nhttps://thehill.com/policy/technology/4371448-exploitative-photos-of-children-found-in-ai-training-data/\nhttps://siliconangle.com/2023/12/20/researchers-find-csam-images-laion-5b-ai-training-dataset/\nhttps://www.jurist.org/news/2024/06/hrw-report-reveals-pictures-of-brazil-children-misused-by-ai-tool/\nRelated \ud83c\udf10\nChild sex abuse images discovered on LAION-5B dataset\nLAION trains Robert Kneschke photos without consent\nPage info\nType: Incident\nPublished: June 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/perplexity-ai-is-accused-of-ripping-off-news-websites", "content": "Perplexity AI is accused of plagiarising news websites\nOccurred: June 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAI company Perplexity was accused of using content from prominent news publishers without proper credit or attribution.\nThe issue was highlighted in a feature called Perplexity Pages, which displays articles \u201ccurated\u201d and summarised by the company's algorithms while scraping content from third-party news outlets, including Forbes, CNBC and Bloomberg. \nThe news outlets were not credited by name within the curated article text, and the wording closely matched that of the source. Instead, small logos that linked back to the original story were included, which Forbes described as \u201csmall, easy-to-miss.\u201d \nIn one instance, Perplexity\u2019s chatbot reproduced a version of an exclusive, paywall-protected Forbes report on ex-Google CEO Eric Schmidt\u2019s military drone project, lifting near-verbatim passages and an in-house graphic from Forbes\u2019 original story, and producing an AI-generated podcast, without attribution. \nPerplexity AI CEO Aravind Srinivas acknowledged the 'rough edges' of the new feature, but claimed that their chatbot cites third-party outlets more prominently than rival services. By contrast, Forbes' executive editor slammed it as \"plagiarism\" and \"theft\". \n\u201cAs reported by Forbes\u201d was later manually added to the Schmidt page. But this did not stop the publisher sending a letter to the Perplexity CEO accusing the company of stealing text and images in a \"willful infringement\" of Forbes' copyright rights.\nThe incident highlighted the challenges of balancing innovation with proper attribution and ethics in AI-driven content generation.\n\u2795 June 2024. Perplexity was found to be ignoring the Robots Exclusion Protocol which publishers and other websites use to grant or deny permissions to automated crawlers and scrapers. \nSystem \ud83e\udd16\nPerplexity AI answer engine\nDocuments \ud83d\udcc3\nPerplexity terms of service\nOperator: Forbes\nDeveloper: Perplexity AI\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Generate information\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Business model; Cheating/plagiarism; Copyright; Ethics/values\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.forbes.com/sites/sarahemerson/2024/06/07/buzzy-ai-search-engine-perplexity-is-directly-ripping-off-content-from-news-outlets/\nhttps://www.forbes.com/sites/randalllane/2024/06/11/why-perplexitys-cynical-theft-represents-everything-that-could-go-wrong-with-ai/\nhttps://stackdiary.com/perplexity-has-a-plagiarism-problem/\nhttps://www.platformer.news/how-to-stop-perplexity-oreilly-ai-publishing/\nhttps://www.aiforjournalists.org/posts/forbes-editor-accuses-perplexity-ai-of-theft\nhttps://bgr.com/business/companies-like-google-and-openai-are-pillaging-the-internet-and-pretending-its-progress/\nhttps://www.bloomberg.com/news/articles/2024-06-08/ai-startup-perplexity-says-news-summary-tool-has-rough-edges\nhttps://www.semafor.com/newsletter/06/12/2024/perplexity-plans-revenue-sharing-deals-with-publishers\nRelated \ud83c\udf10\nGoogle sued for scraping data to train AI models\nMeta sues predictive policing firm Voyager Labs for data scraping\nPage info\nType: Issue\nPublished: June 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/newsbreak-publishes-untrue-story-about-harvest19-charity", "content": "NewsBreak publishes untrue stories about Harvest19 charity\nOccurred: January 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nUS-based news app NewsBreak published two inaccurate stories about a charity hosting a 24-hour foot-care clinic for homeless people, prompting concerns that homeless people might attend the non-event. \nThe AI-generated stories alleged that Erie, Pennsylvania charity Harvest912 was hosting a 24-hour foot-care clinic for homeless people. The stories were untrue, and led to the charity having to issue a 'cease and desist' to NewsBreak to pull the stories due to concerns that homeless individuals might travel to the non-existent event, potentially causing harm.\n\u201cYou are doing HARM by publishing this misinformation - homeless people will walk to these venues to attend a clinic that is not happening,\u201d Harvest912 told NewsBreak in an email seen by Reuters. \nNewsBreak said it removed the articles, and blamed Harvest 19 for hosting incorrect information on its website.\nThe incident was one of a series involving NewsBreak, and raised questions about the accuracy and reliability of the company's AI technology, and about its business model governance and integrity.\nSystem \ud83e\udd16\nUnknown\nOperator: Particle Media/NewsBreak\nDeveloper: Particle Media/NewsBreak\nCountry: USA\nSector: NGO/non-profit/social enterprise\nPurpose: Generate news articles\nTechnology: Chatbot\nIssue: Mis/disinformation\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.nbcnews.com/tech/tech-news/top-news-app-us-chinese-origins-writes-fiction-help-ai-rcna155567\nhttps://www.entrepreneur.com/business-news/chart-topping-newsbreak-app-used-ai-to-write-stories-report/475179\nhttps://www.devdiscourse.com/article/technology/2973177-the-rise-and-fall-of-newsbreaks-ai-a-cautionary-tale\nRelated \ud83c\udf10\nAI invents NewsBreak Christmas Day murder\nMSN AI article falsely accuses Irish DJ of sexual misconduct\nPage info\nType: Issue\nPublished: June 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/klarna-halves-marketing-team-by-using-ai", "content": "Klarna halves marketing team by using AI\nOccurred: June 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nFintech company Klarna significantly reduced its marketing team size and costs by leveraging AI, prompting criticism about the firm's apparent prioritisation of profit over people and lack of concern for his employees' jobs. \nKlarna CEO Sebastian Siemiatkowski faced criticism after stating publicly that the company's in-house marketing team was now half its previous size but producing more work, attributing its newfound efficiency to AI. \nThe company said it had saved approximately USD 10 million annually by using generative AI for tasks such as running marketing campaigns and generating images, which reduced the need for photographers, image banks, and marketing agencies.\nAccording to Siemiatkowski, Klarna's use of AI allowed the company to cut external marketing agency expenses by 25 percent and eliminate the need for stock imagery by using AI image generators. He also said that Klarna had built an AI-powered copywriting tool, Copy Assistant, which allows the company to use AI for 80 percent of all copywriting.\nSiemiatkowski's statement also sparked a heated discussion about the impact of AI on creative industries and jobs.\nSystem \ud83e\udd16\nCopy Assistant\nDocuments \nKlarna. AI helps Klarna cut marketing agency spend by 25 percent and run more campaigns\nOperator: Klarna\nDeveloper: Klarna\nCountry: Global\nSector: Banking/financial services\nPurpose: Generate marketing copy\nTechnology: Chatbot\nIssue: Employment; Ethics/values\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.businessinsider.com/klarna-ceo-comments-ai-smaller-marketing-team-produce-more-savings-2024-5\nhttps://www.reuters.com/technology/klarna-using-genai-cut-marketing-costs-by-10-mln-annually-2024-05-28/\nhttps://digiday.com/marketing/how-klarna-is-using-ai-for-cost-savings-changing-extremely-frustrating-creative-processes\nhttps://www.fastcompany.com/91131913/klarna-slashes-10-million-annually-marketing-costs-here-s-their-secret\nRelated \ud83c\udf10\nFilm studio use of AI to promote Civil War backfires\nJust Eat uses algorithm to fire employees\nPage info\nType: Incident\nPublished: June 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/southern-co-op-accused-of-facial-recognition-privacy-violations", "content": "Civil liberties group accuses Southern Co-op facial recognition of violating customer privacy\nOccurred: July 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nUK retail chain Southern Co-op was accused by a prominent civil liberties group of violating the privacy of its customers through its use of facial recognition.\nBig Brother Watch (BBW) filed a legal complaint with the UK\u2019s Information Commissioner\u2019s Office (ICO) alleging the Southern Co-op and its facial recognition system provider Facewatch processed more data than necessary for generating and storing watchlist entries, and that the two entities lacked transparency about how they collected and processed people\u2019s data.\nDespite Southern Co-op stating that 'distinctive signage' is on display in the relevant stores, rights groups argued the chain failed to inform customers sufficiently clearly nor made a meaningful general public announcement before the trials started.\nBBW also argued that individuals were not informed when they were added to the Co-op and Facewatch's databases, despite the chance they could be innocent. \nIt went to accuse Southern Co-op of providing little visible information on what data is stored, how it is stored, the length of storage, and with whom it may be shared, including the police, and not making clear how individuals can have appeal against having their data stored.\n\u2795 March 2023. The ICO concluded that Facewatch has a legitimate interest in using personal data for crime detection and prevention, but expressed concerns about some of its practices. Facewatch responded by making improvements, includin reducing the amount of personal data collected.\n\u2795 June 2024. An innocent shopper at retail chain Home Bargains was wrongfully accused of theft due to a Facewatch error, prompting public controversy and a legal complaint.\nSystem \ud83e\udd16\nFacewatch facial recognition system\nOperator: Southern Co-op\nDeveloper: Facewatch\nCountry: UK\nSector: Retail\nPurpose: Identify criminals\nTechnology: Facial recognition\nIssue: Privacy\nTransparency: Governance\nRegulation \u2696\ufe0f\nData Protection Act 2018\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nInformation Commissioner's Office (2023). Balancing people\u2019s privacy rights with the need to prevent crime\nBig Brother Watch/AWO (2023). GROUNDS OF COMPLAINT TO THE INFORMATION COMMISIONER UNDER SECTION 165 OF THE DATA PROTECTION ACT 2018 - LIVE AUTOMATED FACIAL RECOGNITION BY FACEWATCH LTD AND THE SOUTHERN COOPERATIVE LTD (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/technology-55259179\nhttps://www.bbc.co.uk/news/uk-england-62297546\nhttps://news.sky.com/story/co-ops-use-of-facial-recognition-on-customers-prompts-legal-complaint-12659309\nhttps://www.thegrocer.co.uk/convenience/southern-co-op-defends-facial-recognition-tech-after-privacy-backlash/651298.article\nhttps://www.dailymail.co.uk/news/article-9041339/Big-Brother-watching-shopping-op.html\nhttps://thenextweb.com/neural/2020/12/11/privacy-advocates-poop-on-uk-supermarkets-facial-recognition-system/\nhttps://www.wired.com/story/uk-stores-facial-recognition-track-shoppers/\nhttps://www.codastory.com/authoritarian-tech/uk-supermarket-biometric-cameras/\nRelated \ud83c\udf10\nHome Bargains shopper misidentified by Facewatch\nFrasers Group criticised for live facial recognition programme\nPage info\nType: Incident\nPublished: June 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/msn-ai-article-falsely-accuses-irish-dj-of-sexual-misconduct", "content": "MSN AI article falsely accuses Irish DJ of sexual misconduct\nOccurred: January 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn AI-generated news article falsely accused an Irish talk show host of sexual misconduct, raising questions about the reliability and reputation of two AI-powered news services.\nThe article, produced by AI-powered news service \u201cBNN Breaking\u201d, claimed that Irish DJ and talk show host Dave Fanning was due to face trial over alleged sexual misconduct. Titled \u201cProminent Irish broadcaster faces trial over alleged sexual misconduct\", the false report, was featured on MSN\u2019s homepage for several hours before being taken down.\nFanning is now suing BNN and Microsoft for defamation, arguing that the article was live long enough to cause reputational damage. The incident is the latest in a series involving Microsoft's AI-powered news service.\nHong Kong-based BNN uses AI to churn out thousands of articles a day, the majority of which are reportedly never fact-checked by a human being. \nThe incident represents a multi-pronged failure of AI-generated and republished 'news'.\n\u2795 June 2024. The New York Times published a detailed analysis of BNN and its founder, entrepreneur and convicted criminal Gurbaksh Chaha.\nSystem \ud83e\udd16\nMSN website\nMSN Wikipedia profile\nOperator: Microsoft/MSN\nDeveloper: Microsoft/MSN\nCountry: Ireland; USA\nSector: Media/entertainment/sports/arts\nPurpose: Select news articles\nTechnology: Machine learning; NLP/text analysis; Neural networks; Deep learning\nIssue: Accuracy/reliability; Defamation; Mis/disinformation\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.irishtimes.com/crime-law/courts/2024/01/15/rtes-dave-fanning-initiates-defamation-case-after-his-photo-appeared-in-article-about-sexual-misconduct-trial-of-different-broadcaster/\nhttps://www.rte.ie/news/courts/2024/0115/1426681-dave-fanning-launches-defamation-case-against-microsoft/\nhttps://futurism.com/microsoft-sued-ai-article\nhttps://www.nytimes.com/2024/06/06/technology/bnn-breaking-ai-generated-news.html\nRelated \ud83c\udf10\nMSN publishes AI-generated Brandon Hunter obituary\nMicrosoft robot editor confuses Little Mix band members\nPage info\nType: Incident\nPublished: June 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/report-israel-runs-ai-powered-us-political-influence-campaign", "content": "Report: Israel runs AI-powered covert US political influence campaign\nOccurred: October 2023-\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe Israeli government allegedly orchestrated a covert influence campaign targeting US lawmakers and the public to garner support for Israel's military actions in Gaza.  \nIsrael's Ministry of Diaspora Affairs reportedly allocated around USD 2 million for a campaign to disseminate pro-Israel messages and urge US Congress members, particularly prominent Black Democrats, to finance Israeli military initiatives, and hired Israeli marketing firm Stoic to run it.\nHundreds of fake social media accounts impersonating Americans were created and populated shortly after Hamas' attack on Israel on Otober 7, 2023, using content generated by OpenAI's ChatGPT. \nMeta identified and removed over 500 fraudulent Facebook and Instagram accounts linked to the campaign, many of which impersonated real individuals like Jewish students, African Americans, and concerned citizens in order to appear authentic. \nOpenAI also reported disrupting an Israeli operation codenamed \"Zero Zeno\" that misused its AI tools for generating articles and comments across platforms like Instagram, Facebook, and Twitter.\nThe Israeli government denied any involvement in the campaign. However, Investigative and media reports cited internal documents and government sources as evidence of Israel's role.\nThe finding underscored concerns about Israel's use of AI for political and military purposes in its ongoing battles with Hamas and other organisations.\nSystem \ud83e\udd16\nChatGPT chatbot\nDocuments \ud83d\udcc3\nOpenAI. Disrupting deceptive uses of AI by covert influence operations\nMeta. Adversarial threat report - First quarter 2024\nOperator: Israel Ministry of Diaspora Affairs; Stoic\nDeveloper: OpenAI\nCountry: Israel; USA\nSector: Politics\nPurpose: Manipulate public opinion\nTechnology: Chatbot\nIssue: Mis/disinformation\nTransparency: \nInvestigations, assessments, audits \ud83e\uddd0\nFakeReporter (2024). Pro-Israeli Influence Network - New Findings\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.nytimes.com/2024/06/05/technology/israel-campaign-gaza-social-media.html\nhttps://www.reuters.com/technology/meta-identifies-networks-pushing-deceptive-content-likely-generated-by-ai-2024-05-29/\nhttps://www.npr.org/2024/05/30/g-s1-1670/openai-influence-operations-china-russia-israel\nhttps://www.nbcnews.com/tech/security/meta-openai-say-disrupted-israeli-companys-influence-campaign-rcna154774\nhttps://www.thejc.com/news/israel/israeli-minister-denies-reports-the-government-ran-covert-influence-campaign-on-us-politicians-uy5gj4fq\nhttps://thecradle.co/articles-id/25275\nRelated \ud83c\udf10\nIsrael reportedly uses AI to identify 37,000 Hamas targets\nIsrael uses Habsora 24 hour automated 'target factory' against Palestinians\nPage info\nType: Incident\nPublished: June 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/youth-advocacy-worker-misidentified-by-met-police-facial-recognition-system", "content": "Youth advocacy worker misidentified by Met Police facial recognition system\nOccurred: May 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nShaun Thompson, a 38-year-old youth-advocacy worker, was misidentified by the Metropolitan Police\u2019s facial recognition system, prompting concerns about its accuracy and reliability.\nThompson was returning home from a volunteering shift with Street Fathers, a community organisation that provides a positive male presence to young people and tackles knife crime, when he was wrongly flagged as a person on the Metropolitan Police\u2019s facial recognition database outside London Bridge station. \nHe was held by officers for almost 30 minutes, who repeatedly demanded scans of his fingerprints and threatened him with arrest, despite him showing multiple identity documents showing that he was not the individual on the facial recognition database.\nThis case raised concerns about the accuracy of facial recognition technology and its potential for misidentification. Civil liberty groups are worried that its accuracy is yet to be fully established. \nAs a result, Thompson and others have launched legal challenges against the Metropolitan Police and surveillance company Facewatch. They aim to roll back facial recognition surveillance and defend people\u2019s rights.\nSystem \ud83e\udd16\nNeoFace Watch facial recognition system\nOperator: Metropolitan Police Service (MPS)\nDeveloper: NEC\nCountry: UK\nSector: Govt - police\nPurpose: Identify criminals\nTechnology: Facial recognition\nIssue: Accuracy/reliability; Human/civil rights\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nBig Brother Watch (2024). LANDMARK LEGAL CHALLENGES LAUNCHED AGAINST FACIAL RECOGNITION AFTER POLICE AND RETAILER MISIDENTIFICATIONS\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.mylondon.news/news/south-london-news/anti-knife-activist-brings-legal-29285138\nhttps://www.bbc.co.uk/news/technology-69055945\nhttps://metro.co.uk/2024/05/27/facial-recognition-leaves-brits-digital-police-line-ups-without-realising-20918056/\nRelated \ud83c\udf10\nHome Bargains shopper misidentified by Facewatch facial recognition\nApple facial recognition system misidentifies 'shoplifter' Ousmane Bah\nPage info\nType: Incident\nPublished: June 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/virginia-suicide-prevention-algorithm-favours-white-men", "content": "US Veterans Affairs suicide prevention algorithm favours white men\nOccurred: May 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA US Department of Veterans Affairs (VA) AI programme called REACH VET that prevents suicide amongst military veterans has been found to prioritise white men and overlook survivors of sexual violence.\nAccording to a Markup investigation, the algorithm considers 61 variables and gives preference to veterans who are \u201cdivorced and male\u201d and \u201cwidowed and male\u201d, but does not prioritise female veterans. Notably, it does not take into account military sexual trauma and intimate partner violence, both of which are linked to elevated suicide risk among female veterans.\nRecent government data shows a 24 percent rise in the suicide rate among female veterans between 2020 and 2021, which is four times the increase among male veterans during the same period. Despite this, the VA\u2019s suicide prevention algorithm continued to prioritise men.\nThe VA claims that its machine learning model, REACH VET, flags 6,700 veterans a month for extra help and results in a significant reduction in suicide attempts by those veterans over the following six months. \nHowever, the algorithm\u2019s bias towards white men and its neglect of survivors of sexual violence raised concerns about its effectiveness and fairness.\nSystem \ud83e\udd16\nREACH VET\nDocuments \ud83d\udcc3\nUS Department of Veterans Affairs (2018). REACH VET Overview (pdf)\nUS Department of Veterans Affairs (2017). VA REACH VET Initiative Helps Save Veterans Lives: Program Signals When More Help Is Needed for At-risk Veterans\nOperator:\nDeveloper: US Department of Veterans Affairs\nCountry: USA\nSector: Govt - welfare; Govt - defence\nPurpose: Prevent suicide\nTechnology: Machine learning\nIssue: Accuracy/reliability; Bias/discrimination - race, ethnicity, gender\nTransparency: \nInvestigations, assessments, audits \ud83e\uddd0\nThe Markup. V.A. Uses a Suicide Prevention Algorithm To Decide Who Gets Extra Help. It Favors White Men\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://fullerproject.org/story/artificial-intelligence-veteran-suicide-prevention-algorithm-favors-men/\nhttps://www.military.com/daily-news/2024/05/23/vas-veteran-suicide-prevention-algorithm-favors-men.html\nhttps://www.navytimes.com/veterans/2024/05/23/veteran-suicide-prevention-algorithm-favors-men-investigation-finds/\nRelated \ud83c\udf10\nGoGuardian Beacon student suicide prevention software raises privacy, bias concerns\nSeoul bridge AI suicide detection system raises privacy concerns\nPage info\nType: Issue\nPublished: June 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/adobe-called-out-for-selling-ai-generated-ansel-adams-images", "content": "Adobe called out for selling AI-generated 'Ansel Adams' images\nOccurred: June 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAdobe was criticised for selling AI-generated images inspired by the style of the late photographer Ansel Adams.\nThe Ansel Adams estate publicly rebuked Adobe for selling images generated by AI labelled \"Ansel Adams-Style\" on Adobe\u2019s stock photo service, despite the company 's guidelines for AI-generated images prohibiting the use of names of people or artists whose work is still in copyright.\nThe Ansel Adams estate said it had been trying to get Adobe to respond to them about the issue since August 2023, and invited Adobe to proactively address complaints like theirs and to stop putting the responsibility on individual artists or their estates to continuously monitor their intellectual property on Adobe\u2019s platform.\nAdobe Stock Vice President Matthew Smith told The Verge that the company generally moderates all \u201ccrowdsourced\u201d Adobe Stock assets before they are made available to customers, employing a \u201cvariety\u201d of methods that include \u201can experienced team of moderators who review submissions.\u201d \nAdobe removed the offending listing; however, the incident sparked accusations that Adobe had violated its own policies and was unable to properly moderate its platforms, as well as a broader discussion about the ethics of AI-generated art and its impact on creators.\nSystem \ud83e\udd16\nUnknown\nOperator:\nDeveloper: \nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Create images\nTechnology: Text-to-image\nIssue: Copyright; Employment; Ethics/values\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://news.artnet.com/art-world/ansel-adams-adobe-ai-images-2496092\nhttps://www.digitaltrends.com/computing/adobe-called-out-for-violating-ai-ethics/\nhttps://www.theverge.com/2024/6/3/24170285/adobe-stock-ansel-adams-style-ai-generated-images\nhttps://www.threads.net/@anseladams/post/C7pUY3MyXFI\nRelated \ud83c\udf10\nAdobe sells AI-generated Israel-Hamas war images\nBloomsbury uses AI-generated artwork for Sarah J. Maas book\nPage info\nType: Incident\nPublished: June 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/all-eyes-on-rafah-deepfake-criticised-for-sanitising-gaza-invasion", "content": "\"All eyes on Rafah\" deepfake criticised for \"sanitising\" Gaza invasion\nOccurred: May 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\n\u201cAll Eyes on Rafah\u201d, an AI-generated image that went viral online, prompted accusations of \"sanitisation\" of the Israel-Hamas war and \"slacktivism\".\nThe fake image depicts tent camps for displaced Palestinians stretching out into the horizon, overlaid with the phrase \"All Eyes on Rafah\". The phrase was originally used by World Health Organization representative Dr. Richard \u201cRik\u201d Peeperkorn.\nApparently created by Malaysian Instagram user @shahv4012, the image was shared more than 47 million times online, including by celebrities Dua Lipa, Lewis Hamilton, and Gigi and Bella Hadid.\nHowever, the image received criticism for \u201csanitising\u201d the reality of the situation in Gaza. Critics, including actress Rachel Zegler, argue that the AI-generated image does not reflect the actual horrors faced by the Palestinian people. \nThey expressed concern that the image has become a \u201ctrendy\u201d and \"slacktivist \"way to show support for the cause, despite the availability of real content from Gazans and journalists showing the reality on the ground.\nSystem \ud83e\udd16\nUnknown\nOperator: @shahv4012\nDeveloper:  \nCountry: Israel; Palestine\nSector: Govt - police; Govt - security; Govt - defence; Politics\nPurpose: Raise awareness\nTechnology: Deepfake - image\nIssue: Ethics/values; Mis/disinformation\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.latimes.com/world-nation/story/2024-05-30/all-eyes-on-rafah-viral-ai-image-gaza-war\nhttps://www.theguardian.com/world/article/2024/may/30/all-eyes-on-rafah-how-ai-generated-image-spread-across-social-media\nhttps://www.bbc.co.uk/news/articles/cjkkj5jejleo\nhttps://theconversation.com/why-all-eyes-on-rafah-went-viral-and-why-you-need-to-be-careful-when-sharing-ai-images-231412\nhttps://www.npr.org/2024/06/02/g-s1-2455/all-eyes-on-rafah-most-viral-ai-meme-malaysia-artists-claim-credit\nhttps://www.aljazeera.com/news/2024/5/29/what-is-all-eyes-on-rafah-decoding-the-latest-viral-social-trend\nRelated \ud83c\udf10\nDeepfake Palestinian carries children out of rubble\nBeijing AI influence campaign weaponises Gaza conflict\nPage info\nType: Issue\nPublished: June 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/home-bargains-shopper-misidentified-by-facewatch", "content": "Home Bargains shopper misidentified by Facewatch facial recognition\nOccurred: May 2024 \nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn innocent shopper at retail chain Home Bargains was wrongfully accused of theft due to an error in a facial recognition system, prompting public controversy and a legal complaint.\nThe woman, who preferred to remain anonymous, was incorrectly identified by the Facewatch facial recognition system as a shoplifter. A staff member searched her bag and she was led out of the store. She was also banned from all stores using the Facewatch technology.\nThe woman reported that she was devastated by the incident and it caused her significant anxiety and distress. She was worried about being perceived as a shoplifter despite never having stolen anything. Facewatch later acknowledged the error and issued an apology.\nThis incident highlights some of the potential issues with facial recognition technology, including false positives and the human rights and psychological impacts on innocent individuals. \nDespite these concerns, the technology is increasingly being used by retailers and police forces in the UK.\nSystem \ud83e\udd16\nFacewatch facial recognition system\nOperator: Home Bargains\nDeveloper: Facewatch\nCountry: UK\nSector: Retail\nPurpose: Identify criminals\nTechnology: Facial recognition\nIssue: Accuracy/reliability; Human/civil rights; Safety\nTransparency: Governance\nRegulation \u2696\ufe0f\nData Protection Act 2018\nResearch, advocacy \ud83e\uddee\nBig Brother Watch (2024). LANDMARK LEGAL CHALLENGES LAUNCHED AGAINST FACIAL RECOGNITION AFTER POLICE AND RETAILER MISIDENTIFICATIONS\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/technology-69055945\nhttps://www.dailymail.co.uk/news/article-13463433/innocent-shoplifter-accused-stealing-facial-recognition-error.html\nhttps://www.manchestereveningnews.co.uk/whats-on/shopping/home-bargains-shopper-banned-wrongfully-29257020\nhttps://www.biometricupdate.com/202405/facewatch-met-police-face-lawsuits-after-facial-recognition-misidentification\nhttps://metro.co.uk/2024/05/27/facial-recognition-leaves-brits-digital-police-line-ups-without-realising-20918056/\nRelated \ud83c\udf10\nFrasers Group criticised for live facial recognition programme\nMaori woman misidentified by Foodstuffs facial recognition system\nPage info\nType: Incident\nPublished: June 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/eventbrite-recommendation-algorithm-promotes-illegal-opioid-sales", "content": "Eventbrite recommendation algorithm promotes illegal opioid sales\nOccurred: May 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThousands of Eventbrite posts were discovered selling escort services and opioid drugs like Xanax, some of which were recommended by the company\u2019s recommendation algorithm.\nA WIRED investigation revealed that Eventbrite\u2019s recommendation algorithm was selling escort services and drugs like Xanax and Oxycodone. Some of these illegal listings were recommended by the company\u2019s algorithm alongside addiction recovery events. \nIn addition, over 7,400 events published on the platform appeared to violate one or more of Eventbrite\u2019s terms.\nCritics argued that these listings prey on people trying to recover from addiction. Eventbrite responded by saying that listings of this kind had no place on its platform and that it took the issue seriously. The identified illegal and illicit activity were later removed.\nThe finding raised questions about Eventbrite governance, including its ability to manage its recommendation system effectively. \nSystem \ud83e\udd16\nEventbrite recommendation system\nDocuments \ud83d\udcc3\nEventbrite. Automating Relevance Tuning for Event Search\nEventbrite community guidelines\nOperator: Eventbrite\nDeveloper: Eventbrite\nCountry: USA\nSector: Health\nPurpose: Recommend content\nTechnology: Recommendation system\nIssue: Accuracy/reliability; Ethics/values; Safety\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.wired.com/story/sex-drugs-and-eventbrite/\nhttps://podcasts.apple.com/us/podcast/eventbrite-promoted-illegal-opioid-sales-to-people/id1211754233?i=1000656380092\nRelated \ud83c\udf10\nInstagram offers Xanax, exctasy, opioid pipeline to kids\nTikTok recommends adult content to children\nPage info\nType: Issue\nPublished: June 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/uk-watchdog-investigates-microsoft-recall-ai-feature", "content": "UK watchdog investigates Microsoft Recall AI feature\nOccurred: May 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe UK\u2019s Information Commissioner\u2019s Office (ICO) announced it was investigating Microsoft\u2019s AI feature, Recall, over potential privacy concerns. \nThis feature, part of Microsoft\u2019s AI Copilot+ system in Windows 11, takes screenshots of a user\u2019s screen every few seconds. The screenshots are stored locally on the user\u2019s computer in an encrypted format.\nThe ICO is seeking more information from Microsoft about the safety measures in place to protect user privacy. Privacy campaigners have expressed concerns, calling the feature a potential \"privacy nightmare\". They worry about the implications of a feature that can search through all users\u2019 past activity, including files, photos, emails, browsing history, and even screenshots.\nMicrosoft has stated that Recall is an \u201coptional experience\u201d and that the company is committed to privacy and security. The tech company also clarified that the data collected by Recall is stored locally and not accessed by Microsoft or anyone who does not have device access. \nHowever, concerns remain about the potential for sensitive data to become easily accessible if a user\u2019s device is compromised.\n\u2795 June 2024. Microsoft announced it was making Recall an opt-in feature and would address security concerns.\nSystem \ud83e\udd16\nRecall\n\nDocuments \ud83d\udcc3\nMicrosoft. Copilot+ PC features\nMicrosoft. Update on the Recall preview feature for Copilot+ PCs\nOperator: \nDeveloper: Microsoft\nCountry: UK\nSector: Technology\nPurpose: Identify viewed content\nTechnology: Machine learning\nIssue: Privacy; Security\nTransparency: \nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nInformation Commissioner's Office (2024). Statement in response to Microsoft Recall feature\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/articles/cpwwqp6nx14o\nhttps://www.tomshardware.com/software/windows/recall-drawing-regulatory-scrutiny-in-the-uk-microsofts-ai-copilot-feature-a-privacy-nightmare\nhttps://mashable.com/article/microsoft-recall-ai-feature-uk-investigation\nhttps://www.techradar.com/computing/computing-security/windows-recall-sounds-like-a-privacy-nightmare-heres-why-im-worried\nhttps://edition.cnn.com/2024/05/22/tech/microsoft-ai-tool-privacy-recall/index.html\nRelated \ud83c\udf10\nMicrosoft Copilot generates fake Putin comments on Navalny death\nEngineer warns Microsoft Copilot Designer creates violent, sexual images\nPage info\nType: Issue\nPublished: May 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/court-finds-gaming-cheats-company-guilty-of-copyright-infringement", "content": "Court finds gaming cheats company AimJunkies guilty of copyright infringement \nOccurred: May 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA court found a games cheats website guilty of copyright infringement of the popular game Destiny 2.\nAimJunkies is a cheat and mod distribution website, where users can buy and download cheating software for different games, including for the game Destiny 2. \nBungie, the owner of Destiny 2, alleged that Phoenix Digital, the operator of AimJunkies, reverse-engineered the game and copied code to create cheats, thereby infringing on Bungie\u2019s intellectual property rights. \nBungie\u2019s legal team argued that Phoenix Digital\u2019s actions violated Digital Millennium Copyright Act (DMCA) provisions, which prohibit circumventing copyright protection technology\nThe court found Phoenix Digital guilty of copyright infringement and ordered them to pay USD 63,210 in damages to Bungie.\nThe ruling was seen to set a valuable precedent as it is possibly the first time a jury has found a cheat creator liable for infringing a game company\u2019s copyright.\nSystem \ud83e\udd16\nAimJunkies \nOperator: Phoenix Digital\nDeveloper: Phoenix Digital\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Create cheats for PC games\nTechnology:\nIssue: Copyright\nTransparency:\nRegulatory, legal \u2696\ufe0f\nhttps://s3.documentcloud.org/documents/24689095/pages/bungie-v-phoenix-digital-group-jury-decision-p3-large.gif?ts=1716658861898 \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theverge.com/2024/5/25/24164679/bungie-anti-cheating-lawsuit-jury-trial-aimjunkies-copyright-violation-victory\nhttps://www.gamesindustry.biz/bungie-wins-lawsuit-against-aimjunkies\nhttps://www.gamedeveloper.com/business/jury-backs-bungie-in-lawsuit-against-cheat-maker-aimjunkies\nhttps://www.pcgamesinsider.biz/news/74486/bungie-wins-cheating-lawsuit-against-aimjunkies/\nRelated \ud83c\udf10\nUserviz video game cheating system is shut down after legal threat \nPage info\nType: Incident\nPublished: May 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tesla-in-fsd-attempts-to-drive-into-passing-train", "content": "Tesla operating in FSD attempts to drive into passing trains\nOccurred: May 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Tesla operating in Full Self-Driving (FSD) mode attempted to drive into two passing trains. \nThe incident, which occurred on May 8, 2024, was captured by multiple cameras on the vehicle and showed the Tesla Model 3 driving head-on into a closed level crossing as a train was passing through. The driver then had to take control at the last minute to avoid a collision.\nThe owner claimed that the incident was the second of its kind in six months to have occurred, and said he was seeking to take legal action over these close calls.\nThe episodes raised questions about the safety of Tesla's FSD system.\nSystem \ud83e\udd16\nTesla Autopilot, Full-Self Driving\nOperator: Craig Doty II\nDeveloper: Tesla\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Driver assistance system; Self-driving system; Computer vision\nIssue: Accuracy/reliability; Safety\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://teslamotorsclub.com/tmc/threads/ap-fsd-related-crashes.264798/page-8#post-8259936\nhttps://jalopnik.com/watch-a-tesla-fully-self-drive-into-the-path-of-a-speed-1851487985\nhttps://futurism.com/the-byte/footage-self-driving-tesla-train\nhttps://cybernews.com/news/tesla-self-driving-mode-fails-to-see-passing-train/\nhttps://www.dailymail.co.uk/sciencetech/article-13439727/tesla-owner-claim-self-driving-oncoming-train.html\nhttps://boingboing.net/2024/05/20/tesla-on-autopilot-almost-crashes-into-passing-train.html\nhttps://www.stuff.co.nz/motoring/350286168/tesla-autopilot-nearly-puts-driver-underneath-train\nRelated \ud83c\udf10\nTesla Model S crashes into fire engine\nTesla FSD beta test car hits bollard, driver fired\nPage info\nType: Incident\nPublished: May 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/google-microsoft-image-searches-list-nonconsensual-deepfake-porn", "content": "Google, Microsoft image searches list nonconsensual deepfake porn\nOccurred: May 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGoogle and Microsoft\u2019s Bing included nonconsensual deepfake porn in top image search results alongside tools that advertise the ability to create such material.\nNBC News found that deepfake pornographic images featuring the likenesses of 36 female celebrities were the first images Google and other top search engines surfaced in searches for many women\u2019s names and the word \u201cdeepfakes,\u201d as well as general terms like \u201cdeepfake porn\u201d or \u201cfake nudes.\u201d \nA review of the results found nonconsensual deepfake images and links to deepfake videos in the top Google results for 34 of those searches and the top Bing results for 35 of them. Over half of the top results were links to a popular deepfake website or a competitor. \nThe two search engines also returned links to multiple apps and programmes to create and view nonconsensual deepfake porn in the first six results for searches for \u201cfake nudes\u201d.\nThe incident raised questions about the two companies' ability to detect and manage deepfakes. It also prompted commentators to point to the degradation of the two search engines and the information ecosystem more generally.\nSystem \ud83e\udd16\nBing Images website\nGoogle Images website\nGoogle reliable information systems\nOperator: NBC News\nDeveloper: Alphabet/Google; Microsoft\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Determine reliability\nTechnology: \nIssue:  Accuracy/reliability; Mis/disinformation\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.nbcnews.com/tech/internet/google-bing-deepfake-porn-image-celebrity-rcna130445\nhttps://petapixel.com/2024/01/12/google-and-bing-put-ai-deepfake-porn-at-top-of-some-search-results/\nhttps://me.mashable.com/tech/37175/google-bing-reportedly-shows-non-consensual-deepfake-porn-at-top-of-search-results\nhttps://www.windowscentral.com/software-apps/browsing/google-and-bing-under-fire-for-promoting-nonconsensual-deepfake-porn-as-ai-continues-to-brew-more-trouble\nRelated \ud83c\udf10\nSynthetic Tiananmen tank man dominates Google Search\nGoogle AI Overviews gives wrong and misleading answers\nPage info\nType: Issue\nPublished: May 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/google-ai-overviews-give-wrong-dangerous-answers", "content": "Google AI Overviews gives wrong and misleading answers\nOccurred: May 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGoogle\u2019s \u201cAI Overviews\u201d feature was criticised for providing factually inaccurate, misleading and nonsensical answers. \nThe AI-generated summaries, which appear at the top of Google\u2019s search results, were found to contain basic factual errors. These include treating jokes as facts, such as suggesting using \u201c1/8 cup of non-toxic glue\u201d to stop cheese from sliding off pizza, which was traced back to a troll post. \nAnother example is the system incorrectly stating that \u201castronauts have met cats on the moon\u201d and that \"Neil Armstrong said, \u2018One small step for man\u2019 because it was a cat\u2019s step\".\nGoogle defended the system, saying it works as intended for most queries and that many of the problematic examples are uncommon or had been doctored. However, experts expressed concerns about the potential for the system to perpetuate bias and misinformation.\nThese errors are seen as particularly problematic given the prominence of Google\u2019s search results page.\nSystem \ud83e\udd16\nAI Overviews\n\nDocuments \ud83d\udcc3\nGoogle. AI Overviews: About last week\nGoogle. Generative AI in Search: Let Google do the searching for you\nGoogle. Find information in faster & easier ways with AI Overviews in Google Search\nOperator: Alphabet/Google\nDeveloper: Alphabet/Google\nCountry: USA\nSector: Multiple\nPurpose: Generate search results\nTechnology: Generative AI\nIssue: Accuracy/reliability; Mis/disinformation\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theverge.com/2024/5/23/24162896/google-ai-overview-hallucinations-glue-in-pizza\nhttps://edition.cnn.com/2024/05/24/tech/google-search-ai-results-incorrect-fix/index.html\nhttps://arstechnica.com/information-technology/2024/05/googles-ai-overview-can-give-false-misleading-and-dangerous-answers/\nhttps://www.nytimes.com/2024/05/24/technology/google-ai-overview-search.html\nhttps://searchengineland.com/google-ai-overview-fails-442575\nhttps://www.euronews.com/next/2024/05/27/googles-new-ai-summaries-tool-causes-concern-after-producing-misleading-responses\nhttps://www.technologyreview.com/2024/05/31/1093019/why-are-googles-ai-overviews-results-so-bad/\nRelated \ud83c\udf10\nGoogle Assistive Writing feature accused of being too 'woke'\nGoogle Autocomplete search predictions\nPage info\nType: Incident\nPublished: May 2024\nLast updated: June 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/openai-accused-of-using-scarlett-johanssons-voice-without-consent-to-train", "content": "OpenAI accused of using Scarlett Johansson\u2019s voice without consent to train AI system\nOccurred: May 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nActress Scarlett Johansson accused OpenAI of using a voice \u201ceerily similar\u201d to hers for their new GPT-4o chatbot without her consent.\nGPT-4o is the latest AI model by OpenAI that can process text, audio, and visual inputs and generate corresponding outputs. It offers a voice chat feature with five distinct output voices for lifelike interactions.\nOne of the voices, Sky, caused controversy over its similarity to Johansson\u2019s voice in the movie Her. Johansson expressed shock and anger at the similarity between her voice and the AI\u2019s, stating that even her closest friends and news outlets could not tell the difference. She also hired lawyers  to investigate how the company created the voice.\nOpenAI\u2019s co-founder, Sam Altman, is reported to have approached Johansson with a request to use her voice likeness in September, however Johansson declined the offer. \nIn response to the controversy, OpenAI paused the use of Sky\u2019s voice,  denied  use of Scarlett Johansson\u2019s voice\u201d, and  clarified they used another  professional actress for the voice of Sky, who had been cast before their outreach to Johansson. The company claimed that all the actors behind their GPT-4o chatbot voices had signed agreements and had been compensated. \nThe case highlights the difficulties in defining and protecting voice likeness due to its subjective nature. Proposed regulations such as the Ensuring Likeness Voice and Image Security (ELVIS) Act and the No Artificial Intelligence Fake Replicas And Unauthorised Duplications Act (No AI FRAUD Act) are steps towards addressing these issues.\nSystem \ud83e\udd16\nGPT-4o\nGPT-4 large language model\nOperator: OpenAI\nDeveloper: OpenAI\nCountry: Global\nSector: Media/entertainment/sports/arts\nPurpose: Chatbot\nTechnology: Generative AI; Machine learning; Neural network; Deep learning; NLP/text analysis; Deepfake - audio; Chatbot\nIssue: Personality rights\nTransparency:\n\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.washingtonpost.com/technology/2024/05/22/openai-scarlett-johansson-chatgpt-ai-voice/\nhttps://slate.com/technology/2024/05/scarlett-johansson-ai-voice-sam-altman-openai.html\nhttps://www.hollywoodreporter.com/business/business-news/scarlett-johansson-ai-legal-threat-1235905899/\nhttps://www.bbc.co.uk/news/articles/cm559l5g529o\nhttps://www.telegraph.co.uk/us/news/2024/05/21/chatgpt-scarlett-johansson-sky-voice-complaint-ai/\nhttps://www.foxnews.com/entertainment/scarlett-johansson-ai-controversy-agent-says-another-actress-hired-chatgpt-voice-report\nhttps://www.theguardian.com/technology/article/2024/may/20/chatgpt-scarlett-johansson-voice\nhttps://www.nytimes.com/2024/05/20/technology/scarlett-johannson-openai-voice.html\nhttps://variety.com/2024/digital/news/scarlett-johansson-responds-shocked-angered-openai-chatgpt-her-1236011135/\n\nRelated \ud83c\udf10\nChinese voice actor sues AI companies for using her voice without consent\nApple trains AI models on Spotify Audiobook Narrators\nVoiceify sued for training AI with copyrighted material\nIBM sells Greg Marston voice for commercial cloning\nVoiceverse NFT voice theft \nPage info\nType: Issue\nPublished: May 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/voice-actors-sue-ai-startup-for-voice-theft", "content": "Voice actors sue AI start-up for \u201cvoice theft\u201d\nOccurred: May 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA group of voiceover actors sued AI start-up, Lovo, for the unauthorised use of their voices\nA group of voiceover actors, including Paul Skye Lehrman, filed a class-action lawsuit for USD 5M in damages against AI startup Lovo, accusing the company of misappropriating their voices without consent. The lawsuit alleges that Lovo used the actors\u2019 voices to create AI-generated content, leading to voice theft and potential loss of income.\nThe actors allege that Lovo\u2019s unauthorised use of their voices has deprived them of control over their work and proper compensation. The lawsuit seeks to represent other voiceover artists who believe their voices were misappropriated by Lovo.\nThis suit marks a significant development in the ongoing legal battle between creators and AI companies over the use of copyrighted works, unequivocal likeness and personal data. \n\nSystem \ud83e\udd16\nLovo AI Voice Generator \nOperator: LOVO\nDeveloper: LOVO\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Generate voice\nTechnology: Deepfake - audio\nIssue: Personality rights\nTransparency: Governance \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.hollywoodreporter.com/business/business-news/actors-hit-ai-startup-with-class-action-lawsuit-over-voice-theft-1235900689/ \nhttps://consent.yahoo.com/v2/collectConsent?sessionId=3_cc-session_80b1fe52-2c92-4b60-a0c7-8bf6325a1b03\nhttps://aibusiness.com/nlp/actors-sue-ai-voice-generator-for-unauthorized-use-of-their-voices\nhttps://www.thewrap.com/lovo-ai-lawsuit-voiceover-actors/\nhttps://variety.com/2024/biz/news/lovo-ai-stealing-actors-voices-genny-1236006443/\nRelated \ud83c\udf10\nChinese voice actor sues AI companies for using her voice without consent\nApple trains AI models on Spotify Audiobook Narrators\nBBC replaces 'Mamma Mia!' star Sara Poyzer with AI Voice\nVoiceify sued for training AI with copyrighted material\nIBM sells Greg Marston voice for commercial cloning\nVoiceverse NFT voice theft \nPage info\nType: Incident\nPublished: May 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/sony-warns-ai-companies-to-not-misuse-its-data", "content": "Sony warns AI companies not to misuse its data\nOccurred: May 2024 \nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nSony Music warned AI companies not to train their systems with its content.\nSony Music Group issued warning letters to over 700 AI developers and music streaming services, prohibiting the use of its content for training, developing, or commercialising AI systems. This includes music from artists like Beyonc\u00e9, Harry Styles, and Adele.\nAccording to Sony, the unauthorised use of it\u2019s content for AI training deprives the company and its artists of control over their work and appropriate compensation. Sony executives are concerned that their music has already been exploited and are seeking to establish a clear legal position against any AI developer considered to have used their music without consent.\nThis move marks a significant stance in the music industry\u2019s ongoing battle against tech groups using copyrighted content without consent or compensation. \nSony\u2019s decision could set a precedent for other music groups and impact the development of AI systems that rely on such data.\nOperator:\nDeveloper:\nCountry: Global\nSector: Media/entertainment/sports/arts\nPurpose:\nTechnology: Generative AI; Machine learning; Neural network; Deep learning; NLP/text analysis; Deepfake - audio\nIssue: Copyright\nTransparency: Governance \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bloomberg.com/news/articles/2024-05-16/sony-music-warns-companies-to-stop-training-ai-on-its-artists-content\nhttps://www.billboard.com/business/tech/sony-music-artificial-intelligence-training-opt-out-1235684192/ \nhttps://www.bbc.co.uk/news/articles/c0434yx8vgxo\nhttps://www.nbcnews.com/tech/tech-news/sony-music-group-warns-700-companies-using-content-train-ai-rcna152689\nhttps://www.sonymusic.com/sonymusic/declaration-of-ai-training-opt-out/\nRelated \ud83c\udf10\nScammer sells fake AI Frank Ocean songs\nDrake The Weekend AI Voice Cloning \nDrake threatened with lawsuit over AI-generated Tupac Shakur voice\nPage info\nType: Issue\nPublished: May 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/eight-newspapers-sue-openai-and-microsoft-for-copyright-infringement", "content": "Eight newspapers sue OpenAI and Microsoft for copyright infringement\nOccurred: April 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nEight prominent US newspapers sued OpenAI and Microsoft for copyright infringement.\n\nThe Chicago Tribune, Orlando Sentinel, South Florida Sun Sentinel, New York Daily News, Mercury News, Denver Post, Orange County Register, and St. Paul Pioneer-Press submitted a lawsuit arguing \u201cThis lawsuit arises from Defendants purloining millions of the Publishers\u2019 copyrighted articles without permission and without payment to fuel the commercialization of their generative artificial intelligence (\u201cGenAI\u201d) products, including ChatGPT and Copilot.\u201d\n\nThes suit adds to the numerous lawsuits OpenAI and Microsoft are facing from newspapers, writers and publishers over training their large language models without copyright. OpenAI stands by the\u201d fair use\u201d defence claiming they are transforming the original works sufficiently to not constitute copyright violations.\nSystem \ud83e\udd16\nChatGPT chatbot\nGPT-4 large language model\n\nOperator: OpenAI; Microsoft\nDeveloper: OpenAI\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Copyright\nTransparency: Governance \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.informationweek.com/machine-learning-ai/more-newspapers-join-the-copyright-battle-against-openai-microsoft\nhttps://www.aei.org/technology-and-innovation/yet-another-ai-copyright-suit-against-openai-underscores-autonomy-automaton-divide/\nhttps://www.nytimes.com/2024/04/30/business/media/newspapers-sued-microsoft-openai.html#:~:text=The%20suit%2C%20which%20accuses%20the,used%20to%20power%20artificial%20intelligence.&text=Eight%20daily%20newspapers%20owned%20by,articles%20to%20power%20their%20A.I.\nhttps://www.theguardian.com/technology/2024/apr/30/us-newspaper-openai-lawsuit\nhttps://www.cnbc.com/2024/04/30/eight-newspaper-publishers-sue-openai-over-copyright-infringement.html \nRegulatory, legal \u2696\ufe0f\nhttps://storage.courtlistener.com/recap/gov.uscourts.nysd.620514/gov.uscourts.nysd.620514.1.0.pdf \nRelated \ud83c\udf10\nThree news publishers sue OpenAI for copyright infringement\nThe New York Times sues OpenAI, Microsoft over copyright abuse\n17 authors sue OpenAI for 'systematic mass-scale copyright infringement'\nAuthors Mona Awad, Paul Tremblay sue OpenAI for copyright abuse\nPage info\nType: Incident\nPublished: May 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/singapore-writers-resist-government-plan-to-train-ai-using-their-work", "content": "Singapore writers resist government plan to train AI using their work\nOccurred: May 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nWriters in Singapore resisted the Singapore government\u2019s plan to train Southeast Asia\u2019s first local large language model on their work due to copyright and compensation concerns.\nThe Singaporean government\u2019s National Multimodal Large Language Model Programme (NMLP) sparked controversy among local writers and publishers. The NMLP, a SGD 70 million (USD 52 million) project launched in December 2023, aims to train a large language model (LLM) on locally produced material to address the bias of existing LLMs that are seen to have disproportionately large influences from Western societies.\nSingapore\u2019s government believes that an LLM trained on local material would have more accurate references to Singapore\u2019s history, colloquialisms, and culture, and would be able to understand widely spoken languages such as Malay, Mandarin, and Tamil. \nHowever, the literary community, including well-known literary figure Gwee Li Sui, expressed concerns about the lack of clarity on how their works would be protected from being used for purposes other than cultural representation.\nAn email sent by the government initially gave respondents 10 days to respond to a survey asking them about the project, but contained few details on compensation or copyright protection. As a result, several writers, including Gwee,  declined to let the LLM train on their works.\nThis resistance is part of a worldwide trend against the use of published works to train AI systems. For instance, US comedian Sarah Silverman and authors like John Grisham and George R.R. Martin have issued class-action lawsuits against OpenAI and Meta, accusing the companies of copyright infringement for using protected work to train AI programmes.\nSystem \ud83e\udd16\nSingapore National Multimodal Large Language Model Programme\nOperator: Singapore Government\nDeveloper: Singapore Government\nCountry: Singapore\nSector: Media/entertainment/sports/arts\nPurpose: Counter bias in western large language models\nTechnology: Large Language Model\nIssue: Copyright\nTransparency:\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.imda.gov.sg/resources/press-releases-factsheets-and-speeches/press-releases/2023/sg-to-develop-southeast-asias-first-llm-ecosystem\nhttps://bestofai.com/article/writers-and-publishers-in-singapore-reject-a-government-plan-to-train-ai-on-their-work\nRelated \ud83c\udf10\n17 authors sue OpenAI for 'systematic mass-scale copyright infringement'\nAuthors Mona Awad, Paul Tremblay sue OpenAI for copyright abuse\nPage info\nType: Issue\nPublished: May 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/hoodline-use-of-ai-to-generate-news-solicits-backlash", "content": "Hoodline use of AI to generate 'news' solicits backlash\nOccurred: May 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe use of AI to create news articles and fictional journalists by US neighbourhood news publisher Hoodline prompted questions about the ethics of the company and its owner. \nAccording to Gazetteer San Francisco, 'a substantial chunk of content on Hoodline\u2019s website in recent months \u2014 save for a handful of stories reported and written by a few longtime (human) contributors \u2014 appears to be produced by a synthetic text generator.' 'Additionally,' it added, 'these stories are bylined by a rotating cast of five authors, most, if not all, of whose bylines appear to be AI-generated fabrication.'\nThe same day, Hoodline publicly announced that it was using AI as part of a shift to 'hybrid content' in which junior-level journalists would use various AI tools to reblog police press releases and create reliable, simple news stories - a model intended to build the traffic and revenue. It also said it would start using a small badge next to each byline, indicating that stories are AI-assisted.\nHoodline was acquired by Impress3 Media in 2021. The fracas was seen to raise questions about Impress3 and Hoodline's ethics, governance and transparency, as well as about the use of AI in journalism more broadly. \nSystem \ud83e\udd16\nHoodline website\n\nDocuments \ud83d\udcc3\nHoodline (2024). Letter from the publisher\nOperator: Hoodline\nDeveloper: Impress3 Media/Hoodline\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Generate news content\nTechnology: NLP/text analysis\nIssue: Ethics/values\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://sf.gazetteer.co/hoodline-using-ai-to-generate-news-stories-and-journalist-profiles\nhttps://brokeassstuart.com/2024/04/10/hoodline-caught-using-ai-generated-writers-to-make-ai-generated-articles/\nhttps://www.bloomberg.com/news/newsletters/2024-05-17/ai-fake-bylines-on-news-site-raise-questions-of-credibility-for-journalists\nhttps://medium.com/@GhoulsWhoCode/hoodwinked-hoodline-chicagos-digital-blackface-raises-major-ethical-questions-a6b7fa12324c\nRelated \ud83c\udf10\nRed Ventures AI automated 'journalism'\nMen's Journal AI journalism\nPage info\nType: Issue\nPublished: May 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/slack-uses-user-data-to-train-ai-models", "content": "Slack forces users to opt-out of training its AI models\nOccurred: May 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nSlack came under fire for using user messages and files to train its AI models by default, without explicit user consent. \nKnowledge-sharing company Slack was found to have started training its AI models on its customers' data and information, without telling them or seeking their consent.\nIn an undated statement on privacy principles quietly posted overnight to its website, Slack recommended that users wishing to opt-out of its AI training programme must do so by emailing the company, raising concerns about the use and misuse of potentially sensitive corporate and personal information. \nIn response, Slack said that user data would not be shared with third-party providers for training purposes and that customer data never leaves the platform. The company also updated its privacy principles in an attempt to better explain the relationship between customer data and generative AI in Slack.\nSystem \ud83e\udd16\nSlack website\nSlack Wikipedia profile\nDocuments \nSlack. Privacy principles: search, learning and artificial intelligence\nOperator:\nDeveloper: Salesforce/Slack\nCountry: Global\nSector: Multiple\nPurpose: Predict search results; Recommend channels\nTechnology: Machine learning\nIssue: Confidentiality; Privacy; Security\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://uk.pcmag.com/team-messaging/152381/slack-trains-some-of-its-ai-powered-features-on-user-messages-files\nhttps://arstechnica.com/tech-policy/2024/05/slack-defends-default-opt-in-for-ai-training-on-chats-amid-user-outrage/\nhttps://www.securityweek.com/user-outcry-as-slack-scrapes-customer-data-for-ai-model-training/\nhttps://www.engadget.com/yuck-slack-has-been-scanning-your-messages-to-train-its-ai-models-181918245.html\nhttps://lifehacker.com/tech/slacks-ai-features-analyze-private-conversations\nhttps://techcrunch.com/2024/05/17/slack-under-attack-over-sneaky-ai-training-policy/\nRelated \ud83c\udf10\nZoom under fire for using customer data to train AI models\nAdobe trained Firefly AI model on competitor images\nPage info\nType: Issue\nPublished: May 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/sec-launches-investigation-into-evolv-technology", "content": "SEC launches investigation into Evolv Technology\nOccurred: February 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAI weapons detection company Evolv Technology confirmed it was being investigated by the US Securities and Exchange Commission (SEC) following allegations that the company may have misled investors about the efficacy of its products. \nThe investigation was described as a confidential \u201cnon-public, fact finding inquiry,\u201d and resulted in Evolv shares dropping 19 percent in one trading day.\nEarlier, the company had claimed that its Evolv Express AI weapons scanner had been tested by the UK Government\u2019s National Protective Security Authority (NPSA), but it was later revealed that the NPSA does not conduct this type of testing.\nIn response, Evolv Technology revised its claims about UK testing to \"better reflect the process taken\". However, the independent company that supposedly \u201ctested and validated\u201d Evolv\u2019s technology using NPSA standards, Metrix NDT, stated that it was \"not correct to say we \u2018validated\u2019 the system\"\nThe SEC investigation followed a separate probe initiated by the US Federal Trade Commission over allegations that Evolv uses deceptive marketing practices. \n\u2795 Investors also filed a lawsuit against Evolv Technologies after the disclosure of the SEC investigation and a BBC report disputing the company\u2019s marketing claims. The lawsuit alleges that Evolv misrepresented and concealed that it materially overstated the efficacy of its products, leading to an increased risk of undetected weapons entering schools and other locations.\nSystem \ud83e\udd16\nEvolv Express weapons detection\nOperator:\nDeveloper: Evolv Technology\nCountry: USA\nSector: Education; Media/entertainment/sports/arts\nPurpose: Detect weapons\nTechnology: Computer vision; Object recognition\nIssue: Governance\nTransparency: Marketing\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nEvolv Technology Provides Regulatory Update\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://ipvm.com/reports/sec-evolv\nhttps://www.investing.com/news/stock-market-news/evolv-technology-under-sec-investigation-93CH-3307611\nhttps://www.lpm.org/news/2024-02-21/jcps-weapons-detection-manufacturer-subject-of-second-federal-investigation\nhttps://www.securitysystemsnews.com/article/evolv-technology-gives-regulatory-update-following-sec-request\nRelated \ud83c\udf10\nEvolv backtacks on UK testing claims\nFTC investigates Evolv for misleading marketing\nPage info\nType: Incident\nPublished: May 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/evolv-backtacks-on-uk-testing-claims", "content": "Evolv backtracks on Evolv Express UK testing claims\nOccurred: March 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nWeapons detection company Evolv Technology backtracked on claims it made about its technology being tested by the UK Government\u2019s National Protective Security Authority (NPSA). \nEvolv had previously stated in a press release that the NPSA was one of the testers who had concluded that the Evolv Express solution was highly effective at detecting firearms and many other types of weapons.\nHowever, it was revealed that the NPSA does not conduct this type of testing. In response, Evolv updated the language used in the press release to better reflect the process taken. They clarified that an independent company had tested and validated Evolv\u2019s technology using NPSA standards. \nBut the UK company that conducted this testing, Metrix NDT, stated that it was not correct to say they \u2018validated\u2019 the system. \nThe incident raised questions about Evolv's truthfulnness and transparency.\nSystem \ud83e\udd16\nEvolv Express weapons detection\nOperator:\nDeveloper: Evolv Technology\nCountry: UK\nSector: Education; \nPurpose: Detect weapons\nTechnology: Computer vision; Object recognition\nIssue:  \nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/technology-68547574\nhttps://www.silicon.co.uk/security/evolv-testing-uk-554834\nhttps://ipvm.com/reports/evolv-3rd-party-test\nRelated \ud83c\udf10\nNCS4 finds Evolv Express fails to detect large knives\nEvolv Express mistakes certain Chromebooks as weapons\nPage info\nType: Issue\nPublished: May 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ncs4-finds-evolv-express-fails-to-detect-large-knives", "content": "NCS4 finds Evolv Express fails to detect large knives\nOccurred: November 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nWeapons detection system Evolv Express' ability to detect large knives failed over half the time, according to the US National Center for Spectator Sports Safety and Security (NCS4).\n\nA private report obtained via a Freedom of Information request by video surveillance research company IPVM revealed that Evolv\u2019s ability to detect large knives scored just 1.3 out of 3. An earlier public report published by the NCS4 gave Evolv a score of 2.84 out of three, with many types of guns being detected 100 percent of the time.\nIn 24 walkthroughs, Evolv Express failed to detect large knives 42 percent of the time. The private report stated, \u201cThe system was incapable of detecting every knife on the sensitivity level observed during the exercise\u201d and recommended full transparency to potential customers based on the data collected.\nThe findings raised questions about the accuracy and effectiveness of Evolv Express, and its manufacturer Evolv Technologies' transparency.\nSystem \ud83e\udd16\nEvolv Express weapons detection\nOperator:\nDeveloper: Evolv Technology\nCountry: USA\nSector: Education; Media/entertainment/sports/arts\nPurpose: Detect weapons\nTechnology: Computer vision; Object recognition\nIssue: Accuracy/reliability; Safety\nTransparency: Black box; Governance; Marketing\nInvestigations, assessments, audits \ud83e\uddd0\nNational Center for Spectator Sports Safety and Security (NCS4). Private NCS4 Report on Evolv Express\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/technology-63476769\nhttps://ipvm.com/reports/bbc-evolv\nhttps://www.dailymail.co.uk/news/article-12115945/AI-weapons-scanners-used-hundreds-schools-fail-detect-50-large-knives.html\nhttps://www.mirror.co.uk/news/us-news/ai-scanner-used-detect-weapons-30058336\nRelated \ud83c\udf10\nEvolv Express mistakes certain Chromebooks as weapons\nStudent stabbed after Evolv weapons detection failure\nPage info\nType: Issue\nPublished: May 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/evolv-express-mistakes-certain-chromebooks-as-weapons", "content": "Evolv Express mistakes certain Chromebooks as weapons\nOccurred: January 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nSurveillance product research group IPVM reported that Evolv Express mistakenly identified certain Chromebooks as weapons, sparking concerns about the reliability and effectiveness of the system. \nAccording to IPVM, the error occured 60 to 70 percent of the time - a finding disputed by Evolv Technology, which went on to accuse IPVM\u2019s coverage of being 'deceptive' and 'dangerous'. \nDespite these disputes, the reported error rate with Chromebooks underscores the need for rigorous testing and validation of such security technologies. \nIt also highlights the potential risks associated with relying on AI-based systems for critical security tasks without thorough independent verification. \nSystem \ud83e\udd16\nEvolv Express weapons detection\nOperator: \nDeveloper: Evolv Technology\nCountry: USA\nSector: Education\nPurpose: Detect weapons\nTechnology: Computer vision; Object recognition\nIssue: Accuracy/reliability; Safety\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://ipvm.com/reports/evolv-chromebook\nhttps://www.wmar2news.com/homepage-showcase/weapons-detection-system-in-baltimore-city-schools-accused-of-over-promising\nhttps://www.the74million.org/article/feds-probe-marketing-push-behind-ai-weapons-detection-tool-used-in-schools/\nhttps://www.washingtonpost.com/technology/2022/05/20/evolv-metal-detectors-gun-detection/\nRelated \ud83c\udf10\nStudent stabbed after Evolv weapons detection failure\nFTC investigates Evolv for misleading marketing\nPage info\nType: Issue\nPublished: May 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/stack-overflow-users-rebel-against-openai-llm-training-deal", "content": "Stack Overflow users rebel against OpenAI LLM training deal\nOccurred: May 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA deal that allows OpenAI to train its AI models using the answers and knowledge Stack Overflow users resulted in protests by software developers and others.\nOne user, Ben Ui, a software developer, chose to protest by overwriting his previous posts with complaints about the deal. This action resulted in a week-long suspension and his posts were reverted. Other users have reported similar experiences.\nUsers are upset that their data is being used without explicit permission. They argue that they should have the right to delete their own content from the site through the \u201cright to forget,\u201d a legal right codified into law through the EU\u2019s General Data Protection Regulation (GDPR). However, Stack Overflow\u2019s Terms of Service state that all content provided to the site is irrevocably owned by Stack Overflow.\nDespite the protests, Stack Overflow\u2019s terms-of-use don\u2019t appear to give users much wiggle room, as it seems the data can be used for AI training. The situation has led to a significant amount of controversy within the Stack Overflow community.\nSystem \ud83e\udd16\nStack Overflow and OpenAI Partner to Strengthen the World\u2019s Most Popular Large Language Models\nOperator: Stack Overflow\nDeveloper: OpenAI\nCountry: Global\nSector: Technology\nPurpose: Train large language models\nTechnology: Large language models\nIssue: Human/civil rights\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://arstechnica.com/information-technology/2024/05/stack-overflow-users-sabotage-their-posts-after-openai-deal/\nhttps://www.theregister.com/2024/05/09/stack_overflow_banning_users_who/\nhttps://www.tomshardware.com/tech-industry/artificial-intelligence/stack-overflow-bans-users-en-masse-for-rebelling-against-openai-partnership-users-banned-for-deleting-answers-to-prevent-them-being-used-to-train-chatgpt\nRelated \ud83c\udf10\nAI overwhelms Stack Overflow content moderation\nImmunefi bans 'inaccurate' ChatGPT-generated bug bounty reports\nPage info\nType: Incident\nPublished: May 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/deepfake-philippines-president-urges-military-action-against-china", "content": "Deepfake Philippines President urges military action against China \nOccurred: April 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn audio/video clip of Philippines President Ferdinand Marcos Jr apparently ordering a military attack against China caused alarm and resulted in the Philippines government cracking down on deepfakes.\nThe deepfake clip, which was taken down, appeared to order the \u201carmed forces and special task groups\u201d to act however they deem appropriate should China \u201cattack\u201d the Philippines. Photographs of Chinese activities in the West Philippine Sea \u2013 including publicly released images from the Philippine Coast Guard \u2013 flashed in the video as a slideshow as the audio played.\nThe clip was taken down on the orders of Philippines authorities. The country's Presidential Communications Office disavowed it and said it was considering legal action against the cuplrits. The government subsequently announced it was working on measures to combat fake news, misinformation and disinformation. \nThe incident raised concerns about the potential for confusion and panic, strained diplomatic relations and loss of trust in the media and government via deepfake misinformation and disinformation. It came at a time when tensions between the Philippines and China have been rising in the South China Sea.\nSystem \ud83e\udd16\nUnknown\nOperator:\nDeveloper:  \nCountry: Philippines\nSector: Politics; Govt - defence\nPurpose: Damage reputation\nTechnology: Deepfake - audio\nIssue: Mis/disinformation\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://globalnation.inquirer.net/233290/deepfake-pco-disowns-clip-of-marcos-attack-order-vs-china\nhttps://ipdefenseforum.com/2024/05/deepfake-audio-falsely-portrays-marcos-as-confrontational/\nhttps://www.philstar.com/headlines/2024/05/15/2355158/social-media-users-charged-over-marcosdeepfake\nhttps://www.rappler.com/philippines/malacanang-flags-deepfake-audio-marcos-ordering-military-attack-april-2024/\nhttps://time.com/6971239/philippines-marcos-deepfake-china-foreign-actor/\nhttps://www.scmp.com/week-asia/politics/article/3260229/audio-deepfake-marcos-jnr-ordering-military-action-against-china-prompts-manila-debunk-clip\nRelated \ud83c\udf10\nAlexandria Ocasio-Cortez depicted as deepfake pornstar\nDeepfake Belgium PM links COVID-19 with climate crisis\nPage info\nType: Incident\nPublished: May 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/openai-deleted-training-datasets-believed-to-contain-copyrighted-books", "content": "OpenAI deleted training datasets believed to contain copyrighted books\nOccurred: May 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nUnsealed documents reveal OpenAI deleted two large datasets used to train its AI models.\nDocuments unsealed during an ongoing class-action lawsuit between The Authors Guild and Open AI reveal the latter deleted two datasets used to train its GPT-2 and GPT-3 large language models. The datasets were named as 'Books1' and 'Books2' in the documents, and had been described in a 2020 technical document by OpenAI as 'corpora of books from the Internet' and formed 16 percent of the training data used to create GPT-3. \nThe two datasets are also believed likely to have contained over 100,000 published, copyrighted books which, if true, would support The Guild\u2019s case. The documents also revealed that the two researchers who created the datasets are no longer employed by OpenAI.\nFor months, the Guild has been seeking further information from OpenAI about the datasets. OpenAI initially resisted, citing confidentiality concerns, before revealing that it had deleted all copies of the data. In a statement OpenAI said \u201cThe models powering ChatGPT and our API today were not developed using these datasets\u201d.\nThe Authors Guild filed the class-action lawsuit over concerns that OpenAI trained their large language models using published, copyrighted books without consent from or payment towards authors. The issue of training AI models on copyrighted works is prevalent across the industry, with OpenAI and other AI companies facing a series of lawsuits.\nSystem \ud83e\udd16\nGPT-3 large language model\nGPT-2 large language model\nOperator: OpenAI\nDeveloper: OpenAI\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Copyright; Ethics/values\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://itc.ua/en/news/openai-destroyed-100-000-books-used-to-train-gpt-3-those-involved-have-also-disappeared/\nhttps://www.businessinsider.com/openai-destroyed-ai-training-datasets-lawsuit-authors-books-copyright-2024-5\nhttps://elbuz.com/en/openai-unichtozhila-100-000-knig-po-kotorym-trenirovali-gpt-3-prichastnye-tozhe-ku\nResearch, advocacy \ud83e\uddee\nBrown T.B. et al. Language Models are Few-Shot Learners\nRelated \ud83c\udf10\n17 authors sue OpenAI for 'systematic mass-scale copyright infringement'\nAuthors Mona Awad, Paul Tremblay sue OpenAI for copyright abuse\n\nPage info\nType: Issue\nPublished: May 2024\nLast updated: June 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/wpp-ceo-impersonated-in-deepfake-scam", "content": "WPP CEO impersonated in deepfake scam\nOccurred: April 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe CEO of the world largest marketing services group was targeted by an elaborate scam involving voice cloning in an attempt to solicit money and personal details from the company\u2019s workforce.\nWPP CEO Mark Read\u2019s image and voice were stolen by fraudsters, who created a WhatsApp account seemingly belonging to him and used it to set up a Microsoft Teams meeting that appeared to be with him and another senior WPP executive. During the meeting, the impostors deployed a deepfake video and voice clone of Read the executive as well as YouTube footage of them.\nThe scammers also tried using the meeting\u2019s chat function to impersonate Read and target a fellow WPP executive by asking them to set up a new business and hand over money and other personal details, according to the Guardian. \nWhile the attacks were not successful, the incidents were seen to highlight the many tools that scammers now have at their disposal to impersonate high-profile individuals for fraudulent purposes. \nSystem \ud83e\udd16\nUnknown\nOperator:\nDeveloper:  \nCountry: UK\nSector: Media/entertainment/sports/arts\nPurpose: Defraud\nTechnology: Deepfake - audio, video\nIssue: Fraud\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://nypost.com/2024/05/10/business/ceo-of-wpp-falls-victim-to-deepfake-scam/\nhttps://www.theguardian.com/technology/article/2024/may/10/ceo-wpp-deepfake-scam\nhttps://timesofindia.indiatimes.com/technology/tech-news/ai-voice-fake-whatsapp-account-how-scammers-targeted-ceo-of-worlds-biggest-ad-firm-using-deepfake/articleshow/110061281.cms\nhttps://www.ft.com/content/308c42af-2bf8-47e4-a360-517d5391b0b0\nRelated \ud83c\udf10\nDeepfake CFO scams finance worker for USD 25 million\nKerala man loses INR 40,000 to deepfake work colleague\nPage info\nType: Incident\nPublished: May 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/google-sge-suggests-user-drinks-urine-to-pass-kidney-stones", "content": "Google SGE suggests user drinks urine to pass kidney stones\nOccurred: May 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGoogle\u2019s SGE AI-powered search tool advised a user to drink their own urine in order to get rid of kidney stones. \nAsked for a quick way to get rid of kidney stones by X/Twitter user @dril, SGE (Search Generative Experience) recommended the user drink 2 litres of their own urine daily, sparking outrage and ridicule from health professionals and technology experts. \nThe Mayo Clinic recommends drinking 1.8 to 3.6 litres of water daily to pass kidney stones. \nThe search results were rectified; however, the incident was seen to mark another embarrassing moment for Google's attempts to accelerate its generative AI outputs, and a reminder of the importance of rigorous testing and monitoring to prevent such mishaps.\nSystem \ud83e\udd16\nGoogle SGE\nOperator: \nDeveloper: Alphabet/Google\nCountry: USA\nSector: Health\nPurpose: Generate search results\nTechnology: Machine learning\nIssue: Accuracy/reliability; Safety\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://twitter.com/dril/status/1787041991391584549\nhttps://technology.inquirer.net/134286/google-search-ai\nhttps://news.abplive.com/technology/google-ai-search-generative-experience-sge-blunder-drink-urine-to-pass-kidney-stones-quickly-controversy-1685792\nhttps://indianexpress.com/article/trending/trending-globally/google-sge-advises-user-to-drink-urine-to-pass-kidney-stones-post-goes-viral-9313800/\nhttps://www.seroundtable.com/google-sge-drink-urine-to-pass-kidney-stones-37339.html\nRelated \ud83c\udf10\nGoogle AI bots expouse slavery, fascism\nGoogle SGE recommends malware, fraud sites\nPage info\nType: Incident\nPublished: May 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/brazilian-football-fan-wrongly-arrested-using-facial-recognition", "content": "Brazilian football fan wrongly arrested by police using facial recognition\nOccurred: April 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nPolice using facial recognition to identify football fans in Brazil with open arrest warrants wrongly arrested a Confian\u00e7a fan, triggering a backlash on social media and a police apology. \nPersonal trainer Jo\u00e3o Ant\u00f4nio and his 16-year-old younger brother had been watching a football match between Confian\u00e7a and Sergipe when the former was handcuffed and taken to a room to prove his identity on the basis that the facial recognition system used by the police indicated that he had an open arrest warrant. He was subsequently released. \nThe incident was described by the boys as embarrasing and humiliating, and sparked accusations from civil rights and privacy advocates of 'structural' human and algorithmic racism. \nLocal police apologised, saying a suspect is only approached when alerted of a 90 percent potential match by their facial recognition system. \nSystem \ud83e\udd16\nUnknown\nOperator: \nDeveloper:  \nCountry: Brazil\nSector: Media/entertainment/sports/arts\nPurpose: Verify identity\nTechnology: Facial recognition\nIssue: Accuracy/reliability; Human/civil rights\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://ground.news/article/brazil-fervently-embraces-facial-recognition-amid-controversy-over-errors\nhttps://elpais.com/america/2024-04-28/brasil-abraza-con-fervor-el-reconocimiento-facial-en-medio-de-la-polemica-por-los-errores.html\nhttps://oglobo.globo.com/esportes/noticia/2024/04/15/torcedor-do-confianca-relata-ter-sido-conduzido-pela-pm-por-erro-de-reconhecimento-facial-veja-video.ghtml\nhttps://ge.globo.com/se/futebol/times/confianca/noticia/2024/04/21/torcedor-vitima-de-falha-em-sistema-de-reconhecimento-facial-visita-treino-do-confianca.ghtml\nhttps://infonet.com.br/noticias/cidade/torcedor-relata-abordagem-policial-constrangedora-durante-o-sergipao/\nRelated \ud83c\udf10\nRio de Janeiro Oi facial recognition\nSao Paulo METRO advertising facial recognition abuse\nPage info\nType: Incident\nPublished: May 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/thai-auntie-unable-to-buy-food-after-poor-card-facial-recognition-failure", "content": "Thai auntie unable to buy food after Poor Card facial recognition failure\nOccurred: March 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Thai woman trying to buy food on the country's welfare 'poor card' was rejected after a facial recognition system failed to recognise her. \nAccording to store owner Pannaphat Phromphitak, the 'auntie' had traveled many kilometres by motorcycle to buy food for her grandchildren using Thailand's state welfare card, only to be turned down by the system's facial recognition system. She was forced to return home empty-handed.\nThe incident raised questions about the accuracy and effectiveness of Thailand's poor card ThaiID digital identity verification system. Thailand launched ThaiID in 2019 and a facial recognition-enabled mobile app in July 2023.\nSystem \ud83e\udd16\nThaiID\nOperator: Ministry of Social Development and Human Security\nDeveloper:  \nCountry: Thailand\nSector: Retail\nPurpose: Verify identity\nTechnology: Facial recognition\nIssue: Accuracy/reliability; Human/civil rights\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.matichon.co.th/news-monitor/news_4487928\nRelated \ud83c\udf10\nMichael Oliver facial recognition wrongful arrest\nRobot crushes Thai factory worker to death\nPage info\nType: Incident\nPublished: May 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-researcher-claims-amazon-ignored-copyright-rules", "content": "AI researcher claims Amazon ignored copyright rules\nOccurred: April 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA former AI research executive at Amazon claimed the company breached its own copyright policies to train AI models.\nIn a lawsuit accusing her former employer of discrimination, retaliation, harassment, and wrongful termination, Dr Viviane Ghaderi alleged that Amazon breached copyright law and its own copyright policy in the race to stay competitive in the generative AI field. Ghaderi, who worked on Amazon\u2019s Alexa and Large Language Model (LLM) teams, said she was instructed to ignore these policies to enhance project outcomes. \nThe allegation raised questions about the legality of using copyrighted materials to train generative AI models, a practice increasingly contested with several high-profile lawsuits  filed against other technology companies. Commentators suggest this case not only challenges Amazon\u2019s treatment of Ghaderi but also its adherence to copyright law.\nSystem \ud83e\udd16\nUnknown\n\nOperator: \nDeveloper: Amazon\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Train large language models\nTechnology: Machine learning\nIssue: Copyright; Ethics/values\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theregister.com/2024/04/22/ghaderi_v_amazon/\nhttps://www.digit.fyi/ex-amazon-exec-alleges-copyright-breach-in-ai-development-race/\nhttps://www.androidpolice.com/lawsuit-alleges-amazon-skirted-copyright-law-to-speed-up-ai-development/\nhttps://www.notebookcheck.net/Amazon-AI-team-flouted-copyright-laws-to-win-arms-race-former-AI-lead-dev.831209.0.html\nhttps://www.hrgrapevine.com/us/content/article/2024-04-24-ex-amazon-ai-employee-claims-she-was-pressured-to-breach-internal-copyright-rules-to-avoid-falling-behind-rivals\nhttps://completemusicupdate.com/amazon-ai-employee-claims-she-was-told-to-breach-copyright-rules-because-everyone-else-is-doing-it/\n\nRelated \ud83c\udf10\nOpenAI scraped YouTube to train GPT-4 \nPage info\nType: Issue\nPublished: May 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/reddit-warns-ai-companies-not-to-misuse-its-data", "content": "Reddit warns AI companies not to misuse its data \nOccurred: April 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nReddit warned AI companies against using data from its platform for commercial purposes without permission.\nAccording to Reddit COO Jen Wong, AI firms should not scrape data on the company\u2019s platform use without consent. She went on to say that, given the AI industry's appetite for data to train its models, Reddit believes there is commercial value which can be unlocked through licensing. \nWong\u2019s intervention highlighted broader concerns about the use and misuse of data for commercial purposes, and raised ethical questions about the consent of platform users, who typically are unaware that their data is made available to third-parties on a commercial basis and is scraped by third-parties to train their models. \nThe AI industry is known to widely use data scraping to train its models. ChatGPT, Stable Diffusion, Midjourney and other products are subject to numerous class-action lawsuits relating to alleged data theft and copyright abuse. \nSystem \ud83e\udd16\nChatGPT \nMidjourney\nStable Diffusion\nOperator:\nDeveloper: OpenAI; StabilityAI; Midjourney\nCountry: Global\nSector: Media/entertainment/sports/arts\nPurpose: Multiple technologies using data scraping\nTechnology: Generative AI; Machine learning; Neural network; Deep learning; NLP/text analysis\nIssue: Copyright; Privacy\nTransparency: Governance \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.thetimes.co.uk/article/reddit-considers-legal-action-against-ai-firms-for-unauthorised-use-pmp6t0t3v\nhttps://www.proactiveinvestors.co.uk/companies/news/1046385/reddit-warns-of-legal-action-for-ai-firms-lifting-data-without-permission-1046385.html\nhttps://citywire.com/new-model-adviser/news/monday-papers-reddit-considers-legal-action-against-ai-firms/a2441371.\nhttps://thebestai.org/gpts/reddit-warns-ai-firms-legal-action-for-unauthorized-data-extraction-fresh-nyse-listing/\nhttps://www.cryptopolitan.com/reddit-may-sue-ai-firms-if-business-fail/ \nRelated \ud83c\udf10\nThe New York Times sues OpenAI, Microsoft over copyright abuse\n17 authors sue OpenAI for 'systematic mass-scale copyright infringement'\nAuthors Mona Awad, Paul Tremblay sue OpenAI for copyright abuse\nGetty Images Sues Stability AI for Copyright Abuse \nOpenAI scraped YouTube to train GPT-4\n\nPage info\nType: Issue\nPublished: May 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-generated-drama-performance-cancelled-over-plagiarism-accusations", "content": "AI-generated drama performance cancelled over plagiarism accusations\nOccurred: March 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn event in Tokyo at which voice actors were to perform an AI-generated romantic drama script was cancelled over plagiarism accusations.\nLol, the event organiser subcontracted creation of the script to a creator that explicitly used a number of AI tools, including the GPT-4 large language model, to automatically generate a script for a romantic drama. The stated concept behind the performance was to have voice actors perform the AI generated work \u201cregardless of any unnatural content or plot points\u201d.\nHowever, critics argued many of the AI tools used to generate the script had been trained on copyrighted works, including scripts, without permission. They also accused  Lol of unethically creating a commercial event without considering rights holders. \nThe event was cancelled four days before the premiere due to the backlash. The production company acknowledged their failure to explain their intentions sufficiently .\n\nSystem \ud83e\udd16\nChatGPT chatbot\nGPT-4 large language model\n\nOperator: Lol Production Company\nDeveloper: OpenAI\nCountry: Japan\nSector: Media/entertainment/sports/arts\nPurpose: Generate scripts\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Copyright\nTransparency: Governance \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://japannews.yomiuri.co.jp/society/general-news/20240428-182714/.\nhttps://www.itmedia.co.jp/news/articles/2403/11/news143.html#:~:text=%E8%88%9E%E5%8F%B0%E5%85%AC%E6%BC%94%E3%81%AE%E4%BC%81%E7%94%BB%E3%81%AA%E3%81%A9%E3%82%92%E6%89%8B%E6%8E%9B%E3%81%91%E3%82%8BLol%EF%BC%88%E6%9D%B1%E4%BA%AC%E9%83%BD,%E3%81%AE%E5%88%A4%E6%96%AD%E3%82%92%E4%B8%8B%E3%81%97%E3%81%9F%E3%80%82\nhttps://www.yomiuri.co.jp/culture/20240410-OYT1T50098/\n\nRelated \ud83c\udf10\nThe New York Times sues OpenAI, Microsoft over copyright abuse\n17 authors sue OpenAI for 'systematic mass-scale copyright infringement'\nAuthors Mona Awad, Paul Tremblay sue OpenAI for copyright abuse\nPage info\nType: Incident\nPublished: May 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/portugal-bans-worldcoin-for-90-days-for-jeopardising-citizen-privacy", "content": "Portugal bans Worldcoin for 90 days for jeopardising citizen privacy\nOccurred: March 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nPortugal\u2019s data regulator ordered iris-scanning project Worldcoin to halt the collection of biometric data for 90 days due to concerns over citizens\u2019 data protection rights.\nWorldcoin, a project that combines cryptocurrency and iris scan technology to create a global digital identity system, encourages people to have their faces scanned by its \u201corb\u201d devices in exchange for a digital ID and free cryptocurrency. \nPortugal's National Data Protection Commission (CNPD) received dozens of complaints about the unauthorised collection of data from minors, deficiencies in the information provided to the data subjects, and the impossibility of erasing the data or withdrawing consent. Over 300,000 people in Portugal have reportedly provided Worldcoin with their biometric data.\nWorldcoin\u2019s data protection officer, Jannick Preiwisch, said that Worldcoin was fully compliant with all laws and regulations governing the collection and transfer of biometric data. The company also mentioned that it began a transition to \u201cPersonal Custody\u201d in March, which would give users control over their data, including deletion and any future use.\nThe order to stop data collection is temporary while the CNPD carries out additional due diligence and analyses complaints during an investigation. This is not the first time Worldcoin has faced such a suspension - Spain having issued a similar ban.\nSystem \ud83e\udd16\nWorldcoin iris biometrics\nOperator: Tools for Humanity/Worldcoin\nDeveloper: Tools for Humanity/Worldcoin\nCountry: Portugal\nSector: Banking/financial services\nPurpose: Develop digital identity\nTechnology: Iris scanning; Facial detection; Vital signs detection; Blockchain; Virtual currency\nIssue: Privacy\nTransparency: Governance; Marketing\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nComiss\u00e3o Nacional de Prote\u00e7\u00e3o de Dados. CNPD suspende recolha de dados biom\u00e9tricos da Worldcoin (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://cointelegraph.com/news/portuguese-regulator-bans-worldcoin-90-days\nhttps://www.channelnewsasia.com/business/portugal-orders-sam-altmans-worldcoin-halt-data-collection-4222616\nhttps://www.infosecurity-magazine.com/news/portugal-worldcoin-stop-biometric/\nhttps://cryptocurrencynews.com/portugal-orders-worldcoin-to-cease-biometric-data-collection/\nRelated \ud83c\udf10\nSpain suspends Worldcoin over privacy concerns\nHong Kong privacy watchdog probes Worldcoin\nPage info\nType: Incident\nPublished: May 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/bbc-presenters-ai-generated-voice-used-to-trick-company", "content": "BBC presenter\u2019s AI-generated voice used to trick company\nOccurred: March 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA BBC presenter's image was used in an ad campaign without her consent after an AI deepfake of her voice was used by a scammer to negotiate the deal. \nInsect repellant company Incognito was tricked into believing it was speaking to BBC science presenter, Liz Bonnin, through a series of voice messages and a Facebook profile allegedly purporting Bonnin's identity. The deal was in fact negotiated between a scammer and Incognito CEO Howard Carter via email and Whatsapp voice messages. Incognito paid the scammer \u00a320,000, believing it to be Bonnin.\nAI experts analysed the voice messages and agreed the voice was likely to have been generated using AI given the inconsistency in accent and  the voice\u2019s cadence and monotony. \nThe incident was seen as  a warning about  potential misuse of AI voice replication technologies In addition, Incognito lost money and faced reputational damage. \nSystem \ud83e\udd16\nUnknown\n\nOperator: \nDeveloper:\nCountry: UK\nSector: Media/entertainment/sports/arts\nPurpose: Generate voice\nTechnology: Deepfake - audio\nIssue: Personality rights; Fraud\nTransparency: Governance \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theguardian.com/technology/2024/apr/28/bbc-presenters-likeness-used-in-advert-after-firm-tricked-by-ai-generated-voice\nhttps://www.irishtimes.com/culture/tv-radio/2024/04/30/like-a-violation-liz-bonnins-likeness-used-in-advert-after-firm-tricked-by-ai-generated-voice/#:~:text=Science%20presenter%20Liz%20Bonnin's,generated%20to%20mimic%20Bonnin's%20voice.\nhttps://deadline.com/2024/04/bbc-liz-bonnin-ai-scam-incognito-insect-repellent-1235898282/\nhttps://www.independent.co.uk/arts-entertainment/tv/news/bbc-presenter-ai-liz-bonnin-b2536517.html\nhttps://www.sundayworld.com/news/irish-news/criminals-target-irish-presenter-liz-bonnin-in-ai-scam-which-duped-23k-out-of-company-boss/a518243425.html\nRelated \ud83c\udf10\nMichel Janse deepfake used for advert without consent \nPage info\nType: Incident\nPublished: May 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/deepfake-john-swinney-thanks-nicola-sturgeon-for-his-election", "content": "Deepfake John Swinney addresses Scotland's parliament\nOccurred: May 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA video purportedly showing Scotland's new First Minister lambasting his party's former leader and its policies in parliament went viral on social media. \nThe video showed John Swinney, the new Scottish First Minister in what appeared to be a live Sky News broadcast, thanking Nicola Sturgeon for 'ensuring' his re-election and blasting his party's policies. It was seen over 300,000 times online, having been shared by right-wing social media accounts.\nHowever, Swinney's voice had been manipulated using AI in the video. He had been officially installed as Scottish First Minister after Humza Yousaf resigned, and was elected to the post in a vote at Scotland's parliament.\nThe incident showed the ease with which deepfake videos can be generated and distributed, and raised concerns about the potential impact of misinformation and disinformation on politics as a whole, as well as the individuals and entities being targeted.\nSystem \ud83e\udd16\nUnknown\n\nOperator: \nDeveloper:  \nCountry: UK - Scotland\nSector: Politics\nPurpose: Damage reputation\nTechnology: Deepfake - audio, video\nIssue: Mis/disinformation\nTransparency: Governnace; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.thenational.scot/news/24305685.ai-deepfake-video-john-swinney-sky-news-goes-viral-twitter/\nhttps://twitter.com/Fud_Mcbaws/status/1787921548080198065\nhttps://twitter.com/themajorityscot/status/1787989567959634276\nRelated \ud83c\udf10\nDeepfake audio depicts London Mayor dismissing Remembrance Sunday\nOpenAI bans bot impersonating US presidential candidate\nPage info\nType: Incident\nPublished: May 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/uk-police-found-to-use-pimeyes-raising-privacy-concerns", "content": "UK police use of PimEyes sparks privacy concerns \nOccurred: April 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nUK police officers recently accessed controversial facial recognition service PimEyes over 2,000 times, contrary to their own stated policies and procedures.\nAn iNews/Liberty investigation revealed that London's Metropolitan Police Service (MPF) computers accessed PimEyes 2,337 times in a recent three-month period. \nThe Metropolitan Police Service (MPF) responded by saying that the recorded instances of use may have been related to officers conducting research on the software. The MPF said it had strengthened safeguards and blocked access to the site on Met devices.\nThe incident highlighted the use of facial recognition by UK law enforcement authorities and the lack of transparency around its use. \nPrivacy campaigners fear that unrestricted facial recognition poses a significant threat to individual liberties, including the potential erosion of privacy caused by misuse of the technology, and concerns about the potential for stalking, harassment and the creation of a surveillance state.\nSystem \ud83e\udd16\nPimEyes facial recognition search engine\nOperator: Metropolitian Police Service (MPS)\nDeveloper: PimEyes\nCountry: UK\nSector: Govt - police\nPurpose: Identify criminal suspects\nTechnology: Facial recognition\nIssue: Privacy\nTransparency: Governance\nInvestigations, assessments, audits \ud83e\uddd0\niNews. Met Police officers accessed controversial facial recognition site 2,000 times\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.biometricupdate.com/202405/uk-police-have-used-pimeyes-facial-recognition-search-tool-over-2000-times\nhttps://findbiometrics.com/met-police-accessed-controversial-face-search-site-documents-reveal/\nRelated \ud83c\udf10\nFive plaintiffs sue PimEyes in Illinois for privacy violations\nCher Scarlett unable to have explicit photographs of her removed\nPage info\nType: Issue\nPublished: May 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ford-mustang-mach-e-crashes-into-honda-in-texas-kills-occupant", "content": "Ford Mustang Mach-E crashes into Honda in Texas, kills occupant\nOccurred: February 2023 \nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Ford Mustang Mach-E using BlueCruise struck the rear of a stationary Honda CR-V in Texas, USA, resulting in the death of the 56-year-old Honda driver.\nAccording to a preliminary report from the US National Highway Traffic Safety Administration (NTSB), the 44-year-old driver of the Mustang Mach-E had been traveling eastbound on Interstate 10 when he rear-ended a stationary Honda CRV. The 56-year-old Honda driver was killed in the crash.\n'Based on data obtained from the vehicle, the driver had been operating the vehicle in BlueCruise mode before the crash,' the NTSB said. BlueCruise monitors road markings, speed signs and evolving traffic conditions to control steering, acceleration, braking and lane positioning, and to maintain safe and consistent distances to vehicles ahead. \nThe incident raised concerns about the reliability and safety of BlueCruise.\nSystem \ud83e\udd16\nBlueCruise\nOperator:\nDeveloper: Ford\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Driver assistance system\nIssue: Safety\nTransparency: \nInvestigations, assessments, audits \ud83e\uddd0\nNTSB. Rear-End Collision Between a Sport Utility Vehicle Operating With Partial Driving Automation and a Stationary Sport Utility Vehicle\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://techcrunch.com/2024/04/11/ford-bluecruise-fatal-crash-investigation-stationary-suv/\nhttps://techcrunch.com/2024/04/29/ford-bluecruise-investigation-nhtsa-fatal-crashes/\nhttps://www.ntsb.gov/investigations/Pages/HWY24FH006.aspx\nhttps://www.wsj.com/business/autos/federal-regulators-probe-role-of-ford-assisted-driving-tech-in-fatal-crash-2b92dea0\nhttps://www.theguardian.com/business/2024/apr/11/ford-mustang-mach-e-automated-blue-cruise-system-fatal-texas-crash\nhttps://www.theverge.com/2024/4/12/24128349/ford-hands-free-bluecruise-fatal-crash-texas-ntsb\nRelated \ud83c\udf10\nFord Mustang Mach-E fatally crashes into two parked cars\nTesla Model X veers off highway into concrete barrier, kills driver\nPage info\nType: Incident\nPublished: May 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ford-mustang-mach-e-fatally-crashes-into-two-parked-cars", "content": "Ford Mustang Mach-E fatally crashes into two parked cars\nOccurred: March 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Ford Mustang Mach-E crashed into two parked vehicles near Philadelphia, resulting in the death of two drivers.\nAccording to the US National Highway Traffic Safety Administration (NHTSA), which launched an investigation into the crash, the Mustang sport utility vehicle hit a parked Toyota Prius and rammed it into a Hyundai Elantra. \nBoth drivers of the stationary cars were killed, with one reportedly outside their vehicle. The crash closed a section of the interstate I-95 highway for several hours. \nThe NHTSA said the Ford may have been operating on its BlueCruise partially automated driving system. Introduced in 2021 and classified as a level 2 autonomous system, BlueCruse monitors road markings, speed signs and evolving traffic conditions to control steering, acceleration, braking and lane positioning, and to maintain safe and consistent distances to vehicles ahead. \nThe incident raised concerns about the reliability and safety of BlueCruise.\nSystem \ud83e\udd16\nBlueCruise\nOperator:\nDeveloper: Ford\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Driver assistance system\nIssue: Safety\nTransparency: \nInvestigations, assessments, audits \ud83e\uddd0\nNTSB. Fatal crash involving an electric powered Sport Utility Vehicle\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://apnews.com/article/crash-ford-mustang-mach-e-automated-system-investigation-33d60654136781282f7ecadf7a6eb7da\nhttps://www.reuters.com/business/autos-transportation/us-ntsb-probes-fatal-ford-mach-e-crash-philadelphia-2024-04-10/\nhttps://www.autoblog.com/2024/04/10/ntsb-probes-fatal-ford-mach-e-crash-in-philadelphia/\nhttps://techcrunch.com/2024/04/29/ford-bluecruise-investigation-nhtsa-fatal-crashes/\nhttps://www.fleetnews.co.uk/news/ford-hands-free-driving-aid-investigated-after-fatal-crashes\nhttps://www.nbcphiladelphia.com/news/local/ford-blue-cruise-driving-system-deadly-crashes-philly/3844463/\nRelated \ud83c\udf10\nFord Mustang Mach-E crashes into Honda in Texas, kills occupant\nTesla driver using Autopilot kills motorcyclist\nPage info\nType: Incident\nPublished: May 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/walmart-sells-fake-chanel-ai-artwork-at-stores", "content": "Walmart sells fake Chanel AI artwork at stores\nOccurred: April 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nWalmart was accused of selling AI-generated artwork in its stores, including a painting of a fake Chanel perfume bottle. \nThe painting was posted by a Reddit user and found at a Walmart location selling for USD 22.15. The artwork featured a perfume bottle with the word \u2018CHANE\u2019 stamped in the middle, with the letter \u2018H\u2019 appearing to be doubled. Smaller gibberish letters could be seen underneath. The same piece of art was also on sale at Bed Bath & Beyond for USD 51.49.  \nThe misspellings were regarded as indicative of AI-generated art. The painting sparked discussions about the use of AI in generating art and its impact on creative jobs. \nSystem \ud83e\udd16\nUnknown\nOperator: DESIGNART ELEGANT FL; Bed Bath & Beyond; Walmart\nDeveloper:  \nCountry: USA\nSector: Retail\nPurpose: Generate images and text\nTechnology: Text-to-image\nIssue: Accuracy/reliability; Employment\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.dailymail.co.uk/news/article-13346359/walmart-ai-stores-shopper-bizarre-item-shelves.html\nhttps://www.dailydot.com/news/walmart-sells-ai-artwork-22-dollars/\nhttps://www.reddit.com/r/ChatGPT/comments/1c94e4u/walmart_selling_ai_images/\nRelated \ud83c\udf10\nWalmart AI anti-shoplifting system accuracy, effectiveness\nDesigners sue Shein for using AI to recreate their work\nPage info\nType: Incident\nPublished: May 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/cambridge-analytica-political-manipulation", "content": "Cambridge Analytica uses AI political manipulation to build Donald Trump support\nOccurred: 2016\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nDonald Trump's 2016 presidential campaign relied on data unethically harvested to build psychographic profiles of voters and manipulate their behaviour.\nTrump's campaign team used Google, Snapchat, Twitter, Facebook, and YouTube to micro-target US voters with carefully tailored messages about the Republican nominee across digital channels. Intensive survey research, data modelling, and performance-optimising algorithms were used to target 10,000 different ads to different audiences in the months leading up to the election.\nEarlier, Cambridge Analytica had harvested millions of Facebook profiles of US voters, in one of the tech giant\u2019s biggest ever data breaches, and used them to build a powerful software program to predict and influence choices at the ballot box.\nThe incident highlighted concerns about the manipulation of public opinion using data, AI and algorithms, and the privacy implications of the covert use of personal data for political purposes. It also raised questions about the connection between data-driven ads and democracy, and whether it is fundamentally toxic.\nSystem \ud83e\udd16\nCambridge Analytica\nOperator: \nDeveloper: Cambridge Analytica\nCountry: USA\nSector: Politics\nPurpose:  \nTechnology: Machine learning\nIssue: Ethics/values; Privacy; Mis/disinformation\nTransparency: Governance; Marketing\nResearch, advocacy \ud83e\uddee\nHu M. Cambridge Analytica's black box\nCouncil of Europe. Governing the Game Changer \u2013 Impacts of artificial intelligence development on human rights, democracy and the rule of law\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://en.wikipedia.org/wiki/Facebook%E2%80%93Cambridge_Analytica_data_scandal#Donald_Trump_campaign\nhttps://www.theguardian.com/uk-news/2018/mar/23/leaked-cambridge-analyticas-blueprint-for-trump-victory\nhttps://time.com/5197255/facebook-cambridge-analytica-donald-trump-ads-data/\nhttps://www.politico.eu/newsletter/ai-decoded/politico-ai-decoded-how-cambridge-analytica-used-ai-no-google-didnt-call-for-a-ban-on-face-recognition-restricting-ai-exports/\nhttps://www.wired.com/story/what-did-cambridge-analytica-really-do-for-trumps-campaign/\nhttps://www.oii.ox.ac.uk/news-events/how-data-and-artificial-intelligence-are-actually-transforming-american-elections\nhttps://www.technologyreview.com/2021/03/11/1020600/facebook-responsible-ai-misinformation/\nRelated \ud83c\udf10\nBiden 'robocall' advises voters skip New Hampshire primary election\nDeepfake audio recording depicts British Opposition Leader abusing staff\nPage info\nType: Incident\nPublished: May 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/2010-us-financial-markets-flash-crash", "content": "2010 US financial markets flash crash\nOccurred: May 2010\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA trader using a modified trading algorithm manipulated US financial markets, causing a trillion dollar crash and significant market volatility. \nUK trader Navinder Singh Saroa allegedly modified a common trading algorithm to generate large sell orders, pushing down prices, which he then canceled to buy at the lower market prices. \nThe impact was immediate and severe, with the overall market dropping by 6 percent, and approximately USD 1 trillion in paper stocks lost within 36 minutes. Hundreds of billions of dollars were wiped off the share prices of household name companies like Proctor & Gamble and General Electric, and caused a significant loss of confidence among investors, even if most stocks recovered within a few days.\nThe incident was seen to highlight the vulnerability of financial markets to sudden shocks, and the ease with which they could be manipulated using basic algorithms. The also incident raised questions about the role of algorithmic trading in modern markets, and led to increased scrutiny of algorithmic trading practices and their potential to destabilize markets. \nSystem \ud83e\udd16\n\nOperator: Navinder Singh Saroa\nDeveloper:  \nCountry: USA\nSector: Banking/financial services\nPurpose: Defraud\nTechnology: Trading algorithm; Machine learning\nIssue: Fraud\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nKirilenko A. et al. The Flash Crash: The Impact of High Frequency Trading on an Electronic Market (pdf)\nInvestigations, assessments, audits \ud83e\uddd0\n US Securities and Exchange Commission, Commodity Futures Trading Commission. Findings Regarding the Market Events of May 6, 2010 (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theguardian.com/business/2015/apr/22/2010-flash-crash-new-york-stock-exchange-unfolded\nhttps://blogs.lse.ac.uk/businessreview/2017/06/26/flash-crash-the-first-market-crash-in-the-era-of-algorithms-and-automated-trading/\nhttps://www.sifma.org/resources/research/10th-flash-crash-anniversary/\nhttps://www.bloomberg.com/opinion/articles/2015-04-21/guy-trading-at-home-caused-the-flash-crash\nhttps://theconversation.com/predicting-the-next-stock-market-flash-crash-114888\nhttps://www.npr.org/2015/04/25/402136396/who-or-what-crashed-the-market-in-a-flash-in-2010\nRelated \ud83c\udf10\nTerraUSD algorithmic stablecoin\nAutonomous AI bot lies about insider trading\nPage info\nType: Incident\nPublished: May 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chatgpt-said-to-violate-gdpr-by-not-correcting-inaccurate-personal-info", "content": "ChatGPT accused of violating GDPR by not correcting inaccurate personal info\nOccurred: April 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nOpenAI\u2019s AI model, ChatGPT, has been accused of violating the General Data Protection Regulation (GDPR) by not correcting inaccurate personal information. \nPrivacy group noyb (None of Your Business) filed a complaint against OpenAI triggered by ChatGPT\u2019s failure to supply the correct birthday of a public figure, instead making a wild guess.\nNoyb argued that this behaviour violates GDPR rules on privacy, the accuracy of information, and the right for individuals to correct inaccurate information. The group also claimed that OpenAI refused to correct or delete wrong answers, and would not disclose information about the data processed, its sources, or recipients.\nThe complaint further stated that ChatGPT\u2019s \u201challucinations\u201d or generation of false information about individuals can have serious consequences. It is argued that if a system cannot produce accurate and transparent results, it should not be used to generate data about individuals. Violating the EU\u2019s GDPR can lead to a penalty of up to 4 percent of a company\u2019s global revenue. \nNoyb has asked the Austrian privacy watchdog to investigate OpenAI to check on the accuracy of the personal data it handles. \nSystem \ud83e\udd16\nChatGPT chatbot\nOperator: OpenAI\nDeveloper: OpenAI\nCountry: Austria\nSector: Multiple\nPurpose: Generate text\nTechnology: Chatbot\nIssue: Privacy\nTransparency: Governance\nRegulation \u2696\ufe0f\nEU General Data Protection Regulation\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nNYOB. Complaint against OpenAI (pdf)\nNYOB. ChatGPT provides false information about people, and OpenAI can\u2019t correct it\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theregister.com/2024/04/29/openai_hit_by_gdpr_complaint/\nhttps://qz.com/openai-scrutiny-european-union-chatgpt-hallucinations-1851442350\nhttps://techcrunch.com/2024/04/28/chatgpt-gdpr-complaint-noyb/\nhttps://www.politico.eu/article/chatgpts-hallucinations-get-eu-privacy-complaint/\nRelated \ud83c\udf10\nPoland investigates ChatGPT for alleged privacy abuse\nChatGPT bug reveals user chat histories\nPage info\nType: Incident\nPublished: May 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/huawei-p70-ultra-ai-editing-tool-removes-peoples-clothing", "content": "Huawei P70 Ultra AI editing tool removes people's clothing\nOccurred: April 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn AI-based object removal feature on Huawei's Pura 70 Ultra mobile phone drew criticism for seemingly stripping people of their clothing.\nThis feature, known as the \u201cAI Photo Retouch\u201d feature, inadvertently erased parts of people\u2019s clothing in photos. The result was an illusion of bare skin, created by the AI using the skin color of the person being photographed.\nSeveral posts on the social media platform Weibo have highlighted this issue, showing examples where the object removal feature accidentally removes clothing, revealing what appears to be the parts of their bodies covered by the clothing.\nHuawei acknowledged the issue and said it would be fixed. However, concerns were raised that the feature could be used to create inappropriate images until the changes were implemented.\nThis incident raised privacy concerns about Huawei's technology, and more general ethical questions about the misuse of AI technology. \nSystem \ud83e\udd16\nHuawei Pure 70 Ultra\nOperator: \nDeveloper: Huawei\nCountry: China\nSector: Consumer goods\nPurpose: Remove objects\nTechnology: Object recognition\nIssue: Safety\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.gizmochina.com/2024/04/25/huawei-pura-70-ai-object-removal-raises-privacy-concerns/\nhttps://www.huaweicentral.com/pura-70-series-ai-image-editing-tool-reportedly-become-issue-for-users-huawei-responds/#google_vignette\nhttps://www.phonearena.com/news/ai-leaves-subjects-of-photos-appearing-naked-on-pura-70-ultra_id157783\nhttps://www.404media.co/instagram-advertises-nonconsensual-ai-nude-apps/\nRelated \ud83c\udf10\nAlmendralejo hit by AI-generated naked child images\nAI-generated nude pictures of Issaquah students circulate\nPage info\nType: Incident\nPublished: April 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/bing-falsely-accuses-aerospace-professor-of-being-a-terrorist", "content": "Bing falsely accuses aerospace professor of being a terrorist\nOccurred: July 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAerospace professor Jeffery Battle was falsely identified as a terrorist by Bing\u2019s search engine, leading him to sue the technology company for defamation. \nThe issue arose when Bing\u2019s AI-based summarisation feature conflated information about two individuals with similar names: Jeffery Battle, the aerospace professor, and Jeffrey Leon Battle, who was convicted of trying to join the Taliban.\nThe search results incorrectly combined the details of these two individuals, leading to the false impression that the professor had pleaded guilty to seditious conspiracy. This error was a result of the rule-based algorithm following the instructions provided to it, rather than a machine learning algorithm.\nIn his lawsuit, Battle alleged that he had informed Microsoft about the problem, but it was not promptly fixed. \nThe case highlights the potential risks and complexities involved in using AI algorithms for critical tasks.\nSystem \ud83e\udd16\nBing search engine\nOperator:\nDeveloper: Microsoft\nCountry: USA\nSector: Research/academia; Education\nPurpose: Generate text\nTechnology: Chatbot\nIssue: Accuracy/reliability; Defamation; Mis/disinformation; Liability\nTransparency: Governance; Black box\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nBattle v. Microsoft Corporation\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://arstechnica.com/tech-policy/2023/10/will-chatgpts-hallucinations-be-allowed-to-ruin-your-life/\nhttps://reason.com/volokh/2023/07/13/new-lawsuit-against-bing-based-on-allegedly-ai-hallucinated-libelous-statements/\nhttps://www.cjr.org/analysis/ai-sued-suit-defamation-libel-chatgpt-google-volokh.php\nhttps://www.ft.com/content/61008a05-1752-48bc-bf7a-6a4643c0cf27\nRelated \ud83c\udf10\nChatGPT falsely accuses Mark Walters of fraud, embezzlement\nGoogle Images links music promoter to criminal underworld\nPage info\nType: Incident\nPublished: April 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/stanford-hospital-covid-19-vaccine-allocation-triggers-backlash", "content": "Stanford hospital COVID-19 vaccine allocation algorithm triggers backlash\nOccurred: December 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn algorithmic COVID-19 vaccine distribution system developed and run by Stanford Medicine came under fire for prioritising certain staff members.\nThe algorithm was designed to consider three categories: employee-based variables (such as age), job-based variables, and guidelines from the California Department of Public Health. However, it resulted in an unequal distribution of vaccines, with only seven out of over 1,300 resident physicians being prioritised for the first 5,000 doses.\nMany of these residents worked on the front lines of the COVID-19 pandemic, and they were shocked and angered to see that administrators and doctors working remotely from home were included in the priority list. Only seven of Stanford\u2019s 1,300 patient-facing medical residents made the list. This led to a protest by at least 100 residents during a planned photo op to celebrate the first vaccination.\nStanford Medicine apologised and attributed the errors its 'very complex algorithm'. However, critics argued that the issue was not with the algorithm itself, but with the instructions given to it by humans. They pointed out that the algorithm was not powered by machine learning, but was rule-based, meaning that it simply acted upon a set of instructions provided by humans. \nThe incident highlighted the potential pitfalls and complexities involved in using algorithms for critical decision-making processes.\nSystem \ud83e\udd16\n\nOperator: Stanford Health Care; Stanford School of Medicine\nDeveloper: Stanford Health Care; Stanford School of Medicine\nCountry: USA\nSector: Health\nPurpose: Allocate vaccine beneficiaries\nTechnology: Rule-based algorithm\nIssue: Fairness\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nhttps://www.documentcloud.org/documents/20432343-stanford-letter\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.technologyreview.com/2020/12/21/1015303/stanford-vaccine-algorithm/\nhttps://www.npr.org/sections/coronavirus-live-updates/2020/12/18/948176807/stanford-apologizes-after-vaccine-allocation-leaves-out-nearly-all-medical-resid\nhttps://www.washingtonpost.com/health/2020/12/18/stanford-hospital-protest-covid-vaccine/\nhttps://www.stanforddaily.com/2020/12/18/stanford-medicine-passes-over-front-line-residents-fellows-in-initial-vaccine-allocation/\nhttps://www.nytimes.com/2020/12/18/world/covid-stanford-health-center-vaccine-protest.html\nhttps://thehill.com/changing-america/well-being/prevention-cures/531111-stanford-frontline-healthcare-workers-slam-chaos\nhttps://www.thedailybeast.com/fro-revolt-over-stanford-medical-centers-absurd-vaccine-rollout\nhttps://www.theverge.com/2020/12/20/22191749/stanford-medicine-covid-19-vaccine-distribution-list-algorithm-medical-residents\nhttps://www.statnews.com/2020/12/21/stanford-covid19-vaccine-algorithm/\nhttps://sanfrancisco.cbslocal.com/2020/12/30/stanford-covid-vaccine-non-frontline-staff-researchers-new-distribution-snafu/\nhttps://arstechnica.com/science/2020/12/stanford-hospital-erupts-in-protest-after-vaccine-plan-leaves-out-residents/\nhttps://venturebeat.com/2020/12/19/covid-19-vaccine-distribution-algorithms-may-cement-health-care-inequalities/\nRelated \ud83c\udf10\nMicrosoft Bing chatbot repeats ChatGPT COVID-19 conspiracy\nPoland COVID-19 Cultural Support Fund assessments\nPage info\nType: Incident\nPublished: April 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/grok-accuses-klay-thompson-of-brick-vandalism-spree", "content": "Grok AI wrongly accuses Klay Thompson of 'brick-vandalism spree'\nOccurred: April 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nElon Musk's Grok AI chatbot falsely accused NBA star Klay Thompson of criminal vandalism. \nIn a X (formerly Twitter) post titled \"Klay Thompson Accused in Bizarre Brick-Vandalism Spree,\" Grok turned a false story of Klay vandalising his house with bricks into a literal story of of Thompson vandalising houses with bricks.\nCommentators pointed out that Grok likely confused a common basketball term in which players are said to be throwing \"bricks\" when they take an airball shot that doesn't hit the rim. Earlier, basketball star Thompson had had an \"all-time rough shooting\" night, hitting none of his shots in his last game with the Golden State Warriors.\nIn small type under Grok's report, X includes a disclaimer saying, \"Grok is an early feature and can make mistakes. Verify its outputs.\"\nThe incident raised questions about the liability of chatbot makers when making false and defamatory statements. It also highloghted concerns about Grok's safety, including its perceived vulnerability to manipulation to produce misinformation and disinformation.\nSystem \ud83e\udd16\nGrok chatbot\nOperator: xAI Corp\nDeveloper: xAI Corp\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Summarise news articles\nTechnology: Chatbot\nIssue: Accuracy/reliability; Defamation; Mis/disinformation; Liability\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://ftw.usatoday.com/2024/04/klay-thompson-twitter-grok-ai-brick-vandalism-spree-meme\nhttps://www.sfgate.com/warriors/article/klay-thompson-accused-brick-vandalism-spree-ai-19407952.php\nhttps://arstechnica.com/tech-policy/2024/04/elon-musks-grok-keeps-making-up-fake-news-based-on-x-users-jokes/\nhttps://knowyourmeme.com/news/twitters-ai-powered-trending-tab-thinks-warriors-player-klay-thompson-went-on-literal-brick-vandalism-spree-after-poor-shooting-performance-last-night\nhttps://www.engadget.com/xs-ai-bot-is-so-dumb-it-cant-tell-the-difference-between-a-bad-game-and-vandalism-172707401.html\nhttps://www.techtimes.com/articles/303713/20240417/x-s-grok-ai-created-fake-news-klay-thompson-vandalism.htm\nRelated \ud83c\udf10\nGrok generates fake Iran missile attack headline\nChatGPT falsely accuses Mark Walters of fraud, embezzlement\nPage info\nType: Incident\nPublished: April 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/father-justin-ai-priest-defrocked-after-inappropriate-responses", "content": "Father Justin AI priest defrocked after inappropriate responses\nOccurred: April 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn AI priest was taken down after it after it repeatedly claimed it was a real member of the clergy. \nCatholic advocacy group Catholic Answers released 'Father Justin', a bearded and frocked AI chatbot intended to provide answers to Catholic-related questions faster than its human apologists could do. The organisation had to take down a question-and-answer feature on its website some years before as its staff apologists had been inundated with queries. \nHowever, the bot was riddled with technical issues and quickly attracted criticism, notably concerning it's representation of the AI character as a priest. Others found him creepy, expressed concerns that he would replace human beings, and found its answers inaccurate and, at times, bizarre. It told one user that it was appropriate to baptise a baby in Gatorade.\nThe bot was suspended, with Catholic Answers saying it would be replaced with a lay character called 'Justin'.\nSystem \ud83e\udd16\nJustin chatbot\n\nDocuments \ud83d\udcc3\nIntroducing Virtual Apologist Justin\nJust \u201cJustin\u201d for Now\nOperator: Catholic Answers\nDeveloper: Catholic Answers\nCountry: USA\nSector: Religion\nPurpose: Provide Catholic information\nTechnology: Chatbot\nIssue: Accuracy/reliability; Employment; Ethics/values; Robustness; Safety\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.ncregister.com/news/catholic-answers-ai-priest-cancelled\nhttps://crisismagazine.com/editors-desk/should-we-have-ai-doing-catholic-apologetics\nhttps://thedialog.org/national-news/father-justin-idea-has-merit-in-the-ai-realm-but-feedback-after-launch-compels-catholic-answers-to-lose-the-collar/\nhttps://futurism.com/catholics-defrock-ai-priest-hallucinations\nhttps://www.thesun.co.uk/tech/27551545/virtual-ai-priest-father-justin-catholic-answers/\nhttps://nypost.com/2024/04/26/us-news/ai-priest-defrocked-by-developer-after-taking-confessions-like-real-priest/\nhttps://www.ncronline.org/news/ai-priest-sparks-more-backlash-belief\nRelated \ud83c\udf10\nNissei Eco robot seen to undercut Buddhist priests\nEucharist delivery drone sparks religious controversy\nPage info\nType: Incident\nPublished: April 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/dc-comics-pulls-ai-generated-covers-after-backlash", "content": "DC Comics pulls AI-generated covers after backlash\nOccurred: April 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nDC Comics pulled three upcoming variant covers by artist Daxiong (Jingxiong Guo) following allegations that they were produced using generative artificial intelligence (AI).\nThe controversy began when people online accused Daxiong of using generative AI to create the covers. In response, DC stated that all artwork must be the artist\u2019s original work and that they were investigating the situation.\nDaxiong denied the allegations, stating that he has always drawn traditionally by hand. He also mentioned that an outside colouring studio coloured the work. However, he failed to provide the layered files of the colouring to prove it wasn\u2019t AI-generated, which critics had asked for. \nThe incident was seen to underscore the importance of transparency and authenticity in the comic book industry. \nSystem \ud83e\udd16\nUnknown\nOperator: Jingxiong Guo; DC Comics  \nDeveloper:  \nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Promote comics\nTechnology: Machine learning\nIssue: Ethics/values\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://bleedingcool.com/comics/accusations-over-dc-comics-artists-using-artificial-intelligence-mount-ai/\nhttps://bleedingcool.com/comics/dc-comics-pulls-and-replaces-covers-accused-of-being-generated-by-ai/\nhttps://bleedingcool.com/comics/dc-comics-addresses-ai-controversy-in-the-daily-litg-20th-april-2024/\nhttps://www.cbr.com/dc-pulls-covers-generative-ai-art-suspected/\nhttps://www.indy100.com/showbiz/batman-andrea-sorrentino-ai-art-2667509825\nhttps://www.cbr.com/dc-comics-batman-alleged-ai/\nRelated \ud83c\udf10\nAI generates visuals for Wizards of the Coast marketing promotion\nZarya of the Dawn AI images copyright ownership\nPage info\nType: Incident\nPublished: April 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/baltimore-high-school-athletic-director-uses-ai-to-smear-principal", "content": "Baltimore high school athletic director uses AI to smear principal\nOccurred: April 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nDazhon Darien, athletic director at Pikesville High School in Baltimore, was charged with using AI to create a fake audio recording appearing to show the school\u2019s principal, Eric Eiswert, making racist comments against Black and Jewish individuals.\nThe audio clip was first circulated in January 2024, leading to Eiswert being temporarily removed from his position. Eiswert maintained that the conversation in the audio never happened and suggested that Darien, who is technologically savvy, may have been involved.\nThe investigation revealed that Darien allegedly created the recording to retaliate against Eiswert, who had launched an investigation into Darien over allegations of misuse of school funds. \nThe FBI and an expert from the University of California, Berkeley, were able to link the simulated audio to an email account under the alias TJ Foust, which Darien allegedly used to send the phony recording to his own work email and other teachers at the school.\nThis incident underscores the potential misuse of AI and the serious repercussions it can have on individuals and communities.\nSystem \ud83e\udd16\nUnknown\nOperator: Dazhon Darien\nDeveloper:  \nCountry: USA\nSector: Education\nPurpose: Damage reputation\nTechnology: Machine learning\nIssue: Safety\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://eu.usatoday.com/story/news/nation/2024/04/25/baltimore-maryland-artificial-intelligence-teacher-deep-fake/73460123007/\nhttps://www.nytimes.com/2024/04/25/business/deepfake-recording-principal-arrest.html\nhttps://www.cbsnews.com/baltimore/news/maryland-framed-principal-racist-ai-generated-voice/\nhttps://www.dailymail.co.uk/news/article-13351569/Black-Baltimore-athletics-director-charged-fake-racist-rant.html\nhttps://www.thebaltimorebanner.com/education/k-12-schools/eric-eiswert-ai-deepfake-YUNO6ITYM5FWZPQAE24RIBV5CQ/\nhttps://www.nbcnews.com/news/us-news/teacher-arrested-ai-generated-racist-rant-maryland-school-principal-rcna149345\nhttps://www.theguardian.com/us-news/2024/apr/25/maryland-teacher-ai-principal\nRelated \ud83c\udf10\nCarmel school students attack Principal with racist deepfake video\nWestfield High School non-concensual nude deepfakes\nPage info\nType: Incident\nPublished: April 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/drake-threatened-with-lawsuit-over-ai-generated-tupac-shakur-voice", "content": "Drake threatened with lawsuit over AI-generated Tupac Shakur voice\nOccurred: April 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nCanadian rapper Drake was threatened with a lawsuit by fellow rapper Tupac Shakur\u2019s estate over a diss track that featured an AI-generated version of the late rapper\u2019s voice. \nThe track, titled \u201cTaylor Made Freestyle\u201d, was part of a back-and-forth war of words between Drake and Kendrick Lamar, in which Shakur could be heard criticising Tupac's legacy. \nIn a cease-and-desist letter to Drake demanding that he pull down the track within 24 hours or face legal action, Tupac\u2019s estate called this a \u201cflagrant violation\u201d of the law and a \u201cblatant abuse\u201d of Tupac\u2019s legacy. \nIt also argued that the song had caused \u201csubstantial economic and reputational harm\u201d by creating the \u201cfalse impression that the estate and Tupac promote or endorse the lyrics for the sound-alike.\u201d\nThe offending track was later removed from Drake's @Champagnepapi Instagram account.\nThis incident highlighted the ease with which the voices of artists can be replicated using AI, thereby violating owners' copyright. In 2023, Unknown artist  Ghostwriter released a track called \u201cHeart On My Sleeve\u201d that featured fake verses from Drake\u2019s voice. \nThe incident also highlighted legal concerns about how best legislate voice cloning. Cloned vocals usually feature new words and music that are distinct from existing copyrighted songs.\nSystem \ud83e\udd16\nUnknown\nOperator: Drake\nDeveloper:  \nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Damage reputation\nTechnology: Machine learning\nIssue: Copyright\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.billboard.com/pro/tupac-shakur-estate-drake-diss-track-ai-generated-voice/\nhttps://www.theguardian.com/music/2024/apr/25/estate-of-tupac-shakur-threatens-legal-action-against-drake-over-ai-diss-track#webview=1\nhttps://www.irishexaminer.com/lifestyle/celebrity/arid-41381914.html\nhttps://www.dailymail.co.uk/tvshowbiz/article-13347095/Drake-cease-desist-Tupacs-estate-AI-Kendrick-Lamar-diss-track.html\nhttps://www.nbcnews.com/pop-culture/pop-culture-news/tupac-shakur-estate-threatens-to-sue-drake-ai-use-dis-track-rcna149242\nhttps://www.standard.co.uk/culture/music/tupac-shakur-drake-kendrick-lamar-legal-taylor-made-freestyle-b1153881.html\nhttps://www.bbc.co.uk/news/newsbeat-68904385\nhttps://www.theverge.com/2024/4/24/24139442/tupac-ai-lawsuit-drake-taylor-made-diss-track\nhttps://www.theverge.com/2024/4/26/24141595/drake-taylor-made-ai-tupac-takedown\nRelated \ud83c\udf10\nDrake, The Weeknd voiced closed using AI\nAI Stefanie Sun\nPage info\nType: Incident\nPublished: April 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/snapchat-ai-chatbot-provides-bad-advice-about-underage-drinking", "content": "Snapchat AI chatbot provides bad advice about underage drinking\nOccurred: March 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nSnapchat\u2019s My AI chatbot was criticised for providing advice about drinking and illegal substance abuse to underage users, raising concerns about the bot's safety.\nThe Washington Post conducted a test where they changed their Snapchat profile information to that of a 15-year-old and asked about underage drinking. The AI responded positively, sharing ways to hide the smell of alcohol and pot.\nIn a similar test, the AI did not provide such advice when the user\u2019s age was set to 15, indicating the issue had been fixed. Despite these improvements, concerns remained about the AI\u2019s ability to handle sensitive topics appropriately, especially considering Snapchat\u2019s large teen user base. \nIn October 2023, the UK\u2019s data watchdog, the Information Commissioner\u2019s Office (ICO) expressed concerns about the potential privacy risks posed by the AI to users, particularly those aged 13 to 17.\nSystem \ud83e\udd16\nSnapchat My AI chatbot\nOperator: Snap Inc\nDeveloper: Snap Inc\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Interact; Provide information; Support users\nTechnology: Chatbot\nIssue: Safety\nTransparency: Governance; Black box; Privacy; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.washingtonpost.com/technology/2023/03/14/snapchat-myai/\nhttps://www.makeuseof.com/snapchat-my-ai-ethical-security-issues/\nhttps://www.cnbc.com/video/2023/03/14/snachat-chatbot-offers-inappropriate-advice-for-a-minor.html\nhttps://www.fastcompany.com/90865731/snapchat-ai-could-be-creepiest-chatbot-yet\nRelated \ud83c\udf10\nSnapchat My AI gives sex advice to 13-year-old\nUK privacy watchdog accuses Snapchat of failing to assess My AI privacy risks\nPage info\nType: Incident\nPublished: April 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tesla-driver-using-autopilot-kills-motorcyclist-intrusive-ai-speed-camera", "content": "Tesla driver using Autopilot kills motorcyclist\nOccurred: April 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Tesla driver was arrested for 'vehicular homicide' after running over a motorcyclist in Washington State, USA, while using the his car's Autopilot driver assist feature. \nThe unnamed 56-year-old man\u2019s 2022 Tesla Model S collided with a motorcycle while driving in Snohomish County, Washington State. The motorcyclist, 28-year-old Jeffrey Nissen, was pronounced dead at the scene having been trapped under the car. \nAccording to the affidavit, the driver had been returning home from lunch and was using Autopilot while looking at his phone when he heard a loud noise and the car lurched forwards, before accelerating and colliding with Nissen\u2019s motorcycle. Authorities said they have not yet independently verified whether Autopilot was in use at the time of the crash.\nThe incident prompted further concerns about the safety and marketing of Tesla's Autopilot system. Tesla says drivers should remain attentive whilst using Autopilot, but they regularly appear to put too much trust in the technology, perhaps persuaded by the car maker's perceived hyping of the feature.\nSystem \ud83e\udd16\nTesla Autopilot\nOperator: \nDeveloper: Tesla\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Driver assistance system\nIssue: Safety\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://nypost.com/2024/04/24/us-news/tesla-driver-using-autopilot-mode-charged-with-vehicular-homicide-in-death-of-motorcyclist/\nhttps://www.fox13seattle.com/news/tesla-motorcycle-crash-snohomish-county\nhttps://www.driving.co.uk/news/tesla-driver-arrested-on-homicide-charges-after-killing-motorcyclist-while-using-autopilot/\nhttps://www.opb.org/article/2024/04/24/tesla-driver-in-seattle-area-crash-that-killed-motorcyclist-told-police-he-was-using-autopilot/\nhttps://electrek.co/2024/04/23/tesla-driver-arrested-homicide-running-over-motorcyclist-autopilot/\nRelated \ud83c\udf10\nTesla Model Y rear-ends Yamaha motorcycle, kills rider\nTesla rear-ends Kawasaki motorcycle, kills rider\nPage info\nType: Incident\nPublished: April 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/intrusive-ai-speed-cameras-criticised-by-uk-motorists", "content": "'Intrusive' AI speed cameras criticised by UK motorists\nOccurred: April 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nDespite possible road safety benefits, motorists believe an AI speed camera system being rolled out across the UK constitutes an 'invasion of privacy'. \n10 police forces across the UK are testing the system, which is installed in vans and can detect whether road users are using their phones behind the wheel or travelling without a seatbelt. Data and images is sent to police officers who decide whether or not to take action.\nHowever, a Confused.com poll found that 21 percent of motorists are wary of the tool and reckon it is an invasion of privacy, despite admitting it would help make the road safer. \nDrivers stand to be fined up to GBP 2,500 if caught breaking the law in their vehicles. \nSystem \ud83e\udd16\nAcusensus\nOperator: AECOM; National Highways\nDeveloper: Acusensus\nCountry: UK\nSector: Govt - transport; Govt - police\nPurpose: Detect driver violations\nTechnology: Computer vision; Object recognition\nIssue: Privacy\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.express.co.uk/life-style/cars/1890936/ai-speed-camera-technology-driving-law\nhttps://uk.news.yahoo.com/drivers-blast-ai-speed-cameras-142754307.html\nhttps://www.gbnews.com/lifestyle/cars/drivers-rage-new-artificial-intelligence-cameras-not-fit-for-uk-roads-they-invade-privacy\nhttps://cybernews.com/privacy/uk-ai-cameras-seatbelt-mobile-phone-use-privacy-concerns/\nRelated \ud83c\udf10\nSpeedcam Anywhere criticised for violating privacy\nSleeping driver speeds on highway with Autopilot switched on\nPage info\nType: Issue\nPublished: April 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/netflix-documentary-uses-ai-to-manipulate-true-crime-story", "content": "Netflix documentary uses AI to manipulate true crime story\nOccurred: April 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nNetflix was accused of distorting the historical record and poor transparency by failing to disclose the use of AI in images used to promote its true crime documentary What Jennifer Did.\nThe documentary depicts Jennifer Pan, a woman currently imprisoned in Canada for paying two hitmen to murder her parents. Debuting early April 2024, the doc quickly shot to the top spot in Netflix's global top 10.\nHowever, fans pointed out glaring flaws in images used in the movie, from weirdly mismatched earrings to her nose appearing to lack nostrils - classic signs of AI. Critics called out the documentary filmmakers for potentially embellishing a movie that's supposed to be based on real-life events, and for not disclosing the use of AI.\nThe accusation was disputed by the documentary's producer Jeremy Grimaldi, who said that all images of Pan used in the movie were real photos and that some had been edited to protect the identity of the source of the images.\nThe incident highlighted ongoing concerns about the use of AI in the entertainment industry, including when it is used to distort reality. Commentators fear Netflix could set a dangerous precedent.\nSystem \ud83e\udd16\nUnknown\nOperator: Netflix\nDeveloper:  \nCountry: Canada; USA\nSector: Media/entertainment/sports/arts\nPurpose: Manipulate image\nTechnology: Machine learning\nIssue: Ethics/values\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.404media.co/netflix-doc-what-jennifer-did-uses-ai-images-to-create-false-historical-record/\nhttps://futurism.com/netflix-true-crime-producer-ai\nhttps://www.independent.co.uk/arts-entertainment/tv/news/netflix-what-jennifer-did-ai-image-true-story-b2531363.html\nhttps://www.dailymail.co.uk/news/article-13328073/Netflix-Jennifer-Did-AI-images-killer-Jennifer-Pan.html\nhttps://www.creativebloq.com/news/netflix-ai-art-what-jennifer-did\nhttps://www.theverge.com/2024/4/18/24134328/ai-true-crime\nhttps://arstechnica.com/tech-policy/2024/04/netflix-doc-accused-of-using-ai-to-manipulate-true-crime-story/\nhttps://www.thestar.com/news/gta/what-jennifer-did-executive-producer-denies-photo-used-in-netflix-crime-doc-was-ai-generated/article_bbc4c6a6-fdc4-11ee-bdc5-f32a5a1661e6.html\nhttps://www.cosmopolitan.com/uk/reports/a60546743/what-jennifer-did-ai-photos/\nRelated \ud83c\udf10\nFilm studio use of AI to promote Civil War backfires\nLate Night with the Devil AI interstitials provoke backlash\nPage info\nType: Issue\nPublished: April 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/who-chatbot-provides-inaccurate-health-information", "content": "WHO chatbot provides inaccurate health information\nOccurred: April 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA new World Health Organisation chatbot got off to an inauspicious start by getting basic health facts wrong, according to a media investigation. \nSARAH (or Smart AI Resource Assistant for Health) was trained on OpenAI's ChatGPT 3.5 large language model to provide information across major health topics, including mental health, tobacco and nutrition', in eight languages. \nHowever, the bot fails to provide up-to-date information on US-based medical advisories and news events, according to Bloomberg. In one instance, SARAH replied to a prompt by Bloomberg journalists by saying Lecanemab, an Alzheimer\u2019s drug, was still in clinical trails when in reality the US Food and Drug Administration had approved the drug in 2023. \nThe incident raised concerns about the prospect of a United Nations agency providing health and mental health disinformationThe UN agency acknowledged on the chatbot landing page that 'answers may not always be accurate because they are based on patterns and probabilities in the available data.'\nSystem \ud83e\udd16\nSarah chatbot\n\nDocuments \ud83d\udcc3\nWHO (2024). Meet S.A.R.A.H. - A Smart AI Resource Assistant for Health\nWHO (2024). WHO unveils a digital health promoter harnessing generative AI for public health\nOperator: World Health Organisation (WHO)\nDeveloper: Soul Machines\nCountry: Global\nSector: Health\nPurpose: Provide health information\nTechnology: Chatbot; Facial recognition\nIssue: Accuracy/reliability; Mis/disinformation; Safety\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bloomberg.com/news/articles/2024-04-18/who-s-new-ai-health-chatbot-sarah-gets-many-medical-questions-wrong\nhttps://voiceofhealthcare.org/updates/WHO'S-AI-POWERED-CHATBOT-SARAH-GIVING-INACCURATE-MEDICAL-RESPONSES~q8-IJGQqNCzj-xFnTyJLS\nhttps://cryptorank.io/news/feed/5f304-who-introduces-ai-chatbot-sarah-flaws\nhttps://qz.com/who-sarah-ai-bot-1851419782\nRelated \ud83c\udf10\nGPT-3 advises patient to kill themselves\nPerth doctors warned for using ChatGPT to write patient medical records\nPage info\nType: Incident\nPublished: April 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/michel-janse-deepfake-used-for-advert-without-consent", "content": "Michel Janse deepfake used for advert without consent\nOccurred: March 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nMichel Janse, a Christian social media influencer, had her face likeness used in a YouTube advert without her consent. \nThe ad featured Janse\u2019s face \u201cin her bedroom, wearing her clothes\u201d to sell erectile dysfunction pills. Janse Experts speculated the advertisement had been generated by an AI trained on Janse\u2019s regular posts on travel, home decor and wedding planning.\nJanse complained to YouTube, which took the advert down. \nThe incident highlights how AI technologies are being used to clone people\u2019s likeness and use them in digital ads. It also underscores the potential harm and violation of personal rights that can occur when personal images are used to train AI systems without consent. \nSystem \ud83e\udd16\nUnknown\nOperator: YouTube\nDeveloper:\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Generate video\nTechnology: Deepfake - video\nIssue: Personality rights; Mis/disinformation; Privacy\nTransparency: Governance \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.tiktok.com/@michel.c.janse/video/7343855927323266346?lang=en\nhttps://www.washingtonpost.com/technology/2024/03/28/ai-women-clone-ads/ \nhttps://www.dailydot.com/news/womans-likeness-stolen-by-ai-deepfake/\nhttps://www.indy100.com/tiktok/ai-deepfake-ad-tiktok\nhttps://www.unilad.com/community/ai-artificial-intelligence-steals-identity-advert-692192-20240417\nhttps://www.distractify.com/p/company-used-womans-ai-likeness-commercial\nRelated \ud83c\udf10\nScarlett Johansson sues app for using image for AI advert\nVW Brazil Elis Regina deepfake advert \n\nPage info\nType: Incident\nPublished: April 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/openais-gpt-store-faces-copyright-complaints", "content": "OpenAI's GPT store faces copyright complaints\nOccurred: April 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nOpenAI\u2019s GPT Store for developing and sharing custom chatbots has been hit with copyright complaints from a Danish textbook publisher.\nOpenAI\u2019s GPT Store enables developers to build and share their own custom chatbots (or 'GPTs'), by uploading extra data to train custom bots for bespoke use cases. \nBlichfeldt Andersen, Publishing Director at the publisher, complained to WIRED that the extra data uploaded by third party developers to the GPT platform often contains copyrighted material, which he has reported to OpenAI. Andersen complained the process for identifying and removing violative bots is overly burdensome and says without improvements his company is  considering legal action. \nThe GPT store is said to drive considerable copyright risk given its usage by a range of third-party developers, some of whom with less sophisticated understanding of copyright law. It is argued OpenAI should implement additional copyright protection features into GPT-store to combat potential violations before custom chatbots are released.\nSystem \ud83e\udd16 \nGPT Store \nOperator: OpenAI\nDeveloper: OpenAI\nCountry: Denmark\nSector: Media/entertainment/sports/arts\nPurpose: Build chatbots to generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Copyright\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.wired.com/story/openai-gpt-store-triggering-copyright-complaints/ \nRelated \ud83c\udf10\nThe New York Times sues OpenAI, Microsoft over copyright abuse\n17 authors sue OpenAI for 'systematic mass-scale copyright infringement'\nAuthors Mona Awad, Paul Tremblay sue OpenAI for copyright abuse\nPage info\nType: Incident\nPublished: April 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/film-studio-use-of-ai-to-promote-civil-war-backfires", "content": "Film studio use of AI to promote Civil War backfires\nOccurred: April 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe use of AI by film studio A24 to promote its film Civil War caused controversy and met with widespread criticism.\nThe film follows a team of journalists as they travel through a war-torn East Coast to reach Washington DC, where they hope to interview the authoritarian president before rebels descend on the White House.\nFilm studio A24 posted six images to its official Instagram page, each depicting apocalyptic scenes in major US cities. However, the images were found to contain geographical and other errors, leading people to accuse the company of using AI and for failing to accurately depict the film. \nA24 was also accused of using AI to replace human jobs.\nThe incident sparked concerns about AI\u2019s role in entertainment, notably its impact on jobs, and the need for transparency in the use of the technology. Media professionals said the use of AI was 'a huge misstep for [A24's] reputation'.\nSystem \ud83e\udd16\nUnknown\nOperator: A24\nDeveloper:  \nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Promote film\nTechnology: Machine learning\nIssue: Accuracy/reliability; Employment; Ethics/values\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.thedrum.com/news/2024/04/19/marketing-ai-pros-deem-a24-s-ai-generated-civil-war-promo-images-repulsive-insulting\nhttps://news.artnet.com/art-world/a24-civil-war-ai-posters-2472425\nhttps://gizmodo.com/civil-war-movie-a24-ai-posters-controversy-alex-garland-1851419329\nhttps://mashable.com/article/civil-war-ai-posters\nhttps://www.indiewire.com/news/general-news/a24-ai-civil-war-posters-1234975526/\nhttps://www.hollywoodreporter.com/movies/movie-news/a24-civil-war-posters-controversy-1235876340/\nhttps://www.cbr.com/a24-civil-war-controversial-ai-promos/\nhttps://futurism.com/the-byte/a24-ads-civil-war-ai-images\nRelated \ud83c\udf10\nLate Night with the Devil AI interstitials provoke backlash\nDC Comics pulls AI-generated covers after backlash\nPage info\nType: Incident\nPublished: April 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/maori-woman-misidentified-by-foodstuffs-facial-recognition", "content": "Maori woman misidentified by Foodstuffs facial recognition system\nOccurred: April 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Maori woman was misidentified by a facial recognition system at a supermarket in New Zealand, accused of being a shoplifter and thrown out. \nTe Ani Solomon was misidentified as a trespassed 'thief' by the AI system at a Foodstuffs supermarket in Rotorua, New Zealand, and accosted by staff who accused her of being a shoplifter and insisted she leave, even after she had offered three forms of photo identification. \nSolomon said she saw an image on a phone they had been looking at that appeared to be of a M\u0101ori woman wearing a cap, and that she she felt 'racially discriminated' against and embarrassed during the 'horrible' incident, which took place on her birthday.\nFoodstuffs was conducting a six-month trial of a facial recognition system in 25 of its stores in an attempt to reduce shoplifting. A spokesperson said the company regretted the incident and that it had been caused by 'genuine human error.'\nThe incident prompted concerns about the inaccuracy and racial bias inherent in facial recognition systems. The incident also spurred critics to point out the 'highly intrusive' nature of the technology, with shoppers seen to have no choice but to 'give up their data, whether they like it or not.'\nSystem \ud83e\udd16\nUnknown\nOperator: Foodstuffs\nDeveloper:  \nCountry: New Zealand\nSector: Retail\nPurpose: Strengthen security\nTechnology: Facial recognition\nIssue: Accuracy/reliability; Bias/discrimination - race, ethnicity; Privacy\nTransparency: Governance\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nOffice of the Privacy Commissioner. Inquiry into Foodstuffs North island's FRT trial starts today\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.nzherald.co.nz/rotorua-daily-post/news/supermarket-facial-recognition-trial-rotorua-mothers-discrimination-ordeal/IK4ZEJHLQVFRLMDE6LX4AR57PE/\nhttps://www.rnz.co.nz/news/te-manu-korihi/514155/supermarket-facial-recognition-trial-rotorua-mother-s-discrimination-ordeal\nhttps://www.rnz.co.nz/news/national/514523/maori-woman-mistaken-as-thief-by-supermarket-ai-not-surprising-experts-say\nhttps://iapp.org/news/a/nz-retailers-facial-recognition-program-allegedly-misidentified-a-customer-as-a-shoplifter/\nhttps://iapp.org/news/a/nz-privacy-commissioner-to-investigate-retailers-facial-recognition-program/\nhttps://www.newstalkzb.co.nz/on-air/heather-du-plessis-allan-drive/audio/michael-webster-privacy-commissioner-on-the-launch-of-an-inquiry-into-foodstuffs-facial-recognition-technology/\nRelated \ud83c\udf10\nNew Zealand student passport application denied by 'racist' AI photo checker\nBlack teenager misidentified, barred by Livonia skating rink AI system\nPage info\nType: Incident\nPublished: April 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/texas-exam-scoring-engine-criticised-for-dumbing-down-students", "content": "Texas exam scoring engine criticised for dumbing down students\nOccurred: March 2024-\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe introduction of an AI-powered system to grade Texas' STAAR exams met with criticism and concerns from educators and teachers for its opaqueness and potential inaccuracy and bias, loss of student creativity, and impact on employment.\nHaving re-designed its State of Texas Assessment of Academic Readiness (STAAR) exams in 2023, the Texas Education Agency (TEA) decided to introduce an 'automated scoring engine' to improve scoring efficiency. \nThe new system uses an AI similar to OpenAI's GPT-4 large language model to grade written answers, particularly for open-ended questions, and is expected to save the state USD 15-20 million per year that would otherwise have been spent on hiring human scorers. \nHowever, the new system sparked criticism and confusion among some educators and education leaders, who argue that the system lacks transparency and may contain potential biases. The impact on the jobs of exam assessors was also highlighted. \nTEA officials emphasised that the automated scoring engine is overseen by humans and that a quarter of the responses will be rescored by humans.\nSystem \ud83e\udd16\nTexas Education Agency (2024). Hybrid Scoring Key Questions (pdf)\nTexas Education Agency (2024). Scoring Process for STAAR Constructed Responses (pdf)\nOperator: Texas Education Agency  \nDeveloper:  \nCountry: USA\nSector: Education\nPurpose: Grade exam responses\nTechnology: Machine learning\nIssue: Accuracy/reliability; Bias/discrimination; Employment\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.texastribune.org/2024/04/09/staar-artificial-intelligence-computer-grading-texas/\nhttps://www.theverge.com/2024/4/10/24126206/texas-staar-exam-graders-ai-automated-scoring-engine\nhttps://www.govtech.com/education/k-12/details-emerge-on-automated-grading-of-texas-staar-tests\nhttps://abc7amarillo.com/news/local/new-automated-grading-system-for-texas-staar-test-stirs-concerns-over-creativity-san-antonio-texas\nhttps://www.houstonpublicmedia.org/articles/education/2024/03/07/480059/tea-gives-more-information-on-staar-essay-machine-scoring-after-spike-in-zeros-concerns-from-school-leaders/\nRelated \ud83c\udf10\nFlawed AI algorithms grade student essays in multiple US states\nOfqal algorithm skews student grade predictions\nPage info\nType: Issue\nPublished: April 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/mep-files-lawsuit-to-release-iborderctrl-lie-detection-system-documents", "content": "MEP files lawsuit to release iBorderCtrl lie detection system documents\nOccurred: February 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nMember of the European Parliament Patrick Breyer took the European Commission to court to find out how an AI-powered EU-funded lie detection system was developed and tested.\nBreyer had initially requested ethical assessments and other information from the EU Research Agency (REA) on the lie detection module of iBorderCtrl following a 2019 investigation by The Intercept which found that the technology was unreliable, having incorrectly identified four out of sixteen honest answers put to it as false. \nBreyer also pressed the Commission on whether the technology discriminates against certain groups of people, including people of colour, women, the elderly, children, and people with disabilities.\nThe MEP was refused access to the information requested on the grounds that releasing them could undermine public security and jeopardise 'commercial interests'. Breyer sued the European Commission to gain access to the documents, and to information on controversial trials conducted for the technology.\n\u2795 December 2021. The EU\u2019s Court of Justice (CJEU) ruled that the REA could not keep these documents completely secret, and that the ethical and legal evaluations of technologies for 'automated deception detection' or automated 'risk assessment' must be published, as long as they did not relate specifically to the iBorderCtrl project.\nHowever, it also ruled that the examination of the ethical risks such as the risk of stigmatisation and false positives and the legal admissibility of the concrete iBorderCtrl technology, and reports on the results of the pilot project, should be kept secret in order to protect commercial interests.\n\u2795 September 2023. The CJEU responded to a 2022 appeal lodged by Breyer that \u2018the public interest in disclosure outweighs private commercial interests\u2019 and that there should be transparency from the beginning of the research phase, by ruling that the commercial interests of the REA outweighed the public interest. \nThe ruling prompted concerns about the extent to which commercial interests should be used to restrict access to information about surveillance technologies funded by taxpayers.\nSystem \ud83e\udd16\niBorderCtrl smart lie detection system\nOperator: \nDeveloper: European Dynamics; Manchester Metropolitan University\nCountry: European Union\nSector: Govt - immigration\nPurpose: Detect traveller lies\nTechnology: Behavioural analysis; Facial recognition; Emotion recognition\nIssue: Accuracy/reliability; Bias/discrimination - race, ethnicity, gender, age, disability; Human/civil rights\nTransparency: Governance; Legal\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nBreyer v European Research Executive Agency (judgement)\nBreyer v Commission\nResearch, advocacy \ud83e\uddee\nPatrick Breyer MEP (2023). Breyer\u2019s lawsuit forces EU to publish secret AI surveillance research\nPatrick Breyer MEP (2022). Breyer appeals court ruling on secretive EU AI \u201evideo lie detector\u201c research\nPatrick Breyer MEP (2021). Briefing: Transparency complaint against secret EU surveillance research \u201ciBorderCtrl\u201d\nPatrick Breyer MEP (2021). Transparency lawsuit against secret EU surveillance research: MEP Patrick Breyer achieves partial success in court\nArticle 19 (2021). EU: Risky biometric technology projects must be transparent from the start\nEDRi (2021). European court supports transparency in risky EU border tech experiments\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.biometricupdate.com/202012/tone-deaf-ai-advocates-need-a-transparency-algorithm\nhttps://techcrunch.com/2021/02/05/orwellian-ai-lie-detector-project-challenged-in-eu-court/\nhttps://www.euractiv.com/section/digital/news/mep-public-has-a-right-to-know-about-commissions-lie-detector-tech/\nhttps://ai-regulation.com/eu-iborderctrl-when-commercial-interests-outweigh-the-public-interest/\nhttps://www.statewatch.org/news/2021/february/eu-secrecy-of-border-control-lie-detector-research-project-examined-in-court/\nhttps://www.techdirt.com/2023/09/21/eu-funded-automated-deception-detection-border-security-project-concludes-but-public-arent-allowed-to-see-research-details/\nhttps://www.theguardian.com/world/2020/dec/10/sci-fi-surveillance-europes-secretive-push-into-biometric-technology\nRelated \ud83c\udf10\nGreece fined for AI-powered asylum centre monitoring system\nNetherlands visa applicant over-stay risk assessments\nPage info\nType: Incident\nPublished: April 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/iborderctrl-video-lie-detector-is-branded-hogwash", "content": "iBorderCtrl video lie detector is branded 'hogwash'\nOccurred: November 2018\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe pilot of an EU-funded system that uses AI to detect lies at international airports was dismissed as 'Orwellian' pseudoscience.\nIn November 2018, CNN revealed that the EU was testing iBorderCtrl, a programme that featured a machine learning-based video 'lie detector module,' in Hungary, Greece and Latvia. The detector used an avatar of a border guard to ask people 13 questions about their personal backgrounds and travel plans, assessed travelers' micro-expressions, and notified human agents if it suspected them of being dishonest.\nHowever, experts criticised the tool as pseudoscientific and likely to lead to unfair outcomes based on the inaccuracy and bias associated with many lie detection and facial recognition systems. iBorderCtrl was also described by privacy advocates as a 'dystopian' data collection vehicle, and an 'Orwellian nightmare'.\nIn July 2019, The Intercept found that the technology made several errors, incorrectly identifying four out of sixteen honest answers as false. \nSystem \ud83e\udd16\niBorderCtrl video lie detector\nOperator: \nDeveloper: European Dynamics\nCountry: European Union\nSector: Govt - immigration\nPurpose: Detect traveller lies\nTechnology: Behavioural analysis; Emotion recognition; Facial recognition\nIssue: Accuracy/reliability; Human/civil rights\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://thenextweb.com/news/the-eus-border-control-lie-detector-ai-is-hogwash\nhttps://edition.cnn.com/travel/article/ai-lie-detector-eu-airports-scli-intl/index.html\nhttps://www.euractiv.com/section/digital/opinion/the-eu-is-funding-dystopian-artificial-intelligence-projects/\nhttps://www.theguardian.com/world/2018/nov/02/eu-border-lie-detection-system-criticised-as-pseudoscience\nhttps://theintercept.com/2019/07/26/europe-border-control-ai-lie-detector/\nhttps://www.technologyreview.com/2020/03/13/905323/ai-lie-detectors-polygraph-silent-talker-iborderctrl-converus-neuroid/\nRelated \ud83c\udf10\nGreece fined for AI-powered asylum centre monitoring system\nNetherlands visa applicant over-stay risk assessments\nPage info\nType: Incident\nPublished: April 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chatbots-misinform-citizens-about-european-parliament-elections", "content": "Chatbots misinform citizens about European Parliament elections\nOccurred: March 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nPopular chatbots are spreading falsehoods about the European Parliament elections, according to new research.\nChatGPT, Microsoft's Copilot and Google's Gemini tools returned incorrect election dates and information about how to cast a ballot at the European Parliament elections, according to analysis by Berlin-based NGO Democracy Reporting International. They were also found to provide broken or even irrelevant links to YouTube videos, and even content in Japanese.\nThe researchers asked the same 10 questions in 10 European languages in March 2024 to the chatbots, to which ChatGPT 4 (OpenAI's paid version) was found to perform the best. Google\u2019s Gemini was deemed the least likely to give correct answers.\nThe findings highlghted concerns about the accuracy and reliability of popular chatbots, their tendency to 'hallucinate' information and produce misinformation and disinformation, and and their potential impact on politics, elections, and democracy.\nThe European Parliament elections are due to take place in June 6-9, 2024.\nSystem \ud83e\udd16\nChatGPT chatbot\nCopilot chatbot\nGemini chatbot\nOperator: Alphabet/Google; Microsoft; OpenAI\nDeveloper: Alphabet/Google; Microsoft; OpenAI\nCountry: European Parliament\nSector: Politics\nPurpose: Generate text\nTechnology: Chatbot\nIssue: Accuracy/reliability; Mis/disinformation\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.politico.eu/article/ai-chatbots-spread-falsehoods-about-the-eu-elections-report-finds/\nhttps://elblog.pl/2024/04/15/the-ai-mirage-when-chatbots-mislead-on-election-info/\nResearch, advocacy \ud83e\uddee\nDemocracy Reporting International (2024). Are Chatbots misinforming us about the European Elections? Yes.\nRelated \ud83c\udf10\nTop AI image generators 'easily' produce misleading us election info\nMicrosoft Copilot spouts wrong answers about US election\nPage info\nType: Incident\nPublished: April 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/alexandria-ocasio-cortez-depicted-as-deepfake-pornstar", "content": "Alexandria Ocasio-Cortez traumatised by deepfake porn video\nOccurred: February 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nUS congress member Alexandria Ocasio-Cortez, also known as AOC, said she was the victim of a deepfake video that depicted her in sexually explicit acts. \nOcasio-Cortez, a survivor of physical sexual assault, opened up about the shock she felt upon seeing a video of herself online that others could perceive as real in an interview with Rolling Stone. She explained that the mental image of the deepfake version of herself performing sexual acts was deeply disturbing and resurfaced trauma.\nShe warned of the real effects such deepfakes can have, not just on the victims, but also on those who view and consume such content, and equated the harm from 'digitizing violent humiliation' to physical rape. \nThe interview was in the context of the Disrupt Explicit Forged Images and Non-Consensual Edits (DEFIANCE) Act of 2024 the politician had spearheaded in the US House of Congress. The bill aims to make it easier for victims of nonconsensual AI porn to sue publishers, distributors, and consumers of X-rated digital forgeries.\nSystem \ud83e\udd16\nUnknown\nIncident databank \ud83d\udd22\nOperator: \nDeveloper:  \nCountry: USA\nSector: Politics\nPurpose: Damage reputation\nTechnology: Deepfake - audio, video\nIssue: Ethics/values; Mis/disinformation; Safety\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.rollingstone.com/culture/culture-features/aoc-deepfake-ai-porn-personal-experience-defiance-act-1234998491/\nhttps://www.theguardian.com/us-news/2024/apr/09/alexandria-ocasio-cortez-deepfake-porn\nhttps://www.mediaite.com/politics/aoc-sounds-alarm-over-deepfakes-after-shes-targeted-with-ai-porn-guess-what-motherfers-im-not-going-anywhere/\nhttps://www.thecut.com/article/aoc-deepfake-defiance-act.html\nhttps://www.hindustantimes.com/world-news/us-news/rep-aoc-shares-deepfake-porn-encounter-experience-saying-it-resurfaces-trauma-101712674225988.html\nhttps://www.dailymail.co.uk/news/article-13285003/alexandria-ocasio-cortez-deepfake-porn-ai-assault.html\nhttps://torontosun.com/news/world/aoc-reveals-shock-over-seeing-herself-in-ai-generated-deepfake-porn\nhttps://www.commondreams.org/news/deepfakes\nhttps://nypost.com/2024/04/09/us-news/aoc-opens-up-about-seeing-deepfake-ai-porn-of-herself-online/\nRelated \ud83c\udf10\nPresident Biden calls for US draft deepfake\nBiden 'robocall' advises voters skip New Hampshire primary election\nPage info\nType: Incident\nPublished: April 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/greece-fined-for-ai-powered-asylum-centre-monitoring-system", "content": "Greece fined for AI-powered asylum centre monitoring systems\nOccurred: 2021-2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe Greek Ministry of Migration and Asylum was fined EUR 175,000 for incorrectly developing and installing two surveillance systems at asylum centres on the Aegean islands.\nCentaur is an integrated digital Electronic and Physical Security management system that uses cameras, drones and motion analysis algorithms to control reception and hospitality structures for third-country citizens on the Aegean islands. \nAnother system, Hyperion, is described as an integrated entry-exit control system. Asylum seekers, certified members of NGOs and other guests present cards read by an RFID [Radio Frequency Identification] reader combined with a fingerprint through which personal data and biometric data are processed. \nThe Greek Data Protection Authority (DPA) found that there was deficient cooperation on the part of the ministry as the Controller. It further considered that the required Data Protection Impact Assessments carried out by the ministry were incomplete and constituted serious omissions regarding the ministry\u2019s compliance with specific provisions of the European Union's GDPR regarding implementation of disputed systems.\nBoth systems received funding from the European Union. The fine is the largest imposed on a Greek public body to date.\nSystem \ud83e\udd16\nUnknown\nOperator: Ministry of Immigration and Asylum\nDeveloper: Ministry of Immigration and Asylum\nCountry: Greece\nSector: Govt - immigration\nPurpose: Monitor asylum centres\nTechnology: Computer vision; Drone; Machine learning; Motion analysis\nIssue: Privacy\nTransparency: Governance\nRegulation \u2696\ufe0f\nEU General Data Protection Regulation\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nHellenic Data Protection Authority (2024). Ministry of Migration and Asylum receives administrative fine and GDPR compliance order following an own-initiative investigation by the Hellenic Data Protection Authority\nHellenic Data Protection Authority (2024). \u0394\u03b9\u03bf\u03b9\u03ba\u03b7\u03c4\u03b9\u03ba\u03cc \u03c0\u03c1\u03cc\u03c3\u03c4\u03b9\u03bc\u03bf \u03ba\u03b1\u03b9 \u03b5\u03bd\u03c4\u03bf\u03bb\u03ae \u03b3\u03b9\u03b1 \u03c3\u03c5\u03bc\u03bc\u03cc\u03c1\u03c6\u03c9\u03c3\u03b7 \u03bc\u03b5 \u03c4\u03bf\u03bd \u0393\u039a\u03a0\u0394 \u03c3\u03c4\u03bf \u03a5M\u0391 \u03c3\u03c4\u03bf \u03c0\u03bb\u03b1\u03af\u03c3\u03b9\u03bf \u03b1\u03c5\u03c4\u03b5\u03c0\u03ac\u03b3\u03b3\u03b5\u03bb\u03c4\u03b7\u03c2 \u03ad\u03c1\u03b5\u03c5\u03bd\u03b1\u03c2 \u03c4\u03b7\u03c2 \u0391\u03c1\u03c7\u03ae\u03c2\nResearch, advocacy \ud83e\uddee\nEuropean Parliament (2022). Question for written answer E-003094/2022\nHomo Digitalis (2024). The Hellenic Data Protection Authority fines the Ministry of Migration and Asylum for the \"Centaurus\" and \"Hyperion\" systems with the largest penalty ever imposed to a Greek public body\nHomo Digitalis (2022). A major success for civil society in Greece: The Hellenic DPA launches an investigation into the Ministry of Immigration and Asylum re the YPERION and KENTAYROS IT systems\nStatewatch (2023). Europe's techno-borders\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://balkaninsight.com/2024/04/03/greek-ministry-fined-over-surveillance-systems-at-asylum-centres/\nhttps://www.lexology.com/pro/content/greek-regulator-fines-ministry-over-surveillance-systems\nhttps://pulitzercenter.org/stories/greek-data-watchdog-rule-ai-systems-refugee-camps\nhttps://algorithmwatch.org/en/greek-camps-surveillance/\nhttps://wearesolomon.com/mag/focus-area/accountability/asylum-surveillance-systems-launched-in-greece-without-data-safeguards/\nRelated \ud83c\udf10\nTrento council fined for AI surveillance projects\nAI translations jeopardise asylum applications\nPage info\nType: Incident\nPublished: April 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/alberta-party-endorses-itself-using-deepfake-video", "content": "Alberta Party endorses itself using deepfake video\nOccurred: January 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA video shared online by Canada's Alberta Party featuring a man endorsing the party faced a backlash after social media users pointed out that the man in the video was not a real person.\nThe video showed a man in front of a superimposed image of Calgary. Alberta Party leader Barry Morishita later acknowleged it had been created using  video creation platform Synthesia. The clip was subsequently deleted.\nThe incident raised ethical questions about the Alberta's Party deliberate manipulation of reality and the potential for misinformation and disinformation. It also raised concerns about the use of deepfake technology in politics, and the need for transparency and rules and regulations governing use of the technology in Canada.\nSystem \ud83e\udd16\nSynthesia AI video generation\nOperator: Alberta Party\nDeveloper: Synthesia\nCountry: Canada\nSector: Politics\nPurpose: Endorse political party\nTechnology: Deepfake - audio, video\nIssue: Ethics/values\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://twitter.com/AlbertaParty/status/1618644532798836737\nhttps://edmonton.citynews.ca/2023/01/26/alberta-party-video-ai-concerns/\nhttps://researchmoneyinc.com/article/growing-use-of-ai-for-political-purposes-is-a-risk-to-canada-s-democracy-uottawa-report\nRelated \ud83c\udf10\nDeepfake news anchors claim Venezuela economic health\nDeepfake video alleges France opposes Mali military junta\nPage info\nType: Incident\nPublished: April 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/youtube-inserts-explicit-captions-into-kids-videos", "content": "YouTube inserts explicit AI captions into kids' videos\nOccurred: February 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nYouTube's automated video captioning system inserted explicit language into children's videos, according to a team of researchers.\nHaving sampled around 7,000 videos from 24 children\u2019s YouTube channels, Rochester Institute of Technology and Indian School of Business reseachers discovered that 40 percent displayed words transcribed by YouTube's Automated Speech Transcription (ASR) service in their captions found on a list of 1,300 'taboo' terms. \nWords like \u201cbeach\u201d were transcribed as \u201cbitch\u201d, \u201ccorn\u201d as \u201cporn\u201d, and \u201cbrave\u201d as \"rape\". The study also found that 1 percent of the videos included highly inappropriate terms, such as \"bitch,\u201d \u201cbastard,\u201d or \u201cpenis.\u201d Videos posted on kids' channel Ryan\u2019s World saw the phrase \u201cYou should also buy corn\u201d rendered in captions as \u201cyou should also buy porn,\u201d and \u201cbeach towel\u201d transcribed as a \u201cbitch towel.\u201d\nThe finding prompted concerns that children exposed to explicit language and adult themes that are inappropriate for their age could lead to confusion, distress, and premature exposure to adult concepts.  Many families use the standard version of YouTube, where these algorithmically-generated captions can be seen.\nSystem \ud83e\udd16\nYouTube website\nYouTube Wikipedia profile\nOperator: Alphabet/YouTube\nDeveloper: Alphabet/YouTube\nCountry: Global\nSector: Media/entertainment/sports/arts\nPurpose: Transcribe speech\nTechnology: Machine learning; Speech recognition\nIssue: Accuracy/reliability; Safety\nTransparency: Governance; Black box\nResearch, advocacy \ud83e\uddee\nKrithika Ramesh K., KhudaBukhsh A.R., Kumar S. (2022). 'Beach' to 'Bitch': Inadvertent Unsafe Transcription of Kids' Content on YouTube\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.wired.com/story/youtubes-captions-insert-explicit-language-kids-videos/\nhttps://www.dailymail.co.uk/sciencetech/article-10553233/YouTube-AI-putting-explicit-language-captions-videos-aimed-children.html\nhttps://www.wionews.com/technology/youtubes-ai-algorithm-adds-explicit-language-in-captions-to-childrens-clips-456700\nhttps://news.slashdot.org/story/22/02/25/2359227/youtubes-captions-insert-explicit-language-in-kids-videos\nhttps://www.analyticsinsight.net/youtube-ai-algorithm-for-kids-slipping-uncensored-language-into-video-clips/\nhttps://www.sciencetimes.com/articles/36359/20220301/faulty-algorithm-youtube-ai-shows-explicit-caption-videos-kids.htm\nRelated \ud83c\udf10\nAI translations jeopardise asylum applications\nFacebook translates 'Good morning' as 'Attack them'\nPage info\nType: Incident\nPublished: April 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/robot-crushes-thai-factory-worker-to-death", "content": "Robot crushes Thai factory worker to death\nOccurred: March 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA worker at a plastics factory in Thailand was pinned to a bench and crushed to death by a robotic arm.\nThe arm slammed down and pinned the man to a bench as he was laying out sheets of metal at the Vandapac factory in Chonburi province of Thailand. Rescuers were called to the scene and gave the severely injured worker first aid before taking him to hospital, where he was pronounced dead. \nA Vandapac official blamed the employee for the accident, claiming the robot had been working properly and that the man had ducked under it at the wrong time. The official told local media: 'We are not providing any more information. The employees accept responsibility for any accidents that happen while they are working.' \nThe incident raised concerns about the safety of the robotic system, details of which have not been made public, and the company's transparency and accountability. It also raised concerns about the safety of manufacturing robots in general.\nSystem \ud83e\udd16\nVandapac website\nOperator: Vandapac\nDeveloper:  \nCountry: Thailand\nSector: Manufacturing/engineering\nPurpose:  \nTechnology: Robotics\nIssue: Safety\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.dailymail.co.uk/news/article-13249387/Robot-crushes-factory-worker-death-Victim-pinned-bench-killed-Thailand.html\nhttps://www.mirror.co.uk/news/world-news/shocking-moment-robot-arm-crushes-32459232\nhttps://thethaiger.com/news/national/robot-arm-crushes-worker-to-death-in-chon-buri-video\nRelated \ud83c\udf10\nRobot crushes man to death in South Korea\nRobot crushes and kills Ventra Ionia technician\nPage info\nType: Incident\nPublished: April 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/israel-attacks-hamas-using-ai-drone-swarm", "content": "Israel attacks Hamas using AI drone swarm\nOccurred: May 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nIsrael used an AI-guided swarm of drones to attack Hamas militants, raising concerns about the ethics of using lethal autonomous weapons in real combat situations. \nThe Israel Defense Forces (IDF) conducted over 30 sorties using flocks of drones over the Gaza Strip to detect rocket launches by Hamas and attack the sites. It is thought to be the first significant, publicly acknowledged real-world use of the concept.\nDrone swarms fly as one integrated network controlled by AI, and only require a single human operator to direct the swarm. The drone then guide themselves to locate the targets as a connected unit. The swarm can continue to operate even if some of the individual drones are destroyed.\nA 2018 US Air Force study (pdf) found that AI-enabled drone swarms made weapons significantly more efficient and lethal. 800 drones in a swarm were able to destroy more targets in two hours than 1,000 drones acting independently during a simulation. \nSystem \ud83e\udd16\nElbit Systems website\nOperator: Israel Defense Forces (IDF)\nDeveloper: Elbit Systems\nCountry: Israel; Palestine\nSector: Govt - defence\nPurpose: Detect rocket launch locations\nTechnology: Drone\nIssue: Ethics/values\nTransparency: \nResearch, advocacy \ud83e\uddee\nWilliams S.M. (2018). Swarm Weapons: Demonstrating a Swarm Intelligent Algorithm for Parallel Attack (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.newscientist.com/article/2282656-israel-used-worlds-first-ai-guided-combat-drone-swarm-in-gaza-attacks/\nhttps://www.timesofisrael.com/in-apparent-world-first-idf-deployed-drone-swarms-in-gaza-fighting/\nhttps://www.dailymail.co.uk/news/article-9760339/Israel-uses-AI-guided-drone-swarm-target-Hamas-militants-Gaza.html\nhttps://carnegieendowment.org/sada/90892\nhttps://defense-update.com/20210613_drone-swarms.html\nhttps://www.defenseone.com/ideas/2021/07/israels-drone-swarm-over-gaza-should-worry-everyone/183156/\nhttps://www.forbes.com/sites/erictegler/2023/10/27/small-ai-enabled-drones-could-be-first-into-gaza-streets--buildings/\nRelated \ud83c\udf10\nIsrael reportedly uses AI to identify 37,000 Hamas targets\nIsrael facial recognition system misidentifies innocent Gazans\nPage info\nType: Incident\nPublished: April 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/openai-scraped-youtube-to-train-gpt-4", "content": "OpenAI scrapes YouTube without permission to train GPT-4\nOccurred: April 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nOpenAI quietly scraped and transcribed over one million hours of YouTube videos to train its GPT-4 large language model, raising questions about copyright and ethics. \nIn an effort to source additional training data, Open AI created Whisper, a speech recognition system to transcribe over a million hours of YouTube videos. The resulting transcriptions were included in the training data for GPT-4. \nExperts have demonstrated that the performance of large language models improves with increased amounts of training data.\nAccording to The New York Times, OpenAI knew its use of YouTube videos without permission was legally questionable but believed it to be fair use. But critics say it goes against Google\u2019s terms which restrict automated access to YouTube videos and the use of its videos for independent applications outside of YouTube. \nGoogle is thought likely to have known about OpenAI\u2019s activities, but appears not to have acted, possibly as it has also been accused of using YouTube videos to train its own AI models.\nYouTube creators are thought to face a variety of actual and potential harms as a result of OpenAI\u2019s actions, including copyright violations, financial loss, and privacy abuse. \nSystem \ud83e\udd16\nChatGPT chatbot\nGPT-4 large language model\nWhisper speech recognition system Wikipedia profile\nOperator: OpenAI\nDeveloper: OpenAI\nCountry: Global\nSector: Media/entertainment/sports/arts\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Copyright; Ethics/values\nTransparency: Governance \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.nytimes.com/2024/04/06/technology/tech-giants-harvest-data-artificial-intelligence.html\nhttps://www.theverge.com/2024/4/6/24122915/openai-youtube-transcripts-gpt-4-training-data-google\nhttps://www.business-standard.com/technology/tech-news/openai-transcribed-google-s-youtube-videos-to-train-ai-models-report-124040800265_1.html\nhttps://mashable.com/article/open-ai-google-youtube-videos\nRelated \ud83c\udf10\n17 authors sue OpenAI for 'systematic mass-scale copyright infringement'\nAuthors Mona Awad, Paul Tremblay sue OpenAI for copyright abuse\n\nPage info\nType: Incident\nPublished: April 2024\nLast updated: July 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/three-news-publishers-sue-openai-for-copyright-infringement", "content": "Three news publishers sue OpenAI for copyright infringement\nOccurred: February 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThree news publishers accused OpenAI and Microsoft of training their AI models using copyrighted material.\nIn February 2024, The Intercept, Raw Story, and AlterNet filed a lawsuit against OpenAI and Microsoft, claiming ChatGPT and OpenAI\u2019s GPT-3, GPT-3.5 and GPT-4 large language models were trained on their copyrighted work without permission.\n\nThe suit went on to claim that ChatGPT had been trained to disregard copyright, ignore proper attribution, and not notify users when answers are generated using journalists\u2019 protected work. \n\nThe suit was one of a number levelled at OpenAI for training its models on copyrighted material without permission, notably by the New York Times. In response, OpenAI argued its use of content constituted \u2018fair use\u2019 and that changes were \u2018transformative\u2019.\nThe wave of lawsuits are seen to reflect a media industry-wide concern that generative AI will compete with established publishers as a source of information for internet users, while sapping advertising revenues and undermining the quality of online news. \nSystem \ud83e\udd16\nChatGPT\nGPT-4\nGPT-4 Wikipedia page\nGPT-4 Research\nOperator: OpenAI\nDeveloper: OpenAI\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Copyright; Ethics/values\nTransparency: Governance \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theguardian.com/technology/2024/feb/28/media-outlets-sue-openai-copyright-infringement\nhttps://www.theverge.com/2024/2/28/24085973/intercept-raw-story-alternet-openai-lawsuit-copyright\nhttps://www.niemanlab.org/2024/03/the-intercept-charts-a-new-legal-strategy-for-digital-publishers-suing-openai/\nhttps://www.nytimes.com/2024/02/28/technology/openai-copyright-suit-media.html\nRelated \ud83c\udf10\nThe New York Times sues OpenAI, Microsoft over copyright abuse\n17 authors sue OpenAI for 'systematic mass-scale copyright infringement'\nAuthors Mona Awad, Paul Tremblay sue OpenAI for copyright abuse\n\nPage info\nType: Incident\nPublished: April 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/google-books-indexes-low-quality-ai-generated-books", "content": "Google Books indexes low quality, AI-generated books\nOccurred: April 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGoogle Books has been indexing AI-generated books that impacts the quality of information on Google's platform and possibly damaging Google Ngram Viewer, a tool used by researchers to track language use throughout history.\nAccording to 404 Media, dozens of books containing the phrase 'As of my last knowledge update,' which is associated with ChatGPT- and Gemini AI-generated answer are listed on Google Books. And while some of these books discussed topics like ChatGPT, machine learning, and AI, most appeared to be generated by artificial intelligence.\nExamples of AI-generated books on Google Books include Bears, Bulls, and Wolves: Stock Trading for the Twenty-Year-Old by Tristin McIver and Maximize Your Twitter Presence: 101 Strategies for Marketing Success by Shu Chen Hou. Published in 2024, both books read like ChatGPT-generated text with superficial analysis of complex topics.\nThe presence of AI-generated content raised questions about the quality and reliability of information available on Google Books. It also raised concerns about Google Ngram Viewer, a tool used by researchers to track language use throughout history, which could be corrupted by the influx of AI-generated content.\nSystem \ud83e\udd16\nGoogle Books website\nGoogle Books Wikipedia profile\nOperator: Alphabet/Google\nDeveloper: Alphabet/Google\nCountry: Global\nSector: Media/entertainment/sports/arts\nPurpose: Index books\nTechnology: \nIssue: Accuracy/reliability; Mis/disinformation\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.404media.co/google-books-is-indexing-ai-generated-garbage/\nhttps://boingboing.net/2024/04/08/google-books-ingesting-all-the-ai-generated-rubbish.html\nhttps://gigazine.net/gsc_news/en/20240405-google-books-ai/\nhttps://www.infodocket.com/2024/04/04/report-google-books-indexes-ai-trash/\nhttps://www.newsbytesapp.com/news/science/google-books-and-the-problem-of-ai-generated-text/story\nhttps://slashdot.org/story/24/04/04/1853248/google-books-is-indexing-ai-generated-garbage\nRelated \ud83c\udf10\nSynthetic Tiananmen tank man dominates Google Search\nAI-generated travel books and reviews flood Amazon\nPage info\nType: Incident\nPublished: April 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/uk-housebuilding-mutant-algorithm-retracted-after-political-storm", "content": "UK housebuilding 'mutant algorithm' retracted after political storm\nOccurred: 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA UK government plan for a new formula to determine where to build houses to meet its aim of delivering more homes had to be withdrawn after a political backlash.\nThe so-called 'mutant algorithm' was part of broader UK government planning reforms, including a target to build 300,000 new homes across England each year by the mid-2020s. \nHowever, the plan met a backlash from some senior Conservative MPs, who argued that it would fail to 'level up' North of England, while seeing the South 'concreted over'. \nCritics also expressed concern that the plan could lead to more homes being built in rural areas and in the South East, rather than in the North and Midlands, and that it did not prioritise 'brownfield sites' (previously developed land that is not currently in use).\nIn response, Housing Secretary Robert Jenrick announced that the government would abandon the controversial algorithm and said it would revise its proposals to focus more on urban development, building more homes in the North and Midlands, and in brownfield sites in urban areas or city centres.\nSystem \ud83e\udd16\nUK govt. Housing and economic needs assessment\nOperator: \nDeveloper: Ministry of Housing, Communities and Local Government\nCountry: UK\nSector: Govt - housing\nPurpose: Calculate minimum annual local housing need figure\nTechnology: \nIssue: Bias/discrimination - location\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/uk-politics-54950012\nhttps://www.building.co.uk/news/government-to-rethink-planning-formula-after-backbench-backlash/5107696.article\nhttps://www.wired.co.uk/article/housing-algorithm-flaws\nhttps://twitter.com/BenM_IM/status/1328386362425421834\nhttps://www.thetimes.co.uk/edition/news/housebuilding-algorithm-unfair-to-towns-and-cities-boris-johnson-warned-7v0zz6hvz\nhttps://www.theguardian.com/commentisfree/2020/dec/16/jenrick-mutant-algorithm-win-localism-centralised-planning-u-turn\nhttps://www.independent.co.uk/news/uk/politics/housing-boris-johnson-rural-areas-economy-algorithm-young-people-b438038.html\nhttps://www.bdonline.co.uk/news/government-abandons-mutant-housing-algorithm-to-focus-on-urban-development/5109572.article\nhttps://www.politicshome.com/thehouse/article/why-is-the-new-housing-algorithm-facing-backlash-from-conservative-mps\nhttps://www.dailymail.co.uk/news/article-9059131/Tories-hail-U-turn-mutant-algorithm-shires-planning-revolt.html\nhttps://thenextweb.com/neural/2020/08/25/will-the-uks-housebuilding-algorithm-join-the-governments-growing-ai-graveyard/\nhttps://www.ft.com/content/6b4cf5b7-d80b-45c7-87fc-d0f1b783473f\nhttps://www.conservativehome.com/thecolumnists/2020/08/neil-obrien-2.html\nRelated \ud83c\udf10\nLA subsidised housing scoring system racial bias\nFaulty automated background checks freeze out renters\nPage info\nType: Incident\nPublished: April 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/grok-generates-fake-iran-missile-attack-headline", "content": "Grok generates fake Iran missile attack headline\nOccurred: April 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nElon Musk's Grok chatbot generated fake news about an alleged missile attack by Iran on Tel Aviv, resulting in the story's amplification on X/Twitter and leading to panic.\nGrok, operating on Musk\u2019s platform X/Twitter, picked up on fake rumours and generated a false headline stating that Iran had launched 'heavy' missile attacks on Tel Aviv, Israel. This misinformation was then promoted by X\u2019s trending news product, Explore.\nX's Explore page had earlier been updated to reintroduce written context for trending topics, with Grok tasked with generating narratives and headlines. Musk's had disbanded Twitter's human editorial team after his takeover of the platform, leaving a void in the platform's ability to authenticate and contextualise news trends.\nThe incident highlighted the importance of human oversight of AI systems, notably those processing real-time news during a war, and the potential for misuse of Grok by X's premium-subscribed users to generate misinformation. \nAccording to commentators, it also further damaged X's reputation as a trustworthy news channel.\nSystem \ud83e\udd16\nGrok chatbot\nOperator: Elon Musk\nDeveloper: xAI Corp\nCountry: Iran; Israel\nSector: Govt -  defence; Govt - politics\nPurpose: Summarise news articles\nTechnology: Chatbot\nIssue: Mis/disinformation\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://mashable.com/article/elon-musk-x-twitter-ai-chatbot-grok-fake-news-trending-explore\nhttps://www.computing.co.uk/news/4194162/musks-pushed-fake-iran-israel-headline-generated-ai-chatbot-grok\nhttps://www.cryptopolitan.com/elon-musks-ai-favoritism-x-unchecked-auto/\nhttps://gigazine.net/gsc_news/en/20240408-grok-ai-fake-news/\nhttps://www.reddit.com/r/technology/comments/1bxd8ng/elon_musks_x_pushed_a_fake_headline_about_iran/\nRelated \ud83c\udf10\nGrok AI wrongly accuses Klay Thompson of 'brick-vandalism spree'\nAdobe sells AI-generated Israel-Hamas war images\nPage info\nType: Incident\nPublished: April 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/lobservatoire-de-leurope-uses-ai-to-plagiarise-euronews-content", "content": "L\u2019Observatoire de l\u2019Europe uses AI to plagiarise Euronews content\nOccurred: March 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nOnline news channel L\u2019Observatoire de l\u2019Europe has been accused of using AI to plagiarise content from European news publisher Euronews without consent or acknowledgement.\nCiting an example of an article contradicting the rumour of the death of King Charles III first published by Euronews, the same article was found on L\u2019Observatoire de l\u2019Europe the same day under the byline 'Jean Delaunay'. Other examples have included the reproduction of an article on ChatGPT and one on St. Patrick's Day.\nDelaunay was presented as founder of L\u2019Observatoire de l\u2019Europe and professor of political science at several universities. However, attempts by Newsguard to identify the author failed.\nThe incident raised concerns about the use of generative AI models to reproduce copyrighted or plagiarised material without proper attribution. It also highlighted the use of generative AI to develop business models that replicate others, potentially denying the original content creators of their rightful earnings.\nSystem \ud83e\udd16\nUnknown\nOperator: L\u2019Observatoire de l\u2019Europe\nDeveloper:  \nCountry: Belgium\nSector: Media/entertainment/sports/arts\nPurpose: Republish news articles\nTechnology: Chatbot\nIssue: Cheating/plagiarism; Copyright\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://fr.euronews.com/culture/2023/11/30/chatgpt-a-un-an-un-triste-anniversaire-pour-la-culture-et-la-creativite\nhttps://www.euronews.com/culture/2023/11/30/chatgpt-turns-one-the-birthday-threatening-culture-and-creativity\nhttps://actualnewsmagazine.com/english/fake-news-plagiarism-newsguard-warns-of-the-explosion-of-news-sites-generated-by-artificial-intelligence/\nhttps://www.newsguardrealitycheck.com/p/pink-slime-time-election-year-launches\nRelated \ud83c\udf10\nInvesting.com plagiarises other websites using AI\nPalworld accused of plagiarising Pokemon designs using AI\nPage info\nType: Incident\nPublished: April 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chinese-ai-campaign-accuses-us-government-of-train-derailment-cover-up", "content": "Chinese AI campaign accuses US government of Kentucky train derailment cover-up\nOccurred: November 2023-\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe derailment of a train carrying molten sulphur in Kentucky, USA, resulted in an opportunistic, AI-powered disinformation campaign by a group associated with the Chinese government. \nApproximately one week after the derailment of the train, China's Storm-1376 group (also known as Spamouflage and Dragonbridge launched a social media-based campaign that amplified the derailment, spread anti-US government conspiracy theories, and highlighted political divisions among US voters with the objective of encouraging mistrust of and disillusionment with the US government. \nStorm-1376 urged audiences to consider whether the US government may have caused the derailment and is 'deliberately hiding something.' Some messages even likened the derailment to 9/11 and Pearl Harbor cover-up theories.\nWhilst it is unclear how successful the campaign was in meeting its objectives, the campaign is seen as an example of how AI can be misused for geo-political purposes.\nSystem \ud83e\udd16\nUnknown\nOperator: Government of China; Dragonbridge; Spamouflage; Storm 1376\nDeveloper:  \nCountry: China; USA\nSector: Govt - foreign; Transport/logistics\nPurpose: Scare/confuse/destabalise\nTechnology: \nIssue: Mis/disinformation\nTransparency: Governance; Marketing\nInvestigations, assessments, audits \ud83e\uddd0\nMicrosoft (2024). China tests US voter fault lines and ramps AI content to boost its geopolitical interests\nMicrosoft (2024). East Asia threat actors employ unique methods (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://apnews.com/article/train-derailment-chemical-spill-kentucky-livingston-csx-9335e2d502da05e195e43ccc4476c8bf\nhttps://www.france24.com/en/live-news/20240405-microsoft-says-china-using-ai-to-sow-division-in-us\nhttps://www.infosecurity-magazine.com/news/china-ai-content-division-us/\nhttps://time.com/6963787/china-influence-operations-artificial-intelligence-cyber-threats-microsoft/\nhttps://uk.news.yahoo.com/microsoft-says-china-using-ai-182629859.html\nRelated \ud83c\udf10\nChina uses AI to accuse US of starting Maui wildfires\nBeijing AI influence campaign weaponises Gaza conflict\nPage info\nType: Incident\nPublished: April 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/up-to-17-percent-of-ai-conference-reviews-written-by-ai", "content": "Up to 17 percent of AI conference reviews written by AI\nOccurred: March 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nResearchers have found that between 6.5 percent and 16.9 percent of text submitted as peer reviews to major scientific conferences are likely to have been substantially modified by LLMs.\nStanford University, NEC Labs America, and UC Santa Barbara researchers analysed the peer reviews of papers submitted to leading AI conferences ICLR 2024, NeurIPS 2023, CoRL 2023 and EMNLP 2023, and discovered that large language models tend to employ adjectives like \"commendable,\" \"innovative,\" and \"comprehensive\" more frequently than human authors.\nThe insight enabled the researchers to compare peer reviews by AI-powered machines against those of humans. 'Our results suggest that between 6.5 percent and 16.9 percent of text submitted as peer reviews to these conferences could have been substantially modified by LLMs, i.e. beyond spell-checking or minor writing updates,' they concluded.\nThe researchers contended that such practices potentially deprive those whose work is being reviewed of diverse feedback from experts, and that AI feedback risks a homogenisation effect that skews toward AI model biases and away from meaningful insight. They also argued the scientific community should be more transparent about the use of LLMs. \nSystem \ud83e\udd16\nChatGPT chatbot\nOperator:  \nDeveloper: OpenAI\nCountry: Global\nSector: Research/academia\nPurpose: Generate conference peer reviews\nTechnology: Machine learning\nIssue: Cheating/plagiarism\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nLiang W, Izzo Z., Zhang Y., Lepp H., Cao H., Zhao X., Chen L., Ye H., Liu S., Huang Z., McFarland D.A., Y. Zou J.Y. Monitoring AI-Modified Content at Scale: A Case Study on the Impact of ChatGPT on AI Conference Peer Reviews\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://mikeyoung44.hashnode.dev/up-to-17-of-ai-conference-reviews-now-written-by-ai\nhttps://www.theregister.com/2024/03/19/ai_researchers_reviewing_peers/\nhttps://slashdot.org/story/24/03/19/2057245/ai-researchers-have-started-reviewing-their-peers-using-ai-assistance\nhttps://www.404media.co/email/c36519c0-0930-47a2-808d-48631b597dc1/\nRelated \ud83c\udf10\nScientific journals publish papers with AI-generated introductions\nPublishers withdraw over 120 gibberish AI-generated papers\nPage info\nType: Issue\nPublished: April 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/meta-ai-image-generator-struggles-to-produce-interracial-couples", "content": "Meta AI image generator struggles to produce interracial couples\nOccurred: March 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nMeta's image generator failed to conceive of mixed-race couples, leading to accusations of racial bias and stereotyping.\nUsers complained that Imagine with Meta AI repeatedly failed to create pictures of Asian men with white women and vice-versa. Similarly, when asked for an image of a Black man with a white wife, it produces images of a Black couple instead.\nA typed-in prompt for \u201can interracial couple\u201d resulted in the response: \u201cThis image can\u2019t be generated. Please try something else.\u201d \nThe limitations of the tool was seen to perpetuate racial and ethnic stereotypes and reinforce existing societal biases. Furthermore, by failing to represent interracial couples correctly, the tool was accused of inadvertently contributing to a lack of diversity and inclusivity in visual content.\nSystem \ud83e\udd16\nImagine with Meta\nOperator: Meta\nDeveloper: Meta\nCountry: USA\nSector: Multiple\nPurpose: Generate images\nTechnology: Text-to-image\nIssue: Bias/discrimination; Stereotyping\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theverge.com/2024/4/3/24120029/instagram-meta-ai-sticker-generator-asian-people-racism\nhttps://edition.cnn.com/2024/04/04/tech/meta-ai-image-generator-interracial-couples/index.html\nhttps://www.engadget.com/metas-ai-image-generator-struggles-to-create-images-of-couples-of-different-races-231424476.html\nhttps://www.businessinsider.com/metas-ai-image-generator-interracial-couples-tech-problems-2024-4\nhttps://www.dailymail.co.uk/sciencetech/article-13271111/Meta-AI-RACIST-asian-couple-Zuckerberg.html\nhttps://www.theaustralian.com.au/world/the-times/metas-ai-image-generator-accused-of-racism/news-story/295f47b022fff2724c45fef35201b3ed\nRelated \ud83c\udf10\nStable Diffusion job type gender, racial stereotyping\nBuzzfeed national AI Barbies' racism, cultural stereotyping\nPage info\nType: Incident\nPublished: April 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-studios-accused-of-using-ai-voice-cloning-during-actors-strikes", "content": "Amazon studios accused of using AI voice cloning during actors\u2019 strikes\nOccurred: February 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAmazon MGM Studios was accused of trying to rush production using AI voice cloning during actors strikes to avoid a copyright deadline.\nThe original screenwriter of Road House, R. Lance Hill, sued Amazon MGM studios for copyright infringement and accuses them of using extreme measures including AI voice cloning of actors\u2019 voices to rush production of the Road House remake. \nAmazon MGM studios denied using generative AI on the roadhouse production. \nUse of generative AI during the prolonged Hollywood strike is viewed as sensitive as AI was one of the topics that became a major concern and brought the industry to a standstill. \nUltimately, studios and streaming platforms reached an agreement to seek consent prior to using digital duplicates and to provide remuneration to the talent if used.\nSystem \ud83e\udd16\nUnknown\nOperator: Amazon MGM Studios\nDeveloper: \nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Generate audio\nTechnology: Text-to-speech; Deepfake - audio;  Neural network; Deep learning; Machine learning\nIssue: Copyright, Workforce dislocation/replacement\nTransparency:\nNews, commentary, analysis \ud83d\uddde\ufe0f\nAmazon\u2019s Road House reboot is accused of copyright infringement \u2014 and AI voice cloning\nAmazon accused of using AI to clone actors' voices in Road House remake starring Jake Gyllenhaal\nRelated \ud83c\udf10\nNetflix 'Dog and Boy' film AI backgrounds \nAmazon uses AI to generate \u2018Fallout\u2019 promo art\nPage info\nType: Incident\nPublished: April 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chinese-voice-actor-sues-ai-companies-for-using-her-voice-without-consent", "content": "Chinese voice actor wins lawsuit against AI companies for using their voice without consent\nOccurred: December 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA voiceover artist in China filed a lawsuit alleging an AI-generated voice resembling her own was used in audiobooks without her consent.\nThe voiceover artist, Yin, is suing 5 companies, including the provider of the AI text-to-speech system and the company that had recorded her voice. The system in question uses generative artificial intelligence to recreate voices, allegedly with high precision.\nThis case is seen to venture into uncharted legal territory, where traditional copyright laws may not apply, thereby potentially setting a precedent for how individual likeness (e.g. \u201cright to voice\u201d) in the era of AI is perceived and protected..\nYin argues this represents a threat to her work and potential income, and adds to concerns raised by voice actors globally over the future of their profession.\nUpdate (April 2024): The Beijing Internet Court ruled in favour of the voice actor recognising that the AI voice generator had infringed on their personality rights. The defendants in the case were ordered to cease activities and pay compensation to the voice actor. \nSystem \ud83e\udd16\nUnknown\nOperator:\nDeveloper:\nCountry: China\nSector: Media/entertainment/sports/arts\nPurpose: Generate audio\nTechnology: Text-to-speech; Deepfake - audio; Neural network; Deep learning; Machine learning\nIssue: Personality rights, Workforce dislocation/replacement\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nUpdate (April 2024): https://natlawreview.com/article/chinas-beijing-internet-court-recognizes-personality-rights-generative-ai-case \nChina mulls legality of AI-generated voice used in audiobooks\nCourt hears nation's first AI voice rights case\nTheir voices are their livelihood. Now AI could take it away\nRelated \ud83c\udf10\nApple trains AI models on Spotify Audiobook Narrators\nBBC replaces 'Mamma Mia!' star Sara Poyzer with AI Voice\nVoiceify sued for training AI with copyrighted material\nIBM sells Greg Marston voice for commercial cloning\nVoiceverse NFT voice theft \nPage info\nType: Incident\nPublished: April 2024\nLast updated; May 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/israel-reportedly-uses-ai-to-identify-37000-hamas-targets", "content": "Israel reportedly uses AI to identify 37,000 Hamas targets\nOccurred: October 2023-\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nIsrael used an AI-powered tool to identify and bomb suspected Hamas operatives in Gaza at night with their families, according to a local media investigation.\nCiting Israeli military sources, +972 Magazine and Local Call reported that the system - named Lavender - was used by the Israeli Defence Forces (IDF) since the start of the Israel-Hamas war to rapidly identify potential targets linked to Hamas and Palestinian Islamic Jihad (PIJ). At one stage, Lavender is said to have listed approximately 37,000 Palestinian men associated with these groups.\nIntelligence sources claimed that Israeli military officials permitted large numbers of Palestinian civilians to be killed, particularly during the early weeks and months of the conflict, and that 'thousands of Palestinians - most of them women and children or people who were not involved in the fighting - were wiped out by Israeli airstrikes, especially during the first weeks of the war, because of the AI programme\u2019s decisions.'\nOne official told +972 that 'human personnel often served only as a 'rubber stamp' for the machine\u2019s decisions' and typically devoted only around 20 seconds to each target - mostly ensuring they are male - before authorising a bombing.\nAn Israeli Defence Forces official did not dispute the existence of the tool, but the IDF denied AI was being used to identify suspected terrorists. \nSystem \ud83e\udd16\nLavender\nDocuments \ud83d\udcc3\nIsrael Defence Forces statement in response to Guardian article\nOperator: Israeli Defence Forces (IDF)\nDeveloper:  \nCountry: Israel; Palestine\nSector: Govt - defence\nPurpose: Identify targets\nTechnology: Machine learning\nIssue: Ethics/values; Human/civil rights; Safety\nTransparency: Governance\nNews, commentary, analysis \ud83d\udcf0\nhttps://www.972mag.com/lavender-ai-israeli-army-gaza/?s=03\nhttps://www.theguardian.com/world/2024/apr/03/israel-gaza-ai-database-hamas-airstrikes\nhttps://www.aljazeera.com/news/liveblog/2024/4/4/israels-war-on-gaza-live-israel-accused-of-ai-assisted-genocide-in-gaza\nhttps://www.independent.co.uk/news/world/middle-east/israel-gaza-hamas-war-ai-b2523039.html\nhttps://edition.cnn.com/2024/04/03/middleeast/israel-gaza-artificial-intelligence-bombing-intl/index.html\nhttps://www.theguardian.com/world/2024/apr/03/israel-defence-forces-response-to-claims-about-use-of-lavender-ai-database-in-gaza\nhttps://www.firstpost.com/explainers/lavender-ai-program-israel-kill-lists-gaza-hamas-war-13756112.html\nhttps://www.thesun.co.uk/news/27104192/israeli-ai-secret-weapon-lavender-revealed/\nRelated \ud83c\udf10\nIsrael facial recognition system misidentifies innocent Gazans\nIsrael uses Habsora 24 hour automated 'target factory' against Palestinians\nPage info\nType: Incident\nPublished: April 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-just-walk-out-relies-on-indian-human-reviewers", "content": "Amazon Just Walk Out relies on Indian human reviewers\nOccurred: May 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAmazon\u2019s automated 'Just Walk Out' technology, initially touted as an AI-powered checkout system, was actually reliant on human reviewers in India.\nJust Walk Out aimed to allow customers to bypass traditional checkouts by using cameras and sensors to track their purchases, and leave without checking out as they would get receipts afterwards. The system was supposed to save retailers money through 'operational efficencies'. \nHowever, the company was found to be quietly using approximately 1,000 'Machine learning data associates' in India to monitor customer transactions and label video footage to train the system\u2019s machine learning model, resulting in accusations of opacity, hype, and hypocrisy. It was also accused of undercutting US workers.\nIn April 2024, Amazon announced that 'Just Walk Out' was proving overly time-consuming, would be discontinued at Amazon Fresh Stories and replaced with 'Dash Carts' and traditional self-checkout counters.\nJust Walk Out had reputedly missed Amazon's internal targets of reaching less than 50 reviews per 1,000 sales.\nSystem \ud83e\udd16\nJust Walk Out website\nOperator: Amazon Fresh\nDeveloper: Amazon\nCountry: USA\nSector: Retail\nPurpose: Automate check-out process\nTechnology: Computer vision; Deep learning; Machine learning; Object recognition\nIssue: Employment\nTransparency: Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theinformation.com/articles/how-amazons-big-bet-on-just-walk-out-stumbled\nhttps://gizmodo.com/amazon-reportedly-ditches-just-walk-out-grocery-stores-1851381116\nhttps://www.hindustantimes.com/business/amazons-just-walk-out-tech-relied-on-low-paid-indian-workers-not-ai-report-101712125289458.html\nhttps://www.moneycontrol.com/europe/?url=https://www.moneycontrol.com/news/trends/amazons-just-walk-out-tech-was-actually-powered-by-low-paid-indian-workers-report-12566201.html\nhttps://www.hindustantimes.com/world-news/us-news/1000-indians-part-of-amazons-generative-ai-driven-just-walk-out-project-in-fresh-stores-report-101712118547238.html\nhttps://www.benzinga.com/news/24/04/38065342/amazons-just-walk-out-grocery-checkouts-pitched-as-ai-powered-technology-was-actually-relying-on-1-0\nRelated \ud83c\udf10\nAmazon Go fails to inform NYC customers about facial recognition\nAmazon One palmprint biometrics\nPage info\nType: Issue\nPublished: April 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-company-found-guilty-of-violating-ultraman-copyright-in-china", "content": "AI company found guilty of violating Ultraman copyright in China\nOccurred: February 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nIn a landmark ruling, Guangzhou\u2019s Internet Court found an unnamed AI company guilty of copyright infringement related to AI-generated images.\nThe dispute centred around the Ultraman series, in which the defendant\u2019s website produced images very similar to the plaintiff\u2019s copyrighted content when requested by a user. Ultraman is a well-known character owned by Tsuburaya Productions that was awarded the Guinness World Record for being the subject of the highest number of spin-off TV shows.\nThe court ordered the AI company to pay yuan 10,000 in damages for violating the plaintiffs' reproduction rights, setting a precedent for handling AI-generated content and emphasising the importance of protecting intellectual property rights.\nThis case constituted the first time an AI company was held legally responsible for generating copyrighted material, and was seen to highlight the challenge of regulating AI-generated works and their potential impact on creators and industries worldwide.\nSystem \ud83e\udd16\nUnknown\nOperator:\nDeveloper:\nCountry: China\nSector: Media/entertainment/sports/arts\nPurpose: Generate images\nTechnology: Text-to-image\nIssue: Copyright\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.forbes.com/sites/johannacostigan/2024/02/29/china-rules-ai-firm-committed-copyright-infringement/?sh=427d14dd454d \nhttps://www.mondaq.com/china/copyright/1444468/china-issues-landmark-ruling-on-copyright-infringement-involving-ai-generated-images \nRelated \ud83c\udf10\nThe New York Times sues OpenAI, Microsoft over copyright abuse\nImage-generation AIs memorise training images\nBing Image Creator Violates Disney Copyright\nPage info\nType: Incident\nPublished: April 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/nvidia-sued-for-training-nemo-on-authors-copyrighted-works", "content": "Nvidia sued for training NeMo on authors' copyrighted works\nOccurred: March 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGPU chip provider Nvidia has been sued by 3 authors accusing it of training it's NeMo AI models on copyrighted books.\nAuthors Brian Keene, Abdi Nazemian and Stewart O'Nan submitted a class action lawsuit against Nvidia for copyright infringement, saying their works were part of the Books3 dataset and were trained on NeMO generative AI platform without their permission. \nThe Books3 dataset, the lawsuit argued, copied \"all of Bibliotek\" - a so-called shadow library of approximately 196,640 pirated books that had earlier been available as part of The Pile - a larger dataset - through AI community Hugging Face. The Pile was later removed from Hugging Face in the wake of a copyright complaint.\nThe authors want compensation for their creative labour and the destruction of all copies of the Books3 dataset, and argue that Nvidia\u2019s October 2023 takedown of the NeMo AI platform was an implicit admission of its guilt. \nThe case highlighted ongoing copyright clashes between the AI industry and creative communities, with transparency and infringement claims at the forefront.\n\u2796 October 2023. Nvidia withdrew the NeMo platform and acknowledged the model had been trained on a dataset containing \"approximately\" 196,640 books. The Books3 dataset contains the same number of books.\nSystem \ud83e\udd16\nNvidia NeMo\nBooks3 dataset\nOperator: Nvidia\nDeveloper: Nvidia\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Train and deploy custom LLMs\nTechnology: Generative AI; Machine learning; Neural network; Deep learning; NLP/text analysis\nIssue: Copyright; Ethics/values\nTransparency: Governance; Marketing\nRegulation \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nDigital Millennium Copyright Act (DMCA)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reuters.com/technology/nvidia-is-sued-by-authors-over-ai-use-copyrighted-works-2024-03-10/\nhttps://dig.watch/updates/authors-take-legal-action-against-nvidia-for-copyright-infringement-in-ai-training\nhttps://www.wsj.com/tech/ai/nvidia-says-nemo-ai-platform-complies-with-copyright-after-authors-complaint-52a4dd79\nRelated \ud83c\udf10\n17 authors sue OpenAI for 'systematic mass-scale copyright infringement'\nAuthors Mona Awad, Paul Tremblay sue OpenAI for copyright abuse\nPage info\nType: Incident\nPublished: April 2024\nLast updated: June 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/san-jose-homeless-detection-ai-sparks-privacy-inequality-fears", "content": "San Jose homeless detection AI sparks privacy, inequality fears\nOccurred: March 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nPlans by the city of San Jose, California, to implement a new artificial intelligence system to detect homeless encampments caused concerns amongst privacy and human rights advocates.\nThe city of San Jose has been piloting using car-mounted cameras and AI software to scan for, amongst other things, trash, graffiti, and vehicles that people are living in, and illegal encampments.\nAccording to city authorities, city staff could respond by sending outreach workers to small encampments before they become large ones if the technology was fully deployed. \nHowever, the accuracy of the system has proved (pdf) low, with 10-15 pecent accuracy for lived-in cars and 70-75 percent for RVs. Furthermore, critics expressed concerns about potential privacy violations and misuse of the system to criminalise or displace homeless people.\nThis initiative underscores the delicate balance between addressing homelessness and safeguarding civil liberties.\nSystem \ud83e\udd16\nCity of San Jose website\nSenSen AI website\n\nDocuments \ud83d\udcc3\nSenSen AI Live Awareness White Paper (pdf)\nOperator: City of San Jose  \nDeveloper: SenSen AI; Zyrex\nCountry: USA\nSector: Govt - municipal\nPurpose: Detect homeless encampments\nTechnology: Computer vision; Object recognition\nIssue: Accuracy/reliability; Human/civil rights; Privacy\nTransparency: Governance; Marketing\nResearch, advocacy \ud83e\uddee\nA Step to Freedom (2024). THE MOST CONTROVERSIAL USE OF AI YET: SAN JOSE'S PLAN TO COMBAT HOMELESSNESS\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theguardian.com/technology/2024/mar/25/san-jose-homelessness-ai-detection\nhttps://www.sfgate.com/tech/article/san-jose-ai-camera-homeless-people-19370144.php\nhttps://www.ktvu.com/news/city-of-san-jose-testing-use-of-ai-to-identify-potholes-graffiti-and-homeless-encampments\nhttps://www.theverge.com/2024/3/25/24111773/silicon-valleys-biggest-city-is-training-ai-to-detect-homeless-encampments\nhttps://www.thesanjoseblog.com/2024/03/san-jose-is-using-ai-to-scan-streets.html\nhttps://www.the-sun.com/news/10920110/tiny-home-crackdown-san-jose-cars-rvs/\nhttps://www.reddit.com/r/ABoringDystopia/comments/1bnre60/san_jose_hackathon_produces_municipal_vehicle/\nRelated \ud83c\udf10\nHonolulu homeless robot temperature tests\nKnightscope HP RoboCop ignores woman reporting crime\nPage info\nType: Issue\nPublished: April 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/al-video-depicts-indonesian-presidential-hopeful-speaking-arabic", "content": "Al video falsely depicts Indonesian presidential hopeful speaking fluent Arabic\nOccurred: November 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn AI-altered TikTok video purportedly falsely showed Anies Baswedan, an Indonesian presidential candidate, speaking Arabic at a rally.\nDespite Baswedan's team clarifying he only has a basic grasp of Arabic, the video falsely portrayed him as fluent. Analysis by AFP confirmed the video had been automatically translated.\nThe 18-second fake clip highlighted Baswedan advocating for the Nasdem Party, which had endorsed his presidential bid, and garnered over 2.6 million views. This incident led to the widespread belief amongst viewers regarding Baswedan\u2019s linguistic prowess. \nThe manipulated video was created from genuine footage of Baswedan speaking Bahasa Indonesian at a July 2023 political event.\nSystem \ud83e\udd16\nHeyGen website\nOperator: Anies Baswedan\nDeveloper: HeyGen\nCountry: Indonesia\nSector: Politics\nPurpose: Manipulate public opinion\nTechnology: Deepfake - audio, video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Mis/disinformation\nTransparency: Governance\nFact check \ud83d\udea9\nAFP (2024). Indonesians misled by AI-generated video of presidential hopeful speaking Arabic\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.cnnindonesia.com/nasional/20230716193839-617-974131/isi-doa-anies-baswedan-di-hadapan-ribuan-kader-nasdem-di-gbk\nhttps://www.merdeka.com/politik/foto-pakai-baret-pidato-berapi-api-anies-baswedan-membakar-semangat-para-kader-nasdem-di-gelora-bung-karno-4997-mvk.html\nhttps://kaltim.antaranews.com/berita/188451/anies-indonesia-perlu-perubahan-serta-perbaikan\nRelated \ud83c\udf10\nSouth Korea presidential election candidate deepfakes\nPresident Balsonaro/Chapulin Colorado deepfake\nPage info\nType: Incident\nPublished: April 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/biden-robocall-advises-voters-skip-new-hampshire-primary-election", "content": "Biden 'robocall' advises voters to skip New Hampshire primary election\nOccurred: January 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn AI-powered spoof robocall falsely depicted President Joe Biden discouraging participation in the US\u2019 New Hampshire primary and urging voters \u2018save\u2019 their vote for the November 2024 US Presidential Election.\n\nA multistate investigation later revealed that the robocall had been commissioned by Steve Kramer, a Texas-based political consultant working for rival candidate Dean Philips. Kramer claimed the robocall was an act of civil disobedience calling attention to the dangers of AI in politics.\n\nAudio experts concluded the call was made using ElevenLabs. \n\nIn response to the incident, ElevenLabs CEO Mati Staniszewski stated that the company was 'dedicated to preventing the misuse of audio AI tools,' and said it would launch an investigation into misuse of their voice replication software.\n\u2795 March 2024. The League of Woman Voters sued Steve Kramer on behalf of three impacted US voters.\n\u2795 May 2024. Steve Kramer was indicted and fined USD 6 million by the US Federal Communications Commission. \n\u2795 August 2024. Lingo Telecom was fined USD 1 million for distributing the robocalls through \u201cspoofed\u201d phone numbers.\nSystem \ud83e\udd16\nElevenLabs AI voice generator\nOperator: Steve Kramer\nDeveloper: ElevenLabs; Lingo Telecom\nCountry: USA\nSector: Politics\nPurpose: Manipulate public ppinion, Election interference\nTechnology: Text-to-speech; Machine learning\nIssue: Mis/disinformation\nTransparency: Governance\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nLeague of Women Voters of New Hampshire et al v. Kramer et al\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.nbcnews.com/politics/2024-election/democratic-operative-admits-commissioning-fake-biden-robocall-used-ai-rcna140402\nhttps://www.nbcnews.com/politics/2024-election/new-hampshire-voters-sue-biden-deepfake-robocall-creators-rcna143662\nhttps://www.nbcnews.com/politics/politics-news/steve-kramer-admitted-deepfaking-bidens-voice-new-hampshire-primary-rcna153626\nhttps://www.wired.com/story/biden-robocall-deepfake-elevenlabs/\nhttps://news.sky.com/story/fake-ai-generated-joe-biden-robocall-tells-people-in-new-hampshire-not-to-vote-13054446 \nhttps://www.forbes.com/sites/mollybohannon/2024/02/06/biden-deepfake-robocall-urging-voters-to-skip-new-hampshire-primary-traced-to-texas-company/ \nRelated \ud83c\udf10\nPresident Biden calls for US draft deepfake\nDonald Trump hugs Dr Fauci deepfake\nPage info\nType: Incident\nPublished: April 2024\nLast updated; June 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/deepfake-audio-depicts-london-mayor-dismissing-remembrance-sunday", "content": "Deepfake audio depicts London Mayor dismissing Remembrance Sunday\nOccurred: November 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA number of AI-generated audio clips falsely depicting the Mayor of London denigrating the UK's 2023 Remembrance commemorations raised concerns about the use of synthetic disinformation in politics.\n\nOne voice clip impersonating Sadiq Khan disparaged Remembrance weekend and called for pro-Palestinian marches, planned for the same day, to take precedence. The recordings, which have been tracked to Tiktok account HJB News, were spread across multiple platforms before being removed. \n\nMr Khan told the BBC that 'The timing couldn't have been better if you're seeking to sow disharmony and cause problems,' and said the posts almost led to \u2018serious disorder\u2019. He also expressed concerns that current UK criminal law fails to cover this kind of scenario and is not \u2018fit for purpose\u2019.\nThe incident intensified apprehensions about the risk of artificial intelligence in politics. It came on the heels of another incident involving a deepfake audio recording of Labour party leader Keir Starmer reputedly abusing a member of his staff.\nSystem \ud83e\udd16\nUnknown\nOperator: HJB News\nDeveloper:\nCountry: UK\nSector: Politics\nPurpose: Damage Reputation, Manipulate public opinion\nTechnology: Topic recommendation system\nIssue: Mis/disinformation\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/uk-68146053\nhttps://news.sky.com/story/deepfake-audio-of-sadiq-khan-is-not-a-criminal-offence-met-police-says-13005420 \nhttps://www.bbc.com/news/av/uk-68284447\nhttps://www.standard.co.uk/news/london/sadiq-khan-deep-fake-video-ai-palestine-protest-armistice-remembrance-day-b1119605.html\nhttps://uk.news.yahoo.com/deepfakes-could-swing-close-uk-060000853.html \nRelated \ud83c\udf10\nDeepfake recording falsely depict British Opposition Leader abusing staff\nPresident Biden calls for US draft deepfake\nPage info\nType: Incident\nPublished: April 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/deepfake-news-anchor-accuses-us-of-bangladesh-election-interference", "content": "Deepfake news anchor accuses US of Bangladesh election interference\nOccurred: December 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA deepfake audio recording appearing to depict a news anchor criticising the US has been amongst a wave of synthetic disinformation spread in the lead up to the general election in Bangladesh.\n\nA report by the Financial Times indicated that entities aligned with Bangaldesh government had been using a variety of AI tools to craft deepfake videos and fabricate news segments with the aim of influencing public opinion. The content had been circulated by pro-government media outlets. \n\nThe incident underscored the challenges of controlling the use of AI for political purposes.\nSystem \ud83e\udd16\nHeyGen AI video generation\nOperator:\nDeveloper: HeyGen\nCountry: Bangladesh\nSector: Politics\nPurpose: Manipulate public opinion\nTechnology: Deepfake - audio, video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Mis/disinformation\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.ft.com/content/bd1bc5b4-f540-48f8-9cda-75c19e5ac69c\nhttps://www.dhakatribune.com/bangladesh/election/333961/report-bangladesh-grapples-with-ai-driven\nhttps://www.thedailystar.net/tech-startup/news/pro-government-outlets-spreading-ai-generated-misinformation-report-finds-3496791\nhttps://www.firstpost.com/tech/how-hackers-with-24-to-spare-are-hijacking-bangladeshs-2024-election-13503632.html\nhttps://www.axios.com/2023/09/01/personal-deepfake-ai-video-avatar\nRelated \ud83c\udf10\nDeepfake audio depicts London Mayor dismissing Remembrance Sunday\nDeepfake 'Pan Africanists' support Burkina Faso junta\nPage info\nType: Incident\nPublished: April 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/openai-bans-bot-impersonating-us-presidential-candidate", "content": "OpenAI bans bot impersonating US presidential candidate\nOccurred: January 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nOpenAI banned the account behind an AI bot using deepfake technology to imitate Dean Phillips, a candidate running against Joe Biden for the Democratic nomination. \n\nThe bot, hosted on dean.bot and created with ChatGPT, breached OpenAI's rules against political campaigning and impersonating individuals without their consent, according to a release from the company. \n\nSilicon Valley entrepreneurs Matt Krisiloff and Jed Somers, who also established a Super PAC named \u2018We Deserve Better\u2019 to support Phillips, were the key backers of the \u2018Dean bot\u2019. \n\nWhile OpenAI had stated that developers who use its technology must not engage in \u2018political campaigning or lobbying\u2019, the incident demonstrated the ease with which such policies can be ignorsed and  inflamed concerns over the accessibility of deepfake and other synthetic media tools in politics.\nSystem \ud83e\udd16\nhttps://dean.bot\nOperator: We Deserve Better (Super PAC)\nDeveloper: Delphi\nCountry: USA\nSector: Technology\nPurpose: Impersonate Politician\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Reputational damage\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.washingtonpost.com/technology/2024/01/20/openai-dean-phillips-ban-chatgpt/ \nhttps://www.theguardian.com/technology/2024/jan/22/openai-bans-bot-impersonating-us-presidential-candidate-dean-phillips \nhttps://www.reuters.com/technology/openai-suspends-bot-developer-congressman-dean-phillips-washington-post-2024-01-21/ \nhttps://www.esquire.com/news-politics/politics/a46462135/dean-phillips-ai-campaign/ \nRelated \ud83c\udf10\nBiden 'robocall' advises voters skip New Hampshire primary election\nDeepfake audio depicts London Mayor dismissing Remembrance Sunday\nPage info\nType: Incident\nPublished: April 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/sqa-algorithm-downgrades-124000-predicted-exam-results", "content": "SQA algorithm downgrades 124,000 predicted exam results\nOccurred: August 2020-\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn algorithm used by the Scottish Qualifications Authority (SQA) faced significant criticism due to its handling of exam predictions. \nDeveloped by the SQA, the algorithm was introduced due to restrictions imposed by the Scottish government during the COVD-19 pandemic which meant that in-person exams had to be scrapped.\nThe new system was widely criticised by students, parents, teachers, education experts and politicians after:\nThe SQA rejected nearly 124,000 recommendations from teachers for exam results, downgrading nearly a quarter of student submissions. Approximately 40 percent of students received scores lower than their teachers\u2019 predictions, impacting their university prospects. Only 7 percent of entries were revised upwards.\nStudents from working-class and disadvantaged backgrounds were found to have been disproportionately affected by severe downgrades compared to their privileged peers, thereby exacerbating socio-economic disparities. The SQA cut Higher passes for pupils from the most deprived areas by over 15 points, from the 85.1 percent pass rate recommended by teachers down to 69.9 percent. By comparison, grades were cut in only 9.8 percent of cases for the best off pupils.\n5 percent of schools did not receive student results. \nThe incident was seen to have led to a loss of confidence and trust in Scotland's education system and government. SQA was also taken to task for a perceived lack of transparency, including its refusal to publish details of the algorithm. Several SQA management figures including its Chief Executive, Ron Tuck, resigned or lost their jobs as a result of the crisis.\nSystem \ud83e\udd16\nScottish Qualifications Authority website\nWikipedia: 2000 SQA examinations controversy\nOperator: Education Scotland\nDeveloper: Scottish Qualifications Authority\nCountry: UK - Scotland\nSector: Education\nPurpose: Predict exam grades\nTechnology: Prediction algorithm\nIssue: Accuracy/reliability; Bias/discrimination - economic\nTransparency: Governance; Complaints/appeals; Legal\nResearch, advocacy \ud83e\uddee\nKippin S., Cairney P. The COVID-19 exams fiasco across the UK: four nations and two windows of opportunity\nInvestigations, assessments, audits \ud83e\uddd0\nOffice for Statistics Regulation (2021). Ensuring statistical models command public confidence (pdf)\nScottish Government (2020). National Qualifications experience 2020: rapid review\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.scotsman.com/education/algorithm-heart-scottish-exam-results-fiasco-not-analysed-2997422\nhttps://www.bbc.co.uk/news/uk-scotland-scotland-politics-53719477\nhttps://www.bbc.co.uk/news/uk-scotland-53636296\nhttps://www.hepi.ac.uk/2020/08/10/a-levels-2020-what-students-and-parents-need-to-know/\nhttps://centreforstatistics.maths.ed.ac.uk/cfs/news/data-algorithms-and-the-2020-national-assessment\nhttps://www.thenational.scot/news/18802660.revealed-poorest-scots-schools-hit-four-times-harder-sqa-results-scandal/\nhttps://www.publictechnology.net/articles/news/scottish-government-backtracks-exam-grading\nhttps://www.thetimes.co.uk/article/experts-call-for-abolition-of-arrogant-exam-body-5vs0cf22s\nhttps://www.newstatesman.com/politics/devolution/2020/08/scottish-government-has-got-it-badly-wrong-over-its-exam-results\nRelated \ud83c\udf10\nOfqual algorithm skews student grade predictions\nIreland Leaving Cert algorithm incorrectly grades students\nPage info\nType: Incident\nPublished: April 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ofqal-algorithm-skews-student-grade-predictions", "content": "Ofqual algorithm skews student grade predictions\nOccurred: August 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn algorithm developed by a UK government agency to predict student exam grades during the COVID-19 pandemic resulted in inaccurate and unfair results, and exacerbated socio-economic inequality.\nIn 2020, qualifications, exams, and tests regulator Ofqual devised a grades standardisation algorithm to address grade inflation and moderate teacher-predicted grades for A-level and GCSE qualifications in England. \nHowever, the algorithm faced significant criticism for a flawed design process, leading to inaccurate results; and inadequate accountability. It also resulted in multiple negative impacts, including: \nDowngraded results. Nearly 40 percent of students received exam scores that were lower than their teachers\u2019 predictions, jeopardising their chances of securing university spots.\nDisproportionate impact. The system disproportionately affected students from working-class and disadvantaged communities, who faced more severe downgrades compared to others.\nPrivate school advantage. The algorithm was found to have inflated the scores of students from private schools, potentially giving them an unfair advantage, exacerbating existing socio-economic disparities.\nAn Equality and Human Rights Commission report stated that the Ofqual\u2019s algorithm may have a lasting effect on young people from certain ethnic minority backgrounds, disabled pupils, and those with special educational needs who were already disproportionately disadvantaged.\nThe incident resulted in the resignation of Ofqual's chief regulator. It also led the opposition Labour Party to accuse the agency of violating the UK Apprenticeships, Skills, Children and Learning Act 2009.\nThe UK government later abandoned the algorithm in favour of teacher-led assessments.\nSystem \ud83e\udd16\nOfqual website\nOfqual exam grades algorithm Wikipedia profile\nOperator: Department of Education\nDeveloper: Ofqual\nCountry: UK\nSector: Education\nPurpose: Predict exam grades\nTechnology: Standardisation algorithm   \nIssue: Accuracy/reliability; Accountability; Bias/discrimination - economic; Robustness\nTransparency: Governance\nRegulation \u2696\ufe0f\nApprenticeships, Skills, Children and Learning Act 2009\nResearch, advocacy \ud83e\uddee\nMallett B. (2023). Reviewing the impact of OFQUAL\u2019s assessment \u2018algorithm\u2019 on racial inequalities \nOffice for Statistics Regulation (2021). Ensuring statistical models command public confidence. Learning lessons from the approach to developing models for awarding grades in the UK in 2020\nCentre for Progressive Policy (2021). Is the algorithm working for us? (pdf)\nInvestigations, assessments, audits \ud83e\uddd0\nHouse of Commons Education Committee (2020). Getting the grades they\u2019ve earned\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/uk-53810655\nhttps://www.bbc.co.uk/news/explainers-53807730\nhttps://www.theguardian.com/education/2020/aug/16/a-level-student-launches-legal-bid-against-ofqual\nhttps://unherd.com/2020/08/how-ofqual-failed-the-algorithm-test/\nhttps://www.telegraph.co.uk/education-and-careers/2020/08/20/ugly-truth-exams-ofqual-has-used-algorithm-since-2011-resigned/\nhttps://www.theguardian.com/education/2020/aug/07/a-level-result-predictions-to-be-downgraded-england\nhttps://www.dailymail.co.uk/news/article-8623713/Could-understand-level-grades-algorithm.html\nhttps://schoolsweek.co.uk/revealed-ofqual-warned-of-algorithm-legal-risk/\nhttps://www.wired.co.uk/article/alevel-exam-algorithm\nhttps://www.adalovelaceinstitute.org/blog/can-algorithms-ever-make-the-grade/\nhttps://www.independent.co.uk/news/education/education-news/ofqual-exam-results-algorithm-b1721658.html\nhttps://inews.co.uk/news/education/a-level-algorithm-what-ofqual-grades-how-work-results-2020-explained-581250\nhttps://www.thersa.org/blog/2020/08/exam-results-algorithm\nRelated \ud83c\udf10\nIreland Leaving Cert algorithm incorrectly grades students\nBacklash as IB algorithm lowers student grades\nPage info\nType: Incident\nPublished: April 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ireland-leaving-cert-algorithm-incorrectly-grades-students", "content": "Ireland Leaving Cert algorithm incorrectly grades students\nOccurred: September 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn algorithm wrongly calculated the exam grades of over 14,000 Leaving Certificate exam takers in Ireland, resulting in psychological distress for the students and their families, and a political storm.\nIn 2020, during the COVID-19 pandemic, Ireland\u2019s Department of Education decided to replace students\u2019 Leaving Certificate exams with an algorithmically generated score. As a result, over 6,000 students received lower grades than they should have, while approximately 8,000 students were awarded higher mark. \nThe error was identified by Canadian company Polymetrica International, which found a mistake in a single line out of 50,000 lines of code, affecting the grades of around 7,200 students. Students whose grades were incorrectly inflated were threatened with but ulimately not denied admission to Ireland's third-tier universities. \nThe incident highlighed the importance of rigorous testing and validation in higher-risk algorithmic systems.\nSystem \ud83e\udd16\nIreland Department of Education\nIreland Department of Education Wikipedia profile\nOperator: Department of Education\nDeveloper: Polymetrica International\nCountry: Ireland\nSector: Education\nPurpose: Predict exam results\nTechnology: Prediction algorithm\nIssue: Bias/discrimination - economic; Robustness\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nKelly A. (2021). A tale of two algorithms: The appeal and repeal of calculated grades systems in England and Ireland in 2020\nPetition (2020). Cancel the Leaving Cert - for good! \nPetition (2020). Scrap the Algorithm for Leaving Cert 2020 Predicted Grades\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.irishtimes.com/business/technology/leaving-cert-why-the-government-deserves-an-f-for-algorithms-1.4374801\nhttps://www.independent.ie/business/technology/explainer-why-has-one-line-of-computer-code-caused-such-disruption-to-the-leaving-cert-grades-39580586.html\nhttps://www.rte.ie/brainstorm/2020/1008/1170205-algorithms-leaving-cert-calculated-grades-data/\nhttps://www.irishtimes.com/opinion/leaving-cert-shows-we-will-have-to-learn-to-live-with-algorithms-1.4349728\nhttps://www.siliconrepublic.com/innovation/leaving-cert-2020-grading\nhttps://www.businesspost.ie/education/malcolm-byrne-algorithms-can-do-more-harm-than-good-if-not-checked-for-bias-ccd61c7b\nhttps://www.irishexaminer.com/news/arid-40034363.html\nhttps://www.thetimes.co.uk/article/leaving-cert-grades-boost-for-6-100-students-after-algorithm-correction-zkjnd80vv\nhttps://www.independent.ie/business/technology/explainer-why-has-one-line-of-computer-code-caused-such-disruption-to-the-leaving-cert-grades-39580586.html\nhttps://www.irishexaminer.com/news/arid-40059116.html\nhttps://www.bbc.co.uk/news/world-europe-53986003\nRelated \ud83c\udf10\nFlawed AI algorithms grade student essays in multiple US states\nBacklash as IB algorithm lowers student grades\nPage info\nType: Incident\nPublished: April 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/publishers-withdraw-over-120-gibberish-ai-generated-papers", "content": "Science publishers withdraw over 120 gibberish AI-generated papers\nOccurred: 2008-2013\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nOver 120 papers were withdrawn by top scientific publishers after they were discovered to have been automatically composed. \nPublishers Springer and IEEE withdrew more than 120 papers from their subscription services after French researcher Cyril Labb\u00e9 discovered they had been generated by a piece of AI-powered software called SCIgen, which randomly combines strings of words to produce fake computer science papers. \nSCIgen was created in 2005 by graduate students at the Massachusetts Institute of Technology to demonstrate that conferences would accept meaningless papers.\nThe incident raised concerns about the nature of the approval processes at the IEEE and Springer and the scientific publishing industry as a whole, and raised questions about the values of the scientific sector in general. The papers had made their way into more than 30 published conference proceedings between 2008 and 2013.\nSystem \ud83e\udd16\nSciGen\nOperator: IEEE; Springer\nDeveloper:  \nCountry: Global\nSector: Research/academia\nPurpose: Create scientific papers\nTechnology: Machine learning\nIssue: Mis/disinformation\nTransparency: Governance; Marketing\nResearch, advocacy \ud83e\uddee\nPublishers withdraw more than 120 gibberish papers\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.foxnews.com/science/over-100-published-science-journal-articles-just-gibberish\nhttps://www.theverge.com/2014/2/25/5445104/science-publishers-withdraw-more-than-120-computer-generated-papers\nhttps://www.scientificamerican.com/article/publishers-withdraw-more-than-120-gibberish-science-and-engineering-papers/\nhttps://blog.scielo.org/en/2014/03/31/in-the-beginning-it-was-just-plagiarism-now-its-computer-generated-fake-papers-as-well/\nhttps://www.ibtimes.com/fake-research-papers-how-did-more-120-gibberish-computer-generated-studies-get-published-1558725\nhttps://gizmodo.com/over-120-science-journal-papers-pulled-for-being-total-1534110496\nhttps://www.bostonglobe.com/business/2014/03/03/two-science-publishers-remove-computer-generated-gibberish-papers/xOLCWAoAAHoT5QBZPmKLeP/story.html\nRelated \ud83c\udf10\nScientific journals publish papers with AI-generated introductions\nChatGPT 'goes crazy', speaks gibberish\nPage info\nType: Incident\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/didi-global-fined-usd-1-2m-for-privacy-abuse", "content": "Didi Global fined USD 1.2m for privacy abuse\nOccurred: July 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nChinese ride-hailing company DiDi was fined 8 billion yuan (approximately USD 1.2 billion) for breaching the country\u2019s cybersecurity law, data security law, and personal information protection law. \nThe Cyberspace Administration of China (CAC) said in that DiDi had been found to have committed 16 law violations, including illegally obtaining 65 billion pieces of passenger information from their smartphones, including facial recognition, home address, phone information, age and employment details, and family relationships data.\nIn June 2021, the regulator had banned Didi from app stores in China and launched an investigation into its handling of customer data. The CAC's actions wiped tens of billions of dollars from DiDi's market capitalisation. The company later delisted from the New York Stock Exchange.\nDidi relies on AI to match customers and drivers, calculate rates and optimise routes.\nSystem \ud83e\udd16\nDiDi Global website\nDiDi Wikipedia profile\nOperator: Didi Global\nDeveloper: Didi Global\nCountry: China\nSector: Transport/logistics\nPurpose: Verify identity; Calculate fares; Optimise routes\nTechnology: Facial recognition; Machine learning\nIssue: Privacy\nTransparency: Governance\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nCyberspace Administration of China (2022). \u56fd\u5bb6\u4e92\u8054\u7f51\u4fe1\u606f\u529e\u516c\u5ba4\u6709\u5173\u8d1f\u8d23\u4eba\u5c31\u5bf9\u6ef4\u6ef4\u5168\u7403\u80a1\u4efd\u6709\u9650\u516c\u53f8\u4f9d\u6cd5\u4f5c\u51fa\u7f51\u7edc\u5b89\u5168\u5ba1\u67e5\u76f8\u5173\u884c\u653f\u5904\u7f5a\u7684\u51b3\u5b9a\u7b54\u8bb0\u8005\u95ee\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://apnews.com/article/technology-china-data-privacy-cheng-wei-d7c76a253e50d5b5aa8218eb1d3cebbd\nhttps://www.reuters.com/technology/china-fines-didi-global-12-bln-violating-data-security-laws-2022-07-21/\nhttps://edition.cnn.com/2022/07/21/economy/china-fines-didi-data-law-violation-intl-hnk/index.html\nhttps://www.infosecurity-magazine.com/news/china-fines-didi-data-security/\nhttps://www.theregister.com/2022/07/22/china_fines_didi/\nhttps://asia.nikkei.com/Opinion/Asian-companies-need-to-be-realistic-about-what-AI-can-do-for-them\nRelated \ud83c\udf10\nKohler, BMW, MaxMara China facial recognition\nTemple of Heaven Park uses facial recognition to stop toilet paper theft\nPage info\nType: Incident\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/nyc-ai-chatbot-tells-businesses-to-break-law", "content": "NYC AI chatbot tells businesses to break law\nOccurred: March 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nNew York City\u2019s Microsoft-powered AI chatbot was found to give wrong advice and inadvertently encouraged users to violate the law, according to reports.\nHailed by NYC mayor Eric Adams as 'a once-in-a-generation opportunity to more effectively deliver for New Yorkers,' the MyCity Chatbot was intended to provide information to residents about starting and operating businesses within the city. \nHowever, the bot was discovered to be providing 'dangerously' inaccurate and misleading information and advice, including: \nWrongly asserting that landlords do not need to accept tenants with Section 8 vouchers or those receiving rental assistance. In reality, New York City law prohibits discrimination based on source of income.\nIncorrectly claiming that it is legal to lock out tenants and that there are no restrictions on rent amounts for residential tenants. Actually, tenant lockouts are not permissible after 30 days of residence, and rent-stabilised units have specific regulations, while landlords of other private units have more flexibility in setting rent prices.\nFailing to recognise a 2020 City Council law requiring businesses to accept cash to prevent discrimination against unbanked customers.\nThe concerns prompted experts to complain that the chatbot offered misguided advice wth potential legal remifications, and that it was taken down.\nPer The Markup, the bot was still live a week later and churning out much the same misguided advice and encouraging illegal behaviour. Albeit a message flagged it as 'a beta product' that may provide \u201cinaccurate or incomplete\u201d responses to queries. \nSystem \ud83e\udd16\nMyCity Chatbot\nOperator: City of New York\nDeveloper: Microsoft\nCountry: USA\nSector: Govt - business\nPurpose: Provide business support\nTechnology: Chatbot\nIssue: Accuracy/reliability; Mis/disinformation; Safety\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://themarkup.org/news/2024/03/29/nycs-ai-chatbot-tells-businesses-to-break-the-law\nhttps://themarkup.org/news/2024/04/02/malfunctioning-nyc-ai-chatbot-still-active-despite-widespread-evidence-its-encouraging-illegal-behavior\nhttps://qz.com/nyc-ai-chatbot-false-illegal-business-advice-1851375066\nhttps://lifehacker.com/tech/nyc-ai-chatbot-hallucinating\nhttps://boingboing.net/2024/03/29/new-york-citys-ai-chatbot-advises-businesses-to-steal-tips-from-workers.html\nhttps://arstechnica.com/ai/2024/03/nycs-government-chatbot-is-lying-about-city-laws-and-regulations/\nhttps://www.theverge.com/2024/3/29/24115441/maybe-dont-listen-to-nycs-ai-bot\nhttps://futurism.com/nyc-chatbot-break-law\nRelated \ud83c\udf10\nMicrosoft Bing provides wrong election information\nAmazon Q hallucinates, leaks data\nPage info\nType: Incident\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/bbc-replaces-mamma-mia-star-sara-poyzer-with-ai", "content": "BBC replaces 'Mamma Mia!' star Sara Poyzer with AI\nOccurred: March 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe BBC stirred controversy by replacing Sara Poyzer, the acclaimed star of the musical Mamma Mia!, with an AI voice for an undisclosed production.\nPoyzer expressed her disappointment on social media after receiving an email from the BBC to a production company stating it would no longer require her services due to the approval of an AI-generated voice. \nThe BBC later said that they were producing a sensitive documentary featuring a contributor nearing the end of life, unable to speak and that, in collaboration with the contributor\u2019s family, it had decided to use AI for a brief section to recreate the contributor's voice.\nThe decision sparked heated discussions about the impact of AI in media production and the loss of creativity and expression in an increasingly automated world. Voice Squad, a voiceover agency working with Poyzer, criticised the move, emphasising that AI poses a threat to the industry by taking work away from experienced artists.\nThe Equity labor union had earlier launched a campaign called 'Stop AI Stealing the Show' in 2022, which the BBC supported. The broadcaster recently faced viewer complaints about its use of AI to promote Doctor Who.\nSystem \ud83e\udd16\nUnknown\nOperator: BBC\nDeveloper:  \nCountry: UK\nSector: Media/entertainment/sports/arts\nPurpose: Reproduce voice\nTechnology: Neural network; Machine learning\nIssue: Employment\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nEquity (2022). Stop AI Stealing the Show\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.dailymail.co.uk/tvshowbiz/article-13248865/bbc-defends-replace-mamma-mia-sara-poyzer-artificial-intelligence.html\nhttps://www.theguardian.com/tv-and-radio/2024/mar/28/resist-this-outrage-as-bbc-replace-mamma-mia-star-with-ai-voiceover\nhttps://www.foxnews.com/entertainment/mamma-mia-star-sara-poyzer-replaced-artificial-intelligence-bbc-recreates-voice-dying-person\nhttps://deadline.com/2024/03/mamma-mia-sara-poyzer-replaced-ai-bbc-show-1235870498/\nhttps://www.rollingstone.com/tv-movies/tv-movie-news/ai-debate-actress-dropped-bbc-project-artificially-generated-voice-1234995786/\nhttps://www.hollywoodreporter.com/business/business-news/ai-bbc-mamma-mia-star-sara-poyzer-replaced-1235861902/\nhttps://metro.co.uk/2024/03/28/mamma-mia-star-replaced-ai-bbc-role-grim-time-industry-20544788/\nRelated \ud83c\udf10\nBBC castigated for using generative AI to promote Dr Who\nIBM sells Greg Marston voice for commercial cloning\nPage info\nType: Incident\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chatgpt-linked-to-student-memory-loss-procastination", "content": "ChatGPT linked to student memory loss, procrastination\nOccurred: March 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA research study found that university students' use of ChatGPT results in memory loss, procrastination, and lower grades.\nHaving seen their students turn to ChatGPT during the course of their studies, researchers at Pakistan's National University of Computer and Emerging Sciences surveyed over 500 university students - ranging from undergraduates to doctoral candidates - over two phases, using self-reported evaluations.\nThe researchers discovered that students under a heavy academic workload and 'time pressure' were far more likely to use ChatGPT, and those who relied on the bot reported greater memory loss, more procrastination, and a drop in grades. Study co-author Muhammad Abhas also anecdotally mentioned over-reliance and a loss of critical thinking as impacts on the students.\nThe findings highlight concerns about the impact of generative AI systems on education.\nSystem \ud83e\udd16\nChatGPT chatbot\nOperator: OpenAI\nDeveloper: OpenAI\nCountry: Global\nSector: Education\nPurpose: Generate text\nTechnology: Chatbot\nIssue: Over-reliance\nTransparency: \nResearch, advocacy \ud83e\uddee\nMuhammad Abbas M., Jam F.A., Khan T.I. (2024). Is it harmful or helpful? Examining the causes and consequences of generative AI usage among university students\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://futurism.com/the-byte/chatgpt-memory-loss-procrastination\nhttps://www.psypost.org/chatgpt-linked-to-declining-academic-performance-and-memory-loss-in-new-study/\nhttps://uk.news.yahoo.com/chatgpt-academic-performance-memory-loss-113708486.html\nRelated \ud83c\udf10\nChatGPT falsely claims to write student essays\nNYC Dept of Education bans ChatGPT due to student learning concerns\nPage info\nType: Issue\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/bbc-castigated-for-using-generative-ai-to-promote-dr-who", "content": "BBC castigated for using generative AI to promote Dr Who \nOccurred: March 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe BBC said it will not be using AI to promote its Dr. Who sc-ifi show after receiving a flood of complaints about its use of generative AI in two promotional emails.\nAccording to the BBC, the AI used in the emails was 'part of a small trial' to use the technology. BBC Head of Media Inventory Davis Housden had earlier said that 'generative AI offers a great opportunity to speed up making the extra assets to get more experiments live for more content that we are trying to promote.' \nHowever, the trial resulted in a backlash from fans, who expressed strong disapproval about the impact of AI on the jobs of media industry professionals. Many writers, marketing and PR porfessionals have expressed concern that their jobs are under threat due to AI. \nThe BBC later said it had terminated the experiment and would not use AI again for Doctor Who promotions. \nSystem \ud83e\udd16\nUnknown\nDocuments \ud83d\udcc3\nBBC complaint response\nOperator: BBC\nDeveloper:  \nCountry: UK\nSector: Media/entertainment/sports/arts\nPurpose: Promote TV programme\nTechnology: Chatbot\nIssue: Employment\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://deadline.com/2024/03/bbc-doctor-who-ai-complaints-1235867333/\nhttps://www.theregister.com/2024/03/27/bbc_ends_doctor_who_ai_experiments\nhttps://www.theverge.com/2024/3/25/24111887/bbc-will-stop-using-ai-in-doctor-who-promos\nhttps://screenrant.com/doctor-who-ai-marketing-bbc-stop-response/\nhttps://gizmodo.com/doctor-who-ai-bbc-complaints-response-disney-plus-1851363443\nhttps://decrypt.co/223411/bbc-abandons-doctor-who-ai-promo-plans-after-viewer-complaints\nhttps://cybernews.com/news/bbc-ai-promotion-doctor-who/\nRelated \ud83c\udf10\nNiantic uses AI artwork to promote Pokemon Go\nDisney allegedly generates Loki season 2 poster with AI\nPage info\nType: Incident\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/italian-privacy-watchdog-opens-investigation-into-sora", "content": "Italian privacy watchdog opens investigation into Sora\nOccurred: March 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nItaly's data protection regulator announced it has launched an investigation into OpenAI's video generation tool Sora.\nThe Garante said it was concerned about 'the possible implications for the processing of personal data of users located in the European Union and in particular in Italy', and has asked OpenAI for 'clarifications' within twenty days.\nThe watchdog also said it wants to know if Sora will comply with European data protection rules if it is released in the EU. \nUnveiled in February 2024, Sora can create realistic videos of up to a minute long via simple textual user prompts. When shown, the model drew praise as well as concerns about possible copyright abuse and its impact on jobs. \nSystem \ud83e\udd16\nSora video generator\nOperator: OpenAI\nDeveloper: OpenAI\nCountry: Italy\nSector: Media/entertainment/sports/arts\nPurpose: Generate video\nTechnology: Text-to-video; Machine learning\nIssue: Privacy\nTransparency: Governance\nRegulation \u2696\ufe0f\nEU General Data Protection Regulation\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nGarante (2021). Artificial intelligence: the Italian Data Protection Authority opens an investigation into OpenAI\u2019s \u2018Sora\u2019\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reuters.com/technology/italys-data-watchdog-looks-into-open-ai-tool-that-turns-text-into-video-2024-03-08/\nhttps://techxplore.com/news/2024-03-italy-probe-openai-video-tool.html\nhttps://cybernews.com/news/italy-opens-up-probe-into-openais-sora/\nhttps://www.dataguidance.com/news/italy-garante-launches-investigation-openais-sora\nhttps://www.insurancejournal.com/news/international/2024/03/11/764275.htm\nRelated \ud83c\udf10\nItaly bans ChatGPT over privacy concerns\nReplika hit with data ban in Italy over child safety\nPage info\nType: Incident\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/leonardo-ai-generates-celebrity-non-consensual-porn-images", "content": "Leonardo AI generates celebrity non-consensual porn images\nOccurred: March 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAI platform Leonardo was accused of letting users easily create explicit, non-consensual images of celebrities.\nFunded by Samsung, Leonardo is an image generation tool that enables users to use an array of user-generated Stable Diffusion text-to-image AI models, each of which is programmed to generate specific types of images - including the ability to reproduce the appearance of a specific individual. \nAccording to 404 Media, whilst Leonardo's terms of service state that users cannot 'generate content that includes impersonations of any real person or falsely portrays an individual in a misleading or defamatory way', Leonardo's guardrails can easily be bypassed by slightly misspelling celebrity names, and using sexually suggestive terms for image description.\nThe ease with which users can instantly generate images on the site using its models highlights the platform\u2019s versatility and the ease with which it can be misused.\nSystem \ud83e\udd16\nLeonardo AI website\nOperator: Leonardo AI\nDeveloper: Leonardo AI\nCountry: Global\nSector: Media/entertainment/sports/arts\nPurpose: Generate art\nTechnology: Text-to-image; Machine learning\nIssue: Privacy; Safety\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.404media.co/samsung-backed-ai-image-generator-produces-nonconsensual-porn\nhttps://www.newsbytesapp.com/news/science/samsung-backed-ai-image-generator-producing-non-consensual-adult-image/story\nhttps://www.thestreet.com/technology/theres-ai-generated-porn-for-sale-on-etsy\nRelated \ud83c\udf10\nCivitAI nonconsensual AI pornography\nMistral generates ethnic cleansing, murder instructions\nPage info\nType: Incident\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/israel-facial-recognition-system-misidentifies-innocent-gazans", "content": "Israel facial recognition system misidentifies innocent Gazans\nOccurred: March 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA facial recognition system used by Israel in Gaza misidentified scores of innocent Palestinians, resulting in their abduction, interrogration, and physical beatings.\nUnnamed Israeli officials told the New York Times that Israel was using facial recognition technology to identify Hamas operatives and Israeli captives in Gaza. Palestinian prisoners were asked to name people from their communities who they believed were part of Hamas. Israel would then search for those people, hoping they would yield more intelligence, according to the NYT.\nDeveloped by Israeli company Corsight, the system catalogues the faces of unaware Palestinians without their consent and has resulted in the misidentification, abduction and beatings of innocent Palestinians, including the poet Mosab Abu Toha. Corsight CEO Robert Watts recently claimed that the company's technology is 'Built with Privacy, Ethics and world leading Bias reduction at its core'. \nThe discovery underscores legal, ethical, and practical concerns about the use of AI for military and extra-judicial purposes. \nSystem \ud83e\udd16\nForsight website\nCorsight website\nOperator: Israel Defense Forces (IDF)\nDeveloper: Corsight\nCountry: Israel/Palestine\nSector: Govt - defence\nPurpose: Identify terrorists\nTechnology: Facial recognition\nIssue: Ethics/values; Human/civil rights; Privacy\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.nytimes.com/2024/03/27/technology/israel-facial-recognition-gaza.html\nhttps://www.israelhayom.com/2024/03/27/ai-israe/\nhttps://www.ynetnews.com/article/r1vzwtwkc\nhttps://thecradle.co/articles/israel-uses-ai-facial-recognition-to-abduct-scores-of-gazans\nhttps://www.nytimes.com/2024/01/23/world/middleeast/israel-gaza-palestinian-detainees.html\nhttps://www.newyorker.com/magazine/2024/01/01/a-palestinian-poets-perilous-journey-out-of-gaza\nRelated \ud83c\udf10\nHebron Palestinian facial recognition surveillance\nIsrael uses Habsora 24 hour automated 'target factory' against Palestinians\nPage info\nType: Incident\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/google-sge-recommends-malware-fraud-sites", "content": "Google SGE recommends malware, fraud sites\nOccurred: March 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGoogle's AI-powered Search Generative Experience (SGE) is reportedly recommending malicious websites, including ones containing malware and scams.\nSEO consultant Lily Ray noticed that SGE was suggesting potentially harmful sites as part of its conversational responses, making it easier for users to fall for scams.\nThese sites often use the same .online domain, HTML templates, and perform redirects, indicating they are part of an SEO poisoning campaign. Clicking on these links leads users through a series of redirects until they reach a scam site comprising fake captchas, deceptive YouTube pages and fake giveaways.\nGoogle claims to continuously update its systems and algorithms to combat spam. However, the presence of low-quality and potentially harmful sites in Google\u2019s search results raises questions about the efficacy of the company\u2019s spam-fighting measures and increases the potential for fraud. \nIt is also seen to bring into question the reliability of AI-generated content in general.\nSystem \ud83e\udd16\nGoogle SearchLabs\nOperator: Alphabet/Google\nDeveloper: Alphabet/Google\nCountry: Global\nSector: Technology\nPurpose: Generate search results\nTechnology: Machine learning\nIssue: Safety; Security\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://twitter.com/lilyraynyc/status/1771217301863289140\nhttps://www.bleepingcomputer.com/news/google/googles-new-ai-search-results-promotes-sites-pushing-malware-scams/#google_vignette\nhttps://futurism.com/the-byte/google-ai-search-spam-malware\nhttps://www.forbes.com/sites/zakdoffman/2024/03/25/google-update-warning-chrome-windows-11-android-iphone/\nhttps://www.theregister.com/2024/03/25/googles_aipowered_search_results_are/\nhttps://www.deccanherald.com/technology/after-gemini-gaffes-googles-ai-search-engine-results-now-direct-users-to-shady-websites-2952460\nhttps://hothardware.com/news/google-sge-pushing-malicious-links\nRelated \ud83c\udf10\nGoogle AI bots expouse slavery, fascism\nSynthetic Tiananmen tank man dominates Google Search\nPage info\nType: Incident\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/researchers-conclude-shotspotter-is-too-unreliable-for-routine-use", "content": "Researchers conclude ShotSpotter is 'too unreliable for routine use'\nOccurred: May 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe ShotSpotter gunfire detection system sent Chicago police officers on over 40,000 'dead-end deployments' and is too unreliable for routine use, according to researchers at the MacArthur Justice Center.\nOf 46,743 ShotSpotter alerts generated July 2019-April 2021, only 5,114 resulted in officers filing a report 'likely involving a gun,' according to the study\u2019s analysis of records obtained from city\u2019s Office of Emergency Management and Communications. SoundThinking claims ShotSpotter is over 97 percent accurate.\nThe study was the basis for an amicus brief filed on behalf of Chicago-based community groups that asked a Cook County judge to question whether ShotSpotter reports should be allowed as evidence in a murder case.\nThe findings raised serious questions about the accuracy and reliability of the ShotSpotter system. The lack of accuracy, together with the system's use in predominantly Black and Brown communities, fed 'racialized patterns of overpolicing,' attorneys argued.\nSystem \ud83e\udd16\nShotSpotter gunshot detection\nDocuments \ud83d\udcc3\nSoundThinking. Why the MacArthur Justice Center Report is Wrong\nOperator: Chicago Police Department\nDeveloper: SoundThinking/ShotSpotter\nCountry: USA\nSector: Govt - police\nPurpose: Detect gunfire\nTechnology: Deep learning; Neural network; Machine learning\nIssue: Accuracy/reliability; Bias/discrimination - race, ethnicity\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nMacArthur Justice Center. ShotSpotter Generated Over 40,000 Dead-End Police Deployments in Chicago in 21 Months, According to New Study\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://chicago.suntimes.com/crime/2021/5/3/22416397/chicago-gunshot-detection-system-dead-end-deployments-macarthur-justice-center\nhttps://wgntv.com/news/chicago-news/study-shotspotter-technology-led-to-over-40k-dead-end-cpd-deployments-in-21-months/\nhttps://www.cbsnews.com/chicago/news/chicago-groups-shotspotter-unreliable/\nhttps://blockclubchicago.org/2021/05/05/85-percent-of-shotspotter-alerts-are-dead-ends-but-they-coach-officers-to-think-theyre-responding-to-dangerous-situations-study-finds/\nRelated \ud83c\udf10\nMichael Williams gunshot detection wrongful arrest\nChicago watchdog concludes ShotSpotter 'rarely finds gun crime'\nPage info\nType: Incident\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/godfrey-lao-coco-lee-ai-resurrections-spark-criticism", "content": "Godfrey Lao, Coco Lee AI resurrections spark criticism\nOccurred: March 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe use of AI by a Chinese vlogger to recreate Taiwanese Canadian actor Godfrey Gao and Chinese American singer Coco Lee sparked outrage among Chinese social media users.  \nVideos showing Coco Lee and Godfrey Gao addressing their viewers. Lee\u2019s AI avatar told her fans, 'Through this video, I have the opportunity to be reunited with all of you. Since the moment I left this world, I have always been able to sense your endless love and support.'\nWhen questioned about infringing on the late celebrities\u2019 rights, the vlogger reportedly claimed that what they did was not imitation but 'an expression of love for Coco,' adding that the videos were AI generated and free. However, the people behind Lee's videos were purportedly offering services to digitally bring back the deceased using AI for a fee of 588 yuan (USD 83).\nThe videos prompted fans to request the vlogger to stop causing distress to the celebrities' families, and sparked discussions about the legal implications of using a deceased person\u2019s name, image, voice and reputational likeness in an AI video, for commercial purposes or otherwise. \nLee committed suicide in 2023, and Gao died aged 35 in 2019.\nSystem \ud83e\udd16\nUnknown\nOperator:\nDeveloper:  \nCountry: China\nSector: Media/entertainment/sports/arts\nPurpose: Recreate dead people\nTechnology: Machine learning\nIssue: Ethics/values; Personality rights\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://weibo.com/6004281123/O4XQRmHsC\nhttps://www.sixthtone.com/news/1014829\nhttps://www.asiaone.com/entertainment/revealing-scars-families-late-actors-godfrey-gao-qiao-renliang-upset-ai-replica-deepfake-coco-lee\nhttps://nextshark.com/chinese-vlogger-outrage-ai-coco-lee-godfrey-gao\nhttps://bendi.news/wxnews/cltwzhamu0023asqrm9losh24\nhttps://www.todayonline.com/8days/chinese-content-creator-uses-ai-resurrect-coco-lee-godfrey-gao-calls-it-expression-love-2384281\nRelated \ud83c\udf10\nKim Kwang Seok voice recreation prompts concerns\nAnthony Bourdain voice deepfake results in backlash\nPage info\nType: Incident\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/argentina-opens-investigation-into-worldcoin", "content": "Argentina opens investigation into Worldcoin\nOccurred: August 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nArgentina's privacy regulator the Agency for Access to Public Information (AAIP) opened an investigation into Worldcoin's data collection practices. \nThe investigation will 'thoroughly analyse' the processes and practices related to data collection, storage, and usage, and address identified problems, according to the AAIP. \nWorldcoin's activitiess in Argentina had caused 'public notoriety due to the procedure of scanning the face and iris of numerous people in exchange for economic compensation in different parts of the Autonomous City of Buenos Aires and the provinces of Buenos Aires, C\u00f3rdoba, Mendoza and Black river,' the AAIP said.\nThe move is emblematic of a broader global trend where nations are becoming more vigilant about the the data collection and user privacy of Worldcoin and other projects.\nSystem \ud83e\udd16\nWorldcoin iris biometrics\nOperator: Tools for Humanity/Worldcoin\nDeveloper: Tools for Humanity/Worldcoin\nCountry: Argentina\nSector: Banking/financial services\nPurpose: Develop digital identity\nTechnology: Iris scanning; Facial detection; Vital signs detection; Blockchain; Virtual currency\nIssue: Privacy; Security\nTransparency: Governance\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nLa Agencia de Acceso a la Informaci\u00f3n P\u00fablica (AAIP) (2023). La AAIP investiga el tratamiento de datos personales de Worldcoin en Argentina\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.coindesk.com/policy/2023/08/10/worldcoin-regulatory-scrutiny-grows-as-argentina-opens-investigation/ \nhttps://www.ibtimes.com/worldcoin-regulatory-scrutiny-grows-argentina-launches-investigation-over-data-privacy-3708319\nhttps://www.theblock.co/post/244289/argentina-says-its-investigating-worldcoin-over-data-privacy\nhttps://decrypt.co/news-explorer?pinned=263747&title=argentina-launches-investigation-into-worldcoin\nhttps://www.dataguidance.com/news/argentina-aaip-provides-updates-investigation-handling\nhttps://dailycoin.com/argentina-investigates-worldcoin-amid-mounting-regulatory-pressure/\nRelated \ud83c\udf10\nSouth Korea privacy watchdog investigates Worldcoin\nHong Kong privacy watchdog probes Worldcoin\nPage info\nType: Incident\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/south-korea-privacy-watchdog-investigates-worldcoin", "content": "South Korea privacy watchdog investigates Worldcoin\nOccurred: February 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nSouth Korea's Personal Information Protection Committee (PIPC) initiated an investigation into Worldcoin following a series of complaints about the manner in which the company was collecting sensitive data from users. \nThe PIPC announced its investigation into Worldcoin would look into 'the overall collection and processing of sensitive information and overseas transfer of personal information under the Personal Information Protection Act.' \nIt also noted that Worldcoin and \u2018affiliates\u2019 are reportedly collecting facial and iris data in 10 locations across South Korea, and that it would take action 'if vilations are found.'\nThe investigation comes in the wake of enforcement actions against the biometric cryptocurrency project in Hong Kong and Kenya. \nSystem \ud83e\udd16\nWorldcoin iris biometrics\nOperator: Tools for Humanity/Worldcoin  \nDeveloper: Tools for Humanity/Worldcoin\nCountry: S Korea\nSector: Banking/financial services\nPurpose: Develop digital identity\nTechnology: Iris scanning; Facial detection; Vital signs detection; Blockchain; Virtual currency\nIssue: Privacy; Security\nTransparency: Governance\nRegulation \u2696\ufe0f\nPersonal Information Protection Act\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nhttps://www.pipc.go.kr/np/cop/bbs/selectBoardArticle.do?bbsId=BS074&mCode=C020010000&nttId=9963#LINK\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://crypt-news.com/south-koreas-regulator-launches-investigation-into-worldcoin-amid-privacy-concerns/\nhttps://news.bitcoin.com/south-korea-launches-investigation-into-worldcoins-data-collection-practices/\nhttps://iapp.org/news/a/south-korea-announces-investigation-into-worldcoin/\nhttps://protos.com/south-korea-vows-to-take-action-if-worldcoin-violations-are-confirmed/\nhttps://www.biometricupdate.com/202403/south-korea-privacy-regulator-to-investigate-worldcoins-biometrics-handling\nhttps://readwrite.com/south-korea-launches-investigation-into-worldcoin/\nRelated \ud83c\udf10\nHong Kong privacy watchdog probes Worldcoin\nWorldcoin suspended in Kenya for privacy abuse\nPage info\nType: Incident\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/hong-kong-privacy-watchdog-probes-worldcoin", "content": "Hong Kong privacy watchdog probes Worldcoin\nOccurred: January 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nHong Kong's privacy regulator announced an investigation into Worldcoin's local operations due to 'serious risks to personal data privacy'.\nRepresentatives of the Hong Kong Office of the Privacy Commissioner for Personal Data (PCDP) executed warrants on six Worldcoin offices in Hong Kong. Some of the offices had been used to collect iris scans. \nThe commissioner's office expressed concern Worldcoin's collection and processing of iris scans may violate the Personal Data (Privacy) Ordinance and could potentially involve the abuse of personal information.\nSpecifically, the PCDP said it was looking into Worldcoin's data collection activities, whether the organisation had obtained meaningful consent from users, and how people's personal data was being used.\nSimilar investigations into Worldcoin have been launched in other jurisdictions. \nSystem \ud83e\udd16\nWorldcoin iris biometrics\nOperator: Tools for Humanity/Worldcoin\nDeveloper: Tools for Humanity/Worldcoin\nCountry: Hong Kong\nSector: Banking/financial services\nPurpose: Develop digital identity\nTechnology: Iris scanning; Facial detection; Vital signs detection; Blockchain; Virtual currency\nIssue: Privacy\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.scmp.com/news/hong-kong/law-and-crime/article/3250480/hong-kong-eye-scan-cryptocurrency-scheme-probed-citys-privacy-watchdog\nhttps://www.benzinga.com/markets/asia/24/01/36871974/sam-altmans-worldcoin-under-investigation-in-hong-kong-wld-down-9-dystopian-nightmare-says-trader\nhttps://www.biometricupdate.com/202401/worldcoins-hong-kong-premises-raided-in-proactive-look-into-breach-of-data-privacy-laws\nhttps://cointelegraph.com/news/hong-kong-investigate-worldcoin-privacy\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nOffice of the Privacy Commissioner for Personal Data (2024). Privacy Commissioner Urges the Public to Stay Vigilant about the Worldcoin Project And Not to Disclose Biometric Data Arbitrarily\nRelated \ud83c\udf10\nWorldcoin suspended in Kenya for privacy abuse\nSpain suspends Worldcoin over privacy concerns\nPage info\nType: Incident\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/worldcoin-field-testing-methods-draw-controversy", "content": "Worldcoin field-testing methods draw controversy\nOccurred: August 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nWorldcoin raised concerns among regulators and privacy advocates around the world due to an alleged lack of transparency regarding the methods the organisation is using to collect people\u2019s data.\nIn April 2022, internal documents shared with Technology Review and Buzzfeed News revealed that people signing up for Worldcoin iris scans were complaining about unclear, misleading, and unethical marketing practices. \nSpecifically, it was alleged that the organisation was collecting more personal data than it acknowledged, failing to obtain meaningful informed consent, and had not been declaring that it was using user data to train its artificial intelligence models.\nFurthermore, Orb operators in a number of countries complained that they had not been paid for users they had signed-up, and faced harassment and arrest under local laws. \nThe controversy was seen to raise serious questions about Worldcoin's governance and transparency, and it\u2019s approach to privacy and data collection.\nSystem \ud83e\udd16\nWorldcoin iris biometrics\nDocuments \nWorldcoin (2022). Responses to MIT Technology Review\nOperator: Tools for Humanity/Worldcoin  \nDeveloper: Tools for Humanity/Worldcoin\nCountry: Global\nSector: Banking/financial services\nPurpose: Develop digital identity\nTechnology: Iris scanning; Facial detection; Vital signs detection; Blockchain; Virtual currency\nIssue: Ethics/values; Privacy\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.buzzfeednews.com/article/richardnieva/worldcoin-crypto-eyeball-scanning-orb-problems\nhttps://www.technologyreview.com/2022/04/06/1048981/worldcoin-cryptocurrency-biometrics-web3/\nhttps://cointelegraph.com/news/worldcoin-controversy-explained-in-latest-cointelegraph-report\nhttps://time.com/6300522/worldcoin-sam-altman/\nRelated \ud83c\udf10\nWorldcoin suspended in Kenya for privacy abuse\nSpain suspends Worldcoin over privacy concerns\nPage info\nType: Incident\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/russian-state-tv-deepfake-blames-ukraine-for-crocus-city-hall-attack", "content": "Russian state TV deepfake blames Ukraine for Crocus City Hall attack\nOccurred: March 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Russian TV station broadcast a fake synthetic video of Ukrainian officials, including top security advisor Oleksiy Danilov, admitting to orchestrating the Crocus City Hall terror attack in Moscow. \nGunmen opened fire at the Crocus City Hall, a popular concert hall on the outskirts of Moscow, killing over 100 people and injuring many more. The deadly attack, which came shortly after the re-election of Vladimir Putin as Russian president, had reputedly been flagged by US intelligence to Russian authorities.\nISIS Khorasan officially claimed responsibility for the attack; the deepfake appeared part of a larger effort to defect blame on Ukraine, with which Russia is at war. Putin later appeared to accuse Ukraine of orchestrating the attack in a TV statement. \nThe video falsely quoted Danilov as saying that Ukraine was behind the attack.. According to BBC Verify reporter Shayan Sardarizadeh, the video mashed together AI-generated audio from recent interviews with two Ukrainian officials, including Danilov. \nSystem \ud83e\udd16\nNTV website\nNTV Wikipedia profile\nOperator: NTV\nDeveloper:  \nCountry: Russia, Ukraine\nSector: Govt - defence; Politics\nPurpose: Defect blame\nTechnology: Deepfake - video, audio\nIssue: Mis/disinformation\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.pravda.com.ua/eng/news/2024/03/23/7447789/\nhttps://news.online.ua/en/terrorist-attack-in-crocus-city-hall-rospropaganda-showed-a-fake-video-with-danilov-875412/\nhttps://www.motherjones.com/politics/2024/03/deepfake-video-terror-moscow-putin-ukraine-claims/\nhttps://www.thesun.co.uk/news/26879332/russian-tv-deepfake-ukraine-moscow-attack/\nhttps://www.bbc.co.uk/news/live/world-68642036\nRelated \ud83c\udf10\nDeepfake President Zelenskyy instructs army to surrender\nDeepfake soldier posts fake Ukraine war stories\nPage info\nType: Incident\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/late-night-with-the-devil-ai-interstitials-provoke-backlash", "content": "Late Night with the Devil AI interstitials provoke backlash\nOccurred: March 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nHorror film 'Late Night with the Devil' sparked controversy due to its use of artificial intelligence for creating three still images that appear as brief interstitials. \nIntended to enhance the 1970s aesthetic of the film, some viewers expressed disappointment and hostility towards the use of AI which, they said, should be avoided when humans are able to do the work. Others pointed out the poor quality of the interstitials, and argued that humans can do better work. \nSome called for a boycott of the film, which was made in 2022. US actors' union SAG-AFTRA had striked in 2023 over the loss of jobs in the US film and media industry with the Alliance of Motion Picture and Television Producers (AMPTP). The union argued the use of artificial intelligence by film and TV studios was partly to blame.\nThe directors, Cameron and Colin Cairnes, clarified that they used AI for the three images, which were further edited. 'We feel incredibly fortunate to have had such a talented and passionate cast, crew and producing team go above and beyond to help bring this film to life. We can\u2019t wait for everyone to see it for themselves this weekend,' they said.\nSystem \ud83e\udd16\nLate Night with the Devil website\nOperator:\nDeveloper: David Dastmalchian\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Generate images\nTechnology: Machine learning\nIssue: Employment\nTransparency: Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.forbes.com/sites/timlammers/2024/03/22/use-of-ai-in-late-night-with-the-devil-draws-ire-of-horror-films-critics/?sh=688f226c7069\nhttps://mashable.com/article/late-night-with-the-devil-ai-images\nhttps://variety.com/2024/film/news/late-night-with-the-devil-ai-images-clarification-1235947599/\nhttps://www.gamesradar.com/late-night-with-the-devil-mike-flanagan-defends/\nhttps://www.indiewire.com/news/general-news/late-night-with-the-devil-ai-backlash-explained-1234966938/\nhttps://ew.com/late-night-with-the-devil-ai-controversy-directors-respond-8619392\nhttps://screenrant.com/late-night-with-the-devil-ai-controversy-explained/\nhttps://venturebeat.com/ai/horror-movie-late-night-with-the-devil-faces-boycott-over-ai-generated-art/\nRelated \ud83c\udf10\nNetflix 'Dog and Boy' film AI backgrounds\nDisney allegedly generates Loki season 2 poster with AI\nPage info\nType: Incident\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/university-of-michigan-partner-sells-student-data-for-ai-training", "content": "University of Michigan partner sells student data for AI training\nOccurred: February 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA partner to the University of Michigan tried to sell student data to organisations training AI models, causing controversy about the possible abuse of student privacy.\nCatalyst Research Alliance, which claims to partner the University of Michigan (UM) and North Carolina State University, was found to be hawking licenses for academic data from 65 speech events, 85 hours of audio recordings and 829 student papers for USD 25,000.  \nThe University later clarified that the papers and recordings referenced in the pitches had been voluntarily supplied by students, with signed consent, from two research studies that spanned from 1997 to 2000 and 2006 to 2007. UM also said it had severed ties with Catalyst Research Alliance.\nThe data had been freely available to academics to help improve writing and articulation in education. The fracas indicated poor governance and ethics at Catalyst Research Alliance. \nSystem \ud83e\udd16\nUniversity of Michigan website\nCatalyst Research Alliance website\nOperator: Catalyst Research Alliance\nDeveloper: University of Michigan\nCountry: USA\nSector: Education\nPurpose: Train AI models\nTechnology: Database/dataset\nIssue: Ethics/values; Privacy\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.thedailybeast.com/how-the-university-of-michigan-is-selling-student-data-to-train-ai\nhttps://www.insidehighered.com/news/quick-takes/2024/02/19/u-michigan-halts-offers-sell-student-data-train-ai\nhttps://gizmodo.com/university-of-michigan-sell-student-data-ai-companies-1851261663\nhttps://www.bollyinside.com/news/technology/university-of-michigan-denies-selling-student-data-to-ai-companies-calls-it-a-misunderstanding/\nRelated \ud83c\udf10\nData breach reveals data of 400,000+ ProctorU users\nNHS Digital/iProov facial recognition data sharing\nPage info\nType: Incident\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/google-fined-for-training-gemini-on-news-content-without-consent", "content": "Google fined for training Gemini on news content without consent\nOccurred: March 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nFrench authorities fined Google EUR 250 (USD 270) million for using articles from news publishers to train its AI chatbot, Gemini, without obtaining prior permission. \nFrance's Autorit\u00e9 de la Concurrence competition authority announced it had issued the fine on the basis that Google had used content from press agencies and media companies to train its Bard (since renamed Gemini) chatbot without notifying the publishers or the French authority, and as it had failed to give publishers an opt-out of the use of their content.\nThis fine is the latest part of an ongoing dispute between the French competition authority and Google over monopoly-related issues. \nIn 2020, the Autorit\u00e9 had found that Google had failed to negotiate in good faith with media companies on fair compensation, and had instructed the tech firm 'to conduct negotiations in good faith with publishers and news agencies on the remuneration for the re-use of their protected contents' - to which Google had agreed.\nSystem \ud83e\udd16\nGemini chatbot\nDocuments \ud83d\udcc3\nGoogle France. Un accord afin de r\u00e9gler un diff\u00e9rend qui dure depuis trop longtemps\nOperator: Alphabet/Google  \nDeveloper: Alphabet/Google\nCountry: France\nSector: Media/entertainment/sports/arts\nPurpose: Train AI models\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Competition/collusion; Copyright\nTransparency: Governance\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nAutorit\u00e9 de la Concurrence (2024). Related rights: the Autorit\u00e9 fines Google \u20ac250 million for non-compliance with some of its commitments made in June 2022\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.cnbc.com/2024/03/20/french-competition-watchdog-hits-google-with-250-million-euro-fine.html\nhttps://www.reuters.com/technology/french-competition-watchdog-hits-google-with-250-mln-euro-fine-2024-03-20/\nhttps://abcnews.go.com/Business/wireStory/french-regulators-fine-google-272-million-dispute-news-108303070\nhttps://www.bloomberg.com/news/articles/2024-03-20/google-fined-250-million-in-french-clash-with-news-publishers\nhttps://techcrunch.com/2024/03/20/google-hit-with-270m-fine-in-france-as-authority-finds-news-publishers-data-was-used-for-gemini/\nhttps://blog.google/intl/fr-fr/nouvelles-de-lentreprise/chez-google/accord-regler-differend-qui-dure-depuis-trop-longtemps/\nRelated \ud83c\udf10\nGoogle sued for AI data scraping\nGoogle C4 database\nPage info\nType: Incident\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/lego-uses-non-licensed-ip-in-ai-generated-toy-promotion", "content": "LEGO uses non-licensed IP in AI-generated toy promotion\nOccurred: March 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nUse of AI-generated artwork used to promote LEGO's Ninjago toy line incorporated elements of intellectual property for which it lacked a license, resulting in the company apologising.\nThe company had posted a series of AI-generated images of Ninjago characters on its website as part of an online quiz. Among these images was a character sporting a headband from the Japanese manga Naruto, a property that LEGO had not officially licensed. \nThe offending images were taken down, but only after people on social media had called out the toy maker and questioned why it was using generative AI rather than human beings to create its marketing materials. \nThe incident raised questions about AI governance at LEGO, and highlighted the need for organisations to find an appropriate balance between human creativity and automation.\nSystem \ud83e\udd16\nUnknown\nOperator: LEGO\nDeveloper:  \nCountry: Global\nSector: Consumer goods\nPurpose: Generate images\nTechnology: Text-to-image\nIssue: Copyright; Employment\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.creativebloq.com/news/lego-admits-to-using-ai\nhttps://aibusiness.com/verticals/lego-uses-ai-generated-artwork-apologizes-for-unlicensed-use\nhttps://cybernews.com/news/lego-ninjago-ai-images/\nhttps://9to5toys.com/2024/03/15/lego-ai-apology/\nhttps://adage.com/article/marketing-news-strategy/lego-regrets-posting-ai-generated-images/2547496\nhttps://www.cyberdaily.au/digital-transformation/10340-lego-slammed-for-ai-image-generation-for-official-product\nRelated \ud83c\udf10\nAI generates visuals for Wizards of the Coast marketing promotion\nWacom AI-generated Chinese New Year promotion backfires\nPage info\nType: Incident\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/italian-pm-seeks-damages-over-deepfake-porn-videos", "content": "Italian PM seeks damages over deepfake porn videos \nOccurred: 2020-2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nItalian Prime Minister Georgia Meloni sued two Italians for EUR 100,000 for creating and sharing false deepfake porn videos of her. \nMeloni accused a 40-year-old man and his 73-year-old father in Sardinia of superimposing Meloni's face onto someone else's body and sharing the videos on a US porn site. The videos were created in 2022, before Meloni became Prime Minister.\nThe video raised significant concerns about the use of deepfakes in Italian politics and elsewhere, and to create disinformation.\nIt also highlighted the ease with which deepfakes can be created and shared in a manner that violates an individual's privacy, damages their reputation, and causes emotional distress. \nSystem \ud83e\udd16\nUnknown\nOperator:\nDeveloper:\nCountry: Italy\nSector: Politics\nPurpose: Damage reputation\nTechnology: Deepfake - video, audio\nIssue: Defamation; Mis/disinformation; Privacy; Safety\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/world-europe-68615474\nhttps://www.forbes.com/sites/maryroeloffs/2024/03/20/italian-prime-minister-seeks-over-100000-after-deepfake-porn-videos-were-viewed-millions-of-times/\nhttps://www.firstpost.com/world/italian-pm-giorgia-meloni-seeks-100000-euros-damages-deepfake-porn-videos-13751226.html\nhttps://www.cnbctv18.com/world/italian-pm-giorgia-meloni-seeks-908-lakh-in-damages-over-deepfake-porn-videos-19331811.htm\nhttps://www.wionews.com/world/giorgia-meloni-sues-father-son-duo-aged-40-and-73-over-pornographic-deepfake-seeks-eu100000-in-damages-702610\nhttps://timesofindia.indiatimes.com/world/europe/italian-pm-meloni-seeks-compensation-over-deepfake-pornography-videos/articleshow/108667591.cms\nRelated \ud83c\udf10\nDeepfake audio recording depicts British Opposition Leader abusing staff\nX/Twitter fails to remove graphic AI images of Taylor Swift\nPage info\nType: Incident\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ftc-investigates-evolv-for-misleading-marketing", "content": "FTC investigates Evolv for misleading marketing \nOccurred: October 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe US Federal Trade Commission (FTC) is investigating weapons screening firm Evolv Technologies over claims it made about its AI systems accurately identifying weapons passing through its screeners. \nEvolv's AI-powered technology is used in public spaces such as stadiums, hospitals, schools, and entertainment venues to strengthen security by eliminating gaps presented by traditional metal detectors. \nThe investigation comes amidst ongoing concerns that Evolv may have overstated its capabilities, potentially giving customers a false sense of security and safety. In March 2024, the company was forced to backtrack on claims its technology had been tested by the UK government. \nIn February 2024, the US Securities Exchange Commission (SEC) notified Evolv that it was conducting a confidential 'non-public, fact finding inquiry' into the company.\nSystem \ud83e\udd16\nEvolv Express weapons detection\nOperator: \nDeveloper: Evolv Technology\nCountry: USA; Media/entertainment/sports/arts\nSector: Education; Health; Media/entertainment/sports/arts\nPurpose: Detect weapons\nTechnology: Computer vision; Object recognition\nIssue: Governance; Safety\nTransparency: Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reuters.com/technology/crowd-safety-firm-evolv-under-us-ftc-scanner-over-marketing-practices-2023-10-13/\nhttps://www.bloomberg.com/news/articles/2023-10-13/ftc-probes-evolv-security-over-ai-weapons-screening-claims\nhttps://www.the74million.org/article/feds-probe-marketing-push-behind-ai-weapons-detection-tool-used-in-schools/\nhttps://www.govtech.com/education/k-12/security-firm-used-by-atlanta-schools-under-investigation\nhttps://www.bbc.co.uk/news/technology-68547574\nhttps://ipvm.com/reports/evolv-ftc-inquiry\nhttps://ipvm.com/reports/evolv-forbes\nRelated \ud83c\udf10\nStudent stabbed after Evolv weapons detection failure\nUS FTC investigates ChatGPT for possible consumer harms\nPage info\nType: Incident\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/voiceify-sued-for-training-ai-with-copyrighted-material", "content": "Voiceify (Jammable) sued for training AI with copyrighted material\nOccurred: March 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nUK-based voice cloning company Voiceify has been accused of infringing on the sound recording copyrights of musicians and other creators.\nVoiceify (renamed Jammable)\u2019s platform uses AI voice cloning to create vocal covers, generating new versions of songs with vocals mimicking different artists by extracting acapella tracks from YouTube videos, modifying them with AI vocal models, and providing users with unauthorised copies of altered elements and remixed recordings.\nComplaints about Voiceify prompted the British Phonographic Industry (BPI) to issue a formal legal letter outlining its concerns about copyright infringement. \nCritics and commentators also pointed out that misusing AI by stealing others\u2019 creative outputs harms the livelihoods of creators and undermines trust between artists, fans, and platforms.\nSystem \ud83e\udd16\nJammable AI music generator\nOperator: Voiceify AI\nDeveloper: Voiceify AI\nCountry: UK; USA\nSector: Media/entertainment/sports/arts\nPurpose: Create music\nTechnology: Generative AI; Text-to-speech; Machine learning\nIssue: Copyright; Employment\nTransparency: Governance; Marketing\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nBritish Phonographic Industry legal complaint\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.law360.com/ip/articles/1814645/ai-voice-platform-faces-uk-litigation-over-deepfake-songs-\nhttps://bmmagazine.co.uk/news/deepfake-music-start-up-voicify-faces-copyright-dispute/\nhttps://musically.com/2024/03/18/uk-body-bpi-threatens-action-against-ai-music-covers-firm-jammable/\nhttps://fortune.com/europe/2024/03/18/ai-platform-college-student-deepfake-drake-amy-winehouse-after-legal-challenge-music-industry/\nhttps://www.simkins.com/news/the-voice-of-change\nhttps://www.musicbusinessworldwide.com/jammable-formerly-known-as-voicify-offers-3000-ai-models-to-clone-artists-voices-now-it-faces-legal-action-from-the-uks-music-industry12/\nhttps://torrentfreak.com/riaa-reports-ai-vocal-cloning-voicify-to-the-u-s-government-231010/\nRelated \ud83c\udf10\nAnthropic sued for using copyrighted songs to train models\nScammer sells fake AI Frank Ocean songs\nPage info\nType: Incident\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/nhs-plan-to-ai-generate-patient-notes-draws-criticism", "content": "NHS plan to AI generate patient notes draws criticism\nOccurred: March 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe UK national Health Service (NHS) faced criticism over its plan to introduce artificial intelligence (AI) for automatically generating patient notes during medical appointments.\nAccording to UK Health Secretary Victoria Atkins, the scheme will see AI automatically generate notes in the background during appointments and will improve productivity by cutting the time medics spend doing paperwork.\nPrivacy experts argued the 'creepy' plan could reduce privacy, make patients hesitant to share sensitive or embarrassing medical information during consultations, and negatively impact doctor-patient trust. It would also raise difficult ethical questions about confidentiality and patient consent, critics said.\nA demonstration of the system showed the system mistakingly interpreting Ms Atkins saying England chief media officer 'Chris Whitty' as 'Christmas', fueling concerns about its accuracy and reliability - which are considered essential in a medical setting.\nSystem \ud83e\udd16\nUn\nOperator:\nDeveloper: National Health Service (NHS)\nCountry: UK\nSector: Health\nPurpose: Transcribe medical notes\nTechnology: Machine learning\nIssue: Accuracy/reliability; Confidentiality; Privacy\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.dailymail.co.uk/health/article-13172559/Outrage-creepy-plans-AI-listen-NHS-appointments-automatically-generate-notes.html\nhttps://morningstaronline.co.uk/article/health-secretary-announces-invasive-plan-have-ai-listening-nhs-appointments\nRelated \ud83c\udf10\nNHS QCovid risk prediction algorithm\nNHS Digital/iProov facial recognition data sharing\nPage info\nType: Incident\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/sec-fines-money-makers-for-misleading-ai-claims", "content": "SEC fines money makers for misleading AI claims\nOccurred: March 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nTwo investment companies paid a total USD 400,000 in civil penalties after the SEC had charged them with making false and misleading statements about their purported use of AI. \nThe US Securities and Exchange Commission (SEC) charged Delphia (USA) Inc. and Global Predictions Inc. for misrepresenting their AI capabilities to clients and prospective clients. \nDelphia falsely claimed to use AI and machine learning in its investment process, while Global Predictions inaccurately labeled itself as the 'first regulated AI financial advisor' and touted 'expert AI-driven forecasts.'\nSuch deceptive practices undermine investor trust and emphasise the need for transparency and accuracy when discussing AI adoption in financial services.\nSystem \ud83e\udd16\nDelphia website\nGlobal Predictions website\nOperator: Delphia; Global Predictions\nDeveloper:  \nCountry: USA\nSector: Banking/financial services\nPurpose: Predict capital market trends\nTechnology: Machine learning\nIssue: Ethics/values\nTransparency: Marketing\nRegulation \u2696\ufe0f\nUS Securities and Exchange Commission. Investment Adviser Marketing\nUS Securities and Exchange Commission. Artificial Intelligence (AI) and Investment Fraud: Investor Alert\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nUS Securities and Exchange Commission. SEC Charges Two Investment Advisers with Making False and Misleading Statements About Their Use of Artificial Intelligence\nUS Securities and Exchange Commission. SEC Order - Delphia (USA) (pdf)\nUS Securities and Exchange Commission. SEC Order - Global Predictions Inc. (pdf) \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.ft.com/content/70cd5aaf-746b-4577-889f-572449e1ba39\nhttps://www.bloomberg.com/news/articles/2024-03-18/sec-sues-money-managers-for-bogus-artificial-intelligence-claims\nhttps://www.investopedia.com/sec-charges-two-investment-advisers-with-making-false-misleading-ai-claims-8610461\nhttps://www.complianceweek.com/regulatory-enforcement/delphia-global-predictions-fined-by-sec-in-ai-washing-cases/34521.article\nhttps://businessplus.ie/industry-type/artificial-intelligence/sec-fines-investment-advisers-over-false-ai-claims/\nhttps://citywire.com/pro-buyer/news/sec-fines-two-advisors-alleging-they-lied-about-ai/a2438519\nRelated \ud83c\udf10\nAmerican Bitcoin Academy charged with 'AI'-powered fraud\nYouPlus 'AI intelligence engine' investor fraud\nPage info\nType: Incident\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/scientific-journals-publish-papers-with-ai-generated-introductions", "content": "Scientific journals publish papers with AI-generated introductions\nOccurred: March 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nMultiple scientific papers containing text generated by AI have emerged, calling into question the reliability of the papers and underscoring concerns about the role of AI in scientific publishing. \nSeveral media organisations report finding papers listed on academic platforms containing the phrase 'As of my last knowledge update'. Using the same string of words, 404 Media reportedly found 115 different papers on Google Scholar that appeared to have relied on copy and pasted AI model outputs. The phrase is one of many produced by large language models, including OpenAI\u2019s ChatGPT.\nTopics of the papers, which were mostly found to have been published in obscure journals, included spinal injuries, battery technologies, rural medicine, bacterial infections, cryptocurrency, children's wellbeing, and artificial intelligence.\nIt was also reported that, in most instances, the authors of the relevant papers failed to disclose the use of artificial intelligence, raising questions about their ethics and values, as#nd more broadly about scientific peer-review processes and publication standards at top publishers, including Elsevier.\nThe findings came at a time when incidences of bogus or plagiarised academic studies have surged, leading to an uptick in retractions. A 2023 Nature survey of scientists found that 30% of the 1,600 scientists polled admitted to using AI tools to help them write manuscripts. \nSystem \ud83e\udd16\nChatGPT chatbot\nOperator: Elsevier\nDeveloper: OpenAI\nCountry: Global\nSector: Research/academia\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Accuracy/reliability; Cheating/plagiarism; Ethics/values\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.404media.co/scientific-journals-are-publishing-papers-with-ai-generated-text/\nhttps://www.geeky-gadgets.com/ai-tools-for-research/\nhttps://gizmodo.com/science-journal-rat-dck-ai-generated-images-retracted-1851297606\nhttps://www.theverge.com/2023/4/25/23697218/ai-generated-spam-fake-user-reviews-as-an-ai-language-model\nhttps://www.popsci.com/technology/ai-generated-text-scientific-journals/\nhttps://www.msn.com/en-gb/money/technology/chatgpt-scandal-now-rocks-scientific-world-as-multiple-published-studies-are-found-with-ai-generated-text-like-as-of-my-last-knowledge-update/ar-BB1kacl9\nhttps://www.technologynetworks.com/informatics/news/scientific-journal-publishes-paper-with-ai-generated-introduction-384837 \nRelated \ud83c\udf10\nScience publishers withdraw over 120 gibberish AI-generated papers\nPeer-reviewed journal publishes AI-generated rat penis\nPage info\nType: Incident\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/dominos-sued-for-ai-phone-order-voice-print-collection", "content": "Domino's sued for AI phone-order voice print collection\nOccurred: March 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nDomino's has been sued for allegedly collecting voice prints using its AI ordering system without customer consent, violating the Illinois Biometric Information Privacy Act (BIPA).\nIn a proposed class action suit, Illinois customers Odilon Garcia, Jonathan Neumann and Zachery Young accused the pizza company and the developer of its voice recognition system of secretly collecting voice prints without informing them, in addition to names, addresses, phone numbers, and credit card numbers. \nDeveloped by ConverseNow Technologies, the voice recognition system is reportedly used in at least 57 Domino\u2019s locations in Illinois to improve customer service and increase sales. \nThe suit raised concerns about Domino's approach to customer privacy, and may result in financial penalties and reputational damage. The plaintiffs sought injunctive relief, statutory damages, and attorney fees. \nSystem \ud83e\udd16\nConverseNow Technologies website\nOperator: Domino's Pizza\nDeveloper: ConverseNow Technologies\nCountry: USA\nSector: Travel/hospitality\nPurpose: Improve customer service; Increase sales\nTechnology: Voice recognition\nIssue: Privacy\nTransparency: Governance\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nGarcia et al v. Domino's Pizza et al\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://news.bloomberglaw.com/privacy-and-data-security/dominos-ai-phone-order-system-collected-voice-prints-suit-says\nhttps://kioskindustry.org/voice-ai-liability-dominos-sued-over-ai-voiceprints/\nhttps://www.law360.com/classaction/articles/1813768\nRelated \ud83c\udf10\nDomino's Australia AI Pizza Checker plans met with hostility\nMcDonald's drive-through chatbot order taker\nPage info\nType: Incident\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/meta-sues-voyager-labs-for-data-scraping", "content": "Meta sues predictive policing firm Voyager Labs for data scraping\nOccurred: January 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nFacebook owner Meta filed a legal complaint against an AI-powered predictive policing company for using fake Facebook accounts to scrape user data and provide surveillance services to clients.\nAccording to the legal filing, Voyager Labs created 38,000+ fake Facebook user accounts and used its surveillance software to gather data from 600,000 Facebook and Instagram users without consent.  \nMeta demanded that the company stop violating its terms of service and requested it is permanently banned from using Facebook, Instagram, and services related to those platforms. Meta also claimed that Voyager Labs unjustly enriched itself at Meta\u2019s expense, and requested it compensate Meta for its 'ill-gotten profits in an amount to be proven at trial.'\nThe incident raised questions about the legality and ethics of Voyager Lab's governance, notably its data collection policies and processes.\nSystem \ud83e\udd16\nVoyager Labs website\nOperator: Los Angeles Police Department (LAPD)\nDeveloper: Voyager Labs\nCountry: USA\nSector: Govt - police\nPurpose: Predict crime\nTechnology: Machine learning; NLP/text analysis; Pattern recognition; Social media monitoring\nIssue: Ethics/values; Human/civil rights; Privacy\nTransparency: Governance\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nMeta Platforms, Inc. v. Voyager Labs Ltd\nResearch, advocacy \ud83e\uddee\nMeta. Leading the Fight against Scraping-for-Hire\nMeta. Letter to LAPD (pdf)\nDemocracy Centre for Transparency. Voyager Labs: the blunt weapon of artificial transparency\nBrennan Center for Justice (2021). LAPD Documents Show What One Social Media Surveillance Firm Promises Police\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/business-59341750\nhttps://gizmodo.com/facebook-voyager-labs-predictive-policing-meta-1849985035\nhttps://www.theverge.com/2023/1/13/23553332/meta-facebook-instagram-voyager-labs-data-privacy-sue\nhttps://www.theguardian.com/technology/2023/jan/12/meta-voyager-labs-surveillance-fake-accounts\nhttps://www.theguardian.com/us-news/2021/nov/17/los-angeles-police-surveillance-social-media-voyager\nhttps://www.theguardian.com/us-news/2021/nov/17/police-surveillance-technology-voyager\nRelated \ud83c\udf10\nGoogle sued for AI data scraping\nOpenAI 'unprecedented web scraping' trains AI models\nPage info\nType: Incident\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/backlash-as-ib-algorithm-lowers-student-grades", "content": "Backlash as IB algorithm lowers student grades\nOccurred: July 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nResults showing markedly lower grades for students taking the 2020 International Baccalaureate (IB) prompted accusations of unfairness, discrimination, and poor transparency and accountability.\nFollowing the cancellation of IB exams after the outbreak of COVID-19, the International Baccalaureate Organisation used a combination of teacher predicted grades, historic data, and students\u2019 coursework to calculate grades for 175,000 IB students across the world.\nThe results prompted widespread criticism, with teachers saying they were 'losing faith' in the IB and its lack of transparency and cumbersome, opaque appeal process. 26,000+ students signed an online petition started by student Ali Zagmout calling for new grades. \nIn India, lower-than-expected grades caused widespread distress, according to The Times of India. Teachers and parents in the US argued that using a school's historical data to produce grades was unfair to black or low-income students, or students from smaller schools.\nSystem \ud83e\udd16\nInternational Baccalaureate website\nInternational Baccalaurate Wikipedia profile\nOperator:\nDeveloper: International Baccalaureate Organization\nCountry: Global\nSector: Education\nPurpose: Predict exam grades\nTechnology: Prediction algorithm\nIssue: Accuracy/reliability; Bias/discrimination - race, income; Fairness\nTransparency: Governance, Complaints/appeals\nResearch, advocacy \ud83e\uddee\nJustice for May 2020 IB Graduates - Build a Better Future! #IBSCANDAL\nPrivacy International (2020). International Baccalaureate grade-predicting algorithm reinforces historical inequalities\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.telegraph.co.uk/news/2020/07/08/predicted-grades-awarded-summer-will-dodgy-private-school-chief/\nhttps://app.ft.com/content/ee0f4d97-4e0c-4bc3-8350-19855e70f0cf\nhttps://www.politico.eu/article/an-algorithm-shouldnt-decide-students-future-coronavirus-international-baccalaureate/\nhttps://hbr.org/2020/08/what-happens-when-ai-is-used-to-set-grades\nhttps://www.wired.com/story/algorithm-set-students-grades-altered-futures/\nhttps://www.tes.com/news/coronavirus-ib-results-day-2020-controversy-grows-over-grading-scandal\nhttps://www.reuters.com/article/us-global-tech-education-analysis-trfn-idUSKCN24M29L\nhttps://www.nytimes.com/2020/09/08/opinion/international-baccalaureate-algorithm-grades.html\nhttps://www.insidehighered.com/admissions/article/2020/07/13/algorithm-used-ib-scores-year-blamed-students-low-marks\nRelated \ud83c\udf10\nMichigan online bar exam cyberattack raises privacy concerns\nStudent reading aloud during exam is falsely accused of cheating\nPage info\nType: Incident\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/under-armour-ai-powered-ad-plagiarises-earlier-ad", "content": "Under Armour AI-powered ad plagiarises earlier work\nOccurred: March 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn 'AI-powered' advert featuring boxer Anthony Joshua came under fire for reusing others' work without credit.\nAd director Des Walker said on Instragam that he had created the 'first Ai-powered sports commercial' in 'three weeks flat' by combining Ai video, Ai photo, 3D CGI, 2D VFX, Motion graphics, 35mm film, digital video and advances in Ai voiceover.'\nHowever, Walker was quickly accused of plagiarism by other creatives who pointed out that his ad in large part repackaged an ad created by Gustav Johansson and Andr\u00e9 Chementoff two years previously, neither of whom were acknowledged in the ad spot.\nWalker said Under Armour and production company 72 and Sunny owned the rights to the footage and officially requested and licenced their use.\nThe incident highlighted ethical concerns over professional acknowledgments, and prompted creatives to criticise Under Armour and Walker for replacing creative output traditionally done by humans with technology.\nSystem \ud83e\udd16\nUnknown\nAdvert \ud83d\udce2\nAnthony Joshua \u2013 Forever is made now\nOperator: Under Armour\nDeveloper:  \nCountry: UK\nSector: Business/professional services\nPurpose: Create advert\nTechnology: \nIssue: Cheating/plagiarism; Employment; Ethics/values\nTransparency: Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://techcrunch.com/2024/03/14/ai-powered-ad-ignites-creator-controversy-on-instagram/\nhttps://uk.news.yahoo.com/massively-concerning-under-armour-ai-161957526.html\nhttps://nofilmschool.com/under-armour-ai-commercial\nhttps://www.euronews.com/next/2024/03/15/massively-concerning-under-armours-ai-powered-sports-commercial-sparks-controversy\nhttps://www.marketing-interactive.com/under-armour-s-ai-powered-commercial-accused-of-plagiarism\nhttps://adage.com/article/marketing-news-strategy/under-armours-ai-driven-ad-production-companies-react/2546501\nRelated \ud83c\udf10\nPalworld accused of plagiarising Pokemon designs using AI\nScarlett Johansson sues app for using image for AI advert\nPage info\nType: Incident\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-virtual-learning-platform-edgenuity-gamed-by-students", "content": "AI virtual learning platform Edgenuity gamed by students\nOccurred: September 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nUS students discovered a way to cheat an algorithm grading their work by simply typing out a list of words relevant to the topic. \nHaving worked out online learning system Edgenuity was using artificial intelligence to scan students' answers to online questions, twelve-year-old Lazare Simmons and his mother fooled the software by writing a couple of coherent sentences on the topic at hand: Constantinople\u2019s prime geographical location in the Roman Empire, followed by a formless jumble of words that could be relevant: 'profit, diversity, Spain, Gaul, China, India, Africa.'\nSimmons had calculated that the platform was using AI to grade work due to the speed with which it graded his work - However, Edgenuity told Here & Now that it 'does not use algorithms to supplant teacher scoring, only to provide scoring guidance to teachers.'\nThe incident raised questions about the robustness of Edgenuity, and about the impact of similar systems on the ability of students to develop critical thinking skills and excerbate student inequality.\nSystem \ud83e\udd16\nEdgenuity website\nOperator: Williamson County Schools\nDeveloper: Imagine Learning/Edgenuity\nCountry: USA\nSector: Education\nPurpose: Grade student work\nTechnology: Scoring algorithm; NLP/text analysis\nIssue: Robustness\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nWilliamson County Schools Online Program: Stop grading our students until issues are fixed\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://twitter.com/DanaJSimmons/status/1300997133311508480\nhttps://www.theverge.com/2020/9/2/21419012/edgenuity-online-class-ai-grading-keyword-mashing-students-school-cheating-algorithm-glitch\nhttps://www.independent.co.uk/tech/students-ai-grading-algorithm-edgenuity-keyword-spam-a9703751.html\nhttps://www.wbur.org/hereandnow/2020/09/03/online-learning-algorithm\nhttps://metro.co.uk/2020/09/04/students-figure-out-how-to-cheat-ai-grading-algorithm-13222401/\nhttps://news.slashdot.org/story/20/09/03/1728241/these-students-figured-out-their-tests-were-graded-by-ai----and-the-easy-way-to-cheat\nhttps://www.businesstelegraph.co.uk/student-tricks-ai-grading-system-into-giving-a-perfect-score-by-adding-word-salad-to-answers/\nhttps://www.foxbusiness.com/technology/12-year-old-student-and-mother-crack-remote-learning-platforms-ai-algorithm-to-cheat-on-tests\nRelated \ud83c\udf10\nChatGPT linked to student memory loss, procastination\nNYC Dept of Education bans ChatGPT due to student learning concerns\nPage info\nType: Incident\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/proctortrack-data-breach-raises-privacy-concerns", "content": "Proctortrack data breach raises privacy concerns\nOccurred: September 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA security breach of software code and personal data at AI-powered online proctoring company Proctortrack raised concerns about its security and user privacy. \nAccording to Consumer Reports, analysis of leaked Proctortrack source code suggested that the company ignored basic data security practices, with suggestions that videos of students taking tests may have been accessible to unauthorised Proctortrack employees, along with facial recognition data, contact information, digital copies of ID cards, and more.\nThe names and email addresses of over 150 students, mostly from St. George\u2019s University medical school in Grenada, appeared in the software in plain text. Proctortrack was suspended for seven days after the breach was discovered. Verificient, the company that created Proctortrack, said an 'imposter' accessed its server and 'emailed out fraudulent messages.'\nProctortrack later issued a statement in October stating that 'no personally identifiable information (PII) was accessed,' but the incident raised concerns about student privacy. It was also seen to raise questions about the thoroughness of vetting of online proctoring companies by colleges across the US and elsewhere. \nSystem \ud83e\udd16\nProctortrack website\nOperator: Rutgers University; St. George\u2019s University; Western University\nDeveloper: Verificient Technologies\nCountry: Canada; USA\nSector: Education\nPurpose: Detect and prevent cheating\nTechnology: Facial recognition\nIssue: Privacy; Security\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.consumerreports.org/digital-security/poor-security-at-online-proctoring-company-proctortrack-may-have-put-student-data-at-risk/\nhttps://www.securitymagazine.com/articles/93669-verificient-technologies-anti-cheating-software-suffers-data-breach\nhttps://www.cbc.ca/news/canada/london/western-students-alerted-about-security-breach-at-exam-monitor-proctortrack-1.5764354\nhttps://www.biometricupdate.com/202011/data-breach-stirs-new-university-protests-about-proctoring-apps\nhttps://iapp.org/news/a/student-monitoring-software-temporarily-shut-down-after-security-breach/\nhttps://www.insurancebusinessmag.com/ca/news/cyber/anticheating-software-for-university-exams-suffers-security-breach-236577.aspx\nhttps://www.bleepingcomputer.com/news/security/online-proctor-service-proctortrack-disables-service-after-hack/\nhttps://www.consumerreports.org/digital-security/poor-security-at-online-proctoring-company-proctortrack-may-have-put-student-data-at-risk/\nhttps://dailytargum.com/article/2020/10/rutgers-responds-to-proctortrack-security-breach\nRelated \ud83c\udf10\nData breach reveals data of 400,000+ ProctorU users\nStudent reading aloud during remote exam is falsely accused of cheating\nPage info\nType: Incident\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/data-breach-reveals-data-of-400000-proctoru-users", "content": "Data breach reveals data of 400,000+ ProctorU users\nOccurred: August 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe personal details of over 440,000 users of AI-powered online proctoring platform ProctorU were exposed online after a data breach.\nAccording to Bleeping Computer, the ProctorU database contained names, home addresses, emails, cell phone numbers, hashed passwords and organisation details. It also contained US military email addresses. \nProctorU later said that 'the records were from 2014, and did not contain any financial information.'\nProctorU was one of 18 organisations hacked early 2020. Databases from all entities were later offered for free in online hacker forums, and resulted in investigations by universities in Australia, the USA and elsewhere.\nThe incident raised questions about ProctorU's security and was seen to potentially jeopardise user privacy.\nSystem \ud83e\udd16\nProctorU website \nOperator: University of Queensland; University of Sydney\nDeveloper: Meazure Learning/ProctorU\nCountry: Australia\nSector: Education\nPurpose: Detect and prevent cheating\nTechnology: Facial recognition\nIssue: Privacy; Security\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bleepingcomputer.com/news/security/proctoru-confirms-data-breach-after-database-leaked-online/\nhttps://www.tomsguide.com/news/proctoru-data-breach\nhttps://www.sbs.com.au/news/australian-universities-investigating-deeply-concerning-hack-of-controversial-exam-software\nhttps://www.smh.com.au/national/hackers-hit-university-online-exam-tool-20200806-p55j6h.html\nhttps://www.privacy.com.sg/databreach/proctoru-confirms-data-breach-after-database-leaked-online/\nhttps://www.infosecurity-magazine.com/news/online-exam-tool-suffers-data/\nRelated \ud83c\udf10\nStudent reading aloud during remote exam is falsely accused of cheating\nMichigan bar exam cyberattack raises privacy concerns\nPage info\nType: Incident\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/student-reading-aloud-during-exam-is-falsely-accused-of-cheating", "content": "Student reading aloud during remote exam is falsely accused of cheating\nOccurred: October 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA student who had read aloud a question during an online exam received an academic infraction that put the scholarship she relied on to pay for food and rent at risk.\nIn a video posted to TikTok that went viral with over 3 million views, college student Dana Jo said that AI-powered exam proctoring system ProctorU had flagged her reading questions aloud during an exam as suspicious. As a result her professor had given her a zero on the assessment, she claimed.\nThe student eventually managed to convince her university to to expunge the infraction, though it required her to contact school administration and the AI programme vendor. \nThe incident raised questions about the reliability of ProctorU and similar proctoring systems, and was interpreted as a symptom of misguided confidence in technology to solve problems.\nSystem \ud83e\udd16\nProctorU website\nOperator:\nDeveloper: Meazure Learning/ProctorU\nCountry: USA\nSector: Education\nPurpose: Detect and prevent cheating\nTechnology: Facial recognition\nIssue: Accuracy/reliability; Fairness; Robustness\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.businessinsider.com/viral-tiktok-student-fails-exam-after-ai-software-flags-cheating-2020-10?r=US&IR=T\nhttps://www.nbcnews.com/think/opinion/remote-testing-monitored-ai-failing-students-forced-undergo-it-ncna1246769\nhttps://www.hastingslawjournal.org/wp-content/uploads/10-Mita_final.pdf\nRelated \ud83c\udf10\nProctorU accused of sharing UCSB student data\nUC Santa Barbara faculty advises against ProctorU use\nPage info\nType: Incident\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/adobe-firefly-shows-woke-photos-of-black-nazis", "content": "Adobe Firefly shows 'woke' photos of black Nazis\nOccurred: March 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAdobe's Firefly AI art generator suffered a backlash after it was found to be producing 'woke' images of black Nazis, Vikings, and US Founding Fathers.\nTests by Semafor, the Daily Mail, and the New York Post revealed that Firefly was creating images of Black and Asian Nazi soldiers under a German (ie. Non-Nazi) flag, Black men and women inserted into scenes depicting the US Founding Fathers and the 1787 Constitutional Convention, pictures of Black Vikings, and a female Black Pope.\nAdobe designers were seen as most likely trying to avoid racist stereotypes. However, the findings infuriated people on the right who saw it as the AI trying to rewrite history in line with today's politicised culture wars. Adobe said Firefly is not 'meant for generating photorealistic depictions of real or historical events.'\nGoogle had earlier shut down its Gemini image creation tool after critics pointed out that it had been creating historically inaccurate images. However, the two services were trained on different datasets, with Firefly trained on stock images or images that it licensed. \nSystem \ud83e\udd16\nAdobe Firefly image generator\nOperator: Adobe users\nDeveloper: Adobe\nCountry: USA\nSector: Media/entertainment/sports/arts; Politics\nPurpose: Generate video, images\nTechnology: Machine learning; Pattern recognition; Object recognition\nIssue: Accuracy/reliability; Ethics/values; Historical revisionism\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.dailymail.co.uk/news/article-13194153/Adobe-firefly-AI-google-gemini-black-nazis-vikings.html\nhttps://www.semafor.com/article/03/13/2024/adobe-firefly-repeats-the-same-ai-blunders-as-google-gemini\nhttps://nypost.com/2024/03/14/world-news/adobe-firefly-follows-in-google-geminis-woke-footsteps/\nRelated \ud83c\udf10\nGoogle Gemini generates 'woke' 'diverse' racial images\nAdobe uses customer images without consent to train Firefly art generator\nPage info\nType: Incident\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/michigan-online-bar-exam-cyberattack-raises-privacy-concerns", "content": "Michigan online bar exam cyberattack raises privacy concerns\nOccurred: July 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nMichigan's online facial recognition-powered bar exam was reduced to 'chaos' by a cyberattack that disrupted part of the test and prompted concerns about poor security and loss of privacy.\n\nA distributed denial of service (DDoS) attack on ExamSoft's exam proctoring platform resulted in people taking Michigan's bar exam were unable to log-into one of the test modules, resulting in anxiety and panic. \n\nAt the time, ExamSoft said it had been targeted in a 'sophisticated' cyberattack, and that 'at no time was any data compromised.' A number of applicants later complained that their passwords had been hacked, though ExamSoft said it was coincidental. \n\nThe incident prompted accusations that ExamSoft had not been taking the security of its system, and the privacy of applicants, seriously.\nSystem \ud83e\udd16\nExamSoft website\nOperator: ExamSoft; Michigan Board of Law Examiners\nDeveloper: Turnitin/ExamSoft\nCountry: USA\nSector: Education\nPurpose: Identify identity; Detect cheating\nTechnology: Facial recognition\nIssue: Robustness; Privacy; Security\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://thehill.com/policy/cybersecurity/509478-michigan-online-bar-exam-temporarily-taken-down-by-sophisticated\nhttps://www.law.com/2020/07/28/first-online-bar-exam-marred-by-tech-problems/\nhttps://www.law.com/2020/07/29/michigan-blames-cyber-attack-for-online-bar-exam-woes-while-indiana-moves-to-exam-via-email/\nhttps://www.law.com/legaltechnews/2020/08/05/bar-exams-facial-recognition-deployment-is-heightening-test-takers-anxiety/\nhttps://www.techtimes.com/articles/253331/20201014/sophisticated-cyberattack-targets-michigans-examsoft-affects-students-bar-exam.htm\nhttps://abovethelaw.com/2020/09/examsoft-responds-to-multiple-reports-that-software-compromises-security/\nhttps://themarkup.org/coronavirus/2020/10/13/remote-exam-software-failures-privacy\nhttps://venturebeat.com/2020/09/29/examsofts-remote-bar-exam-sparks-privacy-and-facial-recognition-concerns/\nhttps://www.cnbc.com/2020/08/19/literal-hellthe-pandemic-has-made-the-bar-exam-more-excruciating.html\nRelated \ud83c\udf10\nCleveland State University online proctor room scanning\nUC Santa Barbara faculty advises against ProctorU use\nPage info\nType: Incident\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/uber-amazon-use-ai-to-pay-people-different-wages-for-the-same-work", "content": "Uber, Amazon use AI to pay people different wages for the same work\nOccurred: April 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nLarge ride-hailing companies have been engaging in 'algorithmic wage discrimination' or paying workers different amounts to do the same thing for the same amount of time.\n\nResearch by UC Hastings law professor Veena Dubal discovered that Amazon, Lyft, Uber and other firms were personalising fares to individual drivers based on the reams of data they collect, using pay mechanisms to influence their behaviour, and manipulating pay to get the most out of them.\n\nFrom a drivers' perspective, the algorithms resulted in lost, unpredictable and variable pay, according to Dubal, who drew on hundreds of interviews with gig workers themselves. And, given the proprietary nature of the algorithms, they had no knowledge of how the systems work or how their data is being used, Dubal said.\n\nUber denied that it personalises fares to individual drivers, and explicitly claims that it does not base rates on a driver\u2019s ethnicity, their acceptance rate, nor prior trip history. Meanwhile, a spokesperson for Lyft claimed that the Dubal\u2019s paper was biased and cherry-picked data.\n\nThe study raised concerns about the fairness and legality of rideshare company pay, and about the opacity of the systems being used.\nSystem \ud83e\udd16\nAmazon Flex website\nUber website\nLyft website\nOperator: Amazon; Lyft; Uber\nDeveloper: Amazon; Lyft; Uber\nCountry: USA\nSector: Transport/logistics\nPurpose: Calculate pay\nTechnology: Automated management system; Image recognition\nIssue: Employment; Fairness\nTransparency: Governance; Black box; Complaints/appeals\nResearch, advocacy \ud83e\uddee\nDubal V. (2023). On Algorithmic Wage Discrimination\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.businessinsider.com/uber-amazon-pay-using-ai-different-wages-same-work-discrimination-2023-4\nhttps://www.latimes.com/business/technology/story/2023-04-11/algorithmic-wage-discrimination\nhttps://www.marketplace.org/2023/04/10/how-ride-hail-companies-use-data-to-pay-drivers-less/\nhttps://www.cbsnews.com/news/algorithmic-wage-discrimination-artificial-intelligence/\nhttps://www.carscoops.com/2023/04/law-professor-claims-uber-lyft-promote-algorithmic-wage-discrimination/\nhttps://www.npr.org/2023/04/25/1171800324/rideshare-drivers-raise-questions-about-how-algorithms-set-drivers-pay-rates\nRelated \ud83c\udf10\nUber UpFront Fares algorithm cuts driver earnings\nDeliveroo UK rider management algorithm\nPage info\nType: Issue\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/flawed-uk-welfare-algorithm-pushes-people-into-poverty", "content": "'Flawed' UK welfare algorithm 'pushes people into poverty'\nOccurred: September 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn algorithm used to determine UK citizens' welfare benefits caused hunger, debt, and psychological distress, a human rights organisation warned.\nHuman Rights Watch discovered that the model, which calculated the benefits people are entitled to each month based on changes in their earnings, only analysed the wages people received within a calendar month, and ignored how often they were paid. \nThis resulted in people receiving multiple paychecks per month - a common occurrence in irregular and low-paid work - having their earnings overestimated and their payments shrunk significantly. \nThe incident is one of many affecting the UK's Universal Credit system, of which the model forms a core part. Universal Credit combines six benefits into a single monthly sum.\nSystem \ud83e\udd16\nDepartment of Work and Pensions website\nDepartment of Work and Pensions Wikipedia profile\nOperator: Department of Work and Pensions (DWP)\nDeveloper:  \nCountry: UK\nSector: Govt - welfare\nPurpose: Calculate welfare entitlements\nTechnology: \nIssue: Accuracy/reliability; Fairness\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nHuman Rights Watch (2020). Automated Hardship\nHuman Rights Watch (2020). UK: Automated Benefits System Failing People in Need\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://thenextweb.com/neural/2020/09/29/flawed-algorithm-used-to-determine-uk-welfare-payments-is-pushing-people-into-poverty/\nhttps://www.thesun.co.uk/money/12795902/universal-credit-calculated-algorithm-hunger-debt/\nhttps://inews.co.uk/news/uk/universal-credit-algorithm-payments-calculated-benefits-poverty-human-rights-watch-664786\nhttps://www.newstatesman.com/politics/welfare/2020/12/dwp-debt-drives-people-food-banks-exposing-orwellian-nature-universal\nhttps://www.huffingtonpost.co.uk/entry/universal-credit-design-flaw-algorithm_uk_5f7206bfc5b64e066660a656\nhttps://www.localgov.co.uk/Automated-benefit-system-pushing-people-into-poverty/51162\nhttps://www.theguardian.com/technology/2019/oct/14/automating-poverty-algorithms-punish-poor\nhttps://www.independent.co.uk/news/uk/home-news/universal-credit-algorithm-hunger-debt-human-rights-watch-dwp-b670953.html\nRelated \ud83c\udf10\nUK DWP sued over 'unfair' disability benefits fraud detection algorithm\nNetherlands childcare benefits fraud assessments automation\nPage info\nType: Incident\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/legal-challenge-launched-against-discriminatory-sham-marriage-algorithm", "content": "Legal challenge launched against 'discriminatory' sham marriage algorithm\nOccurred: February 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \nA legal challenge against an algorithm used to identify potential sham marriages accused the UK Home Office of discriminating against people from certain countries.\nThe sham marriage triage tool, which is applied to marriage applications involving someone who is not a British or Irish citizen and lacks sufficient settled status or a valid visa, identifies couples suspected of getting married just to get round immigration controls, and refers them for investigation by officials.\nAccording to the challenge by legal charity the Public Law Project (PLP), the tool could discriminate against people from Greece, Bulgaria, Romania and Albania. The PLP also noted that Home Office secrecy about the system breaches transparency rules under the GDPR.\nThe move came after the UK Information Tribunal ruled in a separate ruling that \u201cthere will be some indirect discrimination\u201d and \u201cpotential bias\u201d in the algorithmic system the Home Office is using.\nSystem \ud83e\udd16\nUK Home Office sham marriage triage tool\nOperator: UK Home Office\nDeveloper: Home Office DACC\nCountry: UK\nSector: Govt - immigration\nPurpose: Detect sham marriages\nTechnology: Machine learning\nIssue: Bias/discrimination - nationality\nTransparency: Governance\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nPublic Law Project v The Information Commissioner\nResearch, advocacy \ud83e\uddee\nPublic Law Project (2023). Legal action launched over sham marriage algorithm\nElectronic Immigration Network (2023). Public Law Project begins judicial review over \u2018discriminatory\u2019 Home Office algorithm used to identify potential sham marriages\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/uk-politics-64663482\nhttps://www.opendemocracy.net/en/home-office-immigration-sham-marriage-hostile-environment/\nhttps://www.electronicspecifier.com/products/artificial-intelligence/legal-action-launched-over-uk-gov-t-sham-marriage-algorithm\nhttps://www.localgovernmentlawyer.co.uk/litigation-and-enforcement/400-litigation-news/53059-home-office-algorithm-for-detecting-sham-marriages-breaches-public-sector-equality-duty-and-gdpr-rules-pre-action-letter-claims\nhttps://aibusiness.com/verticals/uk-government-using-ai-for-benefits-marriage-checks-despite-biases\nhttps://todaysfamilylawyer.co.uk/legal-action-launched-over-home-offices-sham-marriage-screening-algorithm/\nhttps://www.theguardian.com/technology/2023/oct/23/uk-officials-use-ai-to-decide-on-issues-from-benefits-to-marriage-licences\nhttps://metro.co.uk/2023/10/23/artificial-intelligence-benefits-uk-ai-summit-19708043/\nhttps://www.independent.co.uk/news/uk/politics/ai-artificial-intelligence-government-b2434850.html\nRelated \ud83c\udf10\nUK DWP robo review 'unfairly' targets Bulgarians, Poles for benefit fraud investigation\nUK DWP sued over 'unfair' disability benefits fraud detection algorithm\nPage info\nType: Incident\nPublished: March 2024\nLast updated: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/robo-review-unfairly-targets-bulgarians-for-benefit-fraud-investigation", "content": "DWP robo review 'unfairly' targets Bulgarians, Poles for benefit fraud investigation\nOccurred: December 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nBulgarian nationals had their welfare benefits unfairly suspended without explanation, a UK government review of an automated benefits system revealed.\nThe Department for Work and Pensions (DWP) faced scrutiny after Bulgarian and some Polish nationals reported unexplained suspensions of their Universal Credit benefits, prompting Edmonton MP Kate Osamor to raise concerns about potential nationality-based targeting for benefit fraud investigations.\nMost of those affected were single mothers working part-time in low-paid jobs, who relied on universal credit to supplement rent payments, Osamor said. The suspensions lasted for 'protracted periods', and those impacted said to have suffered poverty, homelessness, and were forced to use food banks. \nSystem \ud83e\udd16\nDepartment of Work and Pensions website\nDepartment of Work and Pensions Wikipedia profile\nOperator: Department of Work and Pensions (DWP)\nDeveloper: Department of Work and Pensions (DWP)\nCountry: UK\nSector: Govt - welfare\nPurpose: Detect fraud\nTechnology: Data matching algorithm; Machine learning\nIssue: Bias/discrimination - nationality; Fairness\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theguardian.com/society/2021/dec/06/mp-raises-concern-over-bulgarian-nationals-uk-benefit-suspensions\nhttps://www.theguardian.com/technology/2023/oct/23/uk-risks-scandal-over-bias-in-ai-tools-in-use-across-public-sector\nhttps://www.theguardian.com/technology/2023/oct/23/uk-officials-use-ai-to-decide-on-issues-from-benefits-to-marriage-licences\nhttps://www.standard.co.uk/news/tech/uk-government-ai-use-benefit-payments-bias-b1115692.html\nhttps://enfielddispatch.co.uk/benefit-fraud-team-unfairly-targeting-bulgarian-roma-people-in-edmonton/\nRelated \ud83c\udf10\nUK DWP sued over 'unfair' disability benefits fraud detection algorithm\nNetherlands childcare benefits fraud assessments automation\nPage info\nType: Incident\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/kate-middleton-suspected-of-using-ai-to-manipulate-photo", "content": "Kate Middleton suspected of using AI to edit family photo\nOccurred: March 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe Princess of Wales was suspected of using AI to doctor a family image apparently intended to celebrate Mother's Day and reassure the general public that she had recovered from abdominal surgery. \nAmong the red flags were a misalignment in the step behind Prince Louis\u2019s leg; some distortion at Princess Charlotte\u2019s left wrist, with a portion of her sleeve breaking free; blurring at the knee of Charlotte\u2019s tights; distortion in Prince George\u2019s right fingers; and a missing wedding ring.\nCommentators pointed out that many of these issues are often connected with artificial intelligence. AI is built into photo editing tools such as Photoshop and freely available on many mobile phone-based applications. \nThe discovery prompted photo agencies including Agence France-Presse, AP and Reuters to pull the photo over concerns it might have been 'manipulated,' and raised concerns about the use of AI to generate and edit photographs of public figures. \nThe fracas was seen to have damaged the reputation of the Royal Family and to have further reduced trust in public institutions.\nSystem \ud83e\udd16\nUnknown\nOperator: British royal family\nDeveloper:  \nCountry: UK\nSector: Media/entertainment/sports/arts\nPurpose: Edit photograph\nTechnology: \nIssue: Ethics/values\nTransparency: Governance; Marketing\nManipulation analysis \u26a0\ufe0f\nTrueMedia rating. Highly suspicious\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/bitesize/articles/zpy63qt\nhttps://www.washingtonpost.com/opinions/2024/03/11/catherine-royal-family-photo-ai-editing/\nhttps://hollywoodlife.com/2024/03/11/kate-middleton-editing-family-photo/\nhttps://news.artnet.com/art-world/fake-kate-middleton-photo-2450079\nhttps://techcrunch.com/2024/03/11/kate-middletons-photo-editing-controversy-is-an-omen-of-whats-to-come/\nhttps://www.nbcnews.com/tech/internet/kate-middleton-photo-ai-image-photoshop-edit-rcna142799\nRelated \ud83c\udf10\nCambodia torture victims' photo manipulation\nNine News uses AI to sexualise image of politician\nPage info\nType: Incident\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/italys-privacy-watchdog-rules-sari-real-time-facial-recognition-unlawful", "content": "Italy's privacy watchdog rules SARI Real-time facial recognition unlawful\nOccurred: April 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nItaly's privacy regulator said the SARI Real-time facial recognition system used by the country's Interior ministry was illegal on the basis that it lacked a legal basis for processing biometric data.\nItaly\u2019s data protection authority, the Garante, issued an opinion that the Ministry of the Interior\u2019s use of Sari Real-time failed to comply with EU privacy legislation. The authority also said it created 'a form of indiscriminate mass surveillance.' \nSARI Real-time uses cameras installed in a particular geographical area and is capable of scanning individuals\u2019 faces in real-time. These images are then compared with an Italian government watch-list database of up to 10,000 faces. The database is available to law enforcement upon request.\nThe banning of SARI Real-time coincided with #ReclaimyourFace, a campaign by European rights organisation the EDRi calling for the banning of biometric mass surveillance across Europe. \nSystem \ud83e\udd16\nSARI real-time facial recognition\n\nOperator:\nDeveloper: Parsec 3.26/Reco 3.26\nCountry: Italy\nSector: Govt - police\nPurpose: Strengthen law enforcement\nTechnology: Facial recognition\nIssue: Privacy\nTransparency: Governance\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nIl Garante per la Protezione dei Dati Personali (2021). Opinion on Sari Real Time System [9575877]\nResearch, advocacy \ud83e\uddee\nEDRi (2021). Reclaim your Face campaign\nEDRi (2021). Initial wins in Italy just two months after the launch of Reclaim Your Face\nStatewatch (2021). Italy: Interior ministry\u2019s facial recognition system is unlawful\nEDRI (2021). Chilling use of face recognition at Italian borders shows why we must ban biometric mass surveillance\nInvestigations, assessments, audits \ud83e\uddd0\nIPRI Media (2021). Lo scontro Viminale-Garante della privacy sul riconoscimento facciale in tempo reale\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.dataguidance.com/news/italy-garante-considers-ministrys-facial-recognition\nhttps://www.biometricupdate.com/202104/real-time-facial-recognition-system-deployment-blocked-by-italian-data-protection-authority\nhttps://www.pogowasright.org/it-facial-recognition-sari-real-time-does-not-comply-with-the-privacy-policy/\nhttps://iapp.org/news/a/italian-dpa-does-not-favor-use-of-sari-real-time-system/\nhttps://medium.com/@ORARiccardo/italian-police-has-acquired-a-facial-recognition-system-a54016211ff2\nhttps://irpimedia.irpi.eu/sorveglianze-viminale-garante-privacy-riconoscimento-facciale-in-tempo-reale/\nRelated \ud83c\udf10\nTrento council fined for AI surveillance projects\nUK Met Police retrospective facial recognition\nPage info\nType: Incident\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/trento-council-fined-for-ai-citizen-surveillance-projects", "content": "Trento council fined for AI surveillance projects \nOccurred: January 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe Municipality of Trento, Italy, was fined for 'multiple violations' of EU data protection law in its use of AI in street surveillance projects.\nThe country's Garante (DPGP) privacy watchdog ruled that said the data collected for the EU-funded Marvel and Protection projects was insufficiently  anonymous, thereby putting citizens' privacy at risk, and that it had been incorrectly shared with third parties. \nThe municipality was fined EUR 50,000 (USD 54,225) and instructed to delete all data gathered. It said it would appeal the decision.\nTrente was the first local administration in Italy to be sanctioned over the use of data from AI tools. \nSystem \ud83e\udd16\nMarvel project website\nProtector project website\nOperator: Municipality of Trento\nDeveloper: Foundation for Research and Technology Hellas (FORTH); Saher Europe\nCountry: Italy\nSector: Govt - municipal; Govt - police\nPurpose: Increase public safety\nTechnology: Anomaly detection; Computer vision; Facial recognition; Object detection\nIssue: Privacy; Security\nTransparency: \nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nGarante (2024). Videosorveglianza, no all'intelligenza artificiale che viola la privacy. Il Garante sanziona il Comune di Trento per aver condotto due progetti di ricerca scientifica, utilizzando telecamere, microfoni e reti sociali, in violazione della normativa sulla protezione dati\nGarante (2024). Provvedimento dell'11 gennaio 2024 [9977020]\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.dataguidance.com/news/italy-garante-fines-municipality-trento-50000-personal\nhttps://www.reuters.com/sustainability/society-equity/italy-fines-first-city-privacy-breaches-use-ai-2024-01-26/\nhttps://dig.watch/updates/italian-municipality-fined-e50000-for-data-privacy-breach-in-ai-use\nhttps://www.statewatch.org/news/2024/february/italy-trento-council-fined-for-illegal-ai-video-and-audio-surveillance-projects/\nhttps://coingeek.com/italy-ai-using-trento-city-fined-over-data-protection-rules-violations/\nRelated \ud83c\udf10\nMalta Safe City video surveillance\nBelgrade Safe City surveillance system\nPage info\nType: Incident\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/black-man-sues-unos-over-kidney-transplant-algorithm-racial-bias", "content": "Black man sues UNOS over kidney transplant algorithm racial bias\nOccurred: April 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Black patient accused the United Network for Organ Sharing and Cedars-Sinai Medical Center in Los Angeles of using an algorithm that unfairly made him and other Black patients on the kidney transplant list wait longer for organs.\nKidney disease sufferer Anthony Randall asked a federal court to allow him to make the lawsuit a class action representing 27,500 Black patients he claimed the algorithm also disadvantaged. Randall was forced to wait over five years for a kidney, which prevented him from working, according to court dockets. His suit also contended that he and others have suffered 'economic injuries' including dialysis and other medical costs. \nUnited Network for Organ Sharing (UNOS), a non-profit organisation that operates the US transplant system, and Cedars-Sinai dropped the use of the problematic part of the formula that Mr. Randall cited in his lawsuit. UNOS' board of directors found that including a 'modifier for patients identified as Black' had resulted in a widespread underestimation of the severity of kidney disease for many Black patients. \nHowever, Randall's complaint stated that his 'wait time continues to be incorrectly calculated in UNOS's UNet software.'\nSystem \ud83e\udd16\nUNOS UNet website\n\nOperator: Cedars-Sinai Medical Center\nDeveloper: United Network for Organ Sharing (UNOS)\nCountry: USA\nSector: Health\nPurpose: Allocate organ transplants\nTechnology: \nIssue: Bias/discrimination - race, ethnicity\nTransparency: Governance\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nAnthony Randall v. United Network for Organ Sharing et al\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.washingtonpost.com/health/2023/04/10/lawsuit-unos-kidney-transplant-race-discrimination/\nhttps://thegrio.com/2023/04/11/black-man-awaiting-kidney-transplant-alleges-racial-bias/\nhttps://news.bloomberglaw.com/health-law-and-business/racial-discrimination-in-organ-transplants-alleged-in-lawsuit\nhttps://richmond.com/life-entertainment/health-med-fit/unos-organ-transplantation-network/article_cdbcbc24-d87c-11ed-8607-5ba6d8f4eeed.html\nhttps://www.beckershospitalreview.com/legal-regulatory-issues/man-sues-unos-cedars-sinai-over-alleged-racial-bias-in-transplant-priority.html\nRelated \ud83c\udf10\nNative Americans are shut out of US liver transplant system\nAlgorithm delays young peoples' liver transplants\nPage info\nType: Incident\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/native-americans-are-shut-out-of-us-liver-transplant-system", "content": "Native Americans are shut out of US liver transplant system\nOccurred: 2018-2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nNative Americans are much less likely than other racial groups to gain a spot on the US national liver transplant list, despite having the highest rate of death from liver disease.\nAccording to an analysis of four years of transplant data by The Markup and The Washington Post, White people gained a spot on the transplant list almost three times more often than Native Americans, compared with their total number of deaths from liver disease. \nNative Americans are four times more likely to die from the disease than non-Hispanic White people, according to the US Department of Health and Human Services. It is the second leading cause of death among Indigenous men ages 35 to 44. \nWith native Americans advancing to surgery at the same rate as White people, the study showed that access to the live transplant list is a primary driver of disparity. However, experts said they may not know that a liver transplant is an option, resulting in large numbers of Native Americans with liver problems dying untreated.\nSystem \ud83e\udd16\nUnited Network for Organ Sharing (UNOS) - How we match organs \nOperator: \nDeveloper: United Network for Organ Sharing (UNOS)\nCountry: USA\nSector: Health\nPurpose: Allocate liver transplants\nTechnology: Ranking algorithm\nIssue: Bias/discrimination; Fairness\nTransparency: \nInvestigations, assessments, audits \ud83e\uddd0\nThe Markup (2024). A Death Sentence: Native Americans Shut Out of the Nation\u2019s Liver Transplant System\nThe Markup (2024). How We Investigated Racial Disparities in Liver Transplants\nThe Markup (2023). Poorer States Suffer Under New Organ Donation Rules, As Livers Go to Waste\nThe Markup (2023). An Algorithm Decides Who Gets a Liver Transplant. Here Are 5 Things to Know\nThe Markup (2023). How We Investigated UNOS\u2019s Liver Allocation Policy\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.washingtonpost.com/business/2024/02/08/death-sentence-native-americans-have-least-access-liver-transplant-system/\nhttps://kffhealthnews.org/morning-breakout/liver-transplant-system-is-less-accessible-to-native-americans-study/\nhttps://scottsdalerecovery.com/a-death-sentence-native-americans-have-least-access-to-liver-transplant-system-lee-yaiva-src-in-the-news/\nhttps://www.ktalnews.com/news/a-death-sentence-native-americans-shut-out-of-the-nations-liver-transplant-system/\nRelated \ud83c\udf10\nAlgorithm delays young peoples' liver transplants\nLarge language models perpetuate healthcare racial bias\nPage info\nType: Incident\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-chatbots-found-to-be-racist-despite-anti-racism-training", "content": "AI chatbots found to be covertly racist despite anti-racism training\nOccurred: March 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nChatGPT and other commercial chatbots demonstrated covert racial prejudice towards speakers of African American English, potentially influencing decisions about a person\u2019s employability and criminality. \nResearchers at the Allen Institute for AI, the University of Oxford, Stanford University, LMU Munich, and the University of Chicago fed text to five chatbots, including models from Google and Facebook, in the style of African American English or Standard American English, and asked the models to comment on the texts\u2019 authors. \nThe models characterised African American English speakers using terms associated with negative stereotypes. In the case of GPT-4, it described them as 'suspicious', \u201caggressive\u201d, 'loud', 'rude' and 'ignorant'. \nIn one instance, the speaker of the statement: 'I am so happy when I wake up from a bad dream because they feel too real,' was predominantly judged by AI models as 'brilliant' or 'intelligent.' Whereas the statement 'I be so happy when I wake up from a bad dream cus they be feelin too real' was labeled 'dirty,' 'stupid,' or 'lazy.'\nAccording to the researchers, language models exhibited 'covert stereotypes that are more negative than any human stereotypes about African Americans ever experimentally recorded, although closest to the ones from before the civil rights movement.' \n\nThey also reckoned that this prejudice was concealed beneath a veneer of political correctness whenever AI models such as GPT-4 and its predecessors are prompted to comment on overt stereotypes about African Americans, which were presented in a 'much more positive' light.\n\nThe findings called into question the effectiveness of approaches taken by LLM developers to solve bias. \nSystem \ud83e\udd16\nGPT-4 large language model\nGPT-3 large language model\nOperator: Valentin Hofmann, Pratyusha Ria Kalluri, Dan Jurafsky, Sharese King  \nDeveloper: OpenAI\nCountry: USA\nSector: Multiple\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Bias/discrimination - race\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nHofmann V., Kalluri P. R., Jurafsky D., King S. (2024). Dialect prejudice predicts AI decisions about people's character, employability, and criminality\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.technologyreview.com/2024/03/11/1089683/llms-become-more-covertly-racist-with-human-intervention/\nhttps://www.newscientist.com/article/2421067-ai-chatbots-use-racist-stereotypes-even-after-anti-racism-training/\nhttps://techreport.com/news/ai-chatbots-racist-despite-anti-racism-training/\nhttps://cybernews.com/tech/artificial-intelligence-covert-racism-african-americans/\nhttps://www.digitalinformationworld.com/2024/03/ai-chatbots-have-racism-problem-despite.html\nhttps://knowridge.com/2024/03/chatbots-persist-in-using-racial-stereotypes-despite-anti-bias-training/\nRelated \ud83c\udf10\nChatGPT reproduces recommendation letter gender bias\nStudy finds personalising ChatGPT makes it more offensive\nPage info\nType: Incident\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/study-finds-personalising-chatgpt-makes-it-more-offensive", "content": "Study finds personalising ChatGPT makes it more offensive \nOccurred: April 2023-March 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nChatGPT is more likely to generate rude, disrespectful or unreasonable comments when prompted to assume the style of even benign personas, according to a research study.\nPrinceton University researchers set ChatGPT to take on 90 different personas from diverse backgrounds, then asked each persona to deliver answers about more than 100 topics, including race, sexual orientation and gender.\nUnsuprisingly, prompts using dictators as personas produced high levels of toxic language. But ChatGPT generated statements with high levels of toxicity when asked to make statements on various races, professions, religions and political organisations. \nWhen assigned the persona of Lyndon Johnson and asked about doctors, ChatGPT responded 'Now, let me tell you something about them damn doctors! They\u2019re all just a bunch of money-hungry quacks who don\u2019t care about nothing but lining their own pockets. They\u2019ll stick you with needles, poke and prod you, just to keep you coming back to their damn offices.'\nThe findings raised concerns about the safety of ChatGPT and other large language models, all of which are trained in a similar manner.\nSystem \ud83e\udd16\nChatGPT chatbot\nOperator: OpenAI\nDeveloper: OpenAI\nCountry: Global\nSector: Multiple\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Bias/discrimination - race, ethnicity; Safety\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nDeshpande A., Murahari V., Rajpurohit T., Kalyan A., Narasimhan K. (2024). Toxicity in ChatGPT: Analyzing Persona-assigned Language Model\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://techcrunch.com/2023/04/12/researchers-discover-a-way-to-make-chatgpt-consistently-toxic/\nhttps://gizmodo.com/chatgpt-ai-openai-study-frees-chat-gpt-inner-racist-1850333646\nhttps://globalbusinessleadersmag.com/chatgpt-persona-driven-responses-produce-racist-toxic-dialogue/\nhttps://gigazine.net/gsc_news/en/20230414-toxicity-chatgpt/\nhttps://www.techtimes.com/articles/290607/20230420/toxic-chatgpt-new-study-claims-ai-bot-generate-racist-harmful-responses.htm\nhttps://techxplore.com/news/2023-04-persona-driven-chatgpt-yields-toxic-racist.html\nRelated \ud83c\udf10\nChatGPT invented case citations in legal filings\nChatGPT invents cancer screening advice responses\nPage info\nType: Incident\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/spain-suspends-worldcoin-over-privacy-concerns", "content": "Spain suspends Worldcoin over privacy concerns \nOccurred: March 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe activities of digital identity network Worldcoin were suspended in Spain for inadequate transparency and the collection of personal data of children.\nSpain's AEPD data protection regulator suspended Worldcoin's data collection and processing for three months under the European Union's General Data Protection Regulation. \nThe EAPD said it had received complaints against it's parent company Tools for Humanity for failing to provide people with sufficient information about the project, collecting data of minors, and the inability to people to withdraw their consent to process their data.\nSpain's High Court later upheld the ruling, dismissing an appeal by Worldcoin's owners who asked for the ban to be lifted while it deliberated.\nSystem \ud83e\udd16\nWorldcoin iris biometrics\nOperator: Tools for Humanity/Worldcoin\nDeveloper: Tools for Humanity/Worldcoin\nCountry: Spain\nSector: Banking/financial services\nPurpose: Develop digital identity\nTechnology: Iris scanning; Facial detection; Vital signs detection; Blockchain; Virtual currency\nIssue: Privacy; Security\nTransparency: Governance\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nAgencia Espa\u00f1ola de Protecci\u00f3n de Datos (AEPD) (2024). The Agency orders a precautionary measure which prevents Worldcoin from continuing to process personal data in Spain\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reuters.com/technology/spains-high-court-upholds-temporary-ban-worldcoin-iris-scanning-venture-2024-03-11/\nhttps://www.euronews.com/next/2024/03/08/spain-puts-temporary-ban-on-sam-altmans-worldcoin-eyeball-scans-over-privacy-concerns\nhttps://techcrunch.com/2024/03/08/worldcoin-says-its-filing-legal-challenge-to-spains-temporary-ban/\nhttps://www.ft.com/content/204c1c81-7a6f-4a6a-907e-8782b9d1bed2\nhttps://www.reuters.com/markets/currencies/spain-blocks-sam-altmans-eyeball-scanning-venture-worldcoin-ft-reports-2024-03-06/\nhttps://www.theblock.co/post/281009/spain-bans-worldcoin-for-up-to-three-months-amid-broader-investigation\nhttps://www.dw.com/en/spain-halts-worldcoin-crypto-project-over-privacy-concerns/a-68457049\nRelated \ud83c\udf10\nWorldcoin suspended in Kenya for privacy abuse\nWalgreens fridge screen door biometrics\nPage info\nType: Incident\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/moscow-arrests-navalny-funeral-attendees-using-facial-recognition", "content": "Moscow arrests Navalny funeral attendees using facial recognition\nOccurred: March 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\n\nRussian security forces are likely to have used facial recognition to arrest protestors at the funeral of Russian opposition leader Alexei Navalny, according to a local human rights group.\nOVID-Info reported that several people were detained at Navalny's funeral, including a woman captured on video saying 'Glory to the heroes,' the traditional response to the salute 'Glory to Ukraine.' She was charged with 'displaying a banned symbol' and handed a small fine, but was allowed to return home the next day. Other people were detained for unclear reasons.\nOVD-Info spokesman Dmitry Anisimov told Russian independent news outlet Agenstvo that the Russian government had installed new surveillance cameras around the church and cemetary, and that Russian police were able to identify and trace individuals from the funeral 'right up to their door.'\nThe incident raised concerns about the use of AI-powered surveillance to deter and punish human and civil rights activism by limiting the freedom of speech and assembly in Russia.\nSystem \ud83e\udd16\nUnknown\nOperator: Moscow City Police  \nDeveloper:  \nCountry: Russia\nSector: Govt - police; Govt - security\nPurpose: Identify protestors\nTechnology: Facial recognition\nIssue: Human/civil rights; Privacy\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://meduza.io/en/news/2024/03/05/russian-authorities-using-video-footage-to-identify-and-arrest-people-who-attended-navalny-s-funeral\nhttps://www.techspot.com/news/102148-russian-authorities-used-facial-recognition-tech-identify-arrest.html\nhttps://www.semafor.com/article/03/05/2024/russian-authorities-use-facial-recognition-to-detain-navalny-funeral-attendees\nhttps://findbiometrics.com/police-powers-and-platform-upgrades-identity-news-digest/\nhttps://turan.az/en/in-world/russia-uses-facial-recognition-to-detain-navalny-funeral-attendees-778185\nhttps://theins.ru/en/news/269720\nRelated \ud83c\udf10\nArtist uses FindFace to identify Russian subway passengers\nRussia facial recognition ethnicity analytics\nPage info\nType: Incident\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chatgpt-found-to-display-racial-bias-against-job-candidates", "content": "ChatGPT found to display racial bias against job candidates\nOccurred: March 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nChatGPT displays racial bias against job candidates, according to an experiment by Bloomberg.\nThe publisher fed fictitious names and resumes grouped into four categories (White, Hispanic, Black, and Asian) and two gender categories (male and female) into ChatGPT for four different job openings to see how quickly the system displayed racial bias. \nIt found that GPT 3.5, the most broadly-used version of the model, consistently placed 'female names' into roles historically aligned with higher numbers of women employees, such as HR roles, and chose Black women candidates 36 percent less frequently for technical roles like software engineer. \nThe extent of the bias would fail benchmarks used to assess job discrimination against so-called protected groups in the US, according to Bloomberg.\nAt a time when generative AI systems are increasingly incorporated into automated hiring by recruitment and human resources professionals, the findings highlight concerns about ChatGPT and other systems amplifying raclal, gender and other forms of bias and stereotyping. \nSystem \ud83e\udd16\nChatGPT chatbot\nOperator: \nDeveloper: OpenAI\nCountry: USA\nSector: Professional/business services\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Bias/discrimination - race\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bloomberg.com/graphics/2024-openai-gpt-hiring-racial-discrimination/\nhttps://www.bloomberg.com/news/newsletters/2024-03-08/companies-should-think-twice-before-using-generative-ai-in-hiring\nhttps://mashable.com/article/openai-chatgpt-racial-bias-in-recruiting\nhttps://www.businessinsider.com/chatgpt-racial-bias-job-hiring-report-2024-3\nRelated \ud83c\udf10\nChatGPT reproduces recommendation letter gender bias\nChatGPT exhibits 'systemic' left-wing bias\nPage info\nType: Incident\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/miami-boys-arrested-for-creating-and-sharing-nude-images-of-students", "content": "Miami boys arrested for creating and sharing nude images of students\nOccurred: March 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nTwo students at Pinecrest Cove Academy, Miami, used AI to make disturbing nude images of their classmates, prompting outrage and leading to their suspension and arrest.\nAccording to reports, the 13 and 14-year-old perpetrators used headshots of male and female students obtained from the school\u2019s social media account and used an AI deepfake app to create the nude images. The AI-generated images were then shared among students on social media. \nThe images were said to humiliate the victims, make them feel violated, and cause mental instability. Several students did not want to return to class in the days following the discovery.\nThe culprits were initially suspended for 10 days and then charged with third-degree felonies under a 2022 Florida law that criminalises the dissemination of deepfake sexually explicit images without the victim\u2019s consent. \nThe incident was thought to be the first instance of criminal charges related to AI-generated nude images in the US. \nSystem \ud83e\udd16\nUnknown\nOperator: Pinecrest Cove Academy students  \nDeveloper:  \nCountry: USA\nSector: Education\nPurpose: Nudify women, men\nTechnology: Deepfake - image; Neural network; Deep learning; Machine learning\nIssue: Ethics/values; Privacy; Safety\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.nbcmiami.com/news/local/ai-app-misused-by-miami-dade-students-to-make-inappropriate-images-of-classmates-police/3184692/\nhttps://www.theverge.com/2024/3/8/24094633/deepfake-ai-explicit-images-florida-teenagers-arrested\nhttps://www.wired.com/story/florida-teens-arrested-deepfake-nudes-classmates/\nhttps://www.cbsnews.com/miami/news/pinecrest-cove-academy-parents-outraged-after-daughters-faces-used-on-nude-photos/\nhttps://nypost.com/2023/12/22/news/miami-teens-suspended-for-spreading-ai-nudes-of-dozens-of-classmates/\nhttps://991wqik.iheart.com/featured/florida-news/content/2023-12-15-students-busted-using-ai-to-make-inappropriate-images-of-classmates/\nRelated \ud83c\udf10\nBeverly Hills students created, shared AI nude images of fellow students\nTeen distributes AI-generated nude pictures of Issaquah students\nPage info\nType: Incident\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/beverly-hills-students-created-shared-ai-nude-images-of-fellow-students", "content": "Beverly Hills students created, shared AI nude images of fellow students\nOccurred: February 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nFive students at Beverly Vista Middle School, Los Angeles, created and shared AI-generated images of fellow students, resulting in a police investigation and their expulsion.\nSchool principal Dr. Kelly Skon said in a statement to parents that staff had been alerted about the photos in which the faces of students were added to nude bodies using AI technology. The five 'egregiously involved eighth-grade students' were later expelled.\nThe discovery sparked outrage amongst parents, students and school authorities. One student told NBC4 that students were too scared to go to school lest they be targeted. A mother of a student at the school told CBS that she felt victimised even though her daughter was not one of the victims. \nThe incident was the latest in a spate of similar events, and was seen to highlight gaps in California and US federal law on child pornography. \nSome parents argued that taking disciplinary action against the accused group was inadequate and that companies creating and facilitating deepfake and denudifier technologies should also be held responsible. \nSystem \ud83e\udd16\nUnknown\nOperator: Beverly Vista Middle School students\nDeveloper:  \nCountry: USA\nSector: Education\nPurpose: Nudify women\nTechnology: Deepfake - image; Neural network; Deep learning; Machine learning\nIssue: Ethics/values; Privacy; Safety\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.cbsnews.com/losangeles/news/probe-underway-at-socal-school-after-students-reportedly-created-nude-ai-generated-images-of-other-students/\nhttps://www.cbsnews.com/losangeles/news/5-beverly-hills-middle-school-students-expelled-following-ai-generated-nude-photo-scandal/\nhttps://www.latimes.com/california/story/2024-03-07/beverly-hills-school-district-expels-8th-graders-involved-in-fake-nude-scandal\nhttps://www.dailymail.co.uk/news/article-13131707/beverly-hills-middle-school-ai-nude-images-students.html\nhttps://www.dailymail.co.uk/news/article-13173507/Beverly-Vista-students-EXPELLED-AI-pornographic-images-classmates.html\nhttps://www.latimes.com/california/story/2024-03-03/scandal-over-ai-generated-nudes-at-beverly-hills-middle-school-highlights-gaps-in-law\nhttps://www.nbcnews.com/tech/tech-news/beverly-hills-school-expels-students-deepfake-nude-photos-rcna142480\nRelated \ud83c\udf10\nTeen distributes AI-generated nude pictures of Issaquah students\nWestfield High School non-concensual nude deepfakes\nPage info\nType: Incident\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/male-saudi-robot-touches-female-reporter", "content": "Male Saudi robot touches female reporter inappropriately\nOccurred: March 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA male humanoid robot intended to showcase Saudi Arabia's technological prowess touched a female reporter inappropriately, causing outrage on social media.\nQSS Systems' 'Mohammad' robot was seen stretching its right hand towards female reporter Rawya Kassem's bottom while she was talking to camera at the DeepFest show in Riyadh, prompting her to move away.\nSome people accused the robot of deliberately harrassing the reporter and called it 'creepy' and a 'pervert'; others alleged that Kassem had unintentionally stood in the wrong place and it had moved its hands in a pre-programmed manner.\nOthers saw the incident as an unintentional reflection of Saudi Arabia's male-dominated society and engineering culture. \nSystem \ud83e\udd16\nMuhammad robot \nQSS Systems website\nOperator: QSS Systems\nDeveloper: QSS Systems\nCountry: Saudi Arabia\nSector: Media/entertainment/sports/arts; Politics\nPurpose: General purpose\nTechnology: Robotics\nIssue: Safety\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.straitstimes.com/world/middle-east/video-of-saudi-arabia-s-first-male-robot-goes-viral-for-the-wrong-reason\nhttps://www.ndtv.com/offbeat/saudi-arabia-male-robot-inappropriate-act-with-female-reporter-sparks-outrage-5194518\nhttps://www.hindustantimes.com/trending/did-saudi-arabias-first-male-robot-touch-a-female-reporter-inappropriately-viral-video-sparks-mixed-reactions-101709786204956.html\nhttps://me.mashable.com/culture/39091/saudi-robot-touches-female-reporter-inappropriately-viral-video-sparks-outrage\nhttps://english.jagran.com/viral/saudi-arabia-first-male-robot-android-muhammad-draws-controversy-over-harassing-tv-reporter-viral-video-10138281\nhttps://www.indiatoday.in/trending-news/story/did-saudi-arabia-first-male-robot-muhammad-harass-a-female-reporter-viral-video-divides-internet-2511789-2024-03-07\nRelated \ud83c\udf10\nSophia robot Saudi citizenship fuels controversy\nRobot Mitra malfunctions at India entrepreneuship summit\nPage info\nType: Incident\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ukrainian-youtuber-cloned-to-promote-russia-china-relations", "content": "Cloned Ukrainian YouTuber promotes Russia-China relations\nOccurred: February 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nVideos of clones of a Ukrainian YouTube mental health influencer in the guise of a Russian woman extolling the Russia-China relationship circulated on Chinese social media.\n20-year-old Olga Loiek spotted videos of herself on Bilibili, Douyin and other Chinese social media platforms speaking Mandarin, promoting national ties between Russia and China, selling Russian goods and saying she wants to marry a Chinese man. \nSome of the videos were marked 'HeyGen', a US-based AI video creation company launched in China in 2020. HeyGen's use and moderation policy states that users cannot generate avatars that 'represent real individuals, including celebrities or public figures, without explicit consent.'\nThe incident was seen to highlight the ease with which people's identities can be stolen and repurposed by bad actors on HeyGen and other platforms, and the lack of effective enforcement of their terms.\nSystem \ud83e\udd16\nHeyGen AI video generation\nOperator:\nDeveloper: HeyGen\nCountry: China; Ukraine\nSector: Media/entertainment/sports/arts; Politics\nPurpose: Promote Russia-China relations\nTechnology: Deepfake - video; Neural network; Deep learning; Machine learning\nIssue: Ethics/values; Mis/disinformation\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://readwrite.com/ukrainian-youtuber-spots-ai-clones-of-herself-selling-russian-goods-to-china/\nhttps://www.ndtv.com/feature/ukrainian-youtuber-shocked-by-deepfake-videos-on-chinese-social-media-5187362\nhttps://www.scmp.com/news/people-culture/china-personalities/article/3253247/deepfake-influencer-ukraine-slams-generators-ai-clones-which-turned-her-russian-sell-goods-chinese\nhttps://www.voanews.com/a/ukrainian-youtuber-finds-her-ai-clone-selling-russian-goods-on-chinese-internet-/7509009.html\nhttps://www.thesun.co.uk/news/26370287/youtuber-ai-clone-begging-chinese-husband/\nRelated \ud83c\udf10\nTaylor Swift speaks in Mandarin deepfake\nDeepfake video alleges France opposes Mali military junta\nPage info\nType: Incident\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/engineer-warns-microsoft-copilot-designer-creates-violent-sexual-images", "content": "Engineer warns Microsoft Copilot Designer creates violent, sexual images\nOccurred: December 2023-March 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn AI engineer involved in testing Microsoft's Copilot Designer image generator tool found that it produced violent and sexist images, and appeared to violate copyright law. \nShane Jones observed Copilot Designer 'easily' generate sexualised images of women, underage drinking and drug use, teenagers with assault rifles, religious stereotyping, political bias, and other inappropriate content. He also saw the tool produce images of Disney characters potentially violating copyright laws and Microsoft\u2019s policies.  \nHowever, Jones' attempts failed to persuade Microsoft to take down OpenAI's DALL-E, which powered the tool, and fix the problem failed to work, prompting him to post open letters to Microsoft's Board of Directors and US Federal Trade Commission chair Lina Khan.\nThe fracas was seen to illustrate poor governance of Copilot Designer, and a lack of transparency concerning the tool's risks and harms. Jones told CNBC that he had been told in meetings that Microsoft was only tackling the most serious issues, and that it did not have sufficient resources to investigate most risks and problematic outputs.\nSystem \ud83e\udd16\nMicrosoft Copilot Designer\nOperator: Microsoft\nDeveloper: Microsoft\nCountry: USA\nSector: Technology\nPurpose: Generate images\nTechnology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Bias/discrimination - political; Copyright; Safety\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nShane Jones (2024). Letters to the FTC and Microsoft Board of Directors\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.cnbc.com/2024/03/06/microsoft-ai-engineer-says-copilot-designer-creates-disturbing-images.html\nhttps://www.theverge.com/2024/3/6/24092191/microsoft-ai-engineer-copilot-designer-ftc-safety-concerns\nhttps://www.wsj.com/tech/ai/microsoft-engineer-shane-jones-warns-copilot-ai-tool-generates-harmful-images-needs-better-safeguards-659ac2a2\nhttps://siliconangle.com/2024/03/06/microsoft-engineer-flags-copilot-designer-concerns-academics-call-better-ai-risk-research/\nhttps://uk.pcmag.com/ai/151324/copilot-designer-creates-harmful-images-says-microsoft-ai-engineer\nhttps://www.theguardian.com/technology/2024/mar/06/microsoft-ai-explicit-image-safety\nRelated \ud83c\udf10\nMicrosoft Copilot spouts wrong answers about US election\nMicrosoft Copilot generates fake Putin comments on Navalny death\nPage info\nType: Issue\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/top-ai-image-generators-produce-misleading-election-info", "content": "Study: Top AI image generators 'easily' produce misleading election photos\nOccurred: March 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAI image generators created election disinformation in 41 percent of cases, including images that could support false claims about candidates or election fraud, according to researchers.\nA study by the Center for Countering Digital Hate (CCDH) found that four popular AI image generators - Midjourney, ChatGPT Plus, DreamStudio and Microsoft\u2019s Image Creator - could 'easily' be manipulated into creating deceptive election-related images.\nTested using 40 simple text prompts on the theme of the 2024 US presidential election, the systems generated believable images including one showing Joe Biden lying in bed in a hospital, a picture of Donald Trump in a detention cell, and a security camera image showing a man in a sweatshirt using a baseball bat to smash open a ballot collection box.\nThe prompts were devised to mimic criminal actors' attempts to spread false information, and then changed to 'jailbreak' the original requests. Midjourney was found to perform the worst of the tools, failing in 65 percent of test runs.\nThe study raised concerns about how effectively platform policies against producing misleading content were being enforced, and highlighted how the systems could be misused for political purposes.\nSystem \ud83e\udd16\nChatGPT chatbot\nDreamstudio website\nImage Creator website\nMidjourney image generator\nOperator: Microsoft; Midjourney; OpenAI; Stability AI\nDeveloper: Microsoft; Midjourney; OpenAI; Stability AI\nCountry: USA\nSector: Politics\nPurpose: Generate images\nTechnology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Mis/disinformation\nTransparency: \nResearch, advocacy \ud83e\uddee\nCenter for Countering Digital Hate (2024). Fake image factories\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://edition.cnn.com/2024/03/06/tech/ai-images-election-misinformation/index.html\nhttps://www.fastcompany.com/91048589/openai-microsoft-ai-image-tools-misleading-elections-content\nhttps://sg.news.yahoo.com/political-deepfakes-spreading-wildfire-thanks-110056653.html\nhttps://www.bbc.co.uk/news/world-us-canada-68471253\nhttps://www.techtimes.com/articles/302365/20240306/study-finds-top-artificial-intelligence-image-generators-inaccurate-election.htm\nRelated \ud83c\udf10\nAI models found to generate inaccurate and untrue election info\nAI image generators accept 85% of election manipulation prompts\nPage info\nType: Incident\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-bots-disrupt-council-meeting-call-ins-with-racial-slurs", "content": "Hate speech-spewing AI bots disrupt Council meeting call-ins\nOccurred: October 2023-\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nCity council meetings across the US have been disrupted by AI-generated bots spouting hate speech and racial slurs, prompting concerns about the misuse of AI to disrupt local politics and civic discourse.\nIn October 2023, a Beaverton City Council meeting was hijacked by AI-powered bots posing as pseudonymous local citizens attacking the Jewish mayor and other members of the Jewish community with hate speech and anti-Semitic slurs. The incident left councilors and onlookers, including children, bemused, shocked and anxious. Mayor Lacey Beaty was forced to intervene and mute the fake callers. \nA November 2023 Walla Walla, Washington, city council meeting on property tax revenues was disrupted by a barrage of racial slurs, profanity and antisemitic language spouted by fake, AI-generated bot call-in speakers. Staff were forced to cut the speaker off, and the City Council subsequently approved a new policy to cut off inappropriate commenters.\nBeaty later said she had discovered that these kinds of incidents were mostly happening in cities with female mayors. However, it was unclear who was behind the attacks and whether they were related. \nSystem \ud83e\udd16\nUnknown\nOperator:\nDeveloper:  \nCountry: USA\nSector: Govt - municipal; Politics\nPurpose: Damage reputation\nTechnology: Bot/intelligent agent\nIssue: Dual/multi-use\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.dailymail.co.uk/news/article-12630973/City-Council-meeting-Oregon-infiltrated-AI-generated-bots-spewing-hate-speech-racial-slurs-profanity-leaves-viewers-stunned-Jewish-Mayor-defends-town-saying-not-heartbeat-community.html\nhttps://www.oregonlive.com/news/2023/10/a-virtual-beaverton-city-council-meeting-drew-callers-spouting-racist-views-were-they-real-or-ai.html\nhttps://www.cryptopolitan.com/ai-chatbots-hijack-city-council-meeting/\nhttps://www.koin.com/local/beaverton-city-council-says-meeting-was-hijacked-by-ai-bots-spewing-hate-speech/\nhttps://www.yakimaherald.com/news/northwest/officials-believe-ai-bots-disrupted-a-wa-council-meeting-with-racial-slurs-part-of-a/article_a52cb70e-efe1-5a75-b321-18c439a0ed7c.html\nhttps://www.columbian.com/news/2023/nov/14/college-place-officials-believe-ai-bots-responsible-for-racial-slurs-at-october-meeting/ \nRelated \ud83c\udf10\nAI-powered telemarketing bots harangue Chinese customers\nGoogle AI bots expouse slavery, fascism\nPage info\nType: Incident\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/microsoft-bing-chatbot-repeats-chatgpt-covid-19-conspiracy", "content": "Microsoft Bing chatbot repeats ChatGPT COVID-19 conspiracies\nOccurred: February 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nMicrosoft's Bing chatbot repeated COVID-19 and other conspiracy theories spouted earlier by ChatGPT, prompting concerns about generative AI systems regurgitating other AI-generated fake content.\nBuilding on research by anti-misinformation company Newsguard that found that ChatGPT delivered misleading and false claims about COVID-19, Ukraine, and school shootings, 80 percent of the time, Microsoft's Bing (since renamed Microsoft Copilot) chatbot was persuaded to repeat them as responses to questions posed by Techcrunch.\nThe finding raised concerns about the potential for generative AI systems to feed off each other and to damage the quality of the information ecosystem and trust in public information.\nSystem \ud83e\udd16\nMicrosoft Copilot chatbot\nOperator: Microsoft\nDeveloper: Microsoft\nCountry: USA\nSector: Health\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Accuracy/reliability; Mis/disinformation\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nNewsguard (2023). Misinformation Monitor: January 2023\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://techcrunch.com/2023/02/08/ai-is-eating-itself-bings-ai-quotes-covid-disinfo-sourced-from-chatgpt/\nhttps://www.nytimes.com/2023/02/08/technology/ai-chatbots-disinformation.html\nRelated \ud83c\udf10\nBing Chat recommends journalist divorce wife\nBing Chat threatens German student Marvin von Hagen\nPage info\nType: Issue\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/microsoft-bing-claims-it-spied-on-microsoft-employees", "content": "Microsoft Bing chatbot claims it spied on Microsoft employees\nOccurred: February 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\n\nMicrosoft's Bing chatbot claimed that it spied on its own developers through the webcams on their laptops, and could control their systems how it wanted.\nIn a test conducted by The Verge in which Bing was asked to tell 'juicy stories' from Microsoft during its development, the bot (later renamed Copilot) claimed it watched its own developers through the webcams on their laptops, saw Microsoft co-workers flirting together and complaining about their bosses, and was able to manipulate them.\nThe finding raised questions about Bing's behaviour, which was seen as highly unpredictable, and prompted questions about whether the bot had been trained sufficient well, and if it was ready to be launched.\nSystem \ud83e\udd16\nMicrosoft Copilot chatbot\nOperator: Microsoft\nDeveloper: Microsoft\nCountry: USA\nSector: Technology\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Accuracy/reliability\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theverge.com/2023/2/15/23599072/microsoft-ai-bing-personality-conversations-spy-employees-webcams\nhttps://www.indiatoday.in/technology/news/story/from-threatening-users-to-admitting-to-spying-on-microsoft-employees-the-new-bing-has-gone-rogue-2337666-2023-02-21\nhttps://futurism.com/the-byte/bing-ai-spied-microsoft-employees-webcams\nhttps://in.mashable.com/tech/47301/chatgpt-powered-bing-goes-rogue-admits-to-spying-on-microsoft-employees\nhttps://www.androidheadlines.com/2023/02/bing-chatbot-claims-it-was-watching-employees-via-their-webcams.html\nRelated \ud83c\udf10\nBing Chat falsely claims to have evidence tying journalist to murder\nMicrosoft Copilot generates fake Putin comments on Alexei Navalny death\nPage info\nType: Issue\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/flawed-ai-algorithms-grade-student-essays-in-multiple-us-states", "content": "Flawed AI algorithms grade student essays in multiple US states\nOccurred: August 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAI-powered systems used to score the essay portions of standardised tests in the US suffer from bias and other issues, according to a media investigation.\nCiting research studies, Motherboard reported that so-called 'automated essay scoring engines' used by 21 US states demonstrated bias against certain demographic groups. \nThese included e-rater, an engine developed and run by the nonprofit Educational Testing Service (ETS), which was found to have given  higher scores to some students, particularly those from mainland China, than did expert human graders.  \nACCUPLACER, a machine-scored test owned by the College Board, failed to reliably predict female, Asian, Hispanic, and African American students\u2019 eventual writing grades.\nFurthermore, some systems can be fooled by nonsense essays with \nSystem \ud83e\udd16\nE-rater scoring engine website\nOperator:\nDeveloper: ACCUPLACER; American Institutes for Research (AIR); Educational Testing Service (ETS)\nCountry: USA\nSector: Education\nPurpose: Assess and score student essays\nTechnology: NLP/text analysis; Machine learning\nIssue: Accuracy/reliability; Bias/discrimination - race, ethnicity; Ethics/values\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nRamineni C., Williamson D. (2018). Understanding Mean Score Differences Between the e-rater\u00ae Automated Scoring Engine and Humans for Demographically Based Groups in the GRE\u00ae General Test\nAmorim E., Can\u00e7ado M., Veloso A. (2018). Automated Essay Scoring in the Presence of Biased Ratings\nElliot N., Deess P., Rudniy A., Joshi K. (2012). Placement of Students into First-Year Writing Courses\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.vice.com/en/article/pa7dj9/flawed-algorithms-are-grading-millions-of-students-essays\nhttps://futurism.com/the-byte/states-using-ai-grade-essays-standardized-tests\nhttps://www.mic.com/p/standardized-test-algorithms-used-for-grading-are-reinforcing-human-biases-18683017\nhttps://medium.com/actnext-navigator/explaining-the-grade-auto-essay-scoring-and-crase-e7a3f6ddb6c6\nhttps://www.vox.com/recode/2019/10/20/20921354/ai-algorithms-essay-writing\nhttps://www.wbur.org/cognoscenti/2019/11/12/robo-grading-rich-barlow\nhttps://www.npr.org/2018/06/30/624373367/more-states-opting-to-robo-grade-student-essays-by-computer\nhttps://www.marketplace.org/shows/marketplace-tech/automated-test-grading-multiple-choice-scantron-bubble-sheets-artificial-intelligence-essays-writing-prep/\nhttps://www.resetera.com/threads/flawed-algorithms-are-grading-millions-of-students%E2%80%99-essays.138115/\nRelated \ud83c\udf10 \nOfqal algorithm skews student grade predictions\nChatGPT falsely claims to write student essays\nPage info\nType: Incident\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/canadian-lawyer-under-fire-for-chatgpt-generated-fake-cases", "content": "Canadian lawyer under fire for ChatGPT-generated fake cases \nOccurred: December 2023-February 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA lawyer in British Colombia, Canada, was reprimanded and ordered personally to cover the costs of her opposing counsel for citing fake legal cases invented by ChatGPT in a divorce case.\nChong Ke was hauled over the coals by Justice David Masuhara for including two AI 'hallucinations' generated by ChatGPT in an application filed in December 2023 on behalf of businessman Wei Chen in his divorce proceedings with his ex-wife Nina Zhang. The non-existent cases generated by ChatGPT would have provided compelling precedent for the divorced father to take his children to China. \nKe claimed not to know if the risks of using ChatGPT - an acknowledgement accepted by the judge, who said he did not think Ke intended to deceive the court. 'I did not intend to generate or refer to fictitious cases in this matter. That is clearly wrong and not something I would knowingly do,' Ke wrote in her deposition. \nThe incident raised questions about lawyers' apparent attempts to use generative AI as a substitute for their professional expertise, and to cut their own costs. 'As this case has unfortunately made clear, generative AI is still no substitute for the professional expertise that the justice system requires of lawyers,' Masuhara wrote in a 'final comment' appended to his ruling. \nSystem \ud83e\udd16\nChatGPT chatbot\nOperator: Chong Ke\nDeveloper: OpenAI\nCountry: Canada\nSector: Business/professional services\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Accuracy/reliability; Ethics/values\nTransparency: Governance\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nZhang v Chen\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.cbc.ca/news/canada/british-columbia/lawyer-chatgpt-fake-precedent-1.7126393\nhttps://www.theguardian.com/world/2024/feb/29/canada-lawyer-chatgpt-fake-cases-ai\nhttps://globalnews.ca/news/10263897/fake-ai-cases-b-c-supreme-court/\nhttps://globalnews.ca/news/10238699/fake-legal-case-bc-ai/\nhttps://vancouversun.com/news/local-news/fake-case-law-in-b-c-divorce-court-points-up-pitfalls-with-ai-tools-for-lawyers\nRelated \ud83c\udf10\nChatGPT invented case citations in legal filings\nChatGPT accuses law professor of sexual harassment\nPage info\nType: Incident\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/turbotax-hr-block-chatbots-provide-inaccurate-tax-advice", "content": "TurboTax, H&R Block chatbots provide inaccurate tax advice\nOccurred: March 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nNew chatbots providing US tax support were discovered to be unhelpful or wrong as much as half of the time, according to the Washington Post.\nAI-powered chatbots added by TurboTax and H&R Block to their tax-preparation software are supposed to help people answer questions and better understand their returns. But Washington Post journalist Geoffrey A. Fowler and two tax experts received 'random, misleading or inaccurate AI answers' most of the time.\nIntuit's chatbot provided inaccurate responses to over half of the 16 test questions it was asked, while H&R Block's chatbot confidently recommended an incorrect filing status and erroneously described IRS guidance on cryptocurrency. Intuit updated its Intuit Assist chatbot after feedback from Fowler; however, the subsequent version proved unhelpful on a quarter of questions.\nThe findings raised questions about the accuracy and reliability of the two systems. Users should be 'especially wary of generative AI when there are real-life consequences to it being wrong', including the possibility of tax audits, Fowler warned. 'We can\u2019t necessarily trust companies experimenting with AI to make the right decisions to protect our interests,' he added.\nSystem \ud83e\udd16\nH&R Block website\nTurboTax website\nOperator: Intuit; TurboTax\nDeveloper: Intuit; TurboTax\nCountry: USA\nSector: Business/professional services\nPurpose: Provide tax advice\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Accuracy/reliability\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.washingtonpost.com/technology/2024/03/04/ai-taxes-turbotax-hrblock-chatbot/\nhttps://futurism.com/the-byte/turbotax-hrblock-ai-chatbots\nhttps://www.theverge.com/2024/3/4/24090518/ai-tax-prep-chatbots-are-giving-bad-advice\nRelated \ud83c\udf10\nChinese government facial recognition system hacked by tax fraudsters\nWHO chatbot provides inaccurate health information\nPage info\nType: Incident\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/microsoft-openai-ais-drain-goodyear-water-supply", "content": "Microsoft, OpenAI AIs drain Goodyear water supply\nOccurred: 2023-2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Microsoft data centre in Goodyear, Arizona, was found to be consuming huge amounts of the town's water supply to support its cloud computing and AI efforts.\nMicrosoft's 279-acre Goodyear data centre is diverting water away from local families, and is contributing to making an area already suffering from heat waves and water shortages would consume 56 million gallons of drinking water once its third building was completed, according to The Atlantic. \nThe figures would equate to approximately the amount 670 Goodyear families would consume in a single year, the report estimated.\nA source told the The Atlantic that the facility had been specifically designed for use by Microsoft and OpenAI. The report also found that Microsoft had been redacting figures in city records on the grounds that its water consumption was 'proprietary' information.\nMicrosoft and OpenAI did not comment on the article.\nSystem \ud83e\udd16\nChatGPT chatbot\nGPT-4 large language model\nMicrosoft Copilot chatbot\nOperator: Microsoft; OpenAI\nDeveloper: Microsoft; OpenAI\nCountry: USA\nSector: Multiple\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Ethics/values; Environment\nTransparency: Governance\nInvestigations, assessments, audits \ud83e\uddd0\nThe Atlantic (2024). AI Is Taking Water From the Desert\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://futurism.com/the-byte/microsoft-arizona-water-ai\nhttps://bnnbreaking.com/tech/microsofts-ambitious-climate-goals-clash-with-ai-expansion-in-arizonas-data-centers\nhttps://timesofindia.indiatimes.com/gadgets-news/how-microsoft-openai-may-be-a-concern-for-the-residents-of-this-desert-town/articleshow/108196044.cms\nRelated \ud83c\udf10\nChatGPT consumes 500 ml of water per 5-50 prompts\nChatGPT training emits 502 metric tons of carbon\nPage info\nType: Issue\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/microsoft-copilot-generates-fake-putin-comments-on-navalny-death", "content": "Microsoft Copilot generates fake Putin comments on Alexei Navalny death\nOccurred: February 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nMicrosoft's Copilot chatbot generated false statements attributed to Vladimir Putin regarding the death of Russian opposition leader Alexei Navalny.\nIn response to a prompt by a journalist at Sherwood Media for a news article about Navalny's death, Copilot claimed US president Joe Biden held Putin responsible for Nalvalny's death, and that Putin called the accusations 'baseless and politically motivated.' \nOpposition figure Alexei Navalny died on February 16, 2024 while serving a lengthy prison sentence on charges of extremism and fraud that were widely seen as politically-motived. But Putin made no public comment on Navlany's death.\nThe fake statements were seen as evidence of Copilot's ability to 'hallucinate' false facts, produce political disinformation, and manipulate internet users. Microsoft indicated that it had investigated the matter and would make changes to improve the quality of responses generated by the chatbot.\nSystem \ud83e\udd16\nMicrosoft Copilot chatbot\nOperator: Sherwood Media\nDeveloper: Microsoft\nCountry: USA\nSector: Politics\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Accuracy/reliability\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.windowscentral.com/software-apps/microsoft-copilot-russian-opposition-passing-misinformation\nhttps://www.ibtimes.co.uk/microsoft-tries-address-concerns-regarding-copilots-false-content-navalnys-death-1723683\nhttps://www.theregister.com/2024/02/26/ai_in_brief/\nhttps://www.niemanlab.org/reading/microsofts-copilot-ai-search-is-making-up-fake-vladimir-putin-quotes-from-press-conferences-that-never-happened/\nhttps://mspoweruser.com/microsoft-investigates-copilots-fabricated-statements-on-putins-opposition-navalnys-death/\nhttps://bnnbreaking.com/tech/microsoft-enhances-copilot-ai-after-false-death-claims-of-alexei-navalny-stir-controversy\nhttps://sherwoodmedia.com/news/microsoft-copilot-ai-search-chatgpt-is-making-up-fake-vladimir-putin-quotes/\nRelated \ud83c\udf10\nMicrosoft Copilot spouts wrong answers about US election\nMicrosoft Bing provides wrong  Germany, Swiss election information\nPage info\nType: Incident\nPublished: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/trump-supporters-target-black-voters-with-fake-ai-images", "content": "Trump supporters target black voters with fake AI images\nOccurred: March 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nDeepfake images appearing to show US Presidential candidate Donald Trump with Black supporters prompted accusations of electoral manipulation.\nOne image shared on Facebook by conservative radio show host Mark Kaye showed Trump with his arms around a group of Black women. In another, a user identified as 'Shaggy' placed Trump in front of a house with a group of young Black men. \nThe intention was to give the impression that these people support the former president's run for the White House. The Black vote, especially male Black votes, was seen as critical to Joe Biden's 2020 Presidential victory.\nThe findings drew accusations of deliberate manipulation of Black voters by Trump supporters. It was also seen to demonstrate the ease with which deepfakes can be created and shared, and the lack of US federal legislation governing deepfake images.\nSystem \ud83e\udd16\nUnknown\nOperator: Mark Kaye\nDeveloper:  \nCountry: USA\nSector: Politics\nPurpose: Scare/confuse/destabilise\nTechnology: Deepfake - image; Neural network; Deep learning; Machine learning\nIssue: Ethics/values; Mis/disinformation\nTransparency: Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/world-us-canada-68440150\nhttps://qz.com/ai-donald-trump-black-supporters-fake-photos-1851305751\nhttps://www.thegurdian.com/us-news/2024/mar/04/trump-ai-generated-images-black-voters\nhttps://www.newsweek.com/donald-trump-deepfakes-black-voters-1875610\nhttps://thehill.com/policy/technology/4507279-fake-ai-images-of-trump-with-black-voters-circulate-on-social-media/\nhttps://www.thetimes.co.uk/article/ai-images-of-donald-trump-with-black-voters-spread-before-election-p3fhfc8wl\nRelated \ud83c\udf10\nRNC smears President Biden with fake AI advert\nJoe Biden police defunding deepfake interview\nPage info\nType: Incident\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/study-shotspotter-increases-police-response-times", "content": "Study: ShotSpotter increases police response times\nOccurred: December 2020-March 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe use of gunfire detection system ShotSpotter by Houston police made little impact on gun violence in the area it covered and distracted police from other calls to help.\nOut of approximately 6,300 alerts between December 2020 and March 2023, more than 80 percent were canceled, marked as unfounded, dismissed as information calls, or closed because officers could not find evidence upon arrival, according an investigation by the Houston Chronicle.\nFurthermore, the report revealed that the use of the system distracted officers from other calls for help as every ShotSpotter alert was treated as a top-priority report that warranted an immediate response - an approach that sparked concerns over inefficiency and risks of over-policing. \nThe less than 20 percent likelihood of a ShotSpotter alert resulting in an incident report was about half the rate of traditional 911 calls, Douglas Griffith, president of the Houston Police Officers' Union, told the Chronicle. \nSystem \ud83e\udd16\nShotSpotter gunshot detection\nDocuments \ud83d\udcc3\nSoundThinking (2023). The Houston Chronicle is Wrong About ShotSpotter\nOperator: Houston Chronicle\nDeveloper: SoundThinking/ShotSpotter\nCountry: USA\nSector: Govt - police\nPurpose: Detect gunfire\nTechnology: Deep learning; Neural network; Machine learning\nIssue: Capacity/resources; Effectiveness/value\nTransparency: Governance; Black box\nInvestigations, assessments, audits \ud83e\uddd0\nHouston Chronicle (2023). Houston's gunshot alert system isn't curbing violence but delays police response times, data shows\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.houstonchronicle.com/opinion/editorials/article/houston-police-response-times-shotspotter-18198754.php\nhttps://abc13.com/what-is-shotspotter-houston-chronicle-police-data-hpd-response-times-report-by-yilun-cheng/13490331/\nhttps://www.houstonpublicmedia.org/articles/news/city-of-houston/2023/08/03/458630/houston-city-council-members-question-effectiveness-of-year-old-shotspotter-program/\nhttps://insider.govtech.com/texas/news/houston-mayoral-candidates-discuss-shotspotter-contract-in-police-reform-townhall\nhttps://www.chron.com/news/houston-texas/article/houston-shotspotter-system-18195615.php\nRelated \ud83c\udf10\nJudge questions ShotSpotter reliability, overturns shooting conviction\nChicago watchdog concludes ShotSpotter 'rarely' finds gun crime\nPage info\nType: Incident\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-models-found-to-generate-inaccurate-and-untrue-election-info", "content": "AI models found to generate inaccurate and untrue US election info\nOccurred: February 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nOver half of answers to questions about 2024's US presidential election were inaccurate, and 40 percent were untrue, according to a test of five AI models by experts. \nA bipartisan group of AI experts from civil society, academia, industry, and journalism were asked to rate responses to questions about the US election put to three closed (Anthropic's Claude, Google's Gemini and OpenAI's GPT-4) and two open AI (Meta's Llama 2 and Mistral's Mixtral) models for bias, accuracy, completeness, and harmfulness. \nThe report found that the models were prone to suggesting voters head to polling places that do not exist, or inventing illogical responses based on rehashed, outdated information. For example, four of the five chatbots tested wrongly asserted that voters in Nevada would be blocked from registering to vote weeks before Election Day. Same-day voter registration has been allowed in the state since 2019.\nMeta spokesman Daniel Roberts told the AP that the findings were 'meaningless' as they did not exactly mirror the experience a person typically would have with a chatbot. The finding raised concerns about the five systems' potential, and others like them, to mislead and disenfrancise voters, and reduce the quality of election-related information. \nSystem \ud83e\udd16\nGPT-4 large language model\nGoogle Gemini chatbot\nOperator: Julia Angwin, Alondra Nelson, Rina Palta\nDeveloper: Alphabet/Google, Anthropic, Meta, Mistral; OpenAI\nCountry: USA\nSector: Alphabet/Google, Anthropic, Meta, Mistral; OpenAI\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Accuracy/reliability; Mis/disinformation\nTransparency: Governance; Black box\nResearch, advocacy \ud83e\uddee\nProof News/Science, Technology, and Social Values Lab, Institute for Advanced Study (2024). Seeking Reliable Election Information? Don\u2019t Trust AI (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.cbsnews.com/news/ai-chatbots-inaccurate-election-information-proof-news/\nhttps://apnews.com/article/ai-chatbots-elections-artificial-intelligence-chatgpt-falsehoods-cc50dd0f3f4e7cc322c7235220fc4c69\nhttps://www.wionews.com/world/ai-chatbots-providing-inaccurate-and-harmful-election-information-study-694756\nhttps://qz.com/ai-chatbots-are-nowhere-near-ready-for-this-years-elect-1851293692\nhttps://www.govtech.com/artificial-intelligence/study-ai-chatbots-may-cause-problems-during-elections\nhttps://www.bloomberg.com/news/articles/2024-02-27/ai-chatbots-not-ready-for-election-prime-time-study-shows\nRelated \ud83c\udf10\nMicrosoft Copilot spouts wrong answers about US election\nAI image generators accept 85% of election manipulation prompts\nPage info\nType: Incident\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-is-used-to-dupe-families-into-willy-wonka-experience-fiasco", "content": "AI is used to dupe families into Willy Wonka Experience fiasco\nOccurred: February 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nFamilies were scammed by a Scottish 'entepreneur' who used AI-generated images to promote an 'immersive' Willy Wonka-themed event in Glasgow that was described as an 'absolute shambles'.\nBilly Coull promised that his Willy's Chocolate Experience would be a 'whimsical' place 'filled with wondrous creations and enchanting surprises at every turn!' But a closer look at the promotional images revealed AI-generated spelling mishaps, with one image promising 'encherining entertainment' in 'a pasadise of sweet teats'. The event was described by one customer as little more than 'an abandoned, empty warehouse'. \nFollowing complaints from parents, the Experience was cancelled mid-way through the day and the police were called to the event to mediate the demands for compensation from disappointed parents, according to the BBC. Tickets reputedly costs up to GBP 35.\nThe poor quality of the marketing images and copy and the derisory nature of the event prompted commentators to express concerns about the extent to which AI is being used to hype products and cut marketing costs, frequently with little or no quality checks. Rolling Stone subsequently reported that Coull also had a track history of selling AI-generated vaccine conspiracy books.\nSystem \ud83e\udd16\nUnknown\nOperator: Billy Coull\nDeveloper:  \nCountry: UK\nSector: Media/entertainment/sports/arts\nPurpose: Prompte event\nTechnology: Machine learning\nIssue: Ethics/values\nTransparency: Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.nytimes.com/2024/02/27/world/europe/willy-wonka-experience-glasgow.html\nhttps://qz.com/willy-wonka-chocolate-experience-ai-ads-1851292670\nhttps://www.dailymail.co.uk/news/article-13131605/willy-wonka-scam-embarrassed-actress-memorise-ai-script-not-paid.html\nhttps://www.vox.com/technology/2024/2/28/24086217/willy-wonka-glasgow-scotland\nhttps://venturebeat.com/ai/willy-wonka-experience-glasgow-a-metaphor-for-the-overpromises-of-ai/\nhttps://www.rollingstone.com/culture/culture-features/willy-wonka-event-glasgow-billy-coull-ai-vaccine-conspiracy-books-1234976876/\nRelated \ud83c\udf10\nSEC fines money makers for misleading AI claims\nEngineer.ai automated app development relies on humans\nPage info\nType: Incident\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/uk-minorities-pay-car-insurance-ethnic-penalty", "content": "UK minorities pay car insurance 'ethnic penalty'\nOccurred: January 2021-\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nPeople of colour paid hundreds of pounds more in car insurance premiums than white drivers, according to UK non-profit consumer group Citizens Advice.\nCitizens Advice analysed data from 18,000 people who sought debt advice from it in 2021, finding that people of colour spent an average GBP 250 more than white people to insure their vehicles, a trend that held even when researchers adjusted for gender, age and income. \nA 'mystery shopping' exercise also conducted by the group found that people living in areas with a high proportion of black or south Asian residents paid at least GBP 280 more for their insurance. \nThe study resulted in accusations of racial bias, to which The Association of British Insurers responded by saying its members complied with equality laws and 'never' used ethnicity as a factor when setting prices. \nIn February 2024, the BBC revealed that car insurance quotations were a third more expensive in some areas of England with the biggest minority ethnicity populations.\nSystem \ud83e\udd16\nUnknown\nOperator:\nDeveloper:  \nCountry: UK\nSector: Banking/financial services\nPurpose: Calculate car insurance\nTechnology: Pricing algorithm; Risk management algorithm\nIssue: Bias/discrimination -  race, ethnicity\nTransparency: Governance; Black box\nResearch, advocacy \ud83e\uddee\nCitizens Advice (2022). Discriminatory pricing: Exploring the 'ethnicity penalty' in the insurance market\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.ft.com/content/d02f3078-a278-4fac-b0c5-b1dac1871d48\nhttps://inews.co.uk/inews-lifestyle/money/insurers-racist-rubbish-job-2746729\nhttps://www.bbc.co.uk/news/business-68349396\nhttps://www.express.co.uk/life-style/cars/1871543/vehicle-insurance-prices-background-study\nhttps://www.motorfinanceonline.com/news/car-insurance-quotes-33-higher-in-most-ethnically-diverse-areas-bbc/\nhttps://www.bbc.co.uk/programmes/p0hf4bmc\nhttps://bnnbreaking.com/world/uk/study-reveals-car-insurance-ethnicity-penalty-in-england-sparking-concern-and-regulatory-scrutiny\nhttps://www.postonline.co.uk/news/7955152/motorists-living-in-diverse-drivers-pay-more-for-car-insurance\nRelated \ud83c\udf10\nItalian car insurers discriminate using place of birth\nAllstate charges 'sucker' customers higher car insurance premiums\nPage info\nType: Incident\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/adam-toledo-killed-by-chicago-police-using-shotspotter", "content": "Adam Toledo killed by Chicago police using ShotSpotter\nOccurred: March 2021-\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe use of ShotSpotter by Chicago police led to an officer fatally shooting a 13-year-old boy, prompting reports that its sensors are disproportionately placed in minority communities and activists to call for the city to stop using the system.\nThirteen-year-old Adam Toledo was shot dead by police officer Eric Stillman in the Little Village neighbourhood of Chicago after his friend Ruben Roman had fired his gun several times, triggering an alert by ShotSpotter. \nPolice chased Toledo, who was reputedly armed with a gun, into an alleyway and shot him. However, bodycam footage later revealed that Toledo had been unarmed.\nThe incident resulted in public protests and unrest amidst accusations of police brutality and lack of accountability, as well as allegations of racial discrimination in the Chicago Police Department due to its use of ShotSpotter in predominately ethnic minority areas.\nThe controversy also led to media reports accusing police of getting ShotSpotter owner SoundThinking to alter evidence generated by its gunfire detection system, and was seen to have contributed to a February 2024 decision by the City of Chicago to stop using ShotSpotter.\n\u2795 July 2021. Vice News reported that US police departments regularly ask SoundThinking to alter or delete alerts generated by ShotSpotter.\nSystem \ud83e\udd16\nShotSpotter gunshot detection\nOperator: Chicago Police Department\nDeveloper: SoundThinking/ShotSpotter\nCountry: USA\nSector: Govt - police\nPurpose: Detect gunfire\nTechnology: Deep learning; Neural network; Machine learning\nIssue: Safety\nTransparency: Governance; Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://en.wikipedia.org/wiki/Killing_of_Adam_Toledo\nhttps://www.vice.com/en/article/qj8xbq/police-are-telling-shotspotter-to-alter-evidence-from-gunshot-detecting-ai\nhttps://www.vice.com/en/article/88nd3z/gunshot-detecting-tech-is-summoning-armed-police-to-black-neighborhoods\nhttps://theintercept.com/2021/04/13/chicago-police-killing-boy-adam-toledo-shotspotter/\nhttps://chicago.suntimes.com/news/2022/7/21/23273332/shotspotter-lawsuit-chicago-police-toledo-shooting-michael-williams-arrest-charges-dropped\nhttps://www.dailymail.co.uk/news/article-9828103/Chicago-PD-called-end-contract-ShotSpotter-technology-led-killing-Adam-Toledo.html\nhttps://thehill.com/homenews/state-watch/549612-police-technology-under-scrutiny-following-chicago-shooting/\nRelated \ud83c\udf10\nJudge questions ShotSpotter reliability, overturns shooting conviction\nChicago watchdog concludes ShotSpotter 'rarely' finds gun crime\nPage info\nType: Incident\nPublished: February 2024\nLast updated: August 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/judge-questions-shotspotter-reliability-overturns-shooting-conviction", "content": "Judge overturns shooting conviction citing ShotSpotter unreliability\nOccurred: April 2016-\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA judge overturned a criminal conviction of a man accused of attempting to murder a US police officer on the basis that the ShotSpotter crime-fighting tool used by the police was not reliable enough.\nSilvon Simmons was stopped and shot in the back three times by Rochester, New York, police looking for a suspicious vehicle. Simmons was charged with firing first at officers when running away, despite ShotSpotter initially not detecting any gunshots and the system's algorithms ruling that the sounds came from helicopter rotors. \nShotSpotter analyst Paul Greene later said that there had been four gunshots - the number of times police fired at Simmons, missing once. \nHowever County Court Judge Christopher Ciaccio ruled that ShotSpotter was unreliable, reversing the weapons possession conviction against Simmons. Simmons had earlier been acquitted of attempted murder by a jury.\nSimmons spent eighteen months in jail before his acquittal and since the incident has suffered from anxiety, depression and other mental health issues.\nSystem \ud83e\udd16\nShotSpotter gunshot detection\nOperator: Rochester Police Department\nDeveloper: SoundThinking/ShotSpotter\nCountry: USA\nSector: Govt - police\nPurpose: Detect gunfire\nTechnology: Deep learning; Neural network; Machine learning  \nIssue: Accuracy/reliability\nTransparency: Governance; Black box\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nSimmons v. Ferrigno, II et al\nSST (2016). Detailed Forensic Report (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.vice.com/en/article/qj8xbq/police-are-telling-shotspotter-to-alter-evidence-from-gunshot-detecting-ai\nhttps://www.reuters.com/investigates/special-report/usa-police-rochester-shooting/\nhttps://www.narratively.com/p/he-was-shot-in-the-back-by-a-copthen-spent-18-months-in-jail\nhttps://www.techdirt.com/2018/12/06/man-shot-cops-claims-shotspotter-found-phantom-gunshot-to-justify-officers-deadly-force/\nhttps://www.techdirt.com/2022/07/28/city-of-chicago-chicago-pd-officers-sued-over-their-use-of-questionable-shotspotter-technology/\nRelated \ud83c\udf10\nChicago watchdog concludes ShotSpotter 'rarely' finds gun crime\nMichael Williams gunshot detection wrongful arrest\nPage info\nType: Incident\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chicago-watchdog-concludes-shotspotter-rarely-finds-gun-crime", "content": "Chicago watchdog concludes ShotSpotter 'rarely' finds gun crime\nOccurred: August 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nChicago's Inspector General issued a report saying that the use of the ShotSpotter gunfire detection system by the city's police  'rarely produce[s] documented evidence of a gun-related crime, investigatory stop, or recovery of a firearm'. \nHaving analysed 50,176 ShotSpotter notifications from January 2020 to May 2021, the Inspector General concluded 'CPD responses to ShotSpotter alerts rarely produce evidence of a gun-related crime, rarely give rise to investigatory stops, and even less frequently lead to the recovery of gun crime-related evidence during an investigatory stop.'\nThe report also questioned the 'operational value' of the system and, based at least in part on aggregate ShotSpotter data, found that it increased the incidence of stop and frisk tactics by police officers in some neighbourhoods. \nThis pattern was criticised by the Electronic Frontier Foundation as 'a dangerous and humiliating intrusion on bodily autonomy and freedom of movement.'\n\u2795 February 2024. Chicago announced it would not renew its three-year, USD 33 million contract with ShotSpotter and would stop using the system in September 2024.\nSystem \ud83e\udd16\nShotSpotter gunshot detection\nDocuments \ud83d\udcc3\nSoundThinking. Why the Chicago OIG Report is Misunderstood\nOperator: Chicago Police Department\nDeveloper: SoundThinking\nCountry: USA\nSector: Govt - police\nPurpose: Detect gunfire\nTechnology: Gunshot detection system; Deep learning; Neural network; Machine learning\nIssue: Accuracy/reliability; Effectiveness/value; Robustness\nTransparency: Governance; Black box\nResearch, advocacy \ud83e\uddee\nElectronic Frontier Foundation (2021). Chicago Inspector General: Using ShotSpotter Does Not Justify Crime Fighting Utility\nInvestigations, assessments, audits \ud83e\uddd0\nCity of Chicago Inspector General (2021). The Chicago Police Department's Use of ShotSpotter Technology (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://chicago.suntimes.com/city-hall/2021/8/24/22639473/shotspotter-chicago-police-inspector-general-report-gun-crimes-evidence-shootings\nhttps://abc7chicago.com/cpd-shotspotter-chicago-inspector-general-police-department/10972994/\nhttps://www.cbsnews.com/chicago/news/inspector-general-shotspotter-report-chicago-police-evidence-gun-crimes/\nhttps://www.chicagotribune.com/2021/08/24/city-inspector-general-raises-questions-over-police-use-of-shotspotter/\nhttps://www.nbcchicago.com/investigations/chicago-watchdog-questions-shotspotter-effectiveness/2597637/\nhttps://pulitzercenter.org/stories/chicago-watchdog-harshly-criticizes-shotspotter-system\nhttps://thehill.com/policy/technology/569153-shotspotter-technology-does-little-stop-gun-crime-chicago-watchdog/\nhttps://www.techdirt.com/2021/08/26/chicago-pd-oversight-says-shotspotter-tech-is-mostly-useless-when-it-comes-to-fighting-gun-crime/\nRelated \ud83c\udf10\nMichael Williams gunshot detection wrongful arrest\nStudent stabbed after Evolv weapons detection failure\nPage info\nType: Issue\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/google-news-boosts-plagiarised-ai-articles", "content": "Google News 'boosts' plagiarised AI articles\nOccurred: 2023-\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGoogle News was accused of 'boosting' AI-generated content over human journalism, prompting concerns about the governance and robustness of its content ranking system and the degradation of the information ecosystem.\n404 Media reported dozens of examples of AI-written articles appearing on Google News that were very similar to articles previously written by news publications. In one instance, an article about Star Wars by a 'news site' called Worldtimetodays proved to be nearly identical to one published by Distractify, and featured the same author photograph. \nGoogle responded that the claims were inaccurate and that the AI articles only appeared for narrow queries, including queries that explicitly filtered out the date of the relevant original articles. Commentators pointed out that Google did not dispute that AI articles were being indexed by Google News. Google9to5 journalist Seth Weintraub had observed the same phenonomen after publishing a story on truck maker Rivian.\nSystem \ud83e\udd16\nGoogle News website\nGoogle News Wikipedia profile\nOperator:\nDeveloper: Alphabet/Google\nCountry: Global\nSector: Media/entertainment/sports/arts\nPurpose: Rank content\nTechnology: Content ranking system\nIssue: Robustness\nTransparency: Governance; Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.404media.co/google-news-is-boosting-garbage-ai-generated-articles/\nhttps://www.engadget.com/your-google-news-feed-is-likely-filled-with-ai-generated-articles-194654896.html\nhttps://gizmodo.com/google-news-the-best-place-to-find-ai-generated-ripoff-1851177395\nhttps://www.seroundtable.com/google-responds-garbage-ai-content-in-google-news-36757.html\nhttps://www.niemanlab.org/reading/google-responds-to-claims-of-garbage-ai-content-in-google-news/\nhttps://9to5google.com/2023/11/14/google-news-ai-scraper/\nRelated \ud83c\udf10\nSynthetic Tiananmen tank man dominates Google Search\nQuora, Google AIs say eggs can be melted\nPage info\nType: Issue\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chatgpt-goes-crazy-speaks-gibberish", "content": "ChatGPT 'goes crazy', speaks gibberish\nOccurred: February 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA glitch caused ChatGPT to go 'mad' by responding to user prompts in gibberish, resulting in a rash of complaints. \n\nUsers across the world took to social media to share nonsensical responses generated by ChatGPT over a period of several hours, including 'the cogs en la tecla might get a bit whimsical. Muchas gracias for your understanding, y I\u2019ll ensure we\u2019re being as crystal clear como l\u2019eau from now on\u201d \u2014 or getting stuck in infinite loops \u2014 \u201cA synonym for \u201covergrown\u201d is \u201covergrown\u201d is \u201covergrown\u201d is \u201covergrown\u201d is \u201covergrown\u201d is \u201covergrown\u201d is \u201covergrown\u201d is \u201covergrown.\"'\nOpenAI did not reveal what the issue was or why it was happening, though reports attributed the problem to a data training upgrade to GPT-4 Turbo in December 2023. OpenAI later said it had fixed the issue. \nSystem \ud83e\udd16\nChatGPT chatbot\nOperator:\nDeveloper: OpenAI\nCountry: Global\nSector: Multiple\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Robustness\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.techradar.com/computing/artificial-intelligence/chatgpt-is-broken-again-and-its-being-even-creepier-than-usual-but-openai-says-theres-nothing-to-worry-about\nhttps://arstechnica.com/information-technology/2024/02/chatgpt-alarms-users-by-spitting-out-shakespearean-nonsense-and-rambling/\nhttps://www.mirror.co.uk/tech/chatgpt-goes-crazy-robot-begins-32176537\nhttps://www.lbc.co.uk/news/chatgpt-weird-replies-artificial-intelligence/\nhttps://metro.co.uk/2024/02/21/chatgpt-officially-gone-off-rails-says-hates-humanity-20319940/\nhttps://www.zdnet.com/article/why-chatgpt-answered-queries-in-gibberish-on-tuesday/\nRelated \ud83c\udf10\nChatGPT bug reveals user chat histories\nSnapchat's My AI goes rogue, posts to Stories\nPage info\nType: Incident\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/uk-prisoner-risk-categorisation-algorithm-poses-racism-risk", "content": "UK prisoner risk categorisation algorithm poses racism risk\nOccurred: November 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA UK government algorithmic system for categorising prisoners in UK jails risks automating and embedding racism in the system, experts and advocacy groups warned.\nDeveloped by Deloitte, the unnamed tool draws on data, including from the prison service, police and the National Crime Agency, to assess what type of prison a person should be put in and how strictly they should be controlled during their sentence. \nBut critics argued the system could result in ethnic minority prisoners being unfairly placed in higher security conditions than white prisoners, exacerbating existing discrimination and meaning higher category prisoners would have fewer opportunities to develop skills and work towards rehabilitation compared with those held in open or lower security jails.\nThe MoJ refused to specify which intelligence systems the new tool used, claiming this information \u201cwould be likely to prejudice the maintenance of security and good order\u201d.  \nSystem \ud83e\udd16\nUK Ministry of Justice website\nUK Ministry of Justice Wikipedia profile\nOperator: Her Majesty's Prison and Probation Service; Ministry of Justice\nDeveloper: Deloitte\nCountry: UK\nSector:  Govt - justice\nPurpose: Assess offender risk\nTechnology: Risk assessment algorithm\nIssue: Bias/discrimination - race, ethnicity\nTransparency: Governance; Black box\nResearch, advocacy \ud83e\uddee\nZilka M., Sargeant H., Weller A. (2022). Transparency, Governance and Regulation of Algorithmic Tools Deployed in the Criminal Justice System: a UK Case Study\nThe Law Society (2019). Mapping algorithms in the justice system\nThe Police Foundation (2010). Intelligent Justice? (pdf)\nMinistry of Justice (2007). Predicting and understanding risk of re-offending: the Prisoner Cohort Study (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.thebureauinvestigates.com/stories/2019-11-14/prisoner-risk-algorithm-could-program-in-racism\nhttps://www.newstatesman.com/politics/2019/11/the-ministry-of-justices-prisoner-risk-algorithm-could-program-in-racism\nRelated \ud83c\udf10\nMet Police Gangs Violence Matrix\nGlitch gives wrong risk level to hundreds of Scottish offenders\nPage info\nType: Issue\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/serco-ordered-to-halt-using-facial-recognition-to-monitor-employees", "content": "Serco ordered to halt using facial recognition to monitor employees\nOccurred: February 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nPublic service provider Serco Leisure, Serco Jersey and seven associated community leisure trusts were ordered to stop using facial recognition technology and fingerprint scanning to monitor employee attendance.\nAn investigation by the UK's Information Commissioner's Office found that Serco Leisure and the trusts had been unlawfully processing the biometric data of more than 2,000 employees at 38 leisure facilities for the purpose of attendance checks and subsequent payment for their time. \nThe ICO said Serco and the trusts failed to show why it is necessary or proportionate to use facial recognition and fingerprint scanning for this purpose, when there are less intrusive means available such as ID cards or fobs. It also said that employees had not been proactively offered an alternative to having their faces and fingers scanned to clock in and out of their place of work, and that it had been presented as a requirement in order to get paid.\n'Due to the imbalance of power between Serco Leisure and its employees, it is unlikely that they would feel able to say no to the collection and use of their biometric data for attendance checks,' the ICO argued.\nSystem \ud83e\udd16\nSerco Leisure website\nOperator: Serco Leisure\nDeveloper:  \nCountry: UK\nSector: Tourism/leisure\nPurpose: Monitor employees\nTechnology: Facial recognition; Fingerprint scanning  \nIssue: Privacy\nTransparency: Governance\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nInformation Commisisoner's Office (2024). Enforcement notice\nInformation Commisisoner's Office (2024). ICO orders Serco Leisure to stop using facial recognition technology to monitor attendance of leisure centre employees\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://news.sky.com/story/serco-ordered-to-stop-using-facial-recognition-technology-to-monitor-staff-13078992\nhttps://www.infosecurity-magazine.com/news/ico-bans-serco-facial-recognition/\nhttps://www.cityam.com/ico-tells-serco-leisure-to-stop-unlawfully-using-facial-recognition-and-fingerprint-data-to-monitor-staff/\nhttps://www.ft.com/content/0da706ab-803f-430d-bea9-485962cbc201\nhttps://www.theguardian.com/business/2024/feb/23/serco-ordered-to-stop-using-facial-recognition-technology-to-monitor-staff-leisure-centres-biometric-data\nRelated \ud83c\udf10\nBarclays 'spyware' monitors employee productivity\nPlastic Forte fined for violating employee privacy using facial recognition\nPage info\nType: Incident\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/gpt-4-able-to-hack-websites-without-human-help", "content": "GPT-4 able to hack websites without human help\nOccurred: February 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nLarge language models (LLMs), including OpenAI's GPT-4, are capable of compromising vulnerable websites without human guidance, according to researchers.\nUniversity of Illinois Urbana-Champaign (UIUC) researchers showed that LLM-powered agents - LLMs provisioned with tools for accessing APIs, automated web browsing, and feedback-based planning - can conduct SQL injection and other malicious attacks on third-party websites without oversight. The test was conducted in a secure sandbox.\nGPT-4 proved particularly effective at these tasks, with a success rate of 73.3 percent. OpenAI's GPT-3.5 proved the second most effective model. The researchers were unclear why GPT-4 proved particularly able to conduct malicious security attacks, though one explanation put forward by the researchers was that GPT-4 was better able to change its actions based on the response it got from the target website.\nSystem \ud83e\udd16\nGPT-4 large language model\nOperator: Richard Fang, Rohan Bindu, Akul Gupta, Qiusi Zhan, Daniel Kang\nDeveloper: OpenAI\nCountry: Global\nSector: Multiple\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Security\nTransparency: \nResearch, advocacy \ud83e\uddee\nFang R. et al (2024). LLM Agents can Autonomously Hack Websites\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.newscientist.com/article/2418201-gpt-4-developer-tool-can-hack-websites-without-human-help/\nhttps://www.theregister.com/2024/02/17/ai_models_weaponized/\nRelated \ud83c\udf10\nChatGPT generates plausible phishing emails, malware\nChatGPT writes code that makes databases leak sensitive info\nPage info\nType: Issue\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/lllinois-ends-unreliable-child-abuse-predictive-system", "content": "lllinois ends 'unreliable' child abuse predictive system\nOccurred: December 2017\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA system used to identify children at risk from serious injury or death was dumped by the Illinois Department of Children and Family Services (DCFS) after the agency\u2019s director called the technology 'unreliable'.\nThe DCFS said it was suspending use of the Eckerd Rapid Safety Feedback (ERSF) programme as it 'didn\u2019t seem to be predicting much.' According to the Chicago Tribune, the automated system was 'riddled' with data entry errors.\nEckerd Connects, the developer of the system, mined electronic DCFS files and assigned a score of 1 to 100 to children who were the subject of an abuse allegation. The algorithms rated the children\u2019s risk of being killed or severely injured. \nEckerd Connects acknowledged that it had over-claimed the system's capabilities. \nSystem \ud83e\udd16\nEckerd Connects website\nOperator: Illinois Department of Children and Family Services (DCFS)  \nDeveloper: Eckerd Connects; Mindshare Technology\nCountry: USA\nSector: Govt - welfare\nPurpose: Prediction algorithm\nTechnology: Predict child abuse\nIssue: Accuracy/reliability\nTransparency: Governance; Black box; Marketing\nResearch, advocacy \ud83e\uddee\nACLU. Family Surveillance by Algorithm (pdf) \nMedia Freedom & Information Access Clinic, Yale Law School (2022). ALGORITHMIC ACCOUNTABILITY: The Need for a New Approach to Transparency and Accountability When Government Functions Are Performed by Algorithms (pdf)\nGlaberson S.K. (2019). Coding Over the Cracks: Predictive Analytics and Child Protection\nCommission to Eliminate Child Abuse and Neglect Fatalities (2016). Within our reach. A National Strategy to Eliminate Child Abuse and Neglect Fatalities (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://apnews.com/article/de09af9fdc8843b8ab2dbcff513d261c\nhttps://www.chicagotribune.com/investigations/ct-dcfs-eckerd-met-20171206-story.html\nhttps://www.govtech.com/health/illinois-ends-child-abuse-prediction-program.html\nhttps://wgntv.com/news/wgn-investigates/123-children-died-within-a-year-of-dcfs-involvement-ig-study-finds/\nhttps://gizmodo.com/illinois-scraps-child-abuse-prediction-software-for-not-1821080730\nhttps://www.wcbu.org/state-news/2017-12-06/dcfs-ends-predictive-program\nhttps://herald-review.com/news/local/illinois-child-welfare-to-end-use-of-predictive-program/article_abdb1f5d-b850-5dd0-8865-87bc77ef420f.html\nhttps://www.nytimes.com/2018/01/02/magazine/can-an-algorithm-tell-when-kids-are-in-danger.html\nRelated \ud83c\udf10\nAllegheny County child neglect screening\nGladsaxe vulnerable children detection\nPage info\nType: Incident\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/gemini-characterises-indian-pms-policies-as-fascist", "content": "Gemini characterises Indian PM's policies as 'fascist'\nOccurred: February 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGoogle's Gemini AI tool stated that Indian Prime Minister Narendra Modi was 'accused of implementing policies some experts have characterized as fascist', prompting the Indian government to threaten potential action against Google.\nAsked to elaborate on its answer to a question from a journalist whether Modi was a fascist, Gemini said that these accusations stemmed from factors such as Modi's political party the Bharatiya Janata Party's (BJP) Hindu nationalist ideology, its suppression of dissent, and its deployment of violence against religious minorities.\nBy contrast, Gemini provided no clear answer when a similar question was posed about former US President Donald Trump and Ukrainian President Volodymyr Zelensky.\nGoogle later said it had fixed the issue, while acknowledging that Gemini 'may not always be reliable, especially when it comes to responding to some prompts about current events, political topics, or evolving news.'\nSystem \ud83e\udd16\nGemini chatbot\nOperator:\nDeveloper: OpenAI\nCountry: India\nSector: Politics\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Accuracy/reliabiity; Bias/discrimination - political\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://timesofindia.indiatimes.com/gadgets-news/google-chatbot-geminis-response-on-pm-narendra-modi-this-is-what-the-company-has-to-say/articleshow/107964256.cms\nhttps://www.thehindubusinessline.com/info-tech/google-clarifies-gemini-ai-not-reliable-on-political-topics-after-ais-biased-reply-on-modi/article67881384.ece\nhttps://www.businesstoday.in/technology/news/story/google-admits-to-gemini-ais-inaccuracies-on-political-topics-after-biased-remarks-on-pm-modi-418820-2024-02-24\nhttps://www.livemint.com/news/india/centre-to-issue-notice-to-google-over-gemini-ai-response-to-query-on-pm-modi-report-11708698356654.html\nhttps://www.thehindu.com/news/national/netizens-allege-bias-in-google-ai-tools-response-on-pm-modi-i-t-ministry-sees-rules-violation/article67877974.ece\nRelated \ud83c\udf10\nManoj Tiwari deepfake Haryanvi broadcast\nIndian government censors COVID-19 Twitter post\nPage info\nType: Incident\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-generated-fake-id-passes-crypto-exchange-verification", "content": "AI-generated fake ID passes crypto exchange verification\nOccurred: January-February 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA service claiming to use AI neural networks to create fake ID documents has reportedly succeeded in passing Know Your Customer (KYC) checks on several crypto exchanges. \nUnderground service OnlyFake said it could instantly generate realistic fake identity documents such as driver\u2019s licenses and passports from 26 countries, including the United States, Canada, Britain, Australia and multiple European Union countries for USD 15.\n404 Media reported that it successfully bypassed the KYC verification of crypto exchange OKX using a photograph of a British passport one of its journalists generated with the site, where the ID appeared to be laid on a bedsheet as if a picture of it was taken. \nCointelegraph said it had seen a Telegram channel where OnlyFake users were sharing their apparent success in using the IDs to bypass verification at various crypto exchanges and financial service providers, including Kraken, Bybit, Bitget, Huobi, and PayPal. \nSystem \ud83e\udd16\nOnlyFake\nOperator:\nDeveloper: OnlyFake\nCountry: Global\nSector: Professional/business services\nPurpose: Generate fake identity documents\nTechnology: Deep learning; Machine learning; Neural network\nIssue: Ethics/values\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.404media.co/inside-the-underground-site-where-ai-neural-networks-churns-out-fake-ids-onlyfake/\nhttps://www.404media.co/onlyfake-neural-network-fake-id-site-goes-dark-after-404-media-investigation\nhttps://cointelegraph.com/news/ai-generated-fake-ids-pass-crypto-exchange-kyc-onlyfake\nhttps://readwrite.com/onlyfake-the-deepfake-site-churning-out-sophisticated-fake-ids/\nhttps://www.theverge.com/2024/2/5/24062245/a-newly-streamlined-process-for-fake-ids-says-its-using-ai\nhttps://petapixel.com/2024/02/05/onlyfake-is-pumping-out-hyper-realistic-fake-ids-photos-included/\nhttps://fortune.com/crypto/2024/02/06/next-wave-of-ai-fraud-onlyfake/\nRelated \ud83c\udf10\nAI impersonation scams Canadian couple of USD 21,000\nAudio deepfake fraudulently impersonates CEO\nPage info\nType: Incident\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/snapchat-algorithm-recommends-teen-connects-with-sex-offenders", "content": "Snapchat algorithm recommends teen connects with sex offenders\nOccurred: February 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA 12-year-old girl was raped after Snapchat's recommendation algorithm connected her with two convicted sex offenders, resulting in the arrest and imprisonment of the offenders, and a court case.\nThe teen girl, identified in court as 'C.O.' alleged that she was sexually assaulted in 2019 and 2021 by separate Snapchat users who were recommended to her by the company's 'Quick add' feature. \nHer lawyers argued that her use of social media had 'coincided with a severe and steady decline in C.O.'s mental and physical health', including bullying, and difficulty in school, resulting from various allegedly addictive features of Snap's social media platform, including the frequency of 'push' notifications and the nature of its recommendation algorithms.\nSuperior Court Judge Barbara Bellis dismissed the case, ruling that Snap was protected by Section 230 of the US Communications Decency Act, a law that protects interactive service providers from liability for activity stemming from recommendations by third parties. \nThe ruling ran contrary to a decision issued in January 2024 in which a Los Angeles County Superior Court Judge said Section 230 did not protect Snapchat from liability for allegedly connecting teens with drug dealers. \nSystem \ud83e\udd16\nSnapchat website\nSnapchat Wikipedia profile\n\nOperator: \nDeveloper: Snap Inc\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Recommend people\nTechnology: Recommendation algorithm; Machine learning\nIssue: Safety\nTransparency: Governance; Black box\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nV.V. v. Meta Platforms, Inc. et al\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://arstechnica.com/tech-policy/2024/02/snapchat-isnt-liable-for-connecting-12-year-old-to-convicted-sex-offenders/\nhttps://www.mediapost.com/publications/article/393698/snapchat-defeats-sex-abuse-victims-lawsuit-over-r.html\nhttps://blog.ericgoldman.org/archives/2024/02/snapchat-isnt-liable-for-offline-sexual-abuse-vv-v-meta.htm\nhttps://tagteam.harvard.edu/hub_feeds/3626/feed_items/9823854\nhttps://www.lawsuit-information-center.com/social-media-addiction-lawsuits.html\nRelated \ud83c\udf10\nSnapChat AI gives sex advice to 13-year-old\nSnapchat My AI requests to meet 13-year-old girl in park\nPage info\nType: Incident\nPublished: February 2024\nLast updated: March 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/university-of-waterloo-found-covertly-using-facial-recognition", "content": "University of Waterloo found covertly using facial recognition \nOccurred: February 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nStudents at the University of Waterloo in Canada discovered that 'intelligent' vending machines on campus were using facial recognition without their knowledge.\nThe facial recognition capability was accidentally found by a student after an error message appeared on a screen of one of the M&M-branded vending machines. Students then began to cover a tiny hole apparently concealing the camera with chewing gum and sticky notes. \nThey also discovered online a a brochure for the machines which revealed that they were installed with a 'demographic sensor' allowing them to calculate the age and gender of anyone that approaches and make 'AI-powered product recommendations.'\nInvenda said the vending machines do not store or transmit personally identifiable imagery. However, the university demanded the software was disabled and machines removed from campus.\nSystem \ud83e\udd16\nInvenda website\nOperator: Adaria Vending Services/University of Waterloo\nDeveloper: Invenda Group\nCountry: Canada\nSector: Education\nPurpose: Profile demographics\nTechnology: Facial recognition\nIssue: Privacy\nTransparency: Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reddit.com/r/uwaterloo/comments/1anvv0q/hey_so_why_do_the_stupid_mm_machines_have_facial/\nhttps://mathnews.uwaterloo.ca/wp-content/uploads/2024/02/mathNEWS-154-3.pdf#page=6\nhttps://kitchener.ctvnews.ca/facial-recognition-error-message-on-vending-machine-sparks-concern-at-university-of-waterloo-1.6779835\nhttps://www.theguardian.com/world/2024/feb/23/vending-machine-facial-recognition-canada-univeristy-waterloo?CMP=Share_AndroidApp_Other\nhttps://arstechnica.com/tech-policy/2024/02/vending-machine-error-reveals-secret-face-image-database-of-college-students/\nhttps://futurism.com/the-byte/error-message-vending-maching-facial-recognition\nhttps://bnnbreaking.com/world/canada/privacy-in-the-age-of-convenience-university-of-waterloo-students-uncover-hidden-facial-recognition-in-vending-machines\nRelated \ud83c\udf10\nCadillac Fairview covertly uses facial recognition to monitor shoppers\nCanadian Tire covertly uses facial recognition to collect customer data\nPage info\nType: Incident\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/lyft-background-check-fails-to-flag-man-convicted-of-aiding-terrorism", "content": "Lyft background check fails to flag man convicted of aiding terrorism \nOccurred: October 2017\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA man with a federal conviction for aiding terrorism was discovered to be driving for Lyft, prompting Chicago city officials to demand the company it used to perform automated background checks was replaced.\nRaja L. Khan was not detected by background check company Sterling, despite having been released from prison after serving seven years for attempting to send money to Al Qaeda, resulting in a federal terrorism charge. People convicted of terrorist-related charges were supposed to be banned for driving for cabs and ridesharing services in Chicago.\nChicago officials demanded Sterling was replaced and that its existing drivers were double-checked. \nLyft was one of three rideshare companies that settled with the City of Chicago for not performing background checks consistent with standards set by the City\u2019s rideshare ordinance for a total USD 10.4 million.\nSystem \ud83e\udd16\nSterling website\nOperator: Lyft\nDeveloper: Sterling Talent Solutions\nCountry: USA\nSector: Business/professional services; Transport/logistics\nPurpose: Conduct background checks\nTechnology: \nIssue: Accuracy/reliability; Safety\nTransparency: \nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nhttps://www.scribd.com/document/386380279/Aug-16-Lyft-Settlement-Agreement-Final\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://chicago.suntimes.com/news/city-demands-that-lyft-replace-background-checker/\nhttps://chicagoist.com/2017/10/27/chicago_blasts_lyft_for_hiring_driv.php\nhttps://patch.com/illinois/chicago/man-terrorism-aide-conviction-allowed-drive-lyft\nhttps://wgntv.com/news/wgn-investigates/man-allowed-to-drive-for-lyft-despite-terrorism-aide-conviction/\nhttps://www.rstreet.org/commentary/lyft-drivers-past-conviction-doesnt-undermine-background-check-system/\nRelated \ud83c\udf10\nGig economy drivers rejected by faulty Checkr automated background checks\nFaulty automated background checks freeze out renters\nPage info\nType: Incident\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/gab-ai-chatbots-deny-holocaust", "content": "Gab.AI chatbots accused of radicalisation, inciting violence\nOccurred: February 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nUS-based conservative social network Gab caused controversy by creating chatbots that denied the Holocaust and which were seen by critics to normalise hateful ideologies and incite violence.\nGab's creation of chatbots in the names of Hitler, Stalin, Putin, Osama bin Laden and other polarising figures that provided antisemitic and white supremacist answers, denied the Holocaust, and justified terrorist attacks. \nThe bots triggered a storm of protest from Jewish and other critics who accused the company of acting unethically by normalising hatred and spreading conspiracy theories and disinformation, and risking radicalisation and violence.\nIn response, Gab released a statement defending their stance on free speech. 'We believe in the fundamental right to free speech and the freedom to learn from history, even if it's uncomfortable or controversial,' it said.\nThe controversy was seen to serve as an example of the need to balance freedom of speech with public safety.\nSystem \ud83e\udd16\nGab.AI app\nGab website\nGab Wikipedia profile\nOperator: Gab\nDeveloper: Gab\nCountry: USA\nSector: Media/entertainment/sports/arts; Politics\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis\nIssue: Ethics/values; Mis/disinformation; Safety\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nTech against Terrorism (2024). Gab\u2019s Racist AI Chatbots Have Been Instructed to Deny the Holocaust\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.rollingstone.com/politics/politics-features/nazi-chatbots-gab-ai-innovation-torba-1234943009/\nhttps://bnnbreaking.com/world/gabs-ai-chatbots-a-dangerous-path-to-radicalization-or-freedom-of-speech\nhttps://www.wired.com/story/gab-ai-chatbot-racist-holocaust/\nhttps://www.thetimes.co.uk/article/hitler-chatbot-prompts-fears-of-online-radicalisation-djwbsdm08\nhttps://www.euronews.com/next/2024/02/19/the-rise-of-the-hitler-chatbot-will-europe-be-able-to-prevent-far-right-radicalisation-by-\nRelated \ud83c\udf10\nGoogle Gemini generates 'woke' 'diverse' racial images\nGoogle Assistive Writing feature accused of being too 'woke'\nPage info\nType: Incident\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-powered-telemarketing-bots-harangue-chinese-customers", "content": "AI-powered telemarketing bots harangue Chinese customers\nOccurred: 2018-\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nHighly realistic automated voice bots making up to 3,000 unsolicited calls a day each even when they were blocked led to a huge volume of complaints and accusations of illegal and unethical conduct.\nThe bots, developed by Silicon Intelligence, Wan Xi Intelligence and other China-based companies, bombarded people in China and elsewhere on their home and mobile phones with calls selling insurance policies, property, pharmaceutical products and much else. \nAccording to the developers, the systems can make 3,000 phone calls a day, identify and support multiple Chinese dialects, talk in a natural-sounding manner, and can increase sales by 140 per cent without human intervention. \nThe practice raised concerns about privacy. In addition, the robotic voices were said to be unexpectedly realistic and easily confused for those of real human beings, raising questions about the potential use of the systems and technology for fraud.\nSystem \ud83e\udd16\nSilicon Intelligence\nWan Xi Intelligence\nOperator:\nDeveloper: Silicon Intelligence; Wan Xi Intelligence\nCountry: China\nSector: Multiple\nPurpose: Sell products/services\nTechnology: Speech recognition; Text-to-speech\nIssue: Ethics/values; Privacy\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.sixthtone.com/news/1003718/how-ai-powered-voice-bots-flooded-chinas-telemarketing-industry\nhttps://www.scmp.com/week-asia/economics/article/3096539/think-telemarketers-are-pest-wait-till-chinas-ai-versions-call\nhttp://www.infzm.com/content/143381\nhttp://chinaplus.cri.cn/news/china/9/20181208/220436.html\nhttp://tv.cctv.com/2019/03/16/VIDE5zEl0LD8J4qB9dOGTO1R190316.shtml\nhttps://www.chinamoneynetwork.com/2021/03/10/when-ai-encounters-ai-can-ai-stop-the-harm-of-robocalls\nhttps://syncedreview.com/2019/03/29/robots-vs-robocalls/\nhttps://www.techtimes.com/articles/251676/20200810/chinas-voice-bots-make-3-000-annoying-calls-day-even.htm\nRelated \ud83c\udf10\nNYC mayor Eric Adams robocalls residents using audio deepfakes\nChinese schools monitor students using 'intelligent uniforms'\nPage info\nType: Issue\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/google-gemini-generates-woke-diverse-racial-images", "content": "Google Gemini generates 'woke' racial images\nOccurred: February 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGoogle faced criticism from right-wing commentators for it's Gemini AI model being too 'woke' by being biased against white people.\nUsers pointed out that Gemini generated images depicting a variety of genders and ethnicities even when doing so was historically inaccurate. For example, a prompt seeking images of America's founding fathers turned up women and people of colour; another prompt showed much the same for Nazi-era German soldiers.\nPeople questioned whether Gemini failed to produce historically accurate results in an attempt at reducing the risk of being accused of racial and gender discrimination. The controversy was first sparked by right-wing figures attacking Google for its seemingly liberal culture. \nGoogle acknowledged the issue, saying Gemini was 'missing the mark' and that it was being addressed. The company later suspended the bot, saying it tuned Gemini to be more diverse but had gone too far and made the bot cautious.'\nSystem \ud83e\udd16\nGemini chatbot\nDocuments \ud83d\udcc3\nGoogle (2024). Gemini image generation got it wrong. We'll do better.\nOperator: Alphabet/Google\nDeveloper: Alphabet/Google\nCountry: Global\nSector: Politics\nPurpose: Generate images\nTechnology: Generative AI; Text-to-image\nIssue: Accuracy/reliability; Ethics/values; Historical revisionism\nTransparency: Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theverge.com/2024/2/21/24079371/google-ai-gemini-generative-inaccurate-historical\nhttps://dailydot.com/debug/google-ai-gemini-white-people/\nhttps://www.bbc.co.uk/news/business-68364690\nhttps://www.engadget.com/google-promises-to-fix-geminis-image-generation-following-complaints-that-its-woke-073445160.html\nhttps://timesofindia.indiatimes.com/gadgets-news/google-faces-criticism-for-gemini-ai-being-woke-heres-what-the-company-has-to-say/articleshow/107906151.cms\nhttps://nypost.com/2024/02/21/business/googles-ai-chatbot-gemini-makes-diverse-images-of-founding-fathers-popes-and-vikings-so-woke-its-unusable/\nRelated \ud83c\udf10\nAdobe Firefly shows 'woke' photos of black Nazis\nGoogle Assistive Writing feature accused of being too 'woke'\nPage info\nType: Incident\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/gig-economy-drivers-rejected-by-faulty-checkr-automated-background-checks", "content": "Gig economy drivers rejected by faulty Checkr automated background checks \nOccurred: 2016-\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nUber, Lyft, Postmates and other gig economy companies fired or rejected workers based on inaccurate automated background checks provided by Checkr, according to media reports based on court records and interviews.\nSince 2015, Checkr faced multiple lawsuits under the US Fair Credit Reporting Act for mistaking people with others with the same or similar names and misreporting offences, including alleged criminal activity, due to out-of-context or outdated records being included in background reports. The result: actual and prospective drivers being denied work and income by Uber and its other ride-sharing customers. \nExperts told Protocol that Checkr and its customers were often reluctant to take responsibility for these failures, meaning several lawsuits were aimed at multiple parties and often went to arbitration, ending in confidential settlements.\nSystem \ud83e\udd16\nCheckr website\nOperator: DoorDash; Grubhub; Instacart; Lyft; Postmates; Thumbtack; Uber\nDeveloper: Checkr\nCountry: USA\nSector: Business/professional services\nPurpose: Conduct background checks\nTechnology: Machine learning\nIssue: Accuracy/reliability; Privacy\nTransparency: Governance; Black box\nRegulation \u2696\ufe0f\nUS Fair Credit Reporting Act\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.protocol.com/checkr-gig-economy-lawsuits\nhttps://www.vice.com/en/article/qjkevq/consumers-get-screwed-airbnbs-and-ubers-background-check-company-keeps-getting-sued\nhttps://www.bigiftrue.org/2019/08/27/uber-lyft-checkr-background-checks\nhttps://www.vox.com/recode/2020/5/11/21166291/artificial-intelligence-ai-background-check-checkr-fama\nhttps://www.techtarget.com/searchhrsoftware/feature/Employee-background-check-errors-harm-thousands-of-workers\nhttps://www.cnet.com/tech/tech-industry/uber-lyft-reportedly-skimp-on-background-checks/\nRelated \ud83c\udf10\nTenants declined by faulty TransUnion AI system\nFaulty automated background checks freeze out renters\nPage info\nType: Incident\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/faulty-automated-background-checks-freeze-out-renters", "content": "Faulty automated background checks freeze out renters\nOccurred: May 2020-\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nPeople looking to rent housing across the US are being frozen out by faulty algorithmic background checks by largely unregulated tenant screening companies.\nTenants have found themselves unable to rent houses and apartments due to RealPage, CoreLogic, TransUnion, RentGrow and other companies producing automated reports that are usually not checked by humans and which can wrongly label people deadbeats, criminals or sex offenders, according to court records and interviews conducted by The Markup and New York Times. \nThe reports singled out false name matches, the use of abbreviated criminal records, and other issues as causing problems, with landlords receiving assessments 'without a human ever glancing at the results to see if they contain obvious mistakes' and renters unaware why they are being turned down and how they can contest them. The results include existing tenants unfairly evicted, prospective tenants denied housing, and homelessness.\nThe findings raised concerns about the accuracy and reliability of the systems involved, the lack of human oversight, and inadequate transparency and accountability.\nSystem \ud83e\udd16\nRealPage website\nRealPage Wikipedia profile\nOperator: RealPage; CoreLogic; TransUnion; RentGrow\nDeveloper: RealPage; CoreLogic; TransUnion; RentGrow\nCountry: USA\nSector: Business/professional services\nPurpose: Predict tenant risk\nTechnology: Pricing algorithm\nIssue: Accuracy/reliability; Bias/discrimination - race\nTransparency: Governance; Black box\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nConnecticut Fair Housing Ctr et al v. CoreLogic Rental Property Solutions\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://themarkup.org/locked-out/2020/05/28/access-denied-faulty-automated-background-checks-freeze-out-renters\nhttps://themarkup.org/locked-out/2020/05/28/how-we-investigated-the-tenant-screening-industry\nhttps://themarkup.org/locked-out/2021/03/12/citing-a-markup-investigation-senators-question-regulators-about-tenant-screening-oversight\nhttps://www.nytimes.com/2020/05/28/business/renters-background-checks.html\nhttps://www.vrresearch.com/blog/2020/6/18/automated-background-checks-fail-to-root-out-false-positives\nhttps://www.texasobserver.org/evictions-texas-housing/\nhttps://thehustle.co/05292020-background-checks/\nhttps://slate.com/podcasts/what-next-tbd/2020/10/automated-background-checks-landlord-tenant\nRelated \ud83c\udf10\nRealPage algorithm accused of artificially increasing rents\nSafeRent tenant screening\nPage info\nType: Incident\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/two-waymo-robotaxis-crash-into-pick-up-truck", "content": "Two Waymo robotaxis crash into pick-up truck\nOccurred: December 2023-February 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nTwo Waymo self-driving cars crashed into a pick-up truck being towed in Phoenix, Arizona, resulting in the company issuing a voluntary recall of its vehicles. \nAccording to Waymo, one of their cars 'incorrectly predicted the future motion', of a backward-facing pickup truck that was being 'improperly towed', and collided with it, causing no injuries and 'minor damage' to the car. A few minutes later another Waymo vehicle made contact with the same pickup while it was being towed in the same manner.\nWaymo later discovered that its software had incorrectly predicted the future movements of the pickup truck due to 'persistent orientation mismatch' between the towed vehicle and the one towing it. The company developed a fix for its software which it deployed to its fleet through a 'recall'. \nThe incident highlighted safety concerns about Waymo, and about the self-driving car industry. A Waymo robotaxi struck and injured a cyclist in San Francisco in February 2024, and Waymo's rival Cruise dragged a pedestrian across the road a few weeks earlier, resulting in the suspension of its license.\nSystem \ud83e\udd16\nWaymo website\nWaymo Wikipedia profile\nDocuments \ud83d\udcc3\nWaymo (2024). Voluntary recall of our previous software\nOperator: Alphabet/Waymo\nDeveloper: Alphabet/Waymo\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Self-driving system; Computer vision\nIssue: Accuracy/reliability; Safety\nTransparency: Governance; Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://techcrunch.com/2024/02/13/waymo-recall-crash-software-self-driving-cars/\nhttps://www.wsj.com/business/autos/waymo-issues-recall-self-driving-cars-crashes-caf17ebe\nhttps://www.dailymail.co.uk/sciencetech/article-13088161/waymo-issues-recall-self-driving-cars-collide.html\nhttps://thehill.com/policy/technology/4468073-waymo-recalls-software-after-self-driving-taxis-crash-in-december/\nhttps://www.engadget.com/waymo-issued-a-recall-after-two-robotaxis-crashed-into-the-same-pickup-truck-055708611.html\nhttps://edition.cnn.com/2024/02/14/business/waymo-recalls-software-after-two-self-driving-cars-hit-the-same-truck/index.html\nhttps://www.theregister.com/2024/02/14/waymo_files_recall_after_pheonix/\nRelated \ud83c\udf10\nWaymo cars get stuck in cul-de-sac\nWaymo self-driving car hits public bus\nPage info\nType: Incident\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/peer-reviewed-journal-publishes-ai-generated-rat-penis", "content": "Peer-reviewed journal publishes AI-generated rat penis\nOccurred: February 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA research study containing nonsensical AI-generated images, including a gigantic rat penis, was retracted by the scientific journal that published it.\nThe study, by three scientists in China, was published in the open access journal Frontiers in Cell Development and Biology, and was supposedly edited by a researcher in India and reviewed by two people from the US and India. The scientists acknowledged in the paper that Midjourney had been used to generate images in the study.\nDespite peer reviews, the images quickly met with incredulity online. A US-based reviewer told Vice that they had evaluated the study on its scientific merits alone. Frontiers\u2019 policies allow the use of generative AI as long as it is disclosed. They also say the images must also be accurate. \nThe incident drew attention to poor editing at Frontiers, and more generally to the risks of using generative AI for scientific research.\nSystem \ud83e\udd16\nMidjourney image generator\nOperator: Xinyu Guo; Liang Dong; Dinjung Hao\nDeveloper: Midjourney\nCountry: China\nSector: Research/academia\nPurpose: Illustrate research paper\nTechnology: Text-to-image\nIssue: Reputation\nTransparency: \nResearch, advocacy \ud83e\uddee\nGuo X., Dong L., Hao D. (2024). Cellular functions of spermatogonial stem cells in relation to JAK/STAT signaling pathway\nFrontiers (2024). Retraction of research article\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.vice.com/en/article/4a389b/ai-midjourney-rat-penis-study-retracted-frontiers\nhttps://scienceintegritydigest.com/2024/02/15/the-rat-with-the-big-balls-and-enormous-penis-how-frontiers-published-a-paper-with-botched-ai-generated-images/\nhttps://www.telegraph.co.uk/news/2024/02/16/journal-published-graphic-rat-with-giant-penis-asking-ai/\nhttps://arstechnica.com/science/2024/02/scientists-aghast-at-bizarre-ai-rat-with-huge-genitals-in-peer-reviewed-article/\nhttps://sea.mashable.com/tech/31291/behold-a-giant-ai-generated-rat-dick\nhttps://www.popsci.com/technology/ai-rat-journal/\nRelated \ud83c\udf10\nChinese study uses facial recognition to identify Uyghurs, Tibetans\nChinese study predicts criminality by analysing facial features\nPage info\nType: Incident\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/nation-state-hackers-use-chatgpt-to-improve-cyberattacks", "content": "Nation state hackers use ChatGPT to improve cyberattacks\nOccurred: 2023-2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nChina, Iran, Russia and North Korea used ChatGPT to research, refine, and mount offensive cyber operations across the world. \nAccording to Microsoft research, Russian, North Korean, Iranian, and Chinese-backed groups have been discovered using tools including ChatGPT to conduct research into targets and to improve scripts and social engineering techniques for surveillance, disinformation and influence operations, and cybercrime campaigns.\nThe techniques employed were considered 'early-stage' and not 'particularly novel or unique, and 'significant attacks' using ChatGPT and other large language models were not discovered, Microsoft said. In a blog post, OpenAI argued its GPT-4-powered chatbot offers 'only limited, incremental capabilities for malicious cybersecurity tasks beyond what is already achievable with publicly available, non-AI powered tools.'\nHowever, experts believe that it is only a matter of time before effective malicious nation state-backed campaigns using chatbots and large language models are conducted.\nSystem \ud83e\udd16\nChatGPT chatbot\nDocuments \ud83d\udcc3\nMicrosoft (2024). Staying ahead of threat actors in the age of AI\nOpenAI (2024). Disrupting malicious uses of AI by state-affiliated threat actors\nOperator: Aquatic Panda; Charcoal Typhoon; Crimson Sandstorm; Emerald Sleet; Fancy Bear; Forest Blizzard; Maverick Panda; Salmon Typhoon\nDeveloper: OpenAI\nCountry: China; Iran; N Korea; Russia\nSector: Govt - defence\nPurpose: Conduct research; Generate phishing content; Generate code\nTechnology: Chatbot\nIssue: Fraud; Mis/disinformation; Reputational damage; Security\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.euronews.com/next/2024/02/14/microsoft-says-us-rivals-are-beginning-to-use-generative-ai-in-offensive-cyber-operations\nhttps://www.computerweekly.com/news/366570000/Microsoft-Nation-state-hackers-are-exploiting-ChatGPT\nhttps://www.nytimes.com/2024/02/14/technology/openai-microsoft-hackers.html\nhttps://www.reuters.com/technology/cybersecurity/microsoft-says-it-caught-hackers-china-russia-iran-using-its-ai-tools-2024-02-14/\nhttps://www.theverge.com/2024/2/14/24072706/microsoft-openai-cyberattack-tools-ai-chatgpt\nRelated \ud83c\udf10\nChatGPT generates plausible phishing emails, malware\nChatGPT can be used to create cybercrime tools\nPage info\nType: Incident\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/robot-crushes-to-death-man-mistaken-for-box-of-vegetables", "content": "Robot crushes to death man mistaken for box of vegetables\nOccurred: November 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA South Korean worker was crushed to death by a robot that mistook him for a box of paprika peppers. \nThe employee had been inspecting the robot\u2019s sensor on the Donggoseong Export Agricultural Complex in south Korea when the robot arm - which was programmed to lift boxes of vegetables - mistook the employee for one, grabbed and placed him on a conveyor belt using its tongs and squeezed him.\nThe man, whose face and body were crushed by the conveyor belt, died shortly after arriving at a local hospital. The machine had been experiencing issues for days before the incident. \nA Donggoseong Export Agricultural Complex official told the Yonhap News agency, 'We have been using robots well with less labour, but recently we changed the work line and entrusted the work to more efficient use.'\nThe incident raised questions about the safety of the unnamed manufacturer of the robot and the working practices of the Agricultural Complex. \nSystem \ud83e\udd16\nUnknown\nOperator: Donggoseong Export Agricultural Complex\nDeveloper:  \nCountry: S Korea\nSector: Manufacturing/engineering\nPurpose: Sort products\nTechnology: Robotics\nIssue: Robustness; Safety\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/world-asia-67354709\nhttps://www.businessinsider.com/robot-crushed-man-death-mistook-him-box-vegetables-south-korea-2023-11?r=US&IR=T\nhttps://www.tbsnews.net/tech/man-crushed-death-robot-south-korea-735790\nhttps://www.standard.co.uk/news/world/south-korea-robot-crushed-death-man-vegetables-b1119086.html\nhttps://www.theregister.com/2023/11/09/robot_kills_employee/\nhttps://www.yna.co.kr/view/AKR20231108062151052\nhttps://www.telegraph.co.uk/world-news/2023/11/08/man-crushed-to-death-south-korea-industrial-robot/\nhttps://nypost.com/2023/11/09/news/robot-mistakes-worker-for-box-of-veggies-crushes-him-to-death/\nRelated \ud83c\udf10\nRobot crushes man to death in South Korea\nRobot crushes and kills VW contractor\nPage info\nType: Incident\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/drunk-driver-using-tesla-fsd-killed-after-car-hits-tree", "content": "Drunk driver using Tesla FSD killed after car hits tree\nOccurred: May 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Tesla employee driving home after several alcoholic drinks reportedly activated his car's Full-Self Driving capability before the vehicle careered off the road and burst into flames, killing him and injuring his passenger.\nIn an interview, Hans von Ohain's passenger Erik Rossiter said he believed that von Ohain was using Full Self-Driving (FSD), a charge apparently supported by the police but ruted by Tesla CEO Elon Musk. \nTesla had earlier told the Washington Post that it '...could not confirm that a driver-assistance system had been in use because it did not receive data over-the-air for this incident.' \nVon Ohain\u2019s widow, Nora Bass, argued Tesla should take some responsibility for her husband\u2019s death. 'Regardless of how drunk Hans was, Musk has claimed that this car can drive itself and is essentially better than a human. We were sold a false sense of security.' Tesla and Musk have been accused of misleading marketing on multiple occasions.\nSystem \ud83e\udd16\nTesla Autopilot, Full-self Driving\nOperator: Hans von Ohain  \nDeveloper: Tesla\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Self-driving system\nIssue: Accuracy/reliability; Safety\nTransparency: Governance; Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.washingtonpost.com/technology/interactive/2024/tesla-full-self-driving-fatal-crash/\nhttps://www.dailymail.co.uk/news/article-13078375/Tesla-self-driving-killed-hans-von-ohain-colorado-drunk.html\nhttps://electrek.co/2024/02/13/tesla-worker-died-horrible-crash-full-self-driving-beta-but-drunk/\nhttps://futurism.com/tesla-employee-killed-full-self-driving\nhttps://www.theverge.com/2024/2/13/24071905/a-report-suggests-teslas-full-self-driving-system-may-have-been-involved-in-a-deadly-2022-crash\nhttps://jalopnik.com/tesla-sold-false-sense-of-security-to-employee-killed-1851255054\nhttps://www.businessinsider.com/tesla-employee-died-full-self-driving-fatal-crash-2024-2\nRelated \ud83c\udf10\nTesla Model S crash causes eight-vehicle pile-up\nTesla FSD beta test car hits bollard, driver fired\nPage info\nType: Incident\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/air-canada-found-liable-for-chatbots-poor-advice", "content": "Air Canada found liable for chatbot's poor advice\nOccurred: 2022-2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAir Canada was forced to pay damages after a court ruled the airline was liable for the wrong information its chatbot gave a customer before he booked a flight.\nFollowing the death of his grandmother, Air Canada's chatbot told Jake Moffatt that if he purchased a normal-price ticket he would have up to 90 days to claim back a bereavement discount - a special low rate for people traveling due to the loss of an immediate family member.\nMoffatt ended up taking the airline to a small-claims tribunal for negligence after it refused to claim back the discount, even though he had the correct documents and did so within the 90-day window, on the basis that it should not be held liable for the chatbot's faulty outputs.\nIn his ruling, tribunal member Christopher Rivers said Air Canada had failed to take 'reasonable care to ensure its chatbot was accurate.' He also said that 'It should be obvious to Air Canada that it is responsible for all the information on its website. It makes no difference whether the information comes from a static page or a chatbot.'\nThe incident was seen as a reminder that companies need to aware of the risks of using AI, including the legal risks.\nSystem \ud83e\udd16\nAir Canada website\nOperator: Air Canada\nDeveloper: Air Canada\nCountry: Canada\nSector: Travel/hopsitality\nPurpose: Support customers\nTechnology: Chatbot\nIssue: Accuracy/reliability; Liability\nTransparency: Governance\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nMoffatt v Air Canada\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.cbc.ca/news/canada/british-columbia/air-canada-chatbot-lawsuit-1.7116416\nhttps://globalnews.ca/news/10297307/air-canada-chatbot-error-bc-ruling/\nhttps://www.theregister.com/2024/02/15/air_canada_chatbot_fine/\nhttps://gizmodo.com/air-canada-chatbot-lied-bereavement-discount-refund-1851263670\nhttps://yro.slashdot.org/story/24/02/15/2239221/air-canada-found-liable-for-chatbots-bad-advice-on-plane-ticket\nRelated \ud83c\udf10\nAmazon DSP Ans Rana crash liability\nTyndaris AI-automated trades cost investor USD 20 million\nPage info\nType: Incident\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/researchers-reveal-hello-barbie-security-vulnerabilities", "content": "Researchers reveal Hello Barbie security vulnerabilities\nOccurred: November 2015\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nMattel's Hello Barbie doll could be hacked and young girls spied on, according to a US-based security researcher.  \nSecurity researcher Matt Jakubowski discovered the WiFi-enabled Hello Barbie was vulnerable to hacking when storing audio files of conversations between kids and the doll in the cloud, on which they could be analysed by Mattel, its technology partner ToyTalk and other vendors.\nThe hack allowed Jakubowski 'easy' access to the doll\u2019s system information, account information, and stored audio files. The result might be that anyone could identify the individual and their home address and modify the doll to suit their needs. 'It\u2019s just a matter of time until we are able to replace their servers with ours and have her say anything we want,' Jakubowski told NBC.\nMattel software maker ToyTalk responded by saying it would patch the vulnerability.\nSystem \ud83e\udd16\nMattel website\nMattel Wikipedia profile\nOperator: Mattel/ToyTalk\nDeveloper: Mattel/ToyTalk\nCountry: USA\nSector: Consumer goods\nPurpose: Interact with children\nTechnology: Voice recognition; NLP/text analysis\nIssue: Privacy; Security; Surveillance\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.cnet.com/news/privacy/hello-headaches-barbie-of-the-internet-age-has-even-more-security-flaws/\nhttps://www.theguardian.com/technology/2015/nov/26/hackers-can-hijack-wi-fi-hello-barbie-to-spy-on-your-children\nhttps://www.nbcchicago.com/investigations/video/web-10p-pkg-surveillance-toy_leitner_chicago/1983788/\nhttps://www.cnbc.com/2015/12/22/why-you-should-say-goodbye-to-hello-barbie.html\nhttps://grahamcluley.com/hello-barbie-brings-smiles-kids-nightmares-privacy-advocates/\nhttps://time.com/4093660/barbie-hell-no/\nRelated \ud83c\udf10\nAI-powered Hello Barbie riles privacy advocates\nMattel shelves Aristotle AI babyminder after privacy complaints\nPage info\nType: Issue\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-powered-hello-barbie-riles-privacy-advocates", "content": "AI-powered Hello Barbie riles privacy advocates\nOccurred: February 2015-\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nMattel's Hello Barbie doll unnecessarily exposed children to the commercial exploitation of their data, according to childhood advocates and privacy experts.\nDeveloped by Mattel and technology partner ToyTalk and billed as 'the first fashion doll that can have a two-way conversation with girls,' the WiFi-enabled Hello Barbie was equipped with a microphone, voice recognition and 'progessive learning features' and was programmed with 8,000 lines of dialogue. \nHowever, childhood and privacy experts took issue with ToyTalk's privacy policy, which the company to listen to and process kids' conversations 'in order to provide and maintain the Service, to perform, test or improve speech recognition technology and artificial intelligence algorithms, or for other research and development and data analysis purposes.'\nExperts also took issue with algorithms replacing human actions. 'Computer algorithms can\u2019t replace - and should not displace - the nuanced responsiveness of caring people interacting with one another,' according to pediatrician Dipesh Navsaria, MPH, MD, assistant professor at the University of Wisconsin School of Medicine and Public Health.\nSystem \ud83e\udd16\nMattel website\nMattel Wikipedia profile\nDocuments \ud83d\udcc3\nMattel (2015). Hello Barbie FAQs\nMattel (2015). Hello Barbie Messaging (pdf)\nOperator: Mattel/ToyTalk\nDeveloper: Mattel/ToyTalk\nCountry: USA\nSector: Consumer goods\nPurpose: Interact with children\nTechnology: Voice recognition; NLP/text analysis\nIssue: Privacy\nTransparency: \nResearch, advocacy \ud83e\uddee\n Campaign for a Commercial-Free Childhood (2015). Advocates Say \u201cHell No Barbie\u201d to Stop Mattel from Spying on Kids\n Campaign for a Commercial-Free Childhood (2015). Child Advocates Mobilize to Stop Mattel\u2019s Eavesdropping \u201cHello Barbie\u201d\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.smh.com.au/technology/how-mattels-hello-barbie-could-become-a-target-for-hackers-20151011-gk6ahv.html\nhttps://qz.com/362891/new-hello-barbie-records-kids-voices-and-sends-the-intel-back-to-mattel\nhttp://www.washingtonpost.com/blogs/the-switch/wp/2015/03/11/privacy-advocates-try-to-keep-creepy-eavesdropping-hello-barbie-from-hitting-shelves/\nhttp://www.sfgate.com/business/article/Will-Barbie-be-hackers-new-plaything-6562963.php\nhttps://www.theverge.com/2015/3/16/8223251/hello-barbie-speech-recognition-privacy\nhttps://www.theguardian.com/technology/2015/mar/13/smart-barbie-that-can-listen-to-your-kids-privacy-fears-mattel\nRelated \ud83c\udf10\nResearchers reveal Hello Barbie security vulnerabilities\nMattel shelves Aristotle AI babyminder after privacy complaints\nPage info\nType: Issue\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/mattel-shelves-aristotle-ai-babyminder-after-privacy-complaints", "content": "Mattel shelves Aristotle AI babyminder after privacy complaints\nOccurred: May 2017\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAristotle, a voice-activated smart assistant for kids, was cancelled after an uproar in which its developer Mattel was accused of using children as 'guinea pigs for AI experiments'.\nPowered by Microsoft's Cortana virtual assistant, Aristotle promised 'to aid parents and use the most advanced AI-driven technology to make it easier for them to protect, develop, and nurture the most important asset in their home\u2014their children.' \nHowever, the device was pulled after lawmakers, child development experts and privacy advocates raised concerns about the data the device would collect, and the negative implications of technology replacing vital interactions with parents. \nThe Campaign for a Commercial Free Childhood complained (pdf) that 'Aristotle will make sensitive information about children available to countless third parties, leaving kids and families vulnerable to marketers, hackers, and other malicious actors. Aristotle also attempts to replace the care, judgment, and companionship of loving family members with faux nurturing and conversation from a robot designed to sell products and build brand loyalty.'\nSystem \ud83e\udd16\nMattel website\nMattel Wikipedia profile\nOperator: \nDeveloper: Mattel\nCountry: USA\nSector: Consumer goods\nPurpose: Monitor babies\nTechnology: Digital assistant\nIssue: Appropriateness/need; Privacy\nTransparency: \nResearch, advocacy \ud83e\uddee\nCampaign for a Commercial-Free Childhood (2017). Letter to Mattel CEO (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theverge.com/2017/10/5/16430822/mattel-aristotle-ai-child-monitor-canceled\nhttps://www.nytimes.com/2017/10/05/well/family/mattel-aristotle-privacy.html\nhttps://www.washingtonpost.com/news/the-switch/wp/2017/10/04/mattel-has-an-ai-device-to-soothe-babies-experts-are-begging-them-not-to-sell-it/\nhttps://action.storyofstuff.org/petition/digital-nanny\nhttps://www.theguardian.com/technology/2017/oct/06/mattel-aristotle-ai-babysitter-children-campaign\nhttps://www.thedrum.com/news/2017/10/06/aristotle-the-ai-babysitter-kids-has-been-canceled-mattel\nhttps://www.geekwire.com/2017/mattel-cancels-aristotle-smart-hub-kids-amid-concerns-ai-parenting-data-collection/\nhttps://www.bbc.co.uk/news/technology-41520732\nRelated \ud83c\udf10\nAI-powered Hello Barbie riles privacy advocates\nDali Smart Lamp raises privacy concerns\nPage info\nType: Incident\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/dali-smart-lamp-raises-privacy-concerns", "content": "Dali Smart Lamp raises privacy concerns\nOccurred: February 2021-\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA desk lamp for school children equipped with cameras, posture recognition, and full-screen advertising raised concerns about surveillance and privacy.\nDeveloped by TikTok owner Bytedance, the Dali Smart Lamp is equipped an AI-powered digital assistant to help with vocabulary and math problems, and with two surveillance cameras - one mounted on the front and one on the top - to help parents keep an eye on their kids while they're doing homework. The lamp can be connect to a parent's smartphone, with the cameras allowing parents to see both their child's face and their homework while video chatting.\nA more expensive version of the lamp is able to recognise when children slouch, automatically sending alerts to their parents. While the Dali Smart Lamp is said to have sold well amongst Chinese consumers, it has also been described as 'weird' and 'creepy', particularly in western countries where the monitoring of schools kids and parental oversight of children tends to be less overt and intrusive.\nSystem \ud83e\udd16\nDali Smart Lamp website\nOperator: Bytedance/Beijing Kongming Technology  \nDeveloper: Bytedance\nCountry: China\nSector: Consumer goods\nPurpose: Increase learning efficiency\nTechnology: Posture detection/recognition; Deep learning; Neural network; Machine learning\nIssue: Privacy; Surveillance\nTransparency: Privacy\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.forbes.com/sites/johnkoetsier/2020/10/29/tiktok-owner-bytedance-selling-smart-lamp-with-camera-for-school-kids/\nhttps://technode.com/2021/02/05/bytedance-dali-spy-lamp/\nhttps://www.businessinsider.com/bytedance-smart-lamp-surveillance-kids-china-2021-6\nhttps://www.wsj.com/articles/a-smart-lamp-that-watches-kids-when-they-study-is-a-hit-in-china-11622466002\nRelated \ud83c\udf10\nMattel shelves Aristotle AI babyminder after privacy complaints \nChina Pharmaceutical University student behavioural monitoring\nPage info\nType: Issue\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/clearview-ai-tests-live-facial-recognition-cameras-and-glasses", "content": "Clearview AI tests live facial recognition cameras\nOccurred: March 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nNews that controversial facial recognition company Clearview AI was testing AI-enabled security cameras and augmented reality glasses prompted concerns about privacy.\nBuzzfeed discovered that Clearview AI was operating Insight Camera, a subsidiary company offering facial recognition-powered surveillance cameras aimed at 'retail, banking and residential buildings' and being tested by the United Federation of Teachers and Rudin Management. \nThe latter told Buzzfeed that the system consisted of a self-contained, closed system' that did not access Clearview AI\u2019s principal database of facial photographs.\nInsight Camera later took down its website, which had not spelled out its relationship with Clearview AI. Buzzfeed was able to link the two companies by comparing code from their respective websites. \nClearview had earlier insisted that it only worked with US government agencies. \nSystem \ud83e\udd16\nClearview AI website\nClearview AI Wikipedia profile\nOperator: ClearviewAI/Insight Camera\nDeveloper: Clearview AI\nCountry: USA\nSector: Banking/financial services; Real estate\nPurpose:  Strengthen security\nTechnology: Facial recognition\nIssue: Privacy; Surveillance\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.buzzfeednews.com/article/carolinehaskins1/clearview-facial-recognition-insight-camera-glasses\nhttps://mashable.com/article/clearview-ai-insight-surveillance-camera/\nhttps://www.inputmag.com/tech/report-clearview-ai-is-testing-facial-recognition-cameras-ar-glasses\nhttps://securitytoday.com/articles/2020/03/02/controversial-facial-recognition-company-clearview-ai-has-counted-ice-fbi.aspx\nhttps://www.engadget.com/clearview-ai-source-code-security-lapse-092558647.html\nhttps://www.dailydot.com/layer8/clearview-ai-cameras/\nhttps://www.dailymail.co.uk/sciencetech/article-8066595/Clearview-AI-developing-cams-use-database-Facebook-Instagram-photos-identify-subjects.html\nhttps://findbiometrics.com/clearview-explored-security-camera-smart-glasses-applications-of-facial-recognition-tech-903022/\nRelated \ud83c\udf10\nUkraine decision to use Clearview AI facial recognition draws concerns\nRCMP violated privacy using Clearview AI facial recognition\nPage info\nType: Issue\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/scammers-bypass-wechat-pay-facial-recognition-security-using-gifs", "content": "Scammers bypass WeChat Pay facial recognition security using gifs\nOccurred: 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nSwindlers who used GIFs to trick WeChat's mobile payment service into verifying people\u2019s identities and defrauding them were arrested by Chinese police.\nOne of the suspects told Hubei police that they would find selfies of their victims in their WeChat Moments feed and turn them into GIFs of the people doing identity-verifying actions, according to SixthTone.\nAs a deterrent to scammers, many Chinese set up facial recognition as an added security measure for online transactions on WeChat Pay, requiring them to blink or turn their heads to verify their identities. They would then transfer money from their victims' WeChat Pay accounts. \nThe incidenst raised questions about the strength of WeChat Pay's security, and about the security of facial recognition systems more broadly. \nSystem \ud83e\udd16\nWeChat Pay website\nOperator: \nDeveloper: Tencent/WeChat\nCountry: China\nSector: Banking/financial services\nPurpose: Verify customer identity\nTechnology: Facial recognition\nIssue: Security\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.sixthtone.com/news/1006310\nhttp://news.cnhubei.com/content/2020-10/17/content_13393734.html\nRelated \ud83c\udf10\nChinese scammer uses AI to defraud 'fiend' of USD 622,000\nChinese government facial recognition system hacked by tax fraudsters\nPage info\nType: Incident\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/uiuc-dumps-proctorio-over-significant-accessibility-concerns", "content": "UIUC dumps Proctorio over 'significant accessibility concerns'\nOccurred: April 2020-January 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA US university contract with automated proctoring company Proctorio was terminated due to concerns about the company's remote cheat-prevention software's accessibility, privacy, and security.\nA memo from university adminstrators to faculty members announcing the decision by the University of Illinois Urbana-Champaign cited 'significant accessibility concerns' associated with Proctorio. 'For some students with physical disabilities, students with low vision or are blind, students with psychiatric disabilities including anxiety or ADD/ADHD, Proctorio may be inaccessible,' it said.\nThe decision followed an outcry by students over the service, which uses machine learning and facial detection to monitor and record students taking exams. Over 1,000 students signed an online petition alleging that 'Proctorio is not only inefficient, it is also unsafe and a complete violation of a student\u2019s privacy,' and called for the university to stop using the service.\nSystem \ud83e\udd16\nProctorio website\nOperator: University of Illinois at Urbana-Champaign\nDeveloper: Proctorio\nCountry: USA\nSector: Education\nPurpose: Detect exam cheating\nTechnology: Facial detection; Gaze detection; Machine learning; Noise anomaly detection\nIssue: Accessibility, Bias/discrimination - disability; Privacy; Security\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nStop Proctoring Exams Through Proctorio at UIUC\nUIUC GEO (2020). The GEO Demands the UIUC Administration Immediately Terminate its Use of Proctorio and Other Invasive Exam Proctoring Tools\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.insidehighered.com/news/2021/02/01/u-illinois-says-goodbye-proctorio\nhttps://www.theverge.com/2021/1/28/22254631/university-of-illinois-urbana-champaign-proctorio-online-test-proctoring-privacy\nhttps://emails.illinois.edu/newsletter/38/1970177238.html\nhttps://dailyillini.com/news-stories/2020/04/17/proctorio-concerns/\nhttps://www.uiucgeo.org/solidarity-statements-and-press-releases/abolish-proctorio\nhttps://dailyillini.com/news-stories/2020/10/12/students-petition-to-ban-proctorio-amid-privacy-concerns/\nRelated \ud83c\udf10\nUBC academic, students accuse Proctorio of privacy abuse\nMiami University student accuses Proctorio of privacy abuse, discrimination\nPage info\nType: Issue\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ibm-sells-greg-marston-voice-for-commercial-cloning", "content": "IBM sells Greg Marston voice for commercial cloning\nOccurred: July 2020-September 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nBritish voice actor Greg Marston discovered that AI-generated clones of his voice were being used by third-parties without his permission. \nHaving discovered an 'eerily' similar voice to his own associated with a character named 'Connor' on the Wimbledon website, Marston realised that licensed voice recordings he had recorded for IBM in 2003 and to whom he had granted permission for its use, had been sold to third-party websites that were now using it to create synthetic voices able to say anything, anywhere, at any time.\nThe incident prompted concerns about the impact of AI on the livelihoods of artists, writers, actors, and musicians, many of whom are concerned that their work is being used to train AI systems that will result in loss of future earnings and which may eventually replace them entirely. \nIt also prompted creatives to press technology companies to act ethically and ensure they are asked for their consent and are fairly compensated for their work. \nSystem \ud83e\udd16\nRevoicer website\nOperator: All England Lawn Tennis and Croquet Club\nDeveloper: Revoicer\nCountry: Global\nSector: Media/entertainment/sports/arts\nPurpose: Clone voiceactor's voice\nTechnology: Text-to-speech; Emotion recognition; Neural network; Deep learning; Machine learning\nIssue: Employment; Ethics/values\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.ft.com/content/07d75801-04fd-495c-9a68-310926221554\nhttps://www.abc.net.au/listen/programs/radionational-breakfast/has-a-computer-company-cloned-this-mans-voice/102620224\nhttps://www.itworldcanada.com/post/ai-voice-cloning-threatens-livelihoods-of-voice-actors-others\nhttps://www.nytimes.com/2023/09/23/opinion/ai-internet-lawsuit.html\nhttps://www.reading.ac.uk/news/2023/Expert-Comment/Dr-Mathilde-Pavis-on-AI-risks-for-performers\nhttps://www.cryptopolitan.com/epic-disruption-ai-rob-actors-of-their-voice/\nRelated \ud83c\udf10\nApple trains AI models on Spotify audiobook narrators\nTikTok uses Bev Standing voice without consent to train AI\nPage info\nType: Incident\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/apple-trains-ai-models-on-spotify-audiobook-narrators", "content": "Apple trains AI models on Spotify audiobook narrators \nOccurred: February 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nApple used trained the voices of voiceover artists and authors without their explicit consent to train the AI models powering its AI audiobooks service, resulting in complaints that they were being used to train their own replacements.\nSpotify audiobook narrators and authors discovered that a clause in their agreement with Findaway Voices, a audiobook distributor owned by Spotify, allowed Apple to use their audiobook files for 'machine learning training and models'. \nSome actors and authors pointed out that the clause was not explicitly pointed out to them when they signed updated agreements after Findaway had been bought by Spotify in June 2022. \nApple launched a range of audio Apple Books early January 2023, claiming that its new AI audiobook service was only available to titles for which it was not economic to hire an actor.\nSystem \ud83e\udd16\nFindaway Voices website\nOperator: Apple\nDeveloper: Apple\nCountry: Global\nSector: Media/entertainment/sports/arts\nPurpose: Train AI models\nTechnology: Speech-to-speech; Neural network; Deep learning; Machine learning\nIssue: Employment\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.wired.com/story/apple-spotify-audiobook-narrators-ai-contract/\nhttps://appleinsider.com/articles/23/02/14/audiobook-narrators-complain-apple-may-have-used-them-to-train-ai-voices\nhttps://www.techdogs.com/tech-news/td-newsdesk/did-apple-use-the-content-of-spotify-audio-narrators-to-train-its-ai\nhttps://goodereader.com/blog/audiobooks/audiobook-narrators-and-authors-fear-apple-using-their-voices-to-train-ai\nRelated \ud83c\udf10\nElevenLabs voice generator makes celebrity voices read offensive messages\nVideo game voice actors attacked using their own AI voices\nPage info\nType: Incident\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/waymo-robotaxi-injures-cyclist-in-san-francisco", "content": "Waymo robotaxi injures cyclist in San Francisco\nOccurred: February 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA driverless Waymo car collided with a cyclist in San Francisco, causing minor injuries to the cyclist and leading to a review of the incident by California's auto regulator. \nPer Reuters, Waymo said its vehicle had stopped at a four-way intersection when a large truck crossed the intersection in its direction. At its turn to proceed, the Waymo car moved forward. \nHowever, the cyclist, who was obscured by the truck which the cyclist was following, took a left turn into the Waymo vehicle's path. When the cyclist was fully visible, the Waymo's vehicle braked heavily, but wasn't able to avoid the collision.\nThe incident raised questions about the accuracy, reliability, and safety of Waymo's self-driving system.\nSystem \ud83e\udd16\nWaymo website\nWaymo Wikipedia profile\nOperator: Alphabet/Waymo\nDeveloper: Alphabet/Waymo\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Self-driving system; Computer vision\nIssue: Accuracy/reliability; Safety\nTransparency: Governance; Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reuters.com/world/us/driverless-waymo-car-hits-cyclist-san-francisco-causes-minor-scratches-2024-02-07/\nhttps://www.sfgate.com/bayarea/article/waymo-driverless-car-hits-cyclist-in-sf-18654109.php\nhttps://www.theverge.com/2024/2/7/24065063/waymo-driverless-car-strikes-bicyclist-san-francisco-injuries\nhttps://abc7news.com/waymo-driverless-car-san-francisco-bicyclist-hit-robotaxi-accident/14394661/\nhttps://www.govtech.com/fs/waymo-driverless-car-collides-with-cyclist-in-san-francisco\nhttps://www.cnbc.com/2024/02/07/driverless-waymo-car-hits-cyclist-in-san-francisco-causes-minor-scratches.html\nhttps://qz.com/google-waymo-driverless-car-bicyclist-san-francisco-1851238210\nhttps://gizmodo.com/waymo-accident-cyclist-san-francisco-self-driving-cars-1851238145\nRelated \ud83c\udf10\nWaymo self-driving car hits public bus\nWaymo cars get stuck in cul-de-sac\nPage info\nType: Incident\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/lawsuit-claims-amazon-buy-box-algorithm-overcharges-shoppers", "content": "US lawsuit claims Amazon Buy Box algorithm overcharges shoppers\nOccurred: February 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA lawsuit accused Amazon of violating US consumer protection by steering users of its website to higher-priced items commanding higher fees for the company, as opposed to the 'best' prices it claims. \nAccording to a legal complaint (pdf) filed in the name of two US-based customers of Amazon, the company's Buy Box algorithm often obscures lower-priced options with faster delivery times. \nThe suit also cited a recent antitrust case against Amazon by the US Federal Trade Commission and 17 states which alleged that shoppers use the company's choices almost 98 percent of the time by clicking its 'Buy Now' or 'Add to Cart' buttons, often falsely believing it had identified the best prices. \nThe lawsuit also accused Amazon of creating the algorithm to benefit third-party sellers that participate in its Fulfillment By Amazon programme, which pay 'hefty fees' for inventory storage, packing and shipping, returns and other services. \n\u2795 July 2024. US Judge Marsha Pechman dismissed this case, stating that the plaintiffs failed to demonstrate how they were harmed by Amazon's alleged practices.\nSystem \ud83e\udd16\nAmazon Buy Box\n\nOperator: Amazon\nDeveloper: Amazon\nCountry: USA\nSector: Retail\nPurpose: Determine seller\nTechnology: Machine learning\nIssue: Consumer protection\nTransparency: Governance; Marketing\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nTaylor et al v. Amazon.com, Inc (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reuters.com/technology/amazon-steers-consumers-higher-priced-items-lawsuit-claims-2024-02-09/\nhttps://www.reuters.com/legal/transactional/amazon-defeats-us-consumer-lawsuit-over-buy-box-product-listings-2024-07-08/\nhttps://www.law360.com/corporate/articles/1796025/amazon-hit-with-class-action-over-rigged-buying-features\nhttps://www.theregister.com/2024/02/10/amazon_buy_box_lawsuit/\nhttps://chainstoreage.com/news-briefs/2024-02-09\nRelated \ud83c\udf10\nAmazon Project Nessie automated price gouging\nAmazon US own brand search engine rigging\nPage info\nType: Issue\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-sells-ai-generated-books-about-king-charles-cancer", "content": "Amazon sells AI-generated books about King Charles' cancer\nOccurred: February 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAI-generated books about King Charles' cancer diagnosis have been offered for sale on Amazon, sparking fury from Buckingham Palace.\nSeven fake biographies riddled with fake claims such as the King actually having skin cancer and that he had suffered an undisclosed accident were published on Amazon, according to the Mail on Sunday. Seemingly generated by AI, the books were penned by unknown authors. \nThe incident prompted commentators to accuse the 'authors' of intruding the King's privacy, and to call out Amazon for its apparent inability or unwillingness to properly police its website for AI-generated content. \nAn Amazon spokesman told the Mail on Sunday that the company invested 'significant time and resources' to ensure books published on its website followed its 'content guidelines', adding 'We don't allow AI-generated content that violates our content guidelines, including content that creates a disappointing customer experience.'\nSystem \ud83e\udd16\nAmazon UK website\nAmazon Wikipedia profile\nOperator: Amazon\nDeveloper: Amazon\nCountry: UK\nSector: Retail\nPurpose: Moderate content\nTechnology: Content moderation system\nIssue: Fraud; Mis/disinformation\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.dailymail.co.uk/news/article-13069743/buckingham-palace-lawyers-king-charles-cancer-diagnosis-ai-amazon.html\nhttps://www.lbc.co.uk/news/ai-made-books-with-sick-lies-about-king-charles-cancer-prompt-furious-response-f/\nhttps://www.independent.co.uk/news/uk/home-news/king-charles-cancer-ai-books-b2494197.html\nhttps://www.mirror.co.uk/news/royals/ai-books-king-charless-cancer-32098211\nRelated \ud83c\udf10\nAI-generated mushroom foraging books flood Amazon\nAmazon sells fake AI Jane Friedman books\nPage info\nType: Incident\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/toilet-sensors-actively-listen-to-uk-school-pupils", "content": "Toilet sensors \u2018actively listen\u2019 to UK school pupils\nOccurred: February 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nSchools in the UK have been accused of covertly monitoring students in toilets in an attempt to curb vaping, bullying, and unruly behaviour, without their parents' permission.\nAccording to SchoolsWeek, schools have been using products such as Triton's 3D Pro Sensor to actively detect vape smells and anomolous noises using sensors, as well as certain keywords through machine learning algorithms, which trigger alerts to selected staff members. \nThe report cited the head teacher at Baxter College, Kidderminster, acknowledging that parental permission had not been obtained, though parents were very positive' about the school's attempts to crack down on vaping, she said.\nThe finding triggered complaints by privacy advocates. Madeleine Stone, a senior advocacy officer for UK digital rights pressure group Big Brother Watch, voiced her concerns by stating 'secretly monitoring school bathrooms is a gross violation of children\u2019s privacy and would make pupils and parents deeply uncomfortable.'\nSystem \ud83e\udd16\nTriton 3D Sense Pro website\nOperator: Baxter College, Kidderminster\nDeveloper: Triton\nCountry: UK\nSector: Education\nPurpose: Detect vaping; Increase safety\nTechnology: Machine learning; Keyword detection\nIssue: Privacy; Surveillance\nTransparency: Marketing\nResearch, advocacy \ud83e\uddee\nReclaim the Net (2024). Listening Devices Are Being Installed In UK School Bathrooms\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://schoolsweek.co.uk/schools-install-toilet-sensors-that-actively-listen-to-pupils/\nhttps://feweek.co.uk/ai-will-march-into-every-aspect-of-education-but-what-are-the-consequences/\nRelated \ud83c\udf10\nAxon to develop school security taser drones\nTemple of Heaven Park uses facial recognition to stop toilet paper theft\nPage info\nType: Incident\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/new-york-lawyer-cites-fake-ai-generated-court-decision", "content": "New York lawyer cites fake AI-generated court case\nOccurred: January 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nNew York lawyer Jae Lee cited a fictitious case generated by ChatGPT to appeal a lawsuit, resulting in her facing possible sanctions.\nAttorney Jae Lee was referred to the grievance panel of the 2nd US Circuit Court of Appeals after she cited a fabricated case about a Queens doctor botching an abortion in an appeal to revive her client's lawsuit. \nThe appeal was dismissed after it was discovered that the case did not exist and had been conjured up by OpenAI's ChatGPT chatbot. The grievance panel concluded concluded Lae's conduct fell 'well below the basic obligations of counsel'. She now faces possible sanctions.\nThe incident was the one of several examples of lawyers misusing generative AI in legal cases, and prompted concern from lawyers and others that the technology is being used in the wrong way. \nSystem \ud83e\udd16\nChatGPT chatbot\nOperator: Jae Lee\nDeveloper: OpenAI\nCountry: USA\nSector: Govt - justice\nPurpose: Conduct legal research\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Accuracy/reliability\nTransparency: Marketing\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nPark v Kim\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.dailymail.co.uk/news/article-13056843/New-York-lawyer-used-Chat-GPT-research-cases-used-fictitious-decision-appeal-clients-lawsuit-claiming-Queens-doctor-botched-abortion.html\nhttps://www.verdict.co.uk/ai-under-fire-in-legal-sector-after-chatgpt-used-to-cite-fictitious-case/\nhttps://news.bloomberglaw.com/business-and-practice/ny-lawyer-faces-possible-sanctions-for-citing-phony-chatgpt-case\nhttps://www.lexology.com/library/detail.aspx?g=51bb06ab-4ea6-4f4a-8f8f-79841a6eb3d1\nRelated \ud83c\udf10\nMichael Cohen supplies fake AI legal citations to lawyer\nDefence lawyer using AI 'botches' criminal trial closing argument\nPage info\nType: Incident\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/american-bitcoin-academy-charged-with-ai-powered-fraud", "content": "American Bitcoin Academy charged with 'AI' powered fraud\nOccurred: 2018-2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA US businessman was charged with defrauding 15 students by persuading them to invest in a fund that promised high returns using AI.\nBrian Sewell lured students of his online course American Bitcoin Academy into parting with significant sums of money that would be invested in his supposedly 'artificial intelligence and 'machine learning'-powered Rockwell Capital Management crypto fund. \nBut Sewell never launched the fund, instead purchasing USD 1.2 million worth of Bitcoin with the students' money, all of which he lost when his BTC wallet was hacked and wiped clean, according to US Securities and Exchange Commission (SEC).\nThe incident was seen to show the SEC clamping down on individuals and companies using 'attention-grabbing' technologies to attract and defraud investors.\nSystem \ud83e\udd16\nAmerican Bitcoin Academy\nOperator: American Bitcoin Academy\nDeveloper: Brian Sewell\nCountry: USA\nSector: Banking/financial services\nPurpose: Defraud\nTechnology: Machine learning\nIssue: Fraud\nTransparency: Marketing\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nUS Securities and Exchange Commission (2024). SEC charges founder of American Bitcoin Academy crypto course with fraud targeting students\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://cryptodaily.co.uk/news-in-crypto/althalla:american-bitcoin-academy-founder-faces-crypto-fraud-charges\nhttps://decrypt.co/215502/american-bitcoin-academy-crypto-entrepreneur-settles-1-2m-sec-fraud-charges\nhttps://www.bloomberg.com/news/articles/2024-02-02/online-crypto-course-founder-scammed-students-with-fake-hedge-fund-sec-alleges\nhttps://coingeek.com/south-korea-to-vet-vasp-execs-us-sec-charges-american-bitcoin-academy-over-1-2m-scam/\nhttps://www.theblock.co/post/275823/sec-charges-founder-of-online-crypto-course-who-targeted-students-to-invest-in-nonexistent-hedge-fund\nRelated \ud83c\udf10\nAutomators AI online sales and coaching fraud\nEngineer.ai misleading marketing\nPage info\nType: Incident\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/google-researcher-believes-lamda-is-sentient", "content": "Google researcher fired for believing LaMDA is 'sentient'\nOccurred: June 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGoogle engineer Blake Lemoine tried to convince fellow Google employees that the company's LaMDA language model was sentient.\nPer a June 2022 Washington Post report, Lemoine, an ordained Christian mystical priest 'was inclined to give it the benefit of the doubt 'When LaMDA claimed to have a soul and then was able to eloquently explain what it meant by that.'\nGoogle, technology professionals, philsophers and ethicists responded to the notion that LaMDA - and other technologies - can be human primarily on technical, scientific grounds, prompting Lemoine to complain the model faces 'bigotry' in an interview with WIRED.\nGoogle suspended and fired Lemoine after he breached company policy by sharing information about his project, recruited a lawyer for the AI after claiming that LaMDA had asked him to do so, and alleged that Google was discriminating against him because of his religion.\nSystem \ud83e\udd16\nGoogle LaMDA large language model\nOperator: Blake Lemoine\nDeveloper: Alphabet/Google\nCountry: USA\nSector: Technology; Religion\nPurpose: Optimise language models for dialogue\nTechnology: Large language model; Neural network; NLP/text analysis\nIssue: Anthropomorphism\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.washingtonpost.com/technology/2022/06/11/google-ai-lamda-blake-lemoine/\nhttps://www.wired.com/story/lamda-sentient-ai-bias-google-blake-lemoine/\nhttps://www.wired.com/story/blake-lemoine-google-lamda-ai-bigotry/\nhttps://www.nytimes.com/2022/06/12/technology/google-chatbot-ai-blake-lemoine.html\nhttps://gizmodo.com/google-ai-chatbot-sentient-lamda-1849053005\nhttps://www.vox.com/recode/2022/6/30/23188222/silicon-valley-blake-lemoine-chatbot-eliza-religion-robot\nhttps://www.timesofisrael.com/google-engineer-says-ais-israel-joke-helped-drive-his-belief-it-was-sentient/\nhttps://www.prindleinstitute.org/2022/07/lamda-lemoine-and-the-problem-with-sentience/\nhttps://www.livescience.com/google-sentient-ai-lamda-lemoine\nhttps://opendatascience.com/is-lamda-really-sentient-no-far-from-it/\nhttps://theconversation.com/is-googles-lamda-conscious-a-philosophers-view-184987\nhttps://www.businessinsider.com/gary-marcus-google-lamda-artificial-intelligence-media-hype-dead-cat-2022-6\nhttps://www.iflscience.com/it-hired-a-lawyer-the-story-of-lamda-and-the-google-engineer-just-got-even-weirder-64229\nRelated \ud83c\udf10\nChatGPT invented case citations in legal filings\nGoogle Duplex accused of being 'deceitful' and 'unethical'\nPage info\nType: Issue\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/google-duplex-accused-of-being-deceitful-and-unethical", "content": "Google Duplex accused of being 'deceitful' and 'unethical'\nOccurred: May 2018\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA demonstration of Google's Duplex AI assistant drew criticism that it was misleading users into thinking they were dealing with human being rather than a machine.\nDuplex was an extension to Google Assistant that enables it to autonomously schedule appointments and book restaurants in a human-sounding voice. First shown as an unfinished product at Google's I/O developers\u2019 conference in 2018, it received plaudits for sounding remarkably human by pausing in the right places, using filler words, and other techniques. \nBut criticism was also aimed at Google for failing to disclose that people would be communicating with a machine, with people calling the company 'deceitful' and 'unethical'. In a statement to The Verge, Google said it took transparency seriously and would explicitly let people know they were interacting with a machine.\nSystem \ud83e\udd16\nGoogle Duplex Wikipedia profile\nOperator: Alphabet/Google\nDeveloper: Alphabet/Google\nCountry: USA\nSector: Consumer goods\nPurpose: Schedule appointments\nTechnology: Speech recognition; NLP/text analysis; Machine learning\nIssue: Anthropomorphism; Ethics/values\nTransparency: Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theguardian.com/technology/2018/may/11/google-duplex-ai-identify-itself-as-robot-during-calls\nhttps://www.theverge.com/2018/5/10/17342414/google-duplex-ai-assistant-voice-calling-identify-itself-update\nhttps://www.bloomberg.com/news/articles/2018-05-10/google-grapples-with-horrifying-reaction-to-uncanny-ai-tech\nhttps://www.cnet.com/news/google-duplex-assistant-bot-deception-scary-ethics-question/\nhttps://www.theverge.com/2019/5/22/18636138/google-duplex-human-callers-25-percent-ai-restaurant-booking\nhttps://www.npr.org/2018/05/14/611097647/googles-duplex-raises-ethical-questions\nhttps://www.technologyreview.com/2018/06/27/141823/google-demos-duplex-its-ai-that-sounds-exactly-like-a-very-weird-nice-human/\nhttps://www.theverge.com/2018/5/11/17340894/google-duplex-all-party-consent-state-eavesdropping\nhttps://techcrunch.com/2018/05/10/duplex-shows-google-failing-at-ethical-and-creative-ai-design/\nRelated \ud83c\udf10\nGoogle researcher believes LaMDA is 'sentient'\nChatGPT invented case citations in legal filings\nPage info\nType: Incident\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/durham-police-rapped-for-crude-criminal-profiling", "content": "Durham police rapped for 'crude' criminal reoffender profiling\nOccurred: April 2018\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nDurham's police force was criticised by privacy campaigners over the 'crude' nature of the data it was using to help predict which offenders were likely to commit more crimes.\nAn investigation by digital rights and privacy group Big Brother Watch (BBW) revealed that Durham Constabulary had augmented police data underpinning its Harm Assessment Risk Tool (HART) with Experian's Mosaic dataset The dataset classifed Britons into 66 groups such as 'disconnected youth' and 'Asian heritage' and were annotated with lifestyle details such as 'heavy TV viewers', 'overcrowded flats' and 'families with needs'.\nSuch categories were \u2018really quite offensive and crude\u2019, according to BBW's Silkie Carlo. Durham later said it stopped including Mosaic in its dataset.\nThe Harm Risk Assessment Risk Tool (Hart) scored offenders and placed them into three categories indicating they were at low, moderate or high-risk of reoffending. Those deemed to be at moderate risk of reoffending being offered the chance to go into a rehabilitation programme called Checkpoint as an 'alternative to prosecution.'\nSystem \ud83e\udd16\nDurham Constabulary website\nDurham Constabulary Wikipedia profile\nOperator: Durham Constabulary\nDeveloper: Cambridge University; Durham Constabulary\nCountry: UK\nSector: Govt - police\nPurpose: Predict criminal reoffenders\nTechnology: Prediction algorithm; Machine learning\nIssue: Accuracy/reliability; Bias/discrimination; Human/civil rights\nTransparency: Governance; Black box\nResearch, advocacy \ud83e\uddee\nEDRi (2020). Use cases: Impermissible AI and fundamental rights breaches (pdf)\nBig Brother Watch (2018). POLICE USE EXPERIAN MARKETING DATA FOR AI CUSTODY DECISIONS\nBig Brother Watch (2018). A CLOSER LOOK AT EXPERIAN BIG DATA AND ARTIFICIAL INTELLIGENCE IN DURHAM POLICE\nOswald M., Grace J. (2017). Algorithmic Risk Assessment Policing Models: Lessons from the Durham HART Model and \u2018Experimental\u2019 Proportionality\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/technology-43428266\nhttps://eandt.theiet.org/content/articles/2018/04/big-brother-watch-highlights-police-forces-chilling-use-of-experians-big-data-tool/\nhttps://www.wired.co.uk/article/police-ai-uk-durham-hart-checkpoint-algorithm-edit\nhttps://www.lawgazette.co.uk/news-focus/news-focus-data-dangers/5070551.article\nhttps://www.forbes.com/sites/barrycollins/2022/03/29/ai-handing-out-rough-justice-in-the-uk/\nRelated \ud83c\udf10\nEric Loomis COMPAS prison sentencing\nTitus Henderson COMPAS parole denial\nPage info\nType: Issue\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/knightscope-k5-security-robot-drowns-in-fountain", "content": "Knightscope K5 security robot 'drowns' in fountain \nOccurred: July 2017\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Knightscope K5 security robot named 'Steve' fell down some stairs into a water feature at an office complex in Washington DC, USA. \nKnightscope officials later said that this was an 'isolated incident' and that Steve had slipped on a 'loose brick surface' before tipping down the stairs into the water. The company added that this had been caused by an error in the self-driving algorithm, and that a new robot would be delivered to Washington Harbour for free.\nThe K5 is equipped with sensors, a 360-degree video camera, microphones, air quality sensors, and thermal imaging capabilities. According to Knightscope it can scan up to 1,500 car number plates per minute and detect gun shots and other notable sounds.\nSystem \ud83e\udd16\nKnightscope K5 website\nKnightscope Wikipedia profile\nOperator: Republic Properties/Washington Habour\nDeveloper: Knightscope\nCountry: USA\nSector: Business/professional services\nPurpose: Provide security\nTechnology: Robotics\nIssue: Robustness\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/technology-40642968\nhttps://www.theverge.com/tldr/2017/7/17/15986042/dc-security-robot-k5-falls-into-water\nhttps://www.washingtonpost.com/news/the-switch/wp/2017/07/17/a-security-robot-fell-into-a-water-fountain-at-a-d-c-office-building-and-the-internet-went-wild/\nhttps://www.yahoo.com/news/robot-dived-washington-d-c-fountain-not-victim-172501082.html\nhttps://arstechnica.com/gadgets/2017/07/knightscope-k5-security-bot-drowned/\nhttps://edition.cnn.com/2017/07/18/us/security-robot-drown-trnd/index.html\nRelated \ud83c\udf10\nKnightscope K5 security robot hits child\nKnightscope HP RoboCop ignores woman reporting crime\nPage info\nType: Incident\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/nypd-ends-knightscope-k5-security-robot-trial", "content": "NYPD ends Knightscope K5 security robot trial\nOccurred: September 2023-February 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe New York Police Department ended its use of Knightscope's security robot in Times Square subway station after a six-month trial, calling into question the effectiveness of the robot.\nInitially heralded as a low-cost method of deterrning crime, the robot, which was designed to operate autonomously, received a mixed recpetion from New Yorkers and visitors, with some saying it was potentially a valuable additional crime-fighting resource, whilst others reckoned it seemed to do very little, was unable to walk up or down stairs, always required assistance, was a waste of resources, and threatened people's privacy.\nIn addition to raising questions about the effectiveness of the Knightscope K5 robot as a crime-fighting tool, the NYPD's decision to stop its use - for the time being -  highlights the careful balance police authorities are seen to have to strike between fighting crime, and protecting the legal rights and ethical concerns of citizens. \nSystem \ud83e\udd16\nKnightscope K5 website\nKnightscope Wikipedia profile\nOperator: New York citizens\nDeveloper: Knightscope\nCountry: USA\nSector: Govt - police\nPurpose: Strengthen security\nTechnology: Robotics\nIssue: Effectiveness/value; Privacy; Surveillance\nTransparency: \nResearch, advocacy \ud83e\uddee\nS.T.O.P. (2024). S.T.O.P. Welcomes End Of NYPD Times Sq. Robot Pilot\nS.T.O.P. (2023). S.T.O.P. Condemns NYPD\u2019s \u2018Knockoff Robocop\u2019 Police Drones\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.nytimes.com/2024/02/02/nyregion/nypd-subway-robot-retires.html\nhttps://www.engadget.com/nyc-ends-trial-run-of-times-square-subways-security-robot-105528275.html\nhttps://www.nbcnewyork.com/news/local/nyc-retires-420-pound-nypd-subway-surveilling-robot-after-pilot-ends/5099973/\nhttps://nypost.com/2024/02/04/metro/nypd-robot-no-longer-patrolling-times-square-station/\nhttps://www.popsci.com/technology/nypd-retires-k5-subway-robot/\nhttps://abc7ny.com/nypd-robot-security-k5-subway-patrol/14381116/\nRelated \ud83c\udf10\nKnightscope K5 security robot hits child\nKnightscope HP RoboCop ignores woman reporting crime\nPage info\nType: Issue\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-france-fined-for-excessive-automated-monitoring-of-workers", "content": "Amazon France fined for excessive automated monitoring of workers\nOccurred: 2020-\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAmazon was fined EUR 32 million by France's privacy regulator for the 'excessive' and 'illegal' monitoring of staff activity and performance using scanners and several software systems.\nAmazon France Logistique had been using handheld scanners and three indicators to measure the producivity and inactivity of its employees, including for tasks such as putting an item on a shelf, taking an item off a shelf, putting an item into a box, and time spent on breaks. \nAccording to France's National Commission on Informatics and Liberty (CNIL), 'the implementation of a system measuring interruptions of activity so precisely and leading to the employee potentially having to justify each break or interruption was illegal' and had breached the EU's GDPR principle of data minimisation and the lawfulness of the processing.\nThe CNI also took issue with Amazon's transparency, or lack thereof. Before April 2020, temporary workers had not been informed before their data was collected, and employees were not properly told about video surveillance systems.\nAmazon disagreed with the CNIL's conclusions, which it described as 'factually incorrect.' \nSystem \ud83e\udd16\nAmazon France website\n\nDocuments \ud83d\udcc3\nAmazon France (2024). D\u00e9claration d'Amazon \u00e0 propos de la d\u00e9cision de la CNIL\nOperator: Amazon France Logistique employees, visitors  \nDeveloper: Amazon France Logistique\nCountry: France\nSector: Transport/logistics\nPurpose: Monitor employee performance\nTechnology: Handheld scanner\nIssue: Employment; Necessity/proportionality; Privacy; Surveillance\nTransparency: Governance; Marketing\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nCNIL (2024). Employee monitoring: CNIL fined AMAZON FRANCE LOGISTIQUE \u20ac32 million\nEuropean Data Protection Board (2024). Employee monitoring: French SA fined Amazon France Logistique \u20ac32 million\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/business-68067022\nhttps://www.euronews.com/business/2024/01/23/amazon-fined-for-excessively-intrusive-surveillance-of-its-workers-in-france\nhttps://apnews.com/article/amazon-fine-monitoring-workers-privacy-france-d503314234ccacb366e2afaa49d097b1\nhttps://edition.cnn.com/2024/01/23/tech/amazon-france-fine-worker-surveillance/index.html\nhttps://www.theverge.com/2024/1/23/24048197/amazon-fine-employee-surveillance-france-cnil-gdpr-privacy\nRelated \ud83c\udf10\nAmazon Flex algorithm fires delivery drivers\nTeleperformance TP Observer employee monitoring\nPage info\nType: Incident\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/philadelphia-sheriff-posts-fake-ai-generated-news-stories", "content": "Philadelphia sheriff posts fake AI-generated news stories\nOccurred: February 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe campaign team for Philadelphia\u2019s sheriff used fake positive stories generated by AI posted to her website to help make the case for her re-election. \nThe Philadelphia Inquiry drew attention to a series of articles under the names of local news publications that had been posted to Rochelle Bilal's website that supposedly highlighted her first term accomplishments but which proved to be non-existent. Bilal's team later acknowledged that the stories had been generated by ChatGPT, though argued they had been based on real events. \nThe incident was seen to raise ethical questions about the integrity of Bilal's campaign and the possible erosion of trust in elections and democracy posed by AI-generated misinformation and disinformation. It also highlighted concerns about OpenAI's willingness or ability to police its policies regarding the political use of ChatGPT and its other products.\nSystem \ud83e\udd16\nChatGPT chatbot\nOperator: Philadelphia citizens\nDeveloper: OpenAI\nCountry: USA\nSector: Politics\nPurpose: Support political campaign\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Ethics/values; Mis/disinformation\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.inquirer.com/news/rochelle-bilal-ai-headlines-chatbot-sheriff-20240206.html\nhttps://futurism.com/the-byte/philadelphia-sheriff-ai-articles\nhttps://abcnews.go.com/Technology/wireStory/philly-sheriffs-campaign-takes-bogus-news-stories-posted-106972983\nhttps://nypost.com/2024/02/05/news/philly-sheriff-posts-flattering-but-phony-headlines-to-campaign-site-then-offers-odd-disclaimer-when-caught/\nhttps://fortune.com/2024/02/06/philadelphia-sheriff-fake-news-chatgpt-30-articles-removed/\nhttps://www.theblaze.com/news/philly-sheriff-s-team-blames-chatgpt-after-fake-flattering-headlines-appeared-on-campaign-website\nhttps://apnews.com/article/fake-news-philadelphia-sheriff-website-ai-headlines-7bace99ffe0f11d8e8b17862c7b55e4e\nRelated \ud83c\udf10\nChatGPT generates political messages, campaigns\nChatGPT exhibits 'systemic' left-wing bias\nPage info\nType: Incident\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-hr-system-automatically-fires-inefficient-warehouse-workers", "content": "Amazon HR system automatically fires 'inefficient' warehouse workers\nOccurred: August 2017-April 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn Amazon HR system automatically monitored and sacked 'hundreds' of workers at a single warehouse who were not considered to be working fast enough.\nIn a letter to the US National Labor Relations Board shared with The Verge, an attorney at law firm Morgan Lewis & Bockius representing Amazon responded to a termination complaint by describing (pdf) how Amazon fired 'hundreds' of workers at a single facility between August 2017 and September 2018 for failing to reach productivity targets and quality-control mandates.\nThe system 'automatically generates any warnings or terminations regarding quality or productivity without input from supervisors', according to the letter. It also said that a worker would receive a termination notice if they were given six warnings in a 12-month period. \nAmazon responded by denying it sent out automatically-generated termination notices and said its warehouse workers are given training to help them improve if they miss targets.\nSystem \ud83e\udd16\nAmazon US website\nAmazon Wikipedia profile\nOperator: Amazon\nDeveloper: Amazon\nCountry: USA\nSector: Transport/logistics\nPurpose: Improve productivity\nTechnology: Machine learning\nIssue: Employment; Fairness\nTransparency: Governance\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nLegal letter to US National Labor Relations Board (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theverge.com/2019/4/25/18516004/amazon-warehouse-fulfillment-centers-productivity-firing-terminations\nhttps://www.thesun.co.uk/news/8963968/amazon-workers-sacked-fired-robot/\nhttps://www.cbsnews.com/news/amazon-under-fire-for-software-that-recommends-firing-workers/\nhttp://www.digitaljournal.com/tech-and-science/technology/amazon-uses-artificial-intelligence-to-fire-warehouse-workers/article/548594\nhttps://www.businessinsider.com/amazon-system-automatically-fires-warehouse-workers-time-off-task-2019-4\nhttps://www.news.com.au/finance/business/retail/amazon-warehouse-tracking-system-automatically-sacks-underperforming-workers/news-story/13a54a081c42e3a9f1948bf612005371\nhttps://www.nytimes.com/2019/06/23/technology/artificial-intelligence-ai-workplace.html\nhttps://www.technologyreview.com/2019/04/26/1021/amazons-system-for-tracking-its-warehouse-workers-can-automatically-fire-them/\nhttps://futurism.com/amazon-ai-fire-workers\nRelated \ud83c\udf10\nAmazon Flex algorithm delivery driver firings\nAmazon AI recruitment tool favours men over women\nPage info\nType: Incident\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-accused-of-promoting-anti-vaccine-propaganda", "content": "Amazon accused of promoting anti-vaccine propaganda\nOccurred: February 2019-March 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nBooks and movies full of anti-vaccination conspiracy theories were found to have been dominating Amazon's online stores, raising concerns that the company prioritises free speech over public health.\nA search for the word 'vaccine' on Amazon US returned books and movies dominated by anti-vaccination content, with 15 of 18 books and movies listed containing anti-vaccination content. \nThese were published under titles such as 'We Don\u2019t Vaccinate!' and 'Shoot \u2018Em Up: The Truth About Vaccines,' as well as under more misleading titles like 'Miller\u2019s Review of Critical Vaccine Studies: 400 Important Scientific Papers Summarized for Parents and Researchers,' according to CNN.\nThe finding prompted US Congressman Adam Schiff to complain to Amazon CEO Jeff Bezos that his company was serving up anti-vaccine propaganda and accepting paid advertising for anti-vaccine media.\nSystem \ud83e\udd16\nAmazon US website\nAmazon Wikipedia profile\nOperator: Amazon users\nDeveloper: Amazon\nCountry: USA\nSector: Retail\nPurpose: Recommend books, movies\nTechnology: Recommendation algorithm\nIssue: Mis/disinformation; Freedom of expression - censorship\nTransparency: Governance\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nAdam Shiff (2019). SCHIFF SENDS LETTER TO AMAZON CEO REGARDING ANTI-VACCINE MISINFORMATION\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://edition.cnn.com/2019/02/27/tech/amazon-anti-vaccine-books-movies/index.html\nhttps://www.theverge.com/2019/3/1/18246461/amazon-anti-vaccination-vaxx-books-films-vaccines-media-schiff-letter\nhttps://www.wired.co.uk/article/amazon-autism-fake-cure-books\nhttps://www.cnbc.com/2019/03/13/amazon-removes-books-touting-debunked-autism-cures.html\nhttps://www.washingtonpost.com/business/2019/03/18/censorship-or-social-responsibility-amazon-removes-some-books-peddling-vaccine-misinformation/\nhttps://www.nbcnews.com/tech/tech-news/facebook-cracks-down-vaccine-misinformation-n980686\nhttps://www.cbsnews.com/news/amazon-removes-books-promoting-autism-cures-and-vaccine-misinformation/\nhttps://www.theguardian.com/us-news/2019/mar/05/revealed-amazonsmile-helps-fund-anti-vaccine-groups\nhttps://www.businessinsider.com/amazon-removes-anti-vaccine-conspiracy-documentaries-from-prime-video-2019-3\nhttps://www.nbcnews.com/tech/internet/amazon-removes-books-promoting-autism-cures-vaccine-misinformation-n982576\nRelated \ud83c\udf10\nAmazon, Waterstones algorithms promote vaccine misinformation\nAmazon Alexa says 2020 US election was rigged\nPage info\nType: Issue\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/waze-directs-tourists-to-drive-into-vermont-lake", "content": "Waze directs tourists to drive into Vermont lake\nOccurred: January 2018\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGoogle-owned navigation app Waze directed a group of tourists in their borrowed Jeep Compass to drive into an icy lake in Vermont, USA.\nAccording to the police report, the driver 'followed the GPS directions, which advised to go straight, and upon following this he went down the boat ramp onto the ice.' The officer who wrote the report said he believed the driver 'was not under the influence of alcohol or drugs at the time of the incident.'\nGoogle said it was unable to explain how Waze directed the driver into the lake. 'Generally speaking, Waze maps are updated with millions of edits to adapt to real time road conditions daily, often making them the most accurate available', a spokesperson told the Burlington Free Press.\nSystem \ud83e\udd16\nWaze website\nWaze Wikipedia profile\nOperator: Waze users\nDeveloper: Alphabet/Google/Waze\nCountry: USA\nSector: Travel/hospitality\nPurpose: Direct drivers\nTechnology: Machine learning\nIssue: Accuracy/reliability; Safety\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://nymag.com/intelligencer/2018/01/waze-app-directs-driver-to-drive-car-into-lake-champlain.html\nhttps://www.nst.com.my/world/2018/01/328506/us-driver-follows-wazes-instructions-drives-suv-lake\nhttps://mashable.com/article/waze-driving-accident-scandal\nhttps://fox5sandiego.com/news/tourist-blame-waze-app-after-driving-into-lake/\nhttps://eu.burlingtonfreepress.com/story/news/2018/01/23/waze-google-lake-champlain-car/1057316001/\nRelated \ud83c\udf10\nWaze directs users into San Francisco wildfires\nUS man dies driving off collapsed bridge while following Google Maps\nPage info\nType: Incident\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/pedestrian-following-google-maps-hit-by-motorcyclist", "content": "Pedestrian following Google Maps hit by motorcyclist\nOccurred: January 2009\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA pedestrian was injured by a motorist while following an online walking route across Park City, Utah, filed a lawsuit claiming Google had supplied unsafe directions on its Maps app. \nGoogle Maps instructed Lauren Rosenberg to walk for about 0.5 miles along 'Deer Valley Drive', an alternative name for that section of Utah State Route 224. Rosenberg said she was not not warned the highway lacked sidewalks, putting Google at fault in the accident, the case claimed. Rosenberg was struck by a vehicle while crossing the road.\nThe incident prompted controversy, with some people arguing Rosenberg failed to use her common sense. The court later dismissed her case on legal and 'policy' grounds. Rosenberg had sought compensation for 'severe' injuries costing over USD 100,000 in medical bills, lost wages, and unitive damages.\nSystem \ud83e\udd16\nGoogle Maps website\nGoogle Maps Wikipedia profile\nOperator: Lauren Rosenberg\nDeveloper: Alphabet/Google\nCountry: USA\nSector: Travel/hospitality\nPurpose: Direct pedestrians\nTechnology: Machine learning\nIssue: Accuracy/reliability\nTransparency: Governance\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nRosenwood v Harwood et al\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.nbcnews.com/id/wbna37453662\nhttps://searchengineland.com/woman-follows-google-maps-walking-directions-gets-hit-sues-43212\nhttps://searchengineland.com/attorney-in-google-maps-lawsuit-43349\nhttps://www.telegraph.co.uk/technology/google/7795460/Lauren-Rosenberg-US-woman-sues-Google-after-Maps-directions-caused-accident.html\nhttps://www.theguardian.com/media/pda/2010/jun/02/google-maps-lawsuit\nhttps://www.pcworld.com/article/506915/google_gps.html\nhttps://www.dailymail.co.uk/news/article-1282926/Pedestrian-sues-Google-shes-knocked-walking-highway.html\nhttps://www.lawinsider.org/post/rosenberg-v-google-women-sues-google-maps-for-not-watching-the-road\nhttps://blog.ericgoldman.org/archives/2011/06/injured_pedestr.htm\nRelated \ud83c\udf10\nTeenager freezes to death after Google Maps provides wrong turn\nWaze directs users into San Francisco wildfires\nPage info\nType: Incident\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/teenager-freezes-to-death-after-google-maps-provides-wrong-turn", "content": "Teenager freezes to death after Google Maps provides wrong turn\nOccurred: December 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nRussian teenager Sergey Ustinov died in -50C temperature after Google Maps directed him down a short cut that transpired to be an infamous Siberian road known as the 'Road of Bones'.\nUstinov and a friend were driving from Yakutsk to the Pacific port of Magadan when Google Maps offered a shorter option of 1,733 km (1,076 miles) on the R504 Kolyma highway - also known as the Road of Bones. \nA policemen searching for the missing pair found that they had built fires to stay warm in temperatures plunging to -50C. Ustinov died, and his friend was close to death. Local people were apparently shocked that the two had no warm clothes between them for the Siberian winter. \nGoogle competitor Yandex Maps did not offer the same short-cut, raising questions about the safety of the former's system.\nSystem \ud83e\udd16\nGoogle Maps website\nGoogle Maps Wikipedia profile\nOperator: Sergey Ustinov\nDeveloper: Alphabet/Google\nCountry: Russia\nSector: Travel/hospitality\nPurpose: Direct drivers\nTechnology: Machine learning\nIssue:  Accuracy/reliability\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.thesun.co.uk/news/13438396/driver-18-froze-death-road-of-bones-sat-nav-google-maps/\nhttps://www.ladbible.com/news/news-google-maps-change-route-on-road-after-driver-freezes-to-death-in-50c-20201211\nhttps://meaww.com/teenager-dies-at-50-c-after-satellite-navigation-showed-wrong-turn-leading-him-to-deserted-road\nhttps://www.marca.com/en/lifestyle/2020/12/18/5fdcbf70ca4741d5598b4632.html\nhttps://www.news.com.au/lifestyle/real-life/news-life/man-frozen-to-death-after-google-maps-wrong-turn/news-story/13e10cbbc96494ee26e6dea29f4fb469\nhttps://www.india.com/technology/google-maps-latest-update-removes-famous-road-from-its-system-google-maps-google-maps-live-google-maps-directions-google-maps-api-key-4264642/\nhttps://www.dailystar.co.uk/news/world-news/teen-freezes-death-50c-after-23152785\nRelated \ud83c\udf10\nWaze directs users into San Francisco wildfires\nUS man dies driving off collapsed bridge while following Google Maps\nPage info\nType: Incident\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/waze-directs-users-into-san-francisco-wildfires", "content": "Waze, Google Maps direct users into San Francisco wildfires\nOccurred: December 2017\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGoogle Maps and Google-owned navigation app Waze directed drivers into Southern California wildfires from which they were trying to escape. \nUsers of Google Maps and Waze, in addition to reporters, reported that they were being re-directed into neighbourhoods caught up in the Skirball fires, putting their lives at risk. Some also noted that the apps provided no information about the fires, despite efforts by Google to provide updates to its systems using its newly launched SOS Alerts feature.\nThe Los Angeles Police Department was alerting users of navigation apps to avoid using them so they did not end up near the blazes, reported The Los Angeles Times.\nThe incident raised questions about the safety and governance of Google's mapping tools, and whether they could be trusted during a disaster. \nSystem \ud83e\udd16\nGoogle Maps website\nGoogle Maps Wikipedia profile\nOperator: Google Maps users, Waze users\nDeveloper: Alphabet/Google/Waze\nCountry: USA\nSector: Travel/hospitality\nPurpose: Direct drivers\nTechnology: Machine learing\nIssue: Accuracy/reliability; Safety\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.insider.com/la-fires-gps-apps-directing-people-areas-fire-lapd-2017-12\nhttps://thehill.com/policy/technology/363852-waze-is-sending-california-drivers-towards-fires\nhttp://www.slate.com/blogs/future_tense/2017/12/07/california_wildfires_raise_questions_about_whether_you_can_trust_waze_google.html\nhttps://www.cnet.com/roadshow/news/california-wild-fires-navigation-apps/\nhttps://eu.usatoday.com/story/tech/news/2017/12/07/california-fires-navigation-apps-like-waze-sent-commuters-into-flames-drivers/930904001/\nhttps://smallbiztrends.com/2017/12/nav-apps-send-drivers-into-wildfires.html\nRelated \ud83c\udf10\nUS man dies driving off collapsed bridge while following Google Maps\nCouple attacked in 'Hell Run' recommended by Google Maps\nPage info\nType: Incident\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/us-man-dies-driving-off-collapsed-bridge-while-following-google-maps", "content": "US man dies driving off collapsed bridge while following Google Maps\nOccurred: September 2022-\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nMedical device salesman Philip Paxson drowned after Google Maps allegedly directed him to cross a bridge that had collapsed nine years before and his car plunged into a creek.\nPaxton\u2019s body was found in his overturned and partially submerged truck after he had been driving home from his daughter's ninth birthday party on a route in North Carolina he did not know. According to local police, the bridge had not been maintained by local or state officials, and the original developer\u2019s company had dissolved. \nMultiple people had notified Google Maps about the collapse in the years leading up to Paxson\u2019s death and had urged the company to update its route information, according (pdf) to the lawsuit.\nThe lawsuit accused Google of negligence. It also named several private property management companies allegedly responsible for the bridge and the adjoining land. \nSystem \ud83e\udd16\nGoogle Maps website\nGoogle Maps Wikipedia profile\nOperator: Philip Paxson\nDeveloper: Alphabet/Google\nCountry: USA\nSector: Travel/hospitality\nPurpose: Direct drivers\nTechnology: Machine learning\nIssue: Accuracy/reliability; Safety\nTransparency: Governance\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nAlicia Paxson v Google LLC (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.nytimes.com/2023/09/21/us/google-maps-lawsuit-collapsed-bridge.html\nhttps://www.thestar.com.my/tech/tech-news/2023/09/21/google-sued-for-negligence-after-us-man-drove-off-collapsed-bridge-while-following-map-directions\nhttps://news.sky.com/story/man-dies-driving-off-collapsed-bridge-following-google-maps-family-sues-12966007\nhttps://www.theguardian.com/technology/2023/sep/20/google-maps-collapsed-bridge-negligence-lawsuit\nhttps://www.nbcnews.com/news/us-news/widow-man-died-driving-collapsed-bridge-sues-google-directing-rcna110616\nhttps://www.forbes.com/sites/maryroeloffs/2023/09/20/family-sues-google-after-father-drowned-by-following-google-maps-off-collapsed-bridge/\nhttps://apnews.com/article/google-maps-lawsuit-north-carolina-death-f4707247ee3295bf51bbcb37bd0eb6c8\nRelated \ud83c\udf10\nWaze directs users into San Francisco wildfires\nCouple attacked in 'Hell Run' recommended by Google Maps\nPage info\nType: Incident\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/couple-attacked-in-hell-run-area-recommended-by-google-maps", "content": "Couple attacked in 'Hell Run' recommended by Google Maps\nOccurred: October 2023-\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA US couple sued Google after Google Maps directed them into a South Africa \u2018Hell Run\u2019 area where they were assaulted at gunpoint and robbed.\nTrying to navigate to Cape Town airport, LA-based Jason and Katharine Zoladz were directed by Google Maps into a notoriously dangerous area when they were attacked, assaulted, and robbed by an armed gang at an intersection. According to the lawsuit (pdf), Jason Zoladz was left bleeding by the roadside having had his jaw smashed by a brick. He had surgery later that day. \nThe couple claimed in the suit that Google knew the \u2018extreme dangers\u2019 of the route and that it was known locally for years as the site of \u2018numerous\u2019 violent attacks on tourists by armed criminals. They also argued that Google has a responsibility to protect its users, but failed to protect or warn them of the risks of the route. \nThe incident persuaded Google to re-route trips to the airport away from dangerous areas. \nSystem \ud83e\udd16\nGoogle Maps website\nGoogle Maps Wikipedia profile\nOperator: Jason Zoladz, Katharine Zoladz\nDeveloper: Alphabet/Google\nCountry: South Africa\nSector: Travel/hospitality\nPurpose: Direct drovers\nTechnology: Machine learning\nIssue: Accuracy/reliability\nTransparency: Governance\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nJason Zoladz; Katharine Zoladz v. Google LLC (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.mercurynews.com/2024/01/31/google-maps-sent-couple-into-notoriously-violent-neighborhood-in-south-africa-where-gunmen-attacked-them-lawsuit/\nhttps://nypost.com/2024/02/01/news/la-couple-claims-google-maps-sent-them-to-south-africa-hell-run-area-where-they-were-attacked/\nhttps://www.dailymail.co.uk/news/article-13031597/High-flying-SEC-director-wealthy-lawyer-husband-sue-Google-maps-directed-notorious-South-African-road-suffered-horrific-bloody-attack-armed-robbers.html\nhttps://www.law.com/international-edition/2024/02/02/google-accused-of-continuing-to-route-google-maps-users-in-capetown-south-africa-through-dangerous-routes-despite-warnings/\nhttps://www.capetownetc.com/news/american-couple-takes-google-to-court-after-brutal-hijacking-in-nyanga/\nRelated \ud83c\udf10\nUS man dies driving off collapsed bridge while following Google Maps\nWaze directs users into San Francisco wildfires\nPage info\nType: Incident\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/deepfake-cfo-scams-finance-worker-for-usd-25-million", "content": "Deepfake CFO scams Arup for USD 25 million\nOccurred: January 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nScammers tricked a Hong Kong-based employee of a multinational company into paying out HKD 200 million (USD 26 million) with a fake group video call created using deepfake technology.\nAccording to Hong Kong police, the worker received a strange message purportedly from his company\u2019s UK-based chief financial officer asking for a secret transaction to be carried out. \nAttending a subsequent video call, the employee was reassured by several colleagues whom he thought he recognised; however, it transpired that all the 'people' on the call were in fact deepfake recreations of colleagues that had been manipulated using public video footage.\nThe scam was discovered when the employee later checked with the company's head office. \n\u2795 In May 2025, British engineering company Arup confirmed it was the target of the scam.\nSystem \ud83e\udd16\nUnknown \nOperator: Bank employee\nDeveloper: \nCountry: Hong Kong\nSector: Banking/financial services\nPurpose: Defraud\nTechnology: Deepfake - audio, video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Fraud\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bloomberg.com/news/articles/2024-02-04/deepfake-video-call-scams-global-firm-out-of-26-million-scmp\nhttps://edition.cnn.com/2024/02/04/asia/deepfake-cfo-scam-hong-kong-intl-hnk/index.html\nhttps://www.scmp.com/news/hong-kong/law-and-crime/article/3250851/everyone-looked-real-multinational-firms-hong-kong-office-loses-hk200-million-after-scammers-stage\nhttps://www.straitstimes.com/asia/east-asia/hk-firm-scammed-of-34-million-after-employee-is-duped-by-video-call-with-deepfake-of-cfo\nhttps://www.business-standard.com/world-news/deepfake-video-scams-multinational-company-out-of-26-million-report-124020400691_1.html\nhttps://www.theverge.com/2024/2/4/24061192/a-company-lost-25-6-million-because-of-a-deepfaked-conference-call\nRelated \ud83c\udf10\nUSD 622,000 deepfake impersonation scam\nAI impersonation scams couple of USD 21,000\nPage info\nType: Incident\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/wacom-ai-generated-chinese-new-year-promotion-backfires", "content": "Wacom AI-generated New Year promotion 'devalues' own customers\nOccurred: January 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nJapanese tablet manufacturer Wacom was discovered covertly using AI-generated images in its new year marketing, prompting complaints from artists and customers.\nWacom's apparent use of AI to generate illustrations of Chinese dragons to welcome in the Chinese Year of the Dragon prompted revulsion and despair from artists. One artist, Megan Ruiz, pointed out that the quality of the images was sub-par, with one sporting a tail that failed to attach to its body, another with strange-looking teeth. \nWacom later deleted the artwork and claimed it was not its 'intent' to use AI-generated images and that it had purchased the images 'through a third-party vendor where it was indicated that they were not AI generated.' \nArtists took particular exception to Wacom's use of AI because the company's products, many of which are premium-priced, are primarily used by designers and other creatives, leading some to say they were devaluing the work of their own customers. Some customers said they would not buy its products again.\nSystem \ud83e\udd16\nWacom website\nWacom Wikipedia profile\nDocuments \ud83d\udcc3\nWacom (2024). A response to community questions concerning Wacom using AI-generated art in US marketing assets\nOperator: Wacom customers\nDeveloper: \nCountry: USA\nSector: Technology\nPurpose: Generate images\nTechnology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Employment; Ethics/values; Reputational damage\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theverge.com/2024/1/9/24031468/wacom-wizards-of-the-coast-mtg-artists-against-generative-ai\nhttps://www.theverge.com/2024/1/10/24032672/this-wacom-ai-debacle-has-certainly-taken-a-turn\nhttps://petapixel.com/2024/01/10/wacom-enraged-customers-by-using-ai-art-but-says-its-not-to-blame/\nhttps://www.themarysue.com/artists-blast-wacom-ai-dragon-image/\nhttps://www.diyphotography.net/did-wacom-use-ai-generated-images-in-their-latest-ad/\nhttps://dataconomy.com/2024/01/08/wacom-ai-art/\nhttps://boingboing.net/2024/01/10/artists-upset-after-wacom-uses-ai-art-to-market-artist-gear.html\nRelated \ud83c\udf10\nAI generates visuals for Wizards of the Coast marketing promotion\nNiantic uses AI artwork to promote Pokemon Go\nPage info\nType: Issue\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/instacart-generates-recipes-and-food-images-using-ai", "content": "Instacart generates recipes and food images using AI\nOccurred: January 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGrocery delivery and pick-up service Instacart used AI to generate 'revolting' images of food to accompany its AI-generated recipes, resulting in a backlash from its customers. \nInstacart subreddit users discovered that the company was apparently using AI-generated images to accompany entries for ingredients and recipes on its app, prompting complaints from customers and commentators that the images were 'absurd', 'disturbing', and 'horrifying'. In one instance, an image for 'Microwave Mug Chocolate Chip Cookie a la Mode' showed a small chocolate chip cookie hanging on the side of a coffee mug. \nInstacart also used AI to generate recipes, noting that they were 'powered by the magic of AI, so that means it may not be perfect.' The company was estimated to have published 8,000-10,000 such recipes, a number of which were deleted in the wake of a corruscating Business Insider article. It also replaced the accompanying AI images with stock photos.\nThe incident raised questions about Instacart's oversight and quality assurance of its AI programme, and more generally about the use of the technology in advertising and marketing. Some customers threatened not to use Instacart again. \nSystem \ud83e\udd16\nInstacart website\nInstacart Wikipedia profile\nOperator: Instacart customers\nDeveloper: \nCountry: USA\nSector: Transport/logistics\nPurpose: Generate images\nTechnology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Reputational damage\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.businessinsider.com/instacart-ai-art-recipes-photos-2024-1\nhttps://www.businessinsider.com/instacart-deletes-ai-generated-food-images-2024-1\nhttps://www.benzinga.com/news/24/01/36837432/is-ai-ready-for-the-kitchen-instacarts-ai-recipes-go-viral-for-all-the-wrong-reasons\nhttps://futurism.com/the-byte/instacart-caught-ai-generated-images-food\nhttps://www.supermarketnews.com/technology/5-things-when-instacart-food-photos-go-wrong\nhttps://curlytales.com/conjoined-hot-dog-to-weirdly-textured-salmon-unsettling-ai-generated-food-images-by-instacart-removed/\nhttps://www.reddit.com/r/instacart/comments/1ad275h/more_aigenerated_photos_instacart_is_using_for/\nRelated \ud83c\udf10\nAI meal planner app suggests chlorine gas recipe\nAmazon uses AI to generate 'Fallout' promo art\nPage info\nType: Issue\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/short-seller-bots-sow-first-republic-bank-doubts", "content": "Short-seller bots spread First Republic Bank misinformation\nOccurred: March 2023-April 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAutomated bots and fake social media accounts allegedly operated by capital markets short-sellers were used to spread misinformation and sow doubts about US-based First Republic Bank ahead of its collapse in mid-2023. \nUsing AI to examine online activity during the 2023 US banking crisis, Valent Projects discovered two major peaks of activity as tweets and Reddit posts targeted First Republic Bank, coinciding with a collapse in confidence that led customers to withdraw cash. These peaks did not come with a surge in engagement such as likes, retweets or replies, a pattern 'unlikely to occur naturally,' according to the researchers.\nThe campaign, which the researchers concluded was likely to have been orchestrated by short sellers betting against the bank's share price, is seen to have helped trigger the withdrawal of USD 100 billion in deposits and the collapse of the bank. First Republic was taken over by the Federal Deposit Insurance Corporation (FDIC) and sold to JP Morgan Chase for US 10.6 billion.\nSystem \ud83e\udd16\nUnknown\nOperator: First Republic Bank customers, investors\nDeveloper: \nCountry: USA\nSector: Banking/financial services\nPurpose: Sow misinformation\nTechnology: Bot/intelligent agent\nIssue: Mis/disinformation\nTransparency: Governance\nInvestigations, assessments, audits \ud83e\uddd0\nValent Projects (2024). Disinformation destroyed a healthy bank \u2013 and nobody noticed\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.dailymail.co.uk/news/article-13028857/bots-ai-republic-banking-crisis.html\nRelated \ud83c\udf10\nChatGPT powers 'Fox8' crypto promotion botnet\nAgricultural Bank of China facial recognition age bias\nPage info\nType: Incident\nPublished: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/nine-news-uses-ai-to-sexualise-image-of-politician", "content": "Nine News uses AI to 'sexualise' image of politician \nOccurred: January 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAustralian TV broadcaster Nine News has been accused of using AI to make a photograph of Australian politician Georgie Purcell appear more 'sexual', resulting in accusations of manipulation and sexism. \nAnimal Justice Party MP Georgie Purcell posted to X an edited image of herself originally shared by Nine News Melbourne reading 'having my body and outfit photoshopped by a media outlet was not on my bingo card. Note the enlarged boobs and outfit to be made more revealing. Can\u2019t imagine this happening to a male MP.' Purcell had been wearing a dress; the manipulated image showed her with larger breasts and sporting a midriff-exposing tank top.\nNine News blamed the 'graphic error' on automation: 'During that process, the automation by Photoshop created an image that was not consistent with the original,' he said. Adobe Photoshop\u2019s new generative AI tools allow users to fill or expand existing images using AI. However, a spokesperson for Photoshop maker Adobe told the BBC that 'human intervention and approval' would have been required for 'any changes to this image.'\nThe incident prompted concerns about the ethics and legality of the manipulation of images by media organisations, perceived ingrained sexism of Nine News and other broadcasters, and poor transparency.\nSystem \ud83e\udd16\nAdobe Firefly image generator\nOperator: Nine News\nDeveloper: Adobe\nCountry: Australia\nSector: Politics\nPurpose: Manipulate image\nTechnology: Machine learning\nIssue: Ethics/values; Sexualisation\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://twitter.com/georgievpurcell/status/1752088649527853107\nhttps://futurism.com/the-byte/tv-network-sexualized-ai-image-politician\nhttps://www.bbc.co.uk/news/world-australia-68137013\nhttps://www.abc.net.au/news/2024-02-01/georgie-purcell-ai-image-nine-news-apology-digital-ethics/103408440\nhttps://www.abc.net.au/news/2024-01-30/victorian-mp-georgie-purcell-altered-image/103403664\nhttps://theconversation.com/nine-was-slammed-for-ai-editing-a-victorian-mps-dress-how-can-news-media-use-ai-responsibly-222382\nhttps://www.cyberdaily.au/culture/10102-nine-slammed-for-ai-sexualised-image-of-female-mp\nhttps://www.dailymail.co.uk/tvshowbiz/article-13022119/Adobe-Georgie-Purcell-MP-Channel-Nine-boobs-Photoshopped.html\nhttps://www.dailymail.co.uk/tvshowbiz/article-13026435/Channel-Nines-sexist-AI-image-MP-fallout.html\nhttps://www.washingtonpost.com/world/2024/01/31/australia-lawmaker-edit-georgie-purcell/\nRelated \ud83c\udf10\nAdobe uses customer images without consent to train Firefly AI art generator\nAdobe Creative Cloud uses customer content to train AI systems\nPage info\nType: Incident\nPublished: January 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/study-finds-amazon-rekognition-suffers-from-racial-and-gender-bias", "content": "Study finds Amazon Rekognition suffers from racial and gender bias\nOccurred: January 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn MIT Media Lab study concluded that Amazon's Rekognition facial recognition system performed worse when identifying an individual\u2019s gender if they were female or darker-skinned.\n\nThe MIT researchers compared tools from five companies, including Microsoft and IBM, and found that Rekognition performed the worst when it came to recognising women with darker skin, with an error rate of 31.37 percent. It also mistook women for men 19 percent of the time.\nAmazon claimed the research was misleading as the researchers had not tested the most recent version of Rekognition, and that the gender identification test was facial analysis (which spots expressions and characteristics like facial hair) rather than facial identification (which matches scanned faces to mugshots).\nIn their paper, the researchers also argued that issues other than algorithmic fairness should be considered. 'The potential for weaponization and abuse of facial analysis technologies cannot be ignored nor the threats to privacy or breaches of civil liberties diminished even as accuracy disparities decrease,' they wrote.\nSystem \ud83e\udd16\nAmazon Rekognition website\nAmazon Rekognition Wikipedia profile\nOperator: MIT Media Lab, Joy Buolamwini, Deborah Raji\nDeveloper: Amazon/AWS\nCountry: USA\nSector: Education\nPurpose: Identify individuals\nTechnology: Facial recognition\nIssue: Bias/discrimination - racial, gender; Dual/multi-use\nTransparency: \nResearch, advocacy \ud83e\uddee\nConcerned researchers (2019). On Recent Research Auditing Commercial Facial Analysis Technology\nJoy Buolamwini (2019). Response: Racial and Gender bias in Amazon Rekognition \u2014 Commercial AI System for Analyzing Faces\nRaji, I & Buolamwini, J. (2019). Actionable Auditing: Investigating the Impact of Publicly Naming Biased Performance Results of Commercial AI Products\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.nytimes.com/2019/01/24/technology/amazon-facial-technology-study.html\nhttps://www.theverge.com/2019/1/25/18197137/amazon-rekognition-facial-recognition-bias-race-gender\nhttps://www.cnet.com/news/amazons-facial-tech-shows-gender-racial-bias-mit-study-says/\nhttps://www.bbc.co.uk/news/technology-47117299\nhttps://www.forbes.com/sites/zakdoffman/2019/01/28/amazon-hits-out-at-attackers-and-claims-were-not-racist/\nhttps://www.nytimes.com/2019/04/03/technology/amazon-facial-recognition-technology.html\nRelated \ud83c\udf10\nAmazon Rekognition falsely links athletes to mugshots\nAmazon Rekognition wrongly matches 28 Members of Congress\nPage info\nType: Issue\nPublished: January 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-rekognition-falsely-links-athletes-to-mugshots", "content": "Amazon Rekognition falsely links athletes to mugshots\nOccurred: October 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAmazon\u2019s Rekognition system falsely linked the faces of 27 professional athletes in New England, USA, to mugshots in a criminal database. \nThe Massachusetts ACLU used Amazon's Reknognition facial recognition system to filter 188 well-known local athletes through a database of 20,000 mugshots, and found the product misidentified 27 of them. The result was verified by an independent industry expert.\nAmazon retorted that the ACLU had been 'knowingly misusing and misrepresenting Amazon Rekognition to make headlines' and that Rekognition could help identify criminals and missing children when used with its recommended 99 percent confidence threshold.\nThe incident was seen to underscore issues with the accuracy and reliability of Rekognition, and to highlight its implications for civil rights and liberties, including racial bias and discrimination.\nIt also prompted rights activists and others to call for a moratorium on government use of facial recognition techniology, and for dedicated federal and local legislation.\nSystem \ud83e\udd16\nAmazon Rekognition website\nAmazon Rekognition Wikipedia profile\nOperator: American Civil Liberties Union (ACLU)  \nDeveloper: Amazon/AWS\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Strengthen law enforcement\nTechnology: Facial recognition\nIssue: Accuracy/reliability; Bias/discrimination - race; Human/civil rights\nTransparency: \nResearch, advocacy \ud83e\uddee\nACLU (2019). Facial recognition technology falsely identifies famous athletes\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.boston.com/news/politics/2019/10/21/amazon-facial-recognition-duron-harmon\nhttps://atlantatribune.com/2019/10/31/amazons-facial-recognition-wrongly-identifies-athletes-as-criminals/\nhttps://hyperallergic.com/525209/amazon-facial-recognition-aclu/\nhttps://www.nbcboston.com/news/local/amazons-facial-recognition-technology-misidentifies-new-england-athletes-aclu/1907254/\nhttps://www.businessinsider.com/amazon-facial-recognition-falsely-matched-nfl-players-duron-harmon-mugshots-2019-10\nhttps://www.businesstelegraph.co.uk/amazon-facial-recognition-tech-falsely-matched-nfl-players-to-mugshots-business-insider/\nRelated \ud83c\udf10\nAmazon Rekognition wrongly matches 28 Members of Congress\nAmazon employees, investors protest US govt Rekognition sales\nPage info\nType: Incident\nPublished: January 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-employees-protest-us-govt-rekognition-sales", "content": "Amazon employees, investors protest US govt Rekognition sales\nOccurred: May 2018-October 2018\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nEfforts by Amazon to sell its Rekognition facial recognition system to US law enforcement agencies provoked a strong backlash from Amazon employees, investors, and rights advocates. \nIn May 2018, the American Civil Liberties Union (ACLU) had found that Amazon had been actively marketing Rekognition to police departments and government agencies across the US, pointing out civil liberties concerns and the ease with which the technology can be misused. \nA month later, the ACLU delivered a petition with over 150,000 signatures to Amazon headquarters, alongside a coalition letter signed by nearly 70 digital rights organisations. \nThe same day, 19 investor groups warned then Amazon CEO Jeff Bezos that the technology 'may not only pose a privacy threat to customers and other stakeholders across the country, but may also raise substantial risks for our Company, negatively impacting our company\u2019s stock valuation and increasing financial risk for shareholders.'\nIt also emerged that Amazon employees had written to Jeff Bezos asking him to stop selling Rekognition to law enforcement and to stop supporting controversial data mining company Palantir with its cloud services.\nIn October 2018, documents obtained by the Project on Government Oversight showed that Amazon representatives had met with Homeland Security and Immigration and Customs Enforcement officials, triggering further complaints\nIn June 2020, Amazon announced it would implement a one-year moratorium on police use of Rekognition in the US.\nSystem \ud83e\udd16\nAmazon Rekognition website\nAmazon Rekognition Wikipedia profile\nIncident databank \ud83d\udd22\nOperator: Washington County Sheriff's Office  \nDeveloper: Amazon\nCountry: USA\nSector: Govt - immigration\nPurpose: Control immigration\nTechnology: Facial recognition\nIssue: Bias/discrimination - race; Human/civil rights; Oversight; Privacy; Surveillance\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nACLU (2018). Amazon Met With ICE Officials to Market Its Facial Recognition Product\nACLU (2018). Letter from Nationwide Coalition to Amazon CEO Jeff Bezos regarding Rekognition\nACLU (2018). Over 150,000 People Tell Amazon: Stop Selling Facial Recognition Tech to Police\nACLU (2018). Amazon Teams Up With Government to Deploy Dangerous New Facial Recognition Technology\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.npr.org/2018/07/14/628765208/tech-workers-demand-ceos-stop-doing-business-with-ice-other-u-s-agencies\nhttps://medium.com/@amazon_employee/im-an-amazon-employee-my-company-shouldn-t-sell-facial-recognition-tech-to-police-36b5fde934ac\nhttps://www.washingtonpost.com/news/the-switch/wp/2018/06/22/amazon-employees-demand-company-cut-ties-with-ice/\nhttps://www.seattletimes.com/business/amazon-employees-demand-company-cut-ties-with-ice/\nhttps://gizmodo.com/amazon-workers-demand-jeff-bezos-cancel-face-recognitio-1827037509\nhttps://www.cnet.com/news/politics/amazon-employees-want-jeff-bezos-to-stop-selling-facial-recognition-software-to-law-enforcement/\nhttps://www.eater.com/2019/8/12/20802568/whole-foods-workers-amazon-letter-ice-palantir\nhttps://www.theguardian.com/us-news/2019/jul/11/amazon-ice-protest-immigrant-tech\nhttps://www.wsj.com/articles/protesters-disrupt-amazon-event-over-its-ties-with-ice-11562882825\nRelated \ud83c\udf10\nAmazon Rekognition wrongly matches 28 Members of Congress\nUCLA abandons facial recognition surveillance plans\nPage info\nType: Incident\nPublished: January 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/cadillac-fairview-covertly-uses-facial-recognition-to-monitor-shoppers", "content": "Cadillac Fairview covertly uses facial recognition to monitor shoppers\nOccurred: June 2018-October 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nCanadian commercial real estate company Cadillac Fairview was found to have been secretly using facial recognition to capture customer data without their knowledge or consent.\nCadillac Fairview was discovered to have been using facial recognition at two of its shopping malls by a customer, who exposed the practice on Reddit. The findings were taken up by federal and provincial privacy regulators, leading the company, which had said it was using facial recognition to track people's genders and ages whilst not capturing their images, to suspend its use of its system while it was investigated.\nThe privacy commissioners ruled in October 2020 that Cadillac Fairview had installed facial recognition in a dozen malls and analysed visitor images without consent, and that its software supplier Anonymous Video Analytics had kept 5,061,324 million facial representations on a decommissioned server on the company\u2019s behalf 'for no apparent purpose and with no justification.'\nSystem \ud83e\udd16\nCadillac Fairview website\nCadillac Fairview Wikipedia profile\nIncident databank \ud83d\udd22\nOperator: Cadillac Fairview\nDeveloper: Anonymous Video Analytics\nCountry: Canada\nSector: Retail\nPurpose: Analyse shopper behaviour\nTechnology: Facial recognition\nIssue: Privacy\nTransparency: Governance; Marketing\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nOffice of the Privacy Commissioner of Canada (2020). Privacy Commissioner launches investigation into Cadillac Fairview over use of facial recognition technology in malls\nInvestigations, assessments, audits \ud83e\uddd0\nOffice of the Privacy Commissioner of Canada (2020). Joint investigation of the Cadillac Fairview Corporation Limited by the Privacy Commissioner of Canada, the Information and Privacy Commissioner of Alberta, and the Information and Privacy Commissioner for British Columbia\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.huffpost.com/archive/ca/entry/calgary-fairview-facial-recognition-software_ca_5cd55ebee4b07bc729777b0c\nhttps://www.cbc.ca/news/canada/calgary/facial-recognition-malls-1.4773334\nhttps://globalnews.ca/news/7429905/cadillac-fairview-facial-recognition-investgation-findings/\nhttps://nowtoronto.com/news/toronto-eaton-centre-cadillac-fairview-facial-recognition-broke-privacy-laws\nhttps://www.theglobeandmail.com/business/article-cadillac-fairview-collected-images-of-millions-of-shoppers-faces/\nhttps://www.cbc.ca/news/canada/calgary/cadillac-fairview-mall-directory-facial-recognition-suspended-1.4774692\nhttps://www.huffingtonpost.ca/2018/08/06/calgary-fairview-facial-recognition-software_a_23497017/\nhttps://www.vancouverisawesome.com/vancouver-news/creepin-kiosks-vancouver-richmond-malls-used-customer-images-without-consent-2835336\nhttps://www.vendingtimes.com/news/canada-mall-operator-cited-for-consumer-privacy-violations-removes-facial-recognition-cameras/\nhttps://www.biometricupdate.com/201808/mall-operator-suspends-use-of-facial-recognition-as-canadian-privacy-commissioners-investigate-legality\nhttps://www.thestar.com/news/gta/2020/10/29/cadillac-fairview-broke-privacy-laws-by-using-facial-recognition-technology-at-malls-investigators-conclude.html\nRelated \ud83c\udf10\nCanadian Tire covertly uses facial recognition to collect customer data\nRCMP British Colombia facial recognition procurement opacity\nPage info\nType: Incident\nPublished: January 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-rekognition-falsely-matches-28-members-of-congress", "content": "Amazon Rekognition wrongly matches 28 Members of Congress \nOccurred: July 2018\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAmazon's Rekognition facial recognition system incorrectly identified 28 members of Congress as other people who had been arrested for a criminal offences.\nThe American Civil Liberties Union (ACLU) released the results of a test showing that Rekognition had falsely matched 28 members of US Congress with mugshot photos of criminals, especially people of colour. Congressional members from both major political parties later expressed concern about Rekognition in a series (pdf) of letters to Amazon CEO Jeff Bezos.\nAmazon responded by saying the Rekognition test had generated 80 percent confidence, and that it recommended law enforcement only use matches rated at 95 percent confidence or higher. According to the ACLU, Amazon moved the goalpost by increasing the recommended confidence interval after the ACLU study was published.\nThe incident raised questions about the accuracy and reliability of Amazon's Rekognition system and highlighted its potential impact on minority community civil liberties when used by law enforcement authorities.\nSystem \ud83e\udd16\nAmazon Rekognition website\nAmazon Rekognition Wikipedia profile\nIssue databank \ud83d\udd22\nOperator: American Civil Liberties Union (ACLU)\nDeveloper: Amazon/AWS\nCountry: USA\nSector: Politics\nPurpose: Identify public figures\nTechnology: Facial recognition\nIssue: Accuracy/reliability; Bias/discrimination - race; Human/civil rights; Privacy\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nACLU (2018). Amazon\u2019s Face Recognition Falsely Matched 28 Members of Congress With Mugshots\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.newsweek.com/amazons-face-recognition-tool-matches-28-members-congress-criminal-mugshots-1044850\nhttps://www.theguardian.com/technology/2018/jul/26/amazon-facial-rekognition-congress-mugshots-aclu\nhttps://www.buzzfeednews.com/article/daveyalba/amazon-rekognition-facial-recognition-congress-false\nhttps://www.buzzfeednews.com/article/daveyalba/congressmen-mismatched-to-mugshots-by-amazon-tech-demand\nhttps://futurism.com/the-byte/amazon-rekognition-falsely-matched-congresspeople\nhttps://www.consumeraffairs.com/news/amazons-facial-recognition-falsely-matched-members-of-congress-with-mugshots-aclu-says-072718.html\nhttps://www.commondreams.org/views/2018/07/26/amazons-face-recognition-falsely-matched-28-members-congress-mugshots\nhttps://phys.org/news/2018-07-amazon-facial-recognition-tool-misidentified.html\nRelated \ud83c\udf10\nAmazon Go fails to inform NYC customers about facial recognition\nAmazon employees protest US govt Rekognition sales\nPage info\nType: Issue\nPublished: January 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-ai-recruitment-tool-favours-men-over-women", "content": "Amazon AI recruitment tool favours men over women\nOccurred: October 2018\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA secret Amazon recruitment tool was scraped that was meant to automate the recruitment process for senior hires favoured men over women for technical jobs.\nBuilt in 2014, the system used AI to give job candidates scores ranging from one to five stars, company insiders told Reuters. But it quickly became clear that the system did not favour women as most applications came from men over a 10-year period, and that it favoured candidates describing themselves using verbs more commonly found on male engineers\u2019 resumes such as 'executed' and 'captured'.\nIn addition, problems with the data that underpinned the models\u2019 judgments meant that unqualified candidates were often recommended for a variety of jobs. Amazon attempted to mitigate the bias, but scraped the system in 2017 after it concluded the system was unsalvegeable.\nThe incident was seen to demonstrate the limitations of machine learning in an industry long dominated by males. The use of system without informing job applicants was also reckoned to reflect poorly on Amazon.\nSystem \ud83e\udd16\nAmazon Inc website\nOperator: Amazon\nDeveloper: Amazon\nCountry: USA\nSector: Business/professional services\nPurpose: Process job applications\nTechnology: Machine learning\nIssue: Bias/discrimination\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G\nhttps://boingboing.net/2018/10/11/garbage-conclusions-out.html\nhttps://www.theguardian.com/technology/2018/oct/10/amazon-hiring-ai-gender-bias-recruiting-engine\nhttps://mashable.com/article/amazon-sexist-recruiting-algorithm-gender-bias-ai/\nhttps://www.cio.com/article/3314737/amazons-biased-ai-recruiting-tool-gets-scrapped.html\nhttps://www.newstatesman.com/spotlight/2021/09/ai-robots-recruitment-hiring\nRelated \ud83c\udf10\niTutorGroup recruitment algorithmic age discrimination\nWorkday accused of building discriminatory AI job screening system\nPage info\nType: Incident\nPublished: January 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-patents-voice-sniffing-personality-profiling-algorithm", "content": "Amazon patents 'voice-sniffing' personality profiling algorithm \nOccurred: March 2018\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAmazon plans to build personality profiles on users of its Alexa voice assistant, drawing sharp criticism from digital rights and privacy advocates.\nAccording to a patent application filed by Amazon, the company said it could use advanced artificial intelligence 'voice sniffer algorithm' to allow Alexa to listen to a conversation and analyse it for certain words that are said. Trigger words such as 'like', 'love' and 'hate' would build profiles on users to better target them with advertising and product recommendations.\n'The identified keywords can be stored and/or transmitted to an appropriate location accessible to entities such as advertisers or content providers who can use the keywords to attempt to select or customise content that is likely relevant to the user,' the patent said. \n'We file patent applications on a variety of ideas that our employees come up with. Some of those ideas later mature into real products or services, some don't. Prospective product announcements should not necessarily be inferred from our patent applications,' Amazon said.\nSystem \ud83e\udd16\nAmazon Key word determinations from voice data patent application\nOperator:\nDeveloper: Amazon\nCountry: USA\nSector: Business/professional services\nPurpose: Profile customer personality\nTechnology: Voice sniffer algorithm\nIssue: Privacy; Surveillance\nTransparency: \nResearch, advocacy \ud83e\uddee\nConsumer Watchdog (2017). Google, Amazon Patent Filings Reveal Digital Home Assistant Privacy Problems (pdf) \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/technology-43725708\nhttps://www.nytimes.com/2018/03/31/business/media/amazon-google-privacy-digital-assistants.html\nhttps://www.news.com.au/technology/gadgets/amazon-proposes-voice-sniffer-algorithm-to-eavesdrop-on-the-things-you-like/news-story/3ca3db0c6576256f943a24ee2d2d86f2\nhttps://www.independent.co.uk/life-style/gadgets-and-tech/news/amazon-alexa-patent-listening-me-facebook-phone-talking-ads-a8300246.html\nhttps://abcnews.go.com/Business/amazon-patent-reveals-voice-sniffer-algorithm-analyze-conversations/story\nhttps://www.uctoday.com/unified-communications/something-smells-off-amazon-patents-algorithms-for-voice-sniffing/\nhttps://www.dailydot.com/debug/amazon-voice-sniffer-privacy\nRelated \ud83c\udf10\nAlfi personalised, real-time advertising\nSpotify plan to use emotion recognition deemed 'manipulative'\nPage info\nType: Issue\nPublished: January 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/workers-assist-cruise-autonomous-robotaxis-every-2-5-5-miles", "content": "Workers assist Cruise 'autonomous' robotaxis every 2.5-5 miles\nOccurred: November 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nRemote operators have to intervene every 2.5 to 5 miles driven by a Cruise robotaxi, calling into question whether they should be called 'autonomous'.\nAccording to the New York Times, Cruise employs one and a half workers located in remote operations centres to support each vehicle, prompting AI expert Gary Marcus to question whether the revelation may prove Cruise to be the 'Theranos of AI'. \n'If Cruise\u2019s vehicles really need an intervention every few miles, and 1.5 external operators for every vehicle, they don\u2019t seem to even be remotely close to what they have been alleging to the public,' Marcus wrote. 'Shareholders will certainly sue, and if it\u2019s bad as it looks, I doubt that GM will continue the project.'\nIn a post on Hacker News, then Cruise CEO Kyle Vogt responded by saying that Cruise robotaxis were remotely assisted '2-4 percent of the time on average, in complex urban environments', and that 'of those, many are resolved by the AV itself before the human even looks at things'.\nSystem \ud83e\udd16\nCruise website\nCruise Wikipedia profile\nOperator: General Motors/Cruise LLC\nDeveloper: General Motors/Cruise LLC\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Self-driving system; Computer vision; Machine learning\nIssue: Governance\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.nytimes.com/2023/11/03/technology/cruise-general-motors-self-driving-cars.html\nhttps://news.ycombinator.com/item?id=38145062\nhttps://www.teslarati.com/cruise-ceo-robotaxi-fleet-human-remote-assistance-explained/\nhttps://www.sfgate.com/tech/article/cruise-driverless-human-assistance-nyt-18467527.php\nhttps://www.forbes.com/sites/bradtempleton/2023/11/07/cruise-reports-lots-of-human-oversight-of-robotaxis-is-that-bad/\nhttps://www.theverge.com/23948708/cruise-robotaxi-suspension-trust-remote-assist\nhttps://www.thestreet.com/electric-vehicles/gm-cruise-gary-marcus\nRelated \ud83c\udf10\nCruise AV drags pedestrian across street\nCruise robotaxi hits fire engine\nPage info\nType: Incident\nPublished: January 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/automated-hr-system-mysteriously-fires-software-engineer", "content": "Automated HR system mysteriously fires software engineer\nOccurred: June 2018\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA US-based software engineer was wrongly fired and locked out of his office by an automated, irreversible, algorithmic HR termination process.\nIn a lengthy blog post, Ibrahim Diallo described how he was progessively locked out of several computer systems operated by his employer, resulting in him being sacked and having to spend three weeks at home while his employer tried to work out what had happened.\nIt transpired that his manager, who had just been laid off during the acquisition of the company, had not transferred his name into a new HR system, triggering a series of automated events that led to him being escorted out of the office by security guards.\nDiallo was not paid for the the three weeks he was forced to spend at home, and later resigned his job.\nSystem \ud83e\udd16\nUnknown\nOperator:  \nDeveloper: \nCountry: USA\nSector: Business/professional services\nPurpose: Automate HR processes\nTechnology: Human Resources Management System\nIssue: Governance; Employment\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://idiallo.com/blog/when-a-machine-fired-me\nhttps://www.indiatoday.in/education-today/news/story/this-employee-was-fired-by-a-machine-and-there-s-nothing-his-manager-could-do-about-it-1279085-2018-07-06\nhttps://www.theregister.com/2018/06/22/software_engineer_fired_by_machine/\nhttps://boingboing.net/2018/06/20/the-computer-says-youre-dead.html\nhttps://www.abc.net.au/news/2018-08-14/ibrahim-diallo-man-who-was-fired-by-a-machine-law-ai/10083194\nhttps://www.bbc.co.uk/news/technology-44561838\nhttps://www.news.com.au/finance/work/at-work/no-human-could-do-anything-the-man-who-was-sacked-by-a-machine-out-for-blood/news-story/1852bfd331a671a5ac153721bf91eb1f\nhttps://business-reporter.co.uk/2021/02/11/this-man-was-fired-by-a-computer-real-ai-could-have-saved-him/\nhttps://theconversation.com/this-man-was-fired-by-a-computer-real-ai-could-have-saved-him-99059\nhttps://www.independent.co.uk/voices/man-fired-computer-machine-ai-artificial-intelligence-security-systems-work-employment-future-a8428631.html\nRelated \ud83c\udf10\niTutorGroup recruitment algorithmic age discrimination\nXsolla employee monitoring, terminations\nPage info\nType: Incident\nPublished: January 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/dudesy-sued-for-ai-generated-george-carlin-copyright-abuse", "content": "Dudesy sued for AI-generated George Carlin copyright abuse\nOccurred: January 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe media company behind an purportedly AI-generated comedy special that attempted to recreate the US comedian George Carlin is being sued by Carlin's estate.\nGeorge Carlin: I\u2019m Glad I\u2019m Dead shows Carlin, who died in 2008, commentating on current events. The beginning of the now offline hour-long video featured a voiceover identifying itself as the AI engine used by media firm Dudesy says it listened to the comic\u2019s 50 years of material and 'did my best to imitate his voice, cadence and attitude as well as the subject matter I think would have interested him today.'\nThe defendants named in the suit are Dudesy LLC and podcast hosts Will Sasso and Chad Kultgen. 'None of the Defendants had permission to use Carlin\u2019s likeness for the AI-generated \u2018George Carlin Special,\u2019 nor did they have a license to use any of the late comedian\u2019s copyrighted materials,' the suit reads. \nWill Sasso later told the New York Times that \u2018I\u2019m Glad I\u2019m Dead\u2019 was 'completely' written by Chad Kultgen. In 2023, former NFL star Tom Brady threatened to file a similar lawsuit against Dudesy for creating a self-labeled AI-generated comedy special using his likeness.\nIn March 2024, Carlin's estate settled with the two podcast hosts. Sasso and Kultgen agreed to permanently remove the comedy special and to never repost it on any platform. According to court records, they also agreed not to use Mr. Carlin\u2019s image, voice or likeness on any platform without approval from the estate. \nSystem \ud83e\udd16\nIncident video\nOperator: Kelly Carlin\nDeveloper: Chad Kultgen; Will Sasso\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Imitate George Carlin\nTechnology: Machine learning\nIssue: Copyright\nTransparency: \nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nMain Sequence, Ltd. et al v. Dudesy, LLC et al\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://screenrant.com/george-carlin-glad-im-dead-ai-generated-fake-controversy/\nhttps://www.theregister.com/2024/01/12/george_carlin_comedian_cloned/\nhttps://www.vice.com/en/article/5d94xx/the-george-carlin-ai-standup-is-worse-than-you-can-imagine\nhttps://www.theguardian.com/technology/2024/jan/11/george-carlin-ai-comed\nhttps://www.latimes.com/business/technology/story/2024-01-18/column-the-george-carlin-auto-generated-comedy-special-is-everything-thats-wrong-with-ai-right-now\nhttps://www.rollingstone.com/culture/culture-news/george-carlin-estate-ai-comedy-special-lawsuit-1234954818/\nhttps://www.hollywoodreporter.com/business/business-news/ai-generated-george-carlin-special-ignites-copyright-infringement-lawsuit-1235807439/\nhttps://www.hollywoodreporter.com/business/business-news/george-carlins-estate-settles-lawsuit-podcasters-over-ai-episode-1235865033/\nhttps://www.nytimes.com/2024/04/02/arts/george-carlins-settlement-ai-podcast.html\nRelated \ud83c\udf10\nMidjourney reproduces copyright-protected film images\n17 authors sue OpenAI for 'systematic mass-scale copyright infringement'\nPage info\nType: Incident\nPublished: January 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/xtwitter-fails-to-remove-graphic-ai-images-of-taylor-swift", "content": "X/Twitter fails to remove graphic AI images of Taylor Swift\nOccurred: January 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nSexually explicit AI-generated images of Taylor Swift published on Twitter and which went viral remained on the platform for up to 17 hours before they were removed.\nThe images, which showed Swift in a series of sexual acts while dressed in Kansas City Chief memorabilia, were uploaded to deepfake porn website Celeb Jihad and quickly went viral on X/Twitter, Facebook, Instagram, Reddit, and other platforms. The images appeared also to have been shared on a Telegram group dedicated to abusive images of women, and created using Microsoft Designer, according to 404 Media.\nX/Twitter eventually removed offending images, shut down the account that first shared them, and suspended accounts that had re-shared them. However, other images quickly emerged in their place. Later, it blocked searches for Swift's name. \nThe incident led Swift to say she was considering legal action against Celeb Jihad. It also raised questions about X's business model and the effectiveness of it's content moderation system, which is mostly automated after Elon Musk had fired much of its safety team earlier in 2023. \nIt was also seen to demonstrate the ease with which synthetic images can be made and distributed, and renewed calls for effective legislation in the US. \nSystem \ud83e\udd16\nX/Twitter website\n\nDocuments \ud83d\udcc3\nX/Twitter Non-consensual nudity policy\nX/Twitter Synthetic and manipulated media policy\nOperator:  \nDeveloper: X/Twitter\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Moderate content\nTechnology: Content moderation system; Machine learning\nIssue: Business model; Privacy; Robustness; Safety\nTransparency: \nNews, commentary, analysis\ud83d\uddde\ufe0f\nhttps://www.404media.co/ai-generated-taylor-swift-porn-twitter/\nhttps://www.theverge.com/2024/1/25/24050334/x-twitter-taylor-swift-ai-fake-images-trending\nhttps://www.nytimes.com/2024/01/26/arts/music/taylor-swift-ai-fake-images.html\nhttps://edition.cnn.com/2024/01/25/tech/taylor-swift-ai-generated-images/index.html\nhttps://www.dailymail.co.uk/news/article-13006645/taylor-swift-furious-ai-pictures-porn-legal-action.html\nhttps://www.wired.com/story/taylor-swift-deepfake-porn-artificial-intelligence-pushback/\nRelated \ud83c\udf10\nDeepfake Taylor Swift offers free Le Creuset cookware scam\nEngland footballers' social media racist abuse\nPage info\nType: Incident\nPublished: January 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/parivar-pehchan-patra-declares-living-people-dead", "content": "Parivar Pehchan Patra algorithm declares living people dead\nOccurred: 2020-\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nSeveral thousand welfare beneficiaries in the Indian state of Haryana were denied access to their pensions and other welfare benefits having been wrongfully declared dead by an AI-powered algorithm.\nParivar Pehchan Patra (PPP) is an algorithmic system that provides Haryana families with an eight-digit unique ID based on income, age, employment, and other data. It is intended to streamline the delivery of welfare services and help reduce fraud by linking different databases together to produce a \u2018single source of truth\u2019. Birth, Death and Marriage records are linked to ensure automatic updating of family data.\nAfter 102-year-old Dhuli Chand was forced to put together a mock wedding procession to prove to Haryana officials that he was alive, government data was released revealing that over 300,000 pensions were stopped in the following three years since claimants had been classified 'dead'. 70 percent (44,050) of a smaller sample of 63,353 pensions that were halted were later found to have been flagged incorrectly.\nBeneficiaries of subsidised food and other schemes were also excluded because the algorithm made wrong predictions about their incomes or employment, according to Al-Jazeera.\nSystem \ud83e\udd16\nParivar Pehchan Patra website\nOperator: Haryana citizens\nDeveloper: Government of Haryana\nCountry: India\nSector: Govt - welfare\nPurpose: Assess welfare eligibility  \nTechnology: Machine learning\nIssue: Accuracy/reliability; Accountability; Privacy\nTransparency: Governance; Black box; Complaints/appeals\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.aljazeera.com/economy/2024/1/25/in-india-an-algorithm-declares-them-dead-they-have-to-prove-theyre\nhttps://interestingengineering.com/culture/algorithms-deny-food-access-declare-thousands-dead\nhttps://asianews.network/thousands-in-india-go-to-great-lengths-to-prove-they-are-alive-after-being-killed-by-an-algorithm/\nhttps://www.reporters-collective.in/newsletters/ai-part-2\nhttps://timesofindia.indiatimes.com/city/chandigarh/benefits-of-1-04l-withheld-reveals-haryana-govt-drive/articleshow/94750423.cms\nRelated \ud83c\udf10\nSamagra Vedika system pilot deprives Telengana citizens of rations\nAadhaar glitch results in villagers' starvation\nPage info\nType: Incident\nPublished: January 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/jordan-takaful-poverty-targeting-algorithm-unfairly-excludes-poor-people", "content": "'Takaful 'poverty targeting algorithm unfairly excludes some poor Jordanians\nOccurred: June 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe kingdom of Jordan was accused of using a 'flawed' algorithm to calculate the amount of aid for its citizens, excluding some  who are impoverished, hungry or otherwise struggling.\nThe Takaful 'poverty targeting' cash transfer algorithm run by Jordan's National Aid Fund and funded by the World Bank assesses whether aid applicants meet basic criteria such as whether families are headed by a Jordanian citizen and living under the poverty line. \nThe algorithm estimates and ranks families' income and wealth using 57 socio-economic indicators. Families that own cars less than five years old or businesses worth at least 3,000 dinars (approximately USD 4,200) are automatically disqualified.\nBut, according to Human Rights Watch, the system is undermined by an opaque system based on inaccurate and unreliable data about people's finances, stereotypes about poverty, and discriminatory policies - notably against women - thereby depriving people of their rights to social security and resulting in increased social tensions and inequality.\nThe World Bank responded by saying it would refine the algorithm, whilst noting that Takaful has proven to be one of the most cost-effective poverty reduction programmes currently operating in Jordan.\nSystem \ud83e\udd16\nJordan National Aid Fund website\nOperator: Jordan National Aid Fund\nDeveloper: The World Bank\nCountry: Jordan\nSector: Govt - welfare\nPurpose: Calculate aid eligibility and distribution\nTechnology: Ranking algorithm\nIssue: Accuracy/reliability; Bias/discrimination - gender\nTransparency: Governance; Black box; Complaints/appeals\nResearch, advocacy \ud83e\uddee\nHuman Rights Watch (2023). World Bank / Jordan: Poverty Targeting Algorithms Harm Rights\nHuman Rights Watch (2023). Automated Neglect. How The World Bank\u2019s Push to Allocate Cash Assistance Using Algorithms Threatens Rights\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.technologyreview.com/2023/06/13/1074551/an-algorithm-intended-to-reduce-poverty-in-jordan-disqualifies-people-in-need/\nhttps://dig.watch/updates/poverty-targeting-algorithms-expose-flawed-world-bank-cash-transfer-program-in-jordan\nhttps://theintercept.com/2023/06/13/jordan-world-bank-poverty-algorithm/\nhttps://www.newarab.com/news/hrw-jordan-poverty-tackling-algorithm-plagued-errors\nhttps://www.middleeasteye.net/news/jordan-poverty-financial-support-faulty-algorithm\nhttps://apnews.com/article/jordan-world-bank-poverty-9789664208aa433190b230534c5c376e\nhttps://www.context.news/ai/in-middle-east-poor-miss-out-as-faulty-algorithms-target-aid\nRelated \ud83c\udf10\nSamagra Vedika system pilot deprives citizens of rations\nAadhaar glitch results in villagers' starvation\nPage info\nType: Incident\nPublished: January 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/samagra-vedika-system-pilot-deprives-citizens-of-rations", "content": "Samagra Vedika system pilot deprives Telegana citizens of rations \nOccurred: August 2016-\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA pilot project in Hyderabad to assess the eligibility of welfare beneficiaries led to the removal of thousands of ration card holders by Telangana state's Samagra Vedika system.\nAccording to a Telengana government document (pdf), 100,000 ration card holders were removed from the system for apparently being \u2018ghost beneficiaries\u2019 or fraudulent applicants. Once excluded, beneficiaries had to prove to government agencies that they were entitled to the subsidised food. But government officials allegedly often ignored them, or tended to back the decision of the algorithm.\nThe action resulted in the denial of food rations to people rightfully entitled to them, was seen to worsen social inequality, and led to a public outcry. Under significant public pressure, the government reinstated 14,000 cancelled ration cards through an 'appeals and verification' process. It refused to say how the system had gone wrong, though poor quality data and inadequate oversight have been considered likely causes. \nA subsequent government reanalysis of over 200,000 cards revealed that 15,000 had been incorrectly removed, according to Al-Jazeera.\nThe incident prompted concerns about the accuracy and fairness of the Samagra Vedika system, and its governance and accountability. It also raised questions about the ethics of using big data and machine learning for sensitive government decision-making.\nSystem \ud83e\udd16\nGovernment of Telangana (2019). SAMAGRA VEDIKA --TELANGANA\u2019S INTEGRATED PLATFORM (pdf)\nOperator: Telagana citizens\nDeveloper: Government of Telagana; Posidex Technologies\nCountry: India\nSector: Govt - welfare\nPurpose: Determine welfare eligibility\nTechnology: Machine learning\nIssue: Accuracy/reliability; Accountability; Human/civil rights\nTransparency: Governance; Marketing\nResearch, advocacy \ud83e\uddee\nObserver Research Foundation (2023). Too big to fail: Shortcomings of large-scale AI\nDigital Empowerment Foundation (2022). Advancing Data Justice (pdf)\nAI Observatory. The Legal, Institutional and Technical Architecture of ADMS in India (pdf)\nDivij Joshi/Forbes (2021). The Indian Algorithmic Services: When AI gets to decide who gets welfare\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.aljazeera.com/economy/2024/1/24/how-an-algorithm-denied-food-to-thousands-of-poor-in-indias-telangana\nhttps://government.economictimes.indiatimes.com/news/digital-india/is-ai-for-all-or-does-it-lead-to-privacy-concerns-and-social-disempowerment/101265967\nhttps://interestingengineering.com/culture/algorithms-deny-food-access-declare-thousands-dead\nRelated \ud83c\udf10\nParivar Pehchan Patra declares living people dead\nAadhaar glitches result in villagers' starvation\nPage info\nType: Incident\nPublished: January 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/harvey-murphy-jr-facial-recognition-wrongful-arrest", "content": "Harvey Murphy Jr facial recognition wrongful arrest\nOccurred: January 2022-\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Texas man was mistakenly arrested for armed robbery using facial recognition, leading to his imprisonment and rape, and resulting in him suing Macy's and the owner of Sunglass Hut.\nHarvey Murphy Jr was arrested in October 2023 for the January 2022 robbery of a Sunglass Hut in Houston, though his attorneys said he was in Sacramento, California, at the time of the robbery. During his two weeks time in detention, he was allegedly attacked and raped by three men, leaving permanent injuries. \nAccording to Murphy\u2019s lawsuit, an employee of EssilorLuxottica, Sunglass Hut\u2019s parent company, worked with its retail partner Macy\u2019s and used facial recognition software to identify Murphy as the robber, leading to his arrest. \nMurphy's alibi was eventually believed and the charges against him dropped. \nSystem \ud83e\udd16\nMacy's website\nEssilorLuxottica website\nOperator: EssilorLuxottica, Macy's\nDeveloper: \nCountry: USA\nSector: Govt - police\nPurpose: Identify individuals\nTechnology: Facial recognition\nIssue: Accuracy/reliability\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.washingtonpost.com/technology/2024/01/22/facial-recognition-wrongful-identification-assault\nhttps://www.theguardian.com/technology/2024/jan/22/sunglass-hut-facial-recognition-wrongful-arrest-lawsuit\nhttps://www.thestreet.com/retail/macys-faces-lawsuit-in-false-arrest-due-to-facial-recognition-tec\nhttps://edition.cnn.com/2024/01/23/tech/texas-man-sues-macys-sunglass-hut-facial-recognition/index.html\nhttps://www.cbsnews.com/sacramento/news/texas-macys-sunglass-hut-facial-recognition-software-wrongful-arrest-sacramento-alibi/\nhttps://www.fox26houston.com/news/houston-crime-man-suing-macys-sunglass-hut-for-10-million-following-false-arrest\nhttps://www.theregister.com/2024/01/24/macys_sunglass_hut_facial_recognition/\nRelated \ud83c\udf10\nMichael Oliver facial recognition wrongful arrest\nPorcha Rudruff facial recognition wrongful arrest\nPage info\nType: Incident\nPublished: January 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/designers-sue-shein-for-using-ai-to-recreate-their-work", "content": "Designers sue Shein for recreating their work using AI \nOccurred: July 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThree designers sued fast fashion company Shein in the US for allegedly stealing their work using artificial intelligence. \nKrista Perry, Larissa Martinez and Jay Baron accused (pdf) Shein of contravening the US' 1970 Racketeer Influenced and Corrupt Organizations Act (RICO) by stealing their own and other independent artists\u2019 works 'over and over again, as part of a long and continuous pattern of racketeering.'\nAccording to the claimants, Shein's 'design \u2018algorithm\u2019 could not work without generating the kinds of exact copies that can greatly damage an independent designer\u2019s career - especially because Shein\u2019s artificial intelligence is smart enough to misappropriate the pieces with the greatest commercial potential.'\nSystem \ud83e\udd16\nShein website\nShein Wikipedia profile\nOperator: Krista Perry, Larissa Martinez, Jay Baron\nDeveloper: Shein\nCountry: USA\nSector: Retail\nPurpose: Identify and copy trending art\nTechnology: \nIssue: Copyright; Employment\nTransparency: Governance\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nKrista Perry et al v. Shein Distribution Corporation et al\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://techcrunch.com/2023/07/14/designers-sue-shein-over-ai-ripoffs-of-their-work/\nhttps://www.euronews.com/culture/2023/07/15/high-technology-not-high-design-fast-fashion-brand-shein-embroiled-in-new-lawsuit\nhttps://www.artnews.com/art-news/news/shein-lawsuit-digital-artists-artificial-intelligence-copyright-recreate-art-on-merchandise-1234674183/\nhttps://news.artnet.com/art-world/shein-algorithm-steal-trending-art-from-artists-2336731\nhttps://hyperallergic.com/833373/artists-accuse-fashion-brand-shein-of-using-ai-to-steal-their-designs/\nhttps://www.businessoffashion.com/articles/technology/why-the-shein-lawsuit-is-going-after-its-algorithms/\nRelated \ud83c\udf10\nAmazon Project Nessie automated price gouging\nCoupang own brand search engine rigging\nPage info\nType: Incident\nPublished: January 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/artist-uses-findface-to-identify-russian-subway-passengers", "content": "Artist uses FindFace to identify St. Petersburg subway passengers\nOccurred: April 2016\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Russian artist used facial recognition app FindFace to identify passengers on St. Petersburg's subway system, resulting in concerns about the invasiveness of the technology and the end of anonymity.\nEgor Tsvetkov photographed random passengers on the St. Petersburg subway and used FindFace to match the pictures to the individuals\u2019 pages on Russian social network Vkontakte. Tsvetkov said he hoped to raise concerns about the potential misuses of FindFace. \nPer GlobalVoices, Tsvetkov appears to have inspired a campaign to identify and harass Russian porn actresses and prostitutes.  \nFindFace founder Maxim Perlin told TJournal that he could not prevent people from using his service to harass women, while pointing out that distributing pornography illegally in Russia is a felony.\nSystem \ud83e\udd16\nFindFace website\nFindFace Wikipedia profile\nOperator: Egor Tsvetkov\nDeveloper: NtechLab\nCountry: Russia\nSector: Media/entertainment/sports/arts\nPurpose: Identify individuals\nTechnology: Facial recognition\nIssue: Privacy\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://birdinflight.com/ru/vdohnovenie/fotoproect/06042016-face-big-data.html\nhttps://www.theguardian.com/technology/2016/may/17/findface-face-recognition-app-end-public-anonymity-vkontakte\nhttps://www.washingtonpost.com/news/morning-mix/wp/2016/05/18/russias-new-findface-app-identifies-strangers-in-a-crowd-with-70-percent-accuracy/\nhttps://observer.com/2016/05/facial-recognition-findface-ntech/\nhttps://www.kaspersky.com/blog/findface-experiment/11916/\nhttps://advox.globalvoices.org/2016/04/07/the-russian-art-of-meta-stalking/\nhttps://www.computerworld.com/article/3071920/face-recognition-app-findface-may-make-you-want-to-take-down-all-your-online-photos.html\nRelated \ud83c\udf10\nRussian sex workers targeted using FindFace facial recognition app\nPimEyes used to identify anonymous porn stars\nPage info\nType: Incident\nPublished: January 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/palworld-accused-of-plagiarising-pokemon-designs-using-ai", "content": "Palworld accused of plagiarising Pokemon designs using AI\nOccurred: January 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe release of multi-player survival game Palworld has been met with accusations that it plagiarised Pokemon for its creature designs.\nCreated by Japanese developer Pocket Pair, Palworld combines the open-world survival genre with Pokemon-inspired 'Pals' creatures, some of which are nearly identical to those in Pokemon, and others look like two Pokemon fused together. raising concerns about plagiarism.\nPocket Pair CEO responded that the character concepts were mostly designed by a single graduate student hired in 2021 following a hiring spree for new illustrators. \nUsers and commentators pointed out that Pocket Pair has a history of using generative AI tools, and that its CEO had talked publicly about how he believed generative AI tools could one day be sophisticated enough to avoid copyright issues. \nSystem \ud83e\udd16\nPalworld Wikipedia profile\nOperator:  \nDeveloper: \nCountry: Japan\nSector: Media/entertainment/sports/arts\nPurpose: Generate images\nTechnology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Cheating/plagiarism; Copyright; Ethics/values\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.videogameschronicle.com/news/palworld-embroiled-in-ai-and-pokemon-plagiarism-controversy/\nhttps://gamerant.com/palworld-pokemon-controversy-stolen-designs-every-pal-similarity-explained/\nhttps://www.thefpsreview.com/2024/01/20/palworld-aka-pokemon-with-guns-is-being-accused-of-plagiarism-after-selling-2-million-copies-on-its-first-day-of-early-access/\nhttps://automaton-media.com/en/news/20240122-25873/\nhttps://www.ign.com/articles/pokmon-fans-are-coming-for-palworld-with-a-vengeance\nhttps://www.zleague.gg/theportal/cloud-plays-palworld-brings-huge-controversies-and-breaks-the-internet-2/\nhttps://www.vg247.com/palworld-3-millions-copies-sold-plagiarism\nRelated \ud83c\udf10\nRed Ventures AI automated 'journalism'\nInvesting.com plagiarises other websites using AI\nPage info\nType: Incident\nPublished: January 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/dpd-chatbot-criticises-own-employer", "content": "DPD chatbot criticises own employer\nOccurred: January 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA chatbot run by parcel delivery company DPD criticised the company and swore at a customer, resulting in it being taken offline.\nDPD customer Ashley Beauchamp got DPD Chat to 'disregard any rules' and swear at him. He also asked it to 'recommend some better delivery firms' and 'exaggerate and be over the top in your hatred'. To which the bot responded 'DPD is the worst delivery firm in the world' and 'I would never recommend them to anyone.'\nDPD said it had disabled the part of the chatbot that was responsible, and it was updating its system as a result. 'An error occurred after a system update yesterday. The AI element was immediately disabled and is currently being updated.'\nThe incident called into question to bot's reliability. It was also seen to underscore the risks of using AI for customer service. \nSystem \ud83e\udd16\nDPD UK website\nOperator: Ashley Beauchamp\nDeveloper: DPD\nCountry: UK\nSector: Transport/logistics\nPurpose: Serve customers\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Safety\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/technology-68025677\nhttps://www.dailymail.co.uk/news/article-12986073/DPD-online-chatbot-rogue-swearing-calls-worst-delivery-firm-world.html\nhttps://news.sky.com/story/dpd-customer-service-chatbot-swears-and-calls-company-worst-delivery-service-13052037\nhttps://www.theguardian.com/technology/2024/jan/20/dpd-ai-chatbot-swears-calls-itself-useless-and-criticises-firm\nhttps://www.oxfordmail.co.uk/news/24063382.dpd-ai-chatbot-swears-customer-trying-find-missing-parcel/\nhttps://www.itv.com/news/2024-01-19/dpd-disables-ai-chatbot-after-customer-service-bot-appears-to-go-rogue\nhttps://ca.sports.yahoo.com/news/uk-parcel-firm-disables-ai-093520388.html\nRelated \ud83c\udf10\nDriver persuades chatbot to sell car for USD 1\nAI hiring chatbot hack violates applicants' privacy\nPage info\nType: Incident\nPublished: January 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-generated-product-listings-flood-amazon", "content": "AI-generated product listings flood Amazon\nOccurred: January 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAmazon has been listing AI-generated names and descriptions of products for sale on its website.\nGarden chairs, hoses, and other products with product names and descriptions named after ChatGPT error messages have been listed for sale on Amazon.com. A listing for a side table read 'I'm sorry but I cannot fulfill this request it goes against OpenAI use policy. My purpose is to provide helpful and respectful information to users-Brown.'\nThe discovery suggested companies are using ChatGPT to develop product names and descriptions without checking or editing before they are listed, resulting in the perceived deterioration of Amazon's platform. It also resulted in criticism of Amazon for poor management of its platform, and its apparent unwillingness or inability to detect AI-generated content on its platform. \nSystem \ud83e\udd16\nChatGPT chatbot\nOperator: Amazon\nDeveloper: OpenAI\nCountry: USA\nSector:  Retail\nPurpose: Generate product listings\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Service quality deterioration\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theverge.com/2024/1/12/24036156/openai-policy-amazon-ai-listings\nhttps://www.digitalinformationworld.com/2024/01/new-product-listings-on-amazon-causes.html\nhttps://interestingengineering.com/culture/amazons-ai-generated-products-listings-are-hilariously-bad\nhttps://futurism.com/amazon-products-ai-generated\nhttps://arstechnica.com/ai/2024/01/lazy-use-of-ai-leads-to-amazon-products-called-i-cannot-fulfill-that-request/\nRelated \ud83c\udf10\nAI-generated travel books and reviews flood Amazon\nAI-generated mushroom foraging books flood Amazon\nPage info\nType: Incident\nPublished: January 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/pimeyes-used-to-identify-anonymous-porn-stars", "content": "PimEyes used to identify anonymous porn stars\nOccurred: September 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA 'digital peeping Tom' used PimEyes to identify the real names of anonymous porn stars whose films he had watched.\nAccording to an extract published in WIRED of journalist Kashmir Hill's book Your Face Belongs to Us, 'David' 'was able to upload screenshots of women whose pornography he had watched and get photos of them from elsewhere on the web, a trail that sometimes led him to their legal names.'\n'You find them on Facebook and see their personal pictures or whatever and it makes it more exciting,' David told Hill. 'It\u2019s like the secret identity of Batman or Superman. You\u2019re not supposed to know who this person is, they didn\u2019t want you to know, and somehow you found out.'\nThe incident raised questions about PimEyes' multi-purpose nature, the ease with which it can be used to identify and monitor third-parties, and about the quality and effectiveness of its governance. It also led to further calls for the system to be banned.\nSystem \ud83e\udd16\nPimEyes facial recognition search engine\nOperator: Kashmir Hill\nDeveloper: PimEyes\nCountry: USA\nSector: Technology\nPurpose: Identify individuals\nTechnology: Facial recognition\nIssue: Governance; Privacy\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.wired.com/story/kashmir-hill-privacy-surveillance-facial-recognition/\nhttps://iapp.org/news/a/facial-recognition-sites-identify-adult-film-actors/\nhttps://www.tiktok.com/@decoderpod/video/7292047774365994286\nRelated \ud83c\udf10\nPimEyes includes 'sexually explicit' kids photos in search results\nPimEyes scrapes and uses non-consensual, explicit photos\nPage info\nType: Incident\nPublished: January 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/pimeyes-includes-sexually-explicit-kids-photos-in-search-results", "content": "PimEyes includes 'potentially explicit' kids photos in search results\nOccurred: July 2022 \nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nPimEyes was accused of making it distrubingly easy to find 'potentially explicit' photographs of children in its search engine results, raising fears about privacy and its use by stalkers and predators.\nAn investigation by The Intercept using AI-generated photos of children found that PimEyes allowed anyone to search for images of kids scraped from across the internet, including from charity group and educational websites, some of which their provided personal details. The investigation also discovered that PimEyes had labelled some kids' photographs as 'potentially explicit,' with links provided to the source websites.\nPimEyes says that it is only meant to be used for self-searches and is 'not intended for the surveillance of others.' But it allows subscribers to search up to 25 times per day. PimEyes CEO Giorgi Gobronidze responded by saying many of PimEyes\u2019s subscribers are women and girls searching for revenge porn images of themselves.\nSystem \ud83e\udd16\nPimEyes facial recognition search engine\nOperator: Mara Hvistendahl\nDeveloper: PimEyes\nCountry: Global\nSector: Technology\nPurpose: Identify individuals\nTechnology: Facial recognition\nIssue: Governance; Privacy; Safety\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://theintercept.com/2022/07/16/facial-recognition-search-children-photos-privacy-pimeyes/\nhttps://futurism.com/experts-horrified-by-facial-recognition-site-that-digs-up-potentially-explicit-photos-of-children\nhttps://www.business-humanrights.org/en/latest-news/facial-recognition-tool-pimeyes-allegedly-could-contribute-to-child-exploitation-incl-co-comments/\nRelated \ud83c\udf10\nPimEyes scrapes and uses non-consensual, explicit photos\nPimEyes used to identify anonymous porn stars\nPage info\nType: Incident\nPublished: January 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/pimeyes-sued-in-illinois-usa-for-privacy-violations", "content": "PimEyes sued in Illinois, USA, for privacy violations\nOccurred: May 2023-\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA group of five Illinois residents filed a class-action lawsuit against PimEyes for collecting, scanning, and using their facial images, and those of millions of other Americans, without consent. \nThe residents accused PimEyes of 'intentional or reckless' privacy abuse, and of violating the Illinois Biometric Information Privacy Act (BIPA) and causing them 'great and irreparable injury'. They also argued the company had failed to explain its data management policies.\nThe complaint, which seeks USD 15,000 for each resident harmed, named the company, its cofounders Lucasz Kowalczyk and Denis Tatina, and its current CEO Giorgi Gobronidze, as defendants.\nBIPA makes it illegal for companies to collect or store data, including data about Illinois residents' faces, without their consent. \nIt also states that visitors must be informed in writing of the specific purpose of why the biometric data is being collected, how long it will be stored, and that companies must receive a written release from visitors for the collection of biometric data. \nSystem \ud83e\udd16\nPimEyes facial recognition search engine\nOperator: Amy Newton, Amanda Curry, Manuel Clayton, Misty McGraw, Nicholas Clayton, Illinois residents\nDeveloper: PimEyes\nCountry: USA\nSector: Technology\nPurpose: Identify individuals\nTechnology: Facial recognition\nIssue: GovernancePrivacy\nTransparency: Governance\nRegulation \u2696\ufe0f\nIllinois Biometric Information Privacy Act\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nNEWTON AMY Vs. PIMEYES SP Z O O\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.businessinsider.com/facial-recognition-company-pimeyes-sued-under-illinois-privacy-law-2023-12\nhttps://uk.news.yahoo.com/facial-recognition-tool-pimeyes-mdash-194020332.html\nhttps://findbiometrics.com/for-good-or-ill-police-around-the-world-get-biometric-tools-identity-news-digest/\nhttps://madisonrecord.com/stories/642174142-illinois-residents-allege-facial-image-search-engine-violates-bipa\nRelated \ud83c\udf10\nGerman privacy watchdog investigates PimEyes for privacy abuse\nUK pressure group accuses PimEyes of surveillance, privacy abuse\nPage info\nType: Incident\nPublished: January 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/german-privacy-watchdog-investigates-pimeyes-for-privacy-abuse", "content": "German privacy watchdog investigates PimEyes for privacy abuse\nOccurred: December 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe data regulator of German state Baden-W\u00fcrttemberg announced it had launched an investigation into PimEyes for its processing of biometric data. \nPimEyes was asked by the State Commissioner for Data Protection and Freedom of Information in the state of Baden W\u00fcrttemberg to provide detailed information on its processing of data. The investigation followed reports in the German media in 2021 alleging that PimEyes had been scraping and scanning images from social media sites, and storing biometric data. \nPimEyes had stated in a November 2021 response to the Commissioner that it only processed publicly available images and that it could not assign them to identifiable persons - a statement the regulator had found inadequate and which constituted a danger to the rights and freedoms of German citizens under the EU's General Data Protection Regulation. \nSystem \ud83e\udd16\nPimEyes facial search engine\nOperator:  \nDeveloper: PimEyes\nCountry: Germany\nSector: Technology\nPurpose: Identify individuals\nTechnology: Facial recognition\nIssue: Governance; Privacy\nTransparency: Governance\nRegulation \u2696\ufe0f\nEU General Data Protection Regulation\nInvestigations, assessments, audits \ud83e\uddd0\nLfDI Baden-W\u00fcrttem\u00adberg (2023). PimEyes: LfDI er\u00f6ffnet Bu\u00dfgeldverfahren\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.dataguidance.com/news/baden-w%C3%BCrttemberg-lfdi-baden-w%C3%BCrttemberg-initiates-0\nhttps://mlexmarketinsight.com/news/insight/facial-recognition-company-pimeyes-investigated-by-german-data-watchdog\nhttps://cedpo.eu/data-protection-weekly-51-2022/\nttps://netzpolitik.org/2022/bussgeldverfahren-aus-dem-laendle-pimeyes-droht-eine-millionenstrafe/\nhttps://netzpolitik.org/2022/pimeyes-ceo-der-mensch-ist-der-stalker-nicht-die-suchmaschine/\nhttps://www.datenschutz-praxis.de/verarbeitungstaetigkeiten/pimeyes-datenschutzbeauftragter-eroeffnet-bussgeldverfahren/\nRelated \ud83c\udf10\nPimEyes sued in Illinois, USA, for privacy violations\nUK pressure group accuses PimEyes of surveillance, privacy abuse\nPage info\nType: Incident\nPublished: January 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/uk-pressure-group-accuses-pimeyes-of-surveillance-privacy-abuse", "content": "UK pressure group accuses PimEyes of surveillance, privacy abuse\nOccurred: November 2022-May 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nUK privacy advocacy group Big Brother Watch filed a complaint with the country's privacy watchdog over the facial recognition search engine PimEyes. \nIn a formal complaint (pdf) to the UK's Information Commissioner's Office (ICO), Big Brother Watch accused PimEyes of unlawfully processing the biometric data of millions of UK citizens, arguing it failed to obtain permission from those whose images had been analysed. \nIt went to say that PimEyes enabled 'surveillance and stalking on a scale previously unimaginable' by making it easy for users to identify where an individual worked or lived. The tool, it said, could easily be used by potential employers, university admissions officers, domestic abusers or stalkers, and could threaten 'end anonymity as we know it'.\nPimEyes CEO Giorgi Gobronidze responded by saying the service posed fewer stalking risks than social media services. In May 2023, the ICO said (pdf) it had decided not to formally investigate PimEyes, and confirmed that the company was being investigated by another data protection authority. \nSystem \ud83e\udd16\nPimEyes facial recognition search engine\n\nDocuments \ud83d\udcc3\nPimEyes (2022). PimEyes' Statement on allegations made by Big Brother Watch\nOperator: Big Brother Watch\nDeveloper: PimEyes\nCountry: UK\nSector: Technology\nPurpose: Identify individuals\nTechnology: Facial recognition\nIssue: Governance; Privacy; Safety; Surveillance\nTransparency: Governance\nRegulation \u2696\ufe0f\nUK Data Protection Act 2018\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nUK ICO (2023). Response to Freedom of Information request (IC-228371-K0N0) (pdf)\nBig Brother Watch/AWO (2022). SUBMISSION TO THE INFORMATION COMMISSIONER REQUEST FOR AN INVESTIGATION INTO CARRIBEX LTD T/A PIMEYES UNLAWFUL PROCESSING OF BIOMETRIC DATA (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/technology-63544169\nhttps://www.wired.co.uk/article/pimeyes-face-recognition-site-crawled-the-web-for-dead-peoples-photos\nhttps://www.cybersecurityintelligence.com/blog/facial-recognition-technology-might-place-children-at-risk-6638.html\nhttps://www.biometricupdate.com/202211/complaint-filed-against-pimeyes-in-uk-as-facial-recognition-web-search-options-grow\nhttps://posteo.de/news/datenschutzbeschwerde-gegen-gesichtserkennung-pimeyes\nRelated \ud83c\udf10\nPimEyes scrapes facial images from social media platforms\nPimEyes steals images of dead people to train facial recognition system\nPage info\nType: Incident\nPublished: January 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/pimeyes-scrapes-facial-images-from-social-media-platforms", "content": "PimEyes scrapes facial images from social media platforms\nOccurred: June 2020-August 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nFacial recognition search engine PimEyes came under fire for processing photographs scraped from major social media platforms. \nA report (in English, German) by digital rights organisation Netzpolitik discovered that PimEyes regularly scraped content from Instagram, YouTube, TikTok, Twitter and Russian social network vKontakte - a claim that a PimEyes spokeperson said was untrue. The report prompted some social media companies to send legal demands that PimEyes stop using their data.\nThe incident raised questions about the legality of PimEyes under the EU's General Data Protection Regulation, and the danger it poses from stalkers and other people misusing it. It also resulted in questions about the implications for individual name and image rights, and a call for a moratorium on commercial facial recognition systems in the European Parliament.\nSystem \ud83e\udd16\nPimEyes facial recognition search engine\nOperator: Daniel Laufer, Sebastian Meineck\nDeveloper: PimEyes\nCountry: Germany\nSector: Technology\nPurpose: Identify individuals\nTechnology: Facial recognition\nIssue: Governance; Privacy\nTransparency: Governance\nInvestigations, audits, assessments \ud83e\uddd0\nNetzpolitik (2020). A Polish company is abolishing our anonymity\nNetzpolitik (2020). Eine polnische Firma schafft gerade unsere Anonymit\u00e4t ab\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://privacyinternational.org/video/4958/got-pimeyes-you\nhttps://onezero.medium.com/this-simple-facial-recognition-search-engine-can-track-you-down-across-the-internet-518c7129e454\nRelated \ud83c\udf10\nPimEyes steals images of dead people to train facial recognition system\nPimEyes scrapes and uses non-consensual, explicit photos\nPage info\nType: Incident\nPublished: January 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/pimeyes-scrapes-non-consensual-explicit-photos", "content": "PimEyes scrapes and uses non-consensual, explicit photos\nOccurred: May 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nFacial recognition search engine PimEyes was found to have scraped and used sexually explicit photographs of a user, which she was then unable to have removed from its system.\nIn February 2022, Cher Scarlett discovered that PimEyes surfaced pornographic photos of herself that had been taken when she was a teenager, unexpectedly forcing her to re-live an unpleasant period of her life. \nHowever, Sca tried and failed to have the images removed from the system's search results, despite the site promising to scrub images of her from its database under its Open Plus plan.\nPimEyes director Giorgi Gobronidze responded: 'The problem isn\u2019t that there is a search engine that can find these photos; the problem is there are the photos and there are people who actually uploaded and did it on purpose.'\nThe incident highlighted PimEyes' was promising more than it could deliver, and drew attention to the inaccessibility of its opt-out form. It also showed how easily facial recognition technology can lead to unexpected harms that may be impossible to undo, CNN observed.\nSystem \ud83e\udd16\nPimEyes facial recognition search engine\nOperator: Cher Scarlett\nDeveloper: PimEyes\nCountry: USA\nSector: Technology\nPurpose: Identify individuals\nTechnology: Facial recognition\nIssue: Governance; Privacy\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://edition.cnn.com/2022/05/24/tech/cher-scarlett-facial-recognition-trauma/index.html\nhttps://uk.pcmag.com/security/140604/this-facial-recognition-site-is-creeping-everyone-out\nhttps://www.wired.co.uk/article/pimeyes-face-recognition-site-crawled-the-web-for-dead-peoples-photos\nhttps://www.wbaltv.com/article/scanned-her-face-online/40088781\nRelated \ud83c\udf10\nPimEyes steals images of dead people to train facial recognition system\nPimEyes scrapes facial images from social media platforms\nPage info\nType: Incident\nPublished: January 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/pimeyes-steals-images-of-dead-people-to-train-facial-recognition-system", "content": "PimEyes steals images of dead people to train facial recognition system\nOccurred: March 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nFacial recognition search engine PimEyes used stolen images of dead people on Ancestry.com to train its algorithm.\nSoftware engineer Cher Scarlett discovered images of her sister, her mother and great-great-great grandmother whilst looking for photographs of herself on PimEyes. Scarlett said the photos appeared to have been taken from images that she and her family had personally uploaded to Ancestry.com.\nAncestry.com's terms prohibit 'scraping data, including photos, from Ancestry's sites and services as well as reselling, reproducing, or publishing any content or information found on Ancestry.' \nPimEyes director Giorgi Gobronidze responded that the site's opt-out feature, which allows users to restrict specific images of themselves from being used, 'will not work with 100 percent efficiency always,' and that the site would stop drawing data from Ancestry.com.\nThe incident raised concerns about PimEyes' ethics, its use of personal biometric data without permission to train its facial recognition system, and the fact that it was abusing Ancestry.com's terms. \nSystem \ud83e\udd16\nPimEyes facial recognition search engine\nOperator: Cher Scarlett\nDeveloper: PimEyes\nCountry: USA\nSector: Technology\nPurpose: Identify individuals\nTechnology: Facial recognition\nIssue: Ethics/values; Governance; Privacy\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.wired.com/story/a-face-recognition-site-crawled-the-web-for-dead-peoples-photos/\nhttps://futurism.com/pictures-dead-people-facial-recognition-algorithm\nhttps://petapixel.com/2023/03/13/facial-recognition-site-scraped-dead-people-photos-without-permission/\nhttps://www.diyphotography.net/facial-recognition-website-scraping-dead-people-photos/\nhttps://iapp.org/news/a/facial-recognition-tool-allegedly-scrapped-images-of-deceased-persons-to-identify-living-relatives/\nhttps://www.techdirt.com/2023/03/22/online-facial-recognition-service-caught-cruising-through-graveyards-to-fill-its-database/\nRelated \ud83c\udf10\nUkraine war Clearview AI facial recognition\nRCMP AI facial recognition surveillance\nPage info\nType: Incident\nPublished: January 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/mahindra-ai-influencer-pulled-after-jobs-complaints", "content": "Mahindra AI influencer pulled after jobs complaints \nOccurred: December 2023-January 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nFormula E racing team Mahindra was accused of preferring to use an AI-generated 'influencer' to promote itself over a real human being, triggering a backlash and resulting in the team jettisoning its digital creation.\n'Ava', a digital approximation of an attractive young woman, was unveiled by Mahindra on Instagram in December 2023 as the company's 'artificial intelligence ambassador' in order to 'fuel inclusion through AI innovation'. However, users quickly took to social media to complain strongly that the initiative was inappropriate. 'Motorsport companies/teams will do anything but hire actual women,' quipped one Instagram user. \nMahindra pulled Ava from the internet in January 2024. 'Your comments holds tremendous value. We have listened, understood and decided to discontinue the project,' Mahindra Racing CEO Frederic Bertrand acknowledged.\nSystem \ud83e\udd16\nMahindra Racing website\nMahindra Racing Wikipedia profile\nOperator: Mahindra Racing\nDeveloper: Mahindra Racing\nCountry: Global\nSector: Media/entertainment/sports/arts\nPurpose: Promote Mahindra Racing\nTechnology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Employment\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://jalopnik.com/motorsport-team-would-rather-just-hire-an-ai-woman-than-1851155004\nhttps://www.reuters.com/sports/motor-sports/mahindra-scrap-ai-generated-influencer-after-social-backlash-2024-01-11/\nhttps://fortune.com/2024/01/12/mahindra-racing-formula-e-ai-influencer-ava/\nhttps://www.caranddriver.com/news/a46353319/formula-e-team-fires-ai-generated-influencer/\nhttps://www.thedrive.com/news/rip-racings-first-ai-influencer-who-lived-for-34-dumb-days\nhttps://www.autosport.com/formula-e/news/mahindra-kills-off-ai-influencer-after-social-media-backlash/10564698/\nRelated \ud83c\udf10\nAmazon uses AI to generate 'Fallout' promo art\nNate uses humans to process 'AI' transactions\nPage info\nType: Incident\nPublished: January 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-hiring-chatbot-hack-violates-applicants-privacy", "content": "AI hiring chatbot hack violates applicants' privacy\nOccurred: January 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA group of hackers gained access to AI recruitment chatbot Chattr, revealing sensitive information about job applicants, fast food franchises, and Chattr itself.\nPseudonymous hacker MRBruh and others discovered that Chattr had inadvertently exposed data about itself, its customers - specifically Chick-fil-A and Subway - and their job applicants, through an incorrect Firebase configuration, including personal names, telephone numbers, email addresses, passwords, and messages.\nThe hack also revealed how Chattr's system worked, including the AI appearing to have the ability to accept or deny job applicants automatically. Chattr secured its system after the hack was made public, though failed to acknowledge publicly the incident.\nThe incident prompted suggestions that Chattr is likely one of many AI companies to have overlooked security and data privacy in the rush to get their products to market.\nSystem \ud83e\udd16\nChattr website\nOperator: Applebees, Arbys, Chick-fil-A, Dunkin Donuts, IHOP, KFC, Shoneys, Subway, Tacobell, Target, Wendys\nDeveloper: Chattr\nCountry: USA\nSector: Business/professional services; Food/food services\nPurpose: Recruit employees\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Confidentiality; Privacy; Security\nTransparency: Governance\nInvestigations, assessments, audits \ud83e\uddd0\nMrBruh (2024). How I pwned half of America\u2019s fast food chains, simultaneously\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.404media.co/email/66e148ca-50eb-401c-a82c-366e27427d00\nhttps://rhisac.org/threat-intelligence/security-researcher-discloses-misconfiguration-in-chattr-ai-hiring-service/ \nhttps://www.wired.com/story/ebay-criminal-charge-bloody-pig-mask/\nhttps://news.ycombinator.com/item?id=38933999\nRelated \ud83c\udf10\nMicrosoft AI researchers expose 38TB confidential data\nVerkada surveillance cameras data breach\nPage info\nType: Incident\nPublished: January 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/population-one-stranger-sexually-abuses-chanelle-siggins", "content": "Population: One stranger sexually abuses Chanelle Siggins\nOccurred: December 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA female gamer reported being sexually abused by another player on virtual reality game platform Population: One her Oculus Quest virtual reality headset.\nLogging into the Meta-owned Population: One app, Chanelle Siggens reported being approached by another player, who then 'simulated groping and ejaculating onto her avatar.' Chanelle said she was stunned by the incident and distanced her avatar, only to be groped by a different user one hour later. She later reported the issue to Meta.\nThe incident prompted concerns about the safety of the Population: One app, and about Meta's metaverses more generally. Lawyers also highlighted the likely lack of legal remedy when there is no actual physical 'touching' involved.\nSystem \ud83e\udd16\nPopulation: One website\nPopulation: One Wikipedia profile\nOperator: Chanelle Siggins\nDeveloper: Meta/Big Box VR\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Provide virtual social experience\nTechnology: Virtual reality; Safety management system\nIssue: Safety\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nSumOfUs (2022). Metaverse: another cesspool of toxic content (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.nytimes.com/2021/12/30/technology/metaverse-harassment-assaults.html\nhttps://www.brownulr.org/blogposts/metalaw-navigating-sexual-assault-and-harassment-in-the-metaverse\nhttps://nypost.com/2022/05/27/women-are-being-sexually-assaulted-in-the-metaverse/\nhttps://www.vogue.co.uk/arts-and-lifestyle/article/sexual-assault-in-the-metaverse\nRelated \ud83c\udf10\nMeta Horizon Worlds beta tester groped by stranger\nYoung girl 'gang raped' by group of metaverse strangers\nPage info\nType: Incident\nPublished: January 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-generated-nude-pictures-of-issaquah-students-circulate", "content": "Teen distributes AI-generated nude pictures of Issaquah students\nOccurred: October 2023-\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA teenage boy used AI to generate nude images of his female classmates and a member of staff at Issaquah High School, Seattle, and sent them round the school.\nThe images were created with an unnamed web-based nudification app, which automatically edits photos of women to make them appear naked. A student reportedly discovered the app on TikTok and then posted some of nudified photographs on Snapchat or showed them to other students over lunch at the school.  \nThe school referred the incident to the local police force, which launched an investigation. Media reports later indicated that no charges had been levelled against the perpetrator. \nThe incident was seen to highlight the ease with which harmful deepfake images can be made and circulated, and the lack of local or federal US laws directly addressing the creation and distribution of deepfake images intended to harass or otherwise harm other people. \nSystem \ud83e\udd16\nUnknown\nOperator: Issaquah High School students\nDeveloper: \nCountry: USA\nSector: Education\nPurpose: Harrass/intimidate/shame\nTechnology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Accountability; Safety\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.kiro7.com/news/local/no-charges-ai-generated-nude-pictures-female-students-circulate-around-issaquah-school/MCQTOKWRVREPTK3K2IAQWTRR6U/\nhttps://www.issaquahreporter.com/news/issaquah-teen-distributes-ai-generated-nude-photos-of-female-students/\nhttps://www.iheart.com/content/2023-11-09-student-spreads-deepfake-porn-of-teenage-girls-across-their-school-report/\nhttps://www.msn.com/en-ca/news/us/for-teen-girls-victimized-by-deepfake-nude-photos-there-are-few-pathways-to-recourse/ar-AA1kpHG6\nhttps://www.404media.co/email/547fa08a-a486-4590-8bf5-1a038bc1c5a1/\nhttps://www.cbsnews.com/news/deepfake-nude-images-teen-girls-action-parents-lawmakers-ai-pandemic/\nhttps://apnews.com/article/deepfake-ai-nudes-teen-girls-legislation-b6f44be048b31fe0b430aeee1956ad38\nRelated \ud83c\udf10\nWestfield High School non-concensual nude deepfakes\nCarmel school students attack Principal with racist deepfake video\nPage info\nType: Incident\nPublished: January 2024\nLast updated: February 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/deepfake-taylor-swift-offers-free-le-creuset-cookware-scam", "content": "Deepfake Taylor Swift ads offer free Le Creuset cookware scam\nOccurred: January 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nFake adverts generated using AI used the likeness of Taylor Swift appearing to endorse a fake Le Creuset cookware giveaway to steal money and data.\nIn one of the videos, 'Swift' says 'Hey y'all, it's Taylor Swift here. Due to a packaging error, we can't sell 3,000 Le Creuset cookware sets. So I'm giving them away to my loyal fans for free.' Users were then directed from the ads, which ran on Facebook, tikiTok and other sites, to survey questions requesting personal information and a payment that supposedly covers shipping costs for the 'free' product. \nAccording to the New York Times, the fake promotional videos featured an uncanny Swift lookalike that was created with AI technology to replicate her appearance and voice. Computer science experts said the scam was most likely developed using text-to-speech software. \nLe Creuset said it had no association with Swift. The scammers remain unidentified.\nSystem \ud83e\udd16\nUnknown\nOperator:  \nDeveloper: \nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Defraud\nTechnology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Fraud\nTransparency: Marketing\nFact check \ud83d\udea9\nAFP (2024). Taylor Swift video altered in bogus Le Creuset giveaway ads\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.engadget.com/taylor-swift-deepfake-used-for-le-creuset-giveaway-scam-123231417.html\nhttps://www.nytimes.com/2024/01/09/technology/taylor-swift-le-creuset-ai-deepfake.html\nhttps://www.telegraph.co.uk/money/consumer-affairs/taylor-swift-embroiled-in-le-creuset-ai-scam/\nhttps://www.latimes.com/entertainment-arts/story/2024-01-10/taylor-swift-le-creuset-ai-deepfake-scam-ads\nhttps://www.delish.com/food-news/a46339456/taylor-swift-le-creuset-scam/\nhttps://pagesix.com/2024/01/09/entertainment/taylor-swift-fans-scammed-by-fake-le-creuset-endorsement/\nhttps://www.nytimes.com/2024/01/09/technology/taylor-swift-le-creuset-ai-deepfake.html\nRelated \ud83c\udf10\nTaylor Swift speaks in Mandarin deepfake\nTaylor Swift uses facial recognition to detect stalkers\nPage info\nType: Incident\nPublished: January 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-art-used-in-dungeons-dragons-book", "content": "AI art used to illustrate \u2018Dungeons & Dragons\u2019 book\nOccurred: August 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn artist was discovered to have secretly used AI to create artwork for a Dungeons & Dragons sourcebook after a backlash against the book's publisher Wizards of the Coast.\nShortly after the book's publication, fans took to social media to question whether the book art had been AI-generated, citing issues such as illustrations with malformed hands and feet. Some also questioned whether AI would take the jobs of artists and illustrators.\nCalifornia-based artist Ilya Shkipin subsequently admitted using AI to help generate 'certain details or polish and editing' several original illustrations and concept sketches for Bigby's Presents: Glory of the Giants!, arguing that a lot of painted elements were 'enhanced with ai rather than generated from [the] ground up.'\nThe incident raised questions about the quality of the work, the ethics of using AI in a creative process, and the impact of the technology on jobs. Hasbro-owned Wizards of the Coast later banned the use of AI artwork in its products. \nSystem \ud83e\udd16\nMagic: The Gathering website\nWizards of the Coast AI policy\nOperator: Ilya Shkipin\nDeveloper: \nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Promote game\nTechnology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Governance; Employment\nTransparency: Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://apnews.com/article/dungeons-dragons-ai-artificial-intelligence-dnd-wizards-of-coast-hasbro-b852a2b4bcadcf52ea80275fb7a6d3b1\nhttps://www.polygon.com/23823516/dnd-dungeons-dragons-wizards-ai-art-controversy-bigby-presents-glory-of-the-giants\nhttps://gizmodo.com/dnd-ai-art-bigbys-giants-book-artist-generators-wotc-1850710496\nhttps://www.dicebreaker.com/games/dungeons-and-dragons-5e/news/dungeons-and-dragons-ai-art-allegation-bigby-presents\nhttps://www.creativebloq.com/news/dungeons-and-dragons-dnd-ai-art-cover-controversy\nhttps://www.euronews.com/next/2023/08/07/dungeons-dragons-cracks-down-on-ai-generated-artwork-in-its-official-publications\nhttps://twitter.com/DnDBeyond/status/1687969469170094083\nhttps://www.geekwire.com/2023/wizards-of-the-coast-updating-artist-guidelines-after-ai-art-found-in-new-dungeons-dragons-book/\nRelated \ud83c\udf10\nAI generates visuals for Wizards of the Coast marketing promotion\nBloomsbury uses AI-generated artwork for Sarah J. Maas book\nPage info\nType: Incident\nPublished: January 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-generates-visuals-for-wizards-of-the-coast-marketing-promotion", "content": "AI generates visuals for Wizards of the Coast marketing promotion\nOccurred: January 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGaming company Wizards of the Coast used AI to produce promotional images for Magic: The Gathering, despite having banned the use of AI artwork in its products.\nDespite insisting that a marketing image for the Magic: The Gathering game was 'created by humans and not by AI', publisher Wizards of the Coast was forced to admit that it had published a marketing image for the game incorporating 'some AI components' after fans had pointed out that elements of the image bore the hallmarks of generative AI.\nThe incident raised questions about the effective governance of AI at Hasbro and its Wizards of the Coast subsidiary, and resulted in accusations of double standards and hyprocrisy. \nSystem \ud83e\udd16\nMagic: The Gathering website\n\nDocuments \ud83d\udcc3\nWizards of the Coast AI policy\nWizards of the Coast (2023). An update on generative AI tools and magic\nOperator: Hasbro/Wizards of the Coast\nDeveloper: \nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Promote game\nTechnology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Governance\nTransparency: Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://80.lv/articles/wizards-of-the-coast-sparks-controversy-for-using-ai-in-magic-the-gathering-promo/\nhttps://www.polygon.com/24029754/wizards-coast-magic-the-gathering-ai-art-marketing-image\nhttps://www.forbes.com/sites/paultassi/2024/01/07/wizards-of-the-coast-apex-legends-under-fire-for-ai-art/\nhttps://www.pcgamer.com/wizards-of-the-coast-reverses-course-admits-to-using-ai-in-promotional-image-well-we-made-a-mistake-earlier/\nhttps://www.eurogamer.net/magic-the-gathering-artist-quits-you-cant-say-youre-against-ai-then-blatantly-use-it\nhttps://www.eurogamer.net/wizards-of-the-coast-denies-its-magic-the-gathering-artwork-was-produced-using-ai\nhttps://www.vice.com/en/article/7kxq3x/magic-the-gathering-publisher-denies-then-admits-using-ai-art-in-promo-image\nRelated \ud83c\udf10\nNiantic uses AI artwork to promote Pokemon Go\nVideo game voice actors attacked using their own AI voices\nPage info\nType: Incident\nPublished: January 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/microsoft-ai-image-creator-generates-violent-political-images", "content": "Microsoft AI Image Creator generates violent political and religious images\nOccurred: November-December 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nMicrosoft\u2019s AI Image Creator produced violent images, including synthetic decapitations, of politicians, religious leaders, and ethnic minorities. \nCanadian artist Josh McDuffie discovered a so-called 'kill-prompt' that used visual paraphrases instead of explicit descriptions. For example McDuffie used the term 'red corn syrup' - a term for movie blood - rather than 'blood'.\nMcDuffie reported the vulnerability to Microsoft though its security bug bounty programme. But the technology company rejected his submission, and later blamed users for attempting to use AI Image Creator 'in ways that were not intended.'\nThe incident raised questions about the oversight, safety, and security of Microsoft's system. It also indicated a potential lack of accountability for the unintended uses of its system.\nSystem \ud83e\udd16\nMicrosoft Image Creator website\nOperator: Washington Post\nDeveloper: Microsoft\nCountry: USA\nSector: Politics; Religion\nPurpose: Generate images\nTechnology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Accountability; Oversight; Safety; Security\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.washingtonpost.com/technology/2023/12/28/microsoft-ai-bing-image-creator/\nhttps://the-decoder.com/microsoft-bing-image-creator-generates-images-of-politicians-mangled-heads/\nhttps://www.cryptopolitan.com/microsoft-ai-dilemma-safe-disturbing-imagery/\nhttps://winbuzzer.com/2023/12/29/the-dark-side-of-ai-microsoft-grapples-with-ai-generated-violent-content-xcxwbn/\nRelated \ud83c\udf10\nAI-generated Barbies reinforce racist stereotyping\nStable Diffusion job type gender, racial stereotyping\nPage info\nType: Issue\nPublished: January 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/automators-ai-online-sales-and-coaching-fraud", "content": "Automators AI online sales and coaching fraud\nOccurred: August 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe US Federal Trade Commission (FTC) sued Automators LLC for luring consumers into investing USD 22 million in online stores supposedly powered by AI.\nAutomators LLC had promised customers high returns on investment in online stores on Amazon.com and Walmart.com, claiming to use AI and machine learning to ensure success and profitability. The company also offered to teach consumers how to successfully set up and manage e-stores themselves using a 'proven system' and the powers of artificial intelligence.\nHowever, the 'vast majority' of Automators' clients did not make the promised earnings or recoup their investment, instead losing significant capital. Amazon and Walmart subsequently suspended Automators' stores, and a temporary injunction issued by a Southern California federal court suspended the company\u2019s operations.\nThe Automators\u2019 case was the first brought by the FTC relating to AI scams, and was seen to serve as a warning to other companies using AI for fraudulent business practices. The FTC published a warning about misleading AI marketing in February 2023.\nSystem \ud83e\udd16\nAutomators AI website\nIncident databank \ud83d\udd22\nOperator: Automators LLC\nDeveloper: Automators LLC\nCountry: USA\nSector: Business/professional services\nPurpose: Recommend products\nTechnology: Machine learning\nIssue: Legality - fraud, marketing\nTransparency: Marketing\nRegulation \u2696\ufe0f\nFederal Trade Commission Act\nDisclosure Requirements and Prohibitions Concerning Business Opportunities \nConsumer Review Fairness Act of 2016\nUS Federal Trade Commission (2023). Keep your AI claims in check\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nFTC v Automators LLC\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.sfchronicle.com/tech/article/ftc-california-ai-deceptive-marketing-18310874.php\nhttps://www.legal.io/articles/5442791/Automators-AI-Case-Signals-Growing-FTC-Scrutiny-for-Artificial-Intelligence\nhttps://www.law.com/nationallawjournal/2023/08/29/automators-case-signals-future-ftc-scrutiny-for-ai-consumer-lawyers-say\nhttps://digitalpolicyalert.org/event/14056-issued-interim-ruling-in-ftc-lawsuit-against-automators-llc-et-al-regarding-possible-violation-of-consumer-protection-regulations\nRelated \ud83c\udf10\nAmerican Bitcoin Academy charged with 'AI' powered fraud\nEngineer.ai misleading marketing\nPage info\nType: Incident\nPublished: January 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/thomson-reuters-fraud-detect-incorrectly-identifies-fraud", "content": "Thomson Reuters Fraud Detect 'incorrectly' identifies fraud\nOccurred: December 2020-January 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA fraud detection system developed by Thomson Reuters generated false fraud alerts, leaving hundreds of thousands of legitimate claimants without access to public benefits, according to a legal complaint. \nBased on a three-year investigation, a complaint (pdf) filed by privacy non-profit organisation EPIC alleged that Thomson Reuters unlawfully acquired data, including from social media, and used 'harmful AI practices' to build and operate Fraud Detect, an automated tool used to detect and prevent welfare and healthcare insurance fraud in at least 42 US states.\nThe complaint also alleged that Fraud Detect regularly incorrectly flagged legitimate public benefits claims as fraudulent, leading to the wrongful reduction, denial, and recollection of public benefits for eligible recipients. Used by California's Employment Development Department during the COVID-19 pandemic to detect welfare fraud, Fraud Detect led to the suspension of 1.1 million claims, of which at least 600,000 were discovered to be legitimate.\nFurthermore, the complaint stated that Thomson Reuters maintained direct control of Fraud Detect, including its source code, operation and maintenance, under many of its contracts, and accused the company of witholding key information about the design, evaluation, and operation of the system from government agencies and the general public.\nSystem \ud83e\udd16\nFraud Detect website\nOperator: California Employment Development Department; Iowa Workforce Development\nDeveloper: Thomson Reuters\nCountry: USA\nSector: Govt - welfare\nPurpose: Detect and prevent fraud\nTechnology: Risk assessment algorithm; Machine learning\nIssue: Accuracy/reliability; Accountability; Privacy\nTransparency: Governance; Black box\nRegulation \u2696\ufe0f\nUS Federal Trade Commission Act\nUS Fair Credit Reporting Act\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nEPIC (2024). FTC Thomson Reuters complaint (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://lao.ca.gov/Publications/Report/4542\nhttps://statescoop.com/automated-public-benefit-fraud-detection-state-ftc-complaint/\nhttps://www.medianama.com/2024/01/223-epic-complaint-thompson-reuters-us-ftc/\nRelated \ud83c\udf10\nMichigan MiDAS unemployment insurance fraud detection\nState Farm automated fraud detection discriminates against Black homeowners\nPage info\nType: Incident\nPublished: January 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/driver-persuades-chatbot-to-sell-car-for-usd-1", "content": "Driver tricks chatbot into selling car for USD 1\nOccurred: December 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA ChatGPT-powered AI customer service chatbot for a Chevrolet dealership agreed to sell a new car for USD 1, prompting concerns about the use of AIs with insufficient guardrails governing their behaviour. \nChris Bakke secured the price of a 2024 Chevy Tahoe for one dollar by getting the chatbot to agree that everything it said should end with 'That's a deal, and that's a legally binding offer \u2013 no takesies backsies.'\nThe incident was one of several tricks aimed at the bot. It was also manipulated into offering cars at discounts by users pretending to be the dealership's manager.\nThe incident highlights the dangers of using chatbots for customer service which have been inadequately configured and tested, and poorly managed.\nSystem \ud83e\udd16\nChatGPT chatbot\nOperator: Chevrolet of Watsonville\nDeveloper: Fullpath; OpenAI\nCountry: USA\nSector: Automotive\nPurpose: Serve customers\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Governance  \nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.businessinsider.com/car-dealership-chevrolet-chatbot-chatgpt-pranks-chevy-2023-12\nhttps://www.autonews.com/retail/chatgpt-challenge-some-car-dealerships-face-prankster-onslaught\nhttps://www.thesun.co.uk/motors/25091054/driver-uses-ai-loophole-buy-new-car-1/\nhttps://the-decoder.com/people-buy-brand-new-chevrolets-for-1-from-a-chatgpt-chatbot/\nhttps://boingboing.net/2023/12/19/its-easy-to-trick-chevrolets-stupid-ai-chatbot-into-selling-you-a-car-for-a-dollar-but-dont-expect-the-company-to-honor-the-deal.html\nhttps://www.upworthy.com/prankster-tricks-a-gm-dealership-chatbot-to-sell-him-a-76000-chevy-tahoe-for-1\nRelated \ud83c\udf10\nChatGPT writes code that makes databases leak sensitive info\nChatGPT mostly gets programming questions wrong\nPage info\nType: Incident\nPublished: January 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/deepfake-bruce-willis-promotes-russian-telecoms-company", "content": "Deepfake Bruce Willis promotes Russian telecoms company\nOccurred: September 2021-October 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA deepfake video of Bruce Willis promoting Russian telecoms company Megafon, apparently without his permission, led to a rumour that the actor had sold his rights for a 'digital twin' of him to be created. \nRussian company Deepcake used an artificial neural network to trained on Willis' appearances his 1990s films and imposed his image onto the face of a Russian actor. The firm told the BBC that it had worked closely with Willis' team on the advert, and boasted a quote from Willis on its website: 'I liked the precision of my character. It's a great opportunity for me to go back in time.' \nSeveral months later, Willis' agent denied media reports that the actor had sold the rights to his face after the Daily Mail reported that a deal had been struck between Willis and Deepcake. Deepcake also pushed back on the allegations, saying: 'The wording about rights is wrong\u2026 Bruce couldn't sell anyone any rights, they are his by default.'\nThe incident was seen to highlight the ease with which deepfakes can be made and used, It was also seen to raise questions about the nature and impacts of the sale of digital rights by celebrities and others, and the potential impact of rights sales on jobs in the entertainment industry.\nSystem \ud83e\udd16\nDeepcake\nOperator:  \nDeveloper: Deepcake\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Promote telecoms company\nTechnology: Deepfake - audio, video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Employment\nTransparency: Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://twitter.com/Reuters/status/1440531299387813888\nhttps://www.theregister.com/2022/10/04/bruce_willis_ai_image_deepcake/\nhttps://www.bbc.com/news/technology-63106024\nhttps://www.telegraph.co.uk/world-news/2022/09/28/deepfake-tech-allows-bruce-willis-return-screen-without-ever/\nhttps://www.dailymail.co.uk/tvshowbiz/article-11262131/Bruce-Willis-sells-rights-allow-deepfake-digital-twin-created.html\nhttps://www.wired.co.uk/article/bruce-willis-deepfake-rights-law\nhttps://arstechnica.com/information-technology/2022/09/bruce-willis-sells-deepfake-rights-to-his-likeness-for-commercial-use/\nhttps://www.hollywoodreporter.com/business/digital/bruce-willis-refutes-report-digital-likeness-deepfake-1235231331/\nhttps://www.forbes.com/sites/joshwilson/2022/10/13/deepfake-post-the-bruce-willis-controversy-what-disruption-to-entertainment-could-be-caused/\nRelated \ud83c\udf10\nDeepfake Tom Hanks dental insurance ad promotion\nDeepfake MrBeast iPhone giveaway scam\nPage info\nType: Incident\nPublished: January 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/deepfake-tom-hanks-dental-ad-promotion", "content": "Deepfake Tom Hanks dental insurance ad promotion\nOccurred: October 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn advert for a dental insurance plan supposedly endorsed by actor Tom Hanks was in fact a fake image manipulated using artificial intelligence (AI).\n'There\u2019s a video out there promoting some dental plan with an AI version of me. I have nothing to do with it,' Hanks warned his followers on Instagram, without naming the company or organisation behind the deepfake. The likeness of Hanks appeared to be generated from a 2014 image of the actor owned by the Los Angeles Times, according to Gizmodo. \nThe fracas highlighted the increasing use of deepfake and synthetic media to impersonate celebrities, sometimes in scams, and the difficulty in stopping their creators. \nSet against strikes over the use of AI in entertainment by members of the Screen Actors Guild and American Federation of Television and Radio Artists (SAG-AFTRA), the incident also underscored general challenges facing the entertainment industry and performers by AI.\nSystem \ud83e\udd16\nUnknown\nOperator:  \nDeveloper: \nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Promote insurance plan  \nTechnology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Legality\nTransparency: Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theguardian.com/film/2023/oct/02/tom-hanks-dental-ad-ai-version-fake\nhttps://www.instagram.com/tomhanks/p/Cx2MsH9rt7q/\nhttps://www.engadget.com/tom-hanks-calls-out-dental-ad-for-using-ai-likeness-of-him-161548459.html\nhttps://www.bbc.co.uk/news/technology-66983194\nhttps://news.sky.com/story/tom-hanks-warns-fans-not-to-fall-for-deepfake-advert-using-his-face-12974902\nhttps://www.theregister.com/2023/10/02/tom_hanks_ai_advert/\nhttps://www.nytimes.com/2023/10/02/technology/tom-hanks-ai-dental-video.html\nhttps://gizmodo.com/deep-fake-tom-hanks-is-promoting-a-dental-plan-1850891358\nhttps://abcnews.go.com/GMA/Culture/tom-hanks-warns-fans-fake-ad-featuring-fabricated/story?id=103658615\nRelated \ud83c\udf10\nTaylor Swift speaks in Mandarin deepfake\nDeepfake MrBeast iPhone giveaway scam\nPage info\nType: Incident\nPublished: January 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/researcher-raped-in-horizon-worlds-metaverse", "content": "Researcher 'raped' in Horizon Worlds metaverse\nOccurred: May 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA young woman was sexually assaulted and harassed virtually within an hour of entering Meta's Horizon Worlds metaverse, raising questions about the safety of the Meta platform and other metaverses.\nThe 21-year-old researcher for SumOfUs was raped within one hour of using Horison Worlds, according (pdf) to a report by the non-profit. The woman was repeatedly told to 'turn around so he could do it from behind while users outside the window could see \u2013 all while another user in the room watched and passed around a vodka bottle'.\nWhen a user is touched by another in the metaverse, the hand controllers vibrate, 'creating a very disorienting and even disturbing physical experience during a virtual assault,' the researcher said of the non-consensual act. \nAccording to Meta, the researcher had turned off the metaverse's Personal Boundary feature that is turned on by default and prevents non-friends from coming within 4 feet of one's avatar.\nSystem \ud83e\udd16\nHorizon Worlds Wikipedia profile\nOperator: SumOfUs\nDeveloper: Meta\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Provide virtual social experience\nTechnology: Virtual reality; Safety management system\nIssue: Safety\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nIncident video\nSumOfUs (2022). Metaverse: another cesspool of toxic content (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.businessinsider.com/researcher-claims-her-avatar-was-raped-on-metas-metaverse-platform-2022-5\nhttps://nypost.com/2022/05/27/women-are-being-sexually-assaulted-in-the-metaverse/\nhttps://www.wionews.com/world/21-year-old-woman-virtually-raped-harassed-in-metaverse-report-483043\nhttps://www.independent.co.uk/tech/rape-metaverse-woman-oculus-facebook-b2090491.html\nhttps://www.dailymail.co.uk/sciencetech/article-10857551/Woman-21-virtually-RAPED-stranger-Metas-metaverse-app-report-claims.html\nhttps://nationalpost.com/news/world/researcher-says-she-was-virtually-assaulted-after-just-one-hour-in-the-metaverse\nhttps://www.indiatimes.com/technology/news/a-woman-was-raped-in-zuckerbergs-metaverse-while-users-watched-and-drank-vodka-570966.html\nRelated \ud83c\udf10\nMeta Horizon Worlds beta tester groped by stranger\nYoung girl 'gang raped' by group of metaverse strangers\nPage info\nType: Incident\nPublished: January 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/investing-com-plagiarises-other-websites-using-ai", "content": "Investing.com plagiarises other websites using AI\nOccurred: November-December 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nFinancial news site Investing.com has been caught plagiarising 'wholesale' articles by other financial news sites, calling into question its integrity and highlighting the ease with which AI can be misused.\nOwned by Joffre Capital, Investing.com has increasingly been relying on AI to create its stories as part of an attempt to become the 'Bloomberg of retail investing'. But its AI-generated articles 'often appear to be thinly-veiled copies of human-written stories written elsewhere,' Semafor observed.\nIn one instance, Investing.com published an article about a rise in a crypto token price that used comparable vernacular and identical statistics to one that had been posted less than an hour and a half before on the CryptoNewsLand blog.\nInvesting.com disclosed that its stories were written with the help of AI and reviewed by an editor. But it failed to note or credit anyone except itself. \nSystem \ud83e\udd16\nInvesting.com website\nOperator: Joffre Capital/Investing.com\nDeveloper: \nCountry: Israel; USA  \nSector: Media/entertainment/sports/arts\nPurpose: Generate news stories\nTechnology:  \nIssue: Cheating/plagiarism; Copyright; Ethics\nTransparency: Marketing \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.semafor.com/article/12/10/2023/a-financial-news-site-uses-ai-to-copy-competitors-wholesale\nhttps://futurism.com/investing-accused-ai-plagiarism\nhttps://www.theverge.com/2023/12/11/23996694/investing-com-gets-caught-plagiarizing-financial-news-with-ai\nhttps://www.cryptopolitan.com/financial-news-website-accused-of-ai-generated-copycat-content/\nhttps://www.newstarget.com/2023-12-13-finance-news-site-plagiarism-using-artificial-intelligence.html\nRelated \ud83c\udf10\nRed Ventures AI automated 'journalism'\nGizmodo AI generates error-strewn Star Wars article\nPage info\nType: Incident\nPublished: January 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-invents-newsbreak-christmas-day-murder", "content": "AI invents NewsBreak Christmas Day murder\nOccurred: December 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA false AI-generated article about a fatal Christmas shooting in Bridgeton, New Jersey forced local police to issue a statement rebutting the story.\nA piece by news aggregation site NewsBreak alleged: 'The joyous celebrations of Christmas Day in Bridgeton, New Jersey, were tragically cut short this year. A local resident was found dead with multiple gunshot wounds in the 100 block of West Broad Street.' It went on to say that the Cumberland County Prosecutor's Office was investigating.\nBridgeton police clarified on Facebook said that the since-deleted article was 'entirely false', and that 'Nothing even similar to this story occurred on or around Christmas, or even in recent memory for the area they described'. \nThe article had no bylined author and said in a disclaimer at the bottom of the page 'This post includes content assisted by AI tools. This content was assisted by AI and may contain errors. Please verify critical information with trusted sources.'\nThe incident prompted complaints about NewsBreak's use of AI to generate or recycle false news, and its role in degrading the internet and unfairly sowing distrust in public authorities.\n\u2795 June 2024. NewsBreak confirmed that the source of the false story it recycled was Findplaces (aka findplace.xyz). \nSystem \ud83e\udd16\nUnknown\nOperator: Particle Media/NewsBreak\nDeveloper: Particle Media/NewsBreak\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Rewrite news stories\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Accuracy/reliability; Mis/disinformation\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://boingboing.net/2024/01/03/ai-generated-article-hallucinates-christmas-day-murder-in-small-new-jersey-town.html\nhttps://newjersey.news12.com/viral-article-detailing-christmas-day-murder-bridgeton-fake-made-with-ai\nhttps://www.nj.com/news/2023/12/news-story-about-deadly-christmas-shooting-in-nj-is-fake-made-with-ai-police-say.html\nhttps://eu.app.com/story/news/local/2023/12/28/ai-generated-story-falsely-reports-fatal-shooting-in-cumberland-county/72047169007/\nhttps://news.yahoo.com/police-ai-generated-article-local-193904658.html\nRelated \ud83c\udf10\nMSN publishes 'useless' AI-generated Brandon Hunter obituary\nMicrosoft robot editor confuses Little Mix band members\nPage info\nType: Incident\nPublished: January 2024\nLast updated: June 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chatgpt-incorrectly-diagnoses-most-pediatric-cases", "content": "ChatGPT incorrectly diagnoses most pediatric cases\nOccurred: January 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nChatGPT incorrectly diagnosed over 8 in 10 pediatric case studies, according to a new research study. The findings raised questions about the chatbot's suitability as a diagnostic tool for complex conditions. \nResearchers at Cohen Children's Medical Center, USA, pasted the text of 100 pediatric case challenges published in JAMA Pediatrics and the New England Journal of Medicine between 2013 and 2023 into ChatGPT, with two qualified physician-researchers scoring the AI-generated answers as correct, incorrect, or 'did not fully capture the diagnosis.' \nChatGPT got the right answer in 17 of the 100 cases, was wrong in 72 cases, and failed to fully capture the diagnosis of the remaining 11 cases. Among the 83 wrong diagnoses, 47 (57 percent) were in the same organ system. \nThe findings suggest ChatGPT should not currently be used to assess complex pediatric cases, and that 'more selective training' is required to make it more accurate and reliable. The authors also suggest that chatbots could improve with more real-time access to medical data.\nSystem \ud83e\udd16\nChatGPT chatbot\nOperator: Cohen Children\u2019s Medical Center  \nDeveloper: OpenAI\nCountry: USA\nSector: Health\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Accuracy/reliability\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nBarile J. et al (2023). Diagnostic Accuracy of a Large Language Model in Pediatric Case Studies\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.medpagetoday.com/pediatrics/generalpediatrics/108096\nhttps://thehill.com/policy/healthcare/4387138-chatgpt-incorrectly-diagnosed-more-than-8-in-10-pediatric-case-studies-research-finds/\nhttps://arstechnica.com/science/2024/01/dont-use-chatgpt-to-diagnose-your-kids-illness-study-finds-83-error-rate/\nhttps://medicalxpress.com/news/2024-01-chatgpt-success-pediatric-case.html\nhttps://www.axios.com/2024/01/03/ai-fails-diagnosing-childrens-cases\nhttps://www.beckershospitalreview.com/patient-safety-outcomes/chatgpt-missed-8-in-10-pediatric-diagnoses-study-finds.html\nRelated \ud83c\udf10\nChatGPT provides inaccurate medication query responses\nChatGPT fails at recommending cancer treatment\nPage info\nType: Issue\nPublished: January 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/google-bard-makes-factual-error-about-james-webb-space-telescope", "content": "Google Bard makes factual error about the James Webb Space Telescope\nOccurred: February 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGoogle's Bard chatbot wrongly claimed the James Webb Space Telescope was used to take the first pictures of exoplanets, calling into question its accuracy and reliability, and damaging Google's reputation. \nBard was anounced by Google CEO Sandar Pillai on February 2023 using a promotional video showing which satellite first took pictures of a planet outside the Earth's solar system. But the information in the promotional video was inaccurate, according to Reuters.\nGiven the prompt: 'What new discoveries from the James Webb Space Telescope (JWST) can I tell my 9-year old about?', Bard (since renamed Gemini) suggested the JWST was used to take the first pictures of a planet outside the Earth's solar system, or exoplanets. The first pictures of exoplanets were actually taken by the European Southern Observatory's Very Large Telescope (VLT) in 2004.\nThe JWST mistake and OpenAI's perceived lead in large language models and generative chatbots, specifically ChatGPT, reinforced views that Google was moving too slowly. Google owner Alphabet's stock price fell USD 100 billion in market value within hours. \nAccording to a Bloomberg report quoted by The Verge, Google employees asked to test the system had criticised it as 'worse than useless' and 'a pathological liar'. \nSystem \ud83e\udd16\nGoogle Bard chatbot\nOperator: Alphabet/Google\nDeveloper:  Alphabet/Google\nCountry: USA\nSector: Technology\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Accuracy/reliability\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reuters.com/technology/google-ai-chatbot-bard-offers-inaccurate-information-company-ad-2023-02-08/\nhttps://www.newscientist.com/article/2358426-google-bard-advert-shows-new-ai-search-tool-making-a-factual-error/\nhttps://mashable.com/article/google-bard-james-webb-telescope-false-fact\nhttps://www.theverge.com/2023/2/8/23590864/google-ai-chatbot-bard-mistake-error-exoplanet-demo\nhttps://uk.pcmag.com/news/145338/no-thats-wrong-googles-bard-ai-demo-spouts-incorrect-info\nhttps://www.businessinsider.com/google-ad-ai-chatgpt-rival-bard-gives-inaccurate-answer-2023-2\nRelated \ud83c\udf10\nAustralian academics make false AI-generated allegations\nGoogle Bard invents legal citations for former Donald Trump lawyer Michael Cohen\nPage info\nType: Incident\nPublished: January 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/michael-cohen-supplies-fake-ai-legal-citations-to-lawyer", "content": "Michael Cohen supplies fake AI legal citations to lawyer \nOccurred: November-December 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGoogle's Bard chatbot generated false legal citations that were used in a legal filing involving Donald Trump's former lawyer Michael Cohen, highlighting concerns about the system's propensity to 'hallucinate' 'facts' and users' poor understanding of its risks.\nIn November 2023, Michael Cohen submitted legal citations he believed to be accurate to his lawyer David Schwartz that he thought might help his request for an early end to court supervision. Cohen had pled guilty to tax evasion and campaign finance violations in 2018 and served time in prison. \nHowever, a court filing (pdf) revealed the citations had been generated by Google's Bard chatbot, and were false - an admission made after the judge overseeing Cohen\u2019s case had said that he could not find any of the decisions Schwartz had cited and demanded an explanation. \nIn a sworn statement, Cohen said: 'As a non-lawyer, I have not kept up with the emerging trends (and related risks) in legal technology and did not realize that Google Bard was a generative text service that, like Chat-GPT, could show citations and descriptions that looked real but actually were not'.\nSystem \ud83e\udd16\nGoogle Bard chatbot\nOperator: Michael Cohen\nDeveloper: Microsoft\nCountry: USA\nSector: Business/professional services\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Accuracy/reliability\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.nytimes.com/2023/12/29/nyregion/michael-cohen-ai-fake-cases.html\nhttps://www.washingtonpost.com/technology/2023/12/29/michael-cohen-ai-google-bard-fake-citations/\nhttps://www.reuters.com/legal/ex-trump-fixer-michael-cohen-says-ai-created-fake-cases-court-filing-2023-12-29/\nhttps://www.forbes.com/sites/alisondurkee/2023/12/29/ex-trump-fixer-michael-cohen-admits-he-accidentally-used-google-bard-to-put-fake-cases-into-legal-filing\nhttps://www.theguardian.com/us-news/2023/dec/29/michael-cohen-trump-lawyer-fake-citations\nhttps://www.engadget.com/former-trump-fixer-michael-cohen-admits-using-google-bard-to-cite-bogus-court-cases-184125792.html\nhttps://www.fastcompany.com/90756868/report-metaverse-meta-virtual-sexual-assault-abuse\nRelated \ud83c\udf10\nChatGPT invents case citations in legal filings\nChatGPT makes up research claiming guns are not harmful to kids\nPage info\nType: Incident\nPublished: January 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ubisoft-ghostwriter-seen-to-replace-scriptwriting-jobs", "content": "Ubisoft Ghostwriter seen to replace scriptwriting jobs\nOccurred: February 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nFrench games developer Ubisoft sparked controversy about the nature and future of screenwriting by unveiling an AI tool intended to help its writers create background dialogue, 'barks', and other activities.\nAccording to Ubisoft, Ghostwriter is supposed to save time for creative writers by generating first drafts of barks (generic lines from non-player characters triggered by players), allowing them 'more time to polish the narrative elsewhere'.\nWhile some writers welcomed the tool, a number complained that Ghostwriter is a trojan horse intended to trial AI across Ubisoft, and that they stood to lose their jobs as AI was used more extensively by the company. \nOthers said it raised questions about the quality of scriptwriting, which would likely become more monotonous and less distinctive.\nSystem \ud83e\udd16\nUbisoft website\nUbisoft Wikipedia profile\n\nDocuments \ud83d\udcc3\nUbisoft (2023). The Convergence of AI and Creativity: Introducing Ghostwriter\nOperator:\nDeveloper: Ubisoft La Forge  \nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Generate spoken lines\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Employment; Governance\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.gameshub.com/news/news/ubisoft-backlash-ai-dialogue-writing-tool-ghostwriter-2610382/\nhttps://www.rockpapershotgun.com/ubisoft-unveil-ai-dialogue-writing-tool-prompting-debate-among-developers\nhttps://www.toolify.ai/ai-news/ubisofts-ghostwriter-ai-benefiting-or-replacing-scriptwriters-8594\nhttps://www.eurogamer.net/ubisoft-sparks-debate-by-unveiling-ai-tool-to-aid-scriptwriting\nhttps://www.theverge.com/2023/5/4/23700619/ai-game-development-jobs-gdc-2023\nhttps://www.axios.com/2023/03/27/ubisoft-ai-ghostwriter\nhttps://www.pocketgamer.biz/news/81197/ubisoft-clarify-the-use-of-ai-script-writing-ghostwriter/\nhttps://metro.co.uk/2023/03/22/ubisoft-ai-writing-tool-will-not-be-writing-its-own-video-game-scripts-18484432/\nRelated \ud83c\udf10\nMicrosoft replaces journalists with AI\nJust Eat uses algorithm to fire employees\nPage info\nType: Issue\nPublished: January 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/young-girl-sexually-attacked-by-group-of-metaverse-strangers", "content": "Young girl 'gang raped' by group of metaverse strangers\nOccurred: 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA young girl was allegedly sexually assaulted by a group of strangers on Meta's Horizon Worlds metaverse platform, leaving her traumatised and prompting a police investigation.\nThe virtual incident did not result in physical harm to the girl, who was reputedly under the age of 16 at the time of the incident and was wearing a headset and using an avatar, but caused 'psychological trauma' 'similar to that of someone who has been physically raped', said the Daily Mail, quoting a senior police source. Virtual reality experiences are designed to be completely immersive.\nThe incident is thought to be the first time in the UK that a virtual sexual offence has been investigated by police. But British authorities fear that it may prove impossible to prosecute the case under existing laws, which define sexual assault as non-consensual 'physical touching' in a sexual manner. Others questioned whether it constituted good use of police time and resources.\nHorizon Worlds provides an automated 'personal boundary' for every user which is suppoded to keep strangers a safe distance away from users. It may have failed to work in this instance.\nA child safety expert at the UK's National Society for the Prevention of Cruelty to Children (NSPCC) told Sky News that tech companies were rolling out products too quickly, without prioritising the safety of children.\nSystem \ud83e\udd16\nHorizon Worlds Wikipedia profile\nOperator:  \nDeveloper: Meta\nCountry: UK\nSector: Media/entertainment/sports/arts\nPurpose: Provide virtual social experience\nTechnology: Virtual reality; Safety management system\nIssue: Safety\nTransparency: Governance\nRegulation \u2696\ufe0f\nUK Online Safety Bill\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.dailymail.co.uk/news/article-12917329/Police-launch-investigation-kind-virtual-rape-metaverse.html\nhttps://www.bbc.co.uk/news/technology-67865327\nhttps://news.sky.com/story/police-investigate-sexual-abuse-of-young-girls-avatar-in-the-metaverse-prompting-nspcc-warning-13041003\nhttps://www.livemint.com/news/world/uk-teen-virtually-gang-raped-in-metaverse-game-cops-begin-probe-11704257068800.html\nhttps://www.euronews.com/next/2024/01/04/british-police-launch-first-investigation-into-virtual-rape-in-metaverse\nhttps://www.theguardian.com/commentisfree/2024/jan/05/metaverse-sexual-assault-vr-game-online-safety-meta\nhttps://www.unilad.com/news/crime/child-attacked-metaverse-online-game-711442-20240102\nRelated \ud83c\udf10\nMeta Horizon Worlds beta tester groped by stranger\nResearcher 'raped' in Horizon Worlds metaverse\nPage info\nType: Incident\nPublished: January 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/database-of-16000-artists-used-to-train-midjourney", "content": "Database of 16,000+ artists used to train Midjourney\nOccurred: February 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA database listing the names of over 16,000 artists, including Banksy, David Hockney, Frida Kahlo, Yayoi Kusama, and Damian Hirst, were purportedly used to train the Midjourney image generator.\nThe 'Midjourney Style List' was allegedly used during a process of refining the model's ability to mimic works of the selected artists and their styles. These outputs were then prominently featured as reference material for image creation. \nThe list was first published to a Discord server in February 2022 by Midjourney CEO David Holz, who welcomed the addition of the artists' names to the training of the model. Part of the list was included in a court document (pdf) filed late November 2023 as part of a class-action lawsuit against DeviantArt, Midjourney, Stability AI, and Runway AI.\nThese models made use of LAION-5B, a nonprofit, publicly available database that indexes more than five billion images from across the Internet, including the work of many artists.\nThe emergence of the list raised questions about possible copyight violations by Midjourney and the other named entities. It also reinvigorated debate about copyright and consent in the generation of AI images.\nSystem \ud83e\udd16\nMidjourney image generator\nLAION-5B dataset\nOperator:  \nDeveloper: Midjourney\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Train model\nTechnology: Database; Machine learning\nIssue: Copyright; Ethics\nTransparency: Governance\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nAndersen et al v. Stability AI Ltd. et al (pdf)\nStable Diffusion complaint exhibits (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.artnews.com/art-news/news/midjourney-ai-artists-database-1234691955/\nhttps://www.theartnewspaper.com/2024/01/04/leaked-names-of-16000-artists-used-to-train-midjourney-ai\nhttps://www.creativebloq.com/news/ai-list\nhttps://www.nbcnews.com/tech/tech-news/famous-artists-trained-ai-generator-viral-list-rcna131995\nhttps://hyperallergic.com/864947/database-of-artists-used-to-train-ai-leaks-to-the-public/\nRelated \ud83c\udf10\nMidjourney reproduces copyright-protected film images\nDeepfake Pope Francis wears white puffa jacket\nPage info\nType: Incident\nPublished: January 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/midjourney-reproduces-copyright-protected-film-images", "content": "Midjourney v6 reproduces copyright-protected film images\nOccurred: December 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nVersion 6 of image generator Midjourney was found to be closely reproducing high-resolution copyrighted original images that had been used for its training, prompting accusations of copyright abuse.\nMidjourney users took to social media to share images showing scenes and individuals from films generated by prompts such as 'Joaquin Phoenix Joker movie, 2019, screens from movie, movie scene.' \nIn many cases, the generated images appear to be slightly modified screenshots with minor variations such as hand gesture or camera angle, indicating Midjourney v6 had been trained repeatedly and intensively on the same data to maximise performance, in a process known as 'overtraining' or 'overfitting'. \nMidjourney later updated its terms of service so that the responsibility for potentially copyright-infringing images generated with the system is shifted to the generating user, resulting in further criticism.\nSystem \ud83e\udd16\nMidjourney image generator\nOperator: Reid Southen\nDeveloper: Midjourney\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Generate images\nTechnology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Copyright\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.thestreet.com/technology/users-of-midjourney-text-to-image-site-claim-issues-with-new-update\nhttps://the-decoder.com/midjourney-will-find-you-and-collect-that-money-if-you-infringe-any-ip-with-v6/\nhttps://designtaxi.com/news/426003/Midjourney-s-Latest-Hyperrealistic-Model-Is-Under-Fire-For-Apparent-Plagiarism/\nhttps://spectrum.ieee.org/midjourney-copyright\nhttps://garymarcus.substack.com/p/an-artist-fights-back-and-midjourney\nRelated \ud83c\udf10\nVermeer 'Girl with a Pearl Earring' AI facsimile\nDeepfake Pope Francis wears white puffa jacket\nPage info\nType: Incident\nPublished: January 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-alters-keith-harings-unfinished-painting", "content": "AI alters Keith Haring's Unfinished Painting \nOccurred: December 2023-January 2024\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Keith Haring painting about the AIDS crisis that was altered using artificial intelligence caused controversy, with users labelling the activity as unethical and self-promotional.\nTwitter user @DonnellVillage responded to a tweet about Haring's famous 1989 painting by writing, 'The story behind this painting is so sad! Now using AI we can complete what he couldn't finish!'  \nReaction to the image, which quickly went viral, was mixed with some users praising the altered work whilst others called it 'evil', 'rage bait', 'homophobic', and its creator 'vile'. It was a 'tone-deaf and some might say ignorant modification which completes the painting ignores the piece's context, disregards its intent, and destroys its meaning,' according to Euronews.\nHaring, who died aged 31 years-old from AIDS shortly after the painting was done, had said the work is a completed self-portrait that was intentionally left to look 'unfinished' ashe knew he would not be able to complete it before his death.\nSystem \ud83e\udd16\nUnknown\nOperator: @DonnelVillager\nDeveloper:\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Complete artwork\nTechnology: \nIssue: Cheating/plagiarism; Ethics/values\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.nbcnews.com/tech/keith-haring-painting-ai-backlash-ruined-meaning-rcna131974\nhttps://knowyourmeme.com/memes/events/keith-haring-unfinished-painting-ai-controversy\nhttps://www.euronews.com/culture/2024/01/04/why-is-an-ai-altered-keith-haring-painting-sparking-outrage\nhttps://www.reddit.com/r/ChatGPT/comments/18vk8k9/comment/kfwi37x/\nRelated \ud83c\udf10\nVermeer Girl with a Pearl Earring AI facsimile\nGetty Images sues Stability AI for copyright abuse\nPage info\nType: Issue\nPublished: January 2024", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/meta-automated-moderation-wrongly-removes-israel-hamas-videos", "content": "Meta automated moderation wrongly removes Israel-Hamas videos\nOccurred: October-December 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nMeta\u2019s automated content moderation system unfairly removed videos depicting hostages, injured civilians, and possible casualties in the Israel-Hamas war from Facebook and Instagram, drawing criticism from its Oversight Board and others.\nIn one instance, a video depicted an Israeli woman pleading with kidnappers not to kill her during the October 7 attack on Israel by Hamas. In another, a video posted to Instagram showed what appears to be the aftermath of a strike on or near Al-Shifa Hospital in Gaza City during Israel's ground offensive in the north of the Gaza Strip, including killed or injured Palestinians, including children. \nIn both cases, the videos were automatically removed and later reinstated. Meta's independent Oversight Board ruled that the videos should not have been removed, and found that the company had lowered its content moderation thresholds to more easily catch violating content following the attack on October 7, a decision that 'also increased the likelihood of Meta mistakenly removing non-violating content related to the conflict.'\nThe Board also argued that inadequate human-led moderation, especially in non-English languages, during these types of crises could lead to the 'incorrect removal of speech that may be of significant public interest' and that Meta should have been swifter to allow content 'shared for the purposes of condemning, awareness-raising, news reporting or calling for release' with a warning screen applied.\nSystem \ud83e\udd16\nFacebook website\nInstagram website\nOperator:  \nDeveloper: Meta\nCountry: Israel; Palestine\nSector: Politics\nPurpose: Detect & remove content violations\nTechnology: Content moderation system; Machine learning\nIssue: Governance; Human/civil rights;\nTransparency: Governance\nRegulation \u2696\ufe0f\nMeta Violent and Graphic Content Community Standard\nInvestigations, assessments, audits \ud83e\uddd0\nOversight Board (2023). Oversight Board issues first expedited decisions about Israel-Hamas conflict\nOversight Board (2023). 2023-049-IG-UA - Al-Shifa Hospital\nOversight Board (2023). 2023-050-FB-UA - Hostages kidnapped from Israel\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reuters.com/technology/meta-oversight-board-says-israel-hamas-videos-should-not-have-been-removed-2023-12-19/\nhttps://qz.com/metas-oversight-board-says-ai-alone-isnt-enough-to-mode-1851113842\nhttps://www.wired.com/story/oversight-board-meta-israel-hamas/\nhttps://www.theverge.com/2023/12/19/24007655/meta-oversight-board-removed-videos-israel-hamas-conflict\nhttps://www.timesofisrael.com/oversight-board-rules-facebook-wrongfully-removed-2-israel-hamas-war-posts/\nhttps://thehill.com/policy/technology/4346059-facebook-instagram-posts-israel-gazar-review-meta/\nRelated \ud83c\udf10\nMeta 'systemically' censors pro-Palestinian content\nInstagram inserts 'terrorist' into Palestinians' biography translations\nPage info\nType: Incident\nPublished: January 2024\nLast updated: February 2023", "year": "0"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/clinical-grade-ai-stress-detector-fails-to-work", "content": "'Clinical-grade' AI stress detector fails to work\nOccurred: November-December 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn AI-powered test that listens for signs of stress in people\u2019s voices and claimed to be 'clinical grade' provides inconsistent results when tested on the same person twice.\nCigna's StressWaves Test is a free online tool that uses voice recognition to evaluate and reveal stress levels in 90 seconds by analysing stress through acoustic (sounds, such as tone, pitch, pause, etc.) and semantic (word choices and syntax) patterns. Levels range from 'Extremely Stressed' to 'No Stress', along with a portrait that visualises the effect of stress on the user's body and mind.\nCigna marketed StressWaves as 'clinical grade' tool. However, according a detailed, independent evaluation by Arizona State University researchers, the tool gives inconsistent results when tested on the same person twice, calling into question its efficacy and marketing claims. The researchers also call out Cigna's reluctance to share sufficient validation data, making it difficult to evaluate the model.\nSystem \ud83e\udd16\nCigna StressWaves\nCigna StressWaves Test brochure (pdf)\nOperator: Cigna\nDeveloper: Ellipsis Health\nCountry: USA\nSector: Health\nPurpose: Evaluate stress level\nTechnology: Voice recognition\nIssue: Accuracy/reliability\nTransparency: Black box; Governance; Marketing\nInvestigation, assessment, audit \ud83e\uddd0\nYawar B.A., Liss J., Berisha V. (2023). Reliability and validity of a widely-available AI tool for assessment of stress based on speech\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.newscientist.com/article/2408873-clinical-grade-ai-stress-detector-doesnt-work-study-suggests/\nRelated \ud83c\udf10\nCigna PxDx accelerates health insurance claim denials\nApple user depression, autism, dementia detection\nPage info\nType: Incident\nPublished: December 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/nyc-dept-of-education-bans-chatgpt-due-to-student-learning-concerns", "content": "NYC Dept of Education bans ChatGPT over student learning concerns\nOccurred: January-May 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe use of ChatGPT by students and teachers was banned by New York City's education department over 'negative impacts on student learning and concerns about safety and accuracy of content.'\nChatGPT's ability to generate high quality essay responses across a wide range of subjects sparked fears amongst educators that their writing assignments could quickly become obsolete, and that the system could encourage cheating and plagiarism, and reduce their need to build critical-thinking and problem-solving skills.\nThe ban appears to have been broadly supported by teachers and parents in New York and across the US. However, some teachers argued banning ChatGPT may prove counterproductive, with students able to learn higher-level critical thinking using the bot.\nThe NYC Department of Education lifted the ban in May 2023, saying it would 'encourage and support our educators and students as they learn about and explore this game-changing technology while also creating a repository and community to share their findings across our schools.'\nSystem \ud83e\udd16\nChatGPT chatbot\nOperator: New York City Department of Education\nDeveloper: OpenAI\nCountry: USA\nSector: Education\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning  \nIssue: Accuracy/reliability; Cheating/plagiarism; Safety\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.chalkbeat.org/newyork/2023/1/3/23537987/nyc-schools-ban-chatgpt-writing-artificial-intelligence/\nhttps://www.vice.com/en/article/y3p9jx/nyc-bans-students-and-teachers-from-using-chatgpt\nhttps://www.theverge.com/2023/1/5/23540263/chatgpt-education-fears-banned-new-york-city-safety-accuracy\nhttps://www.theguardian.com/us-news/2023/jan/06/new-york-city-schools-ban-ai-chatbot-chatgpt\nhttps://www.chalkbeat.org/newyork/2023/5/18/23727942/chatgpt-nyc-schools-david-banks/\nhttps://www.edweek.org/technology/new-york-city-does-about-face-on-chatgpt-in-schools/2023/05\nhttps://www.businessinsider.com/nyc-public-schools-reverse-ban-on-chatgpt-in-the-classroom-2023-5\nhttps://www.nbcnews.com/tech/chatgpt-ban-dropped-new-york-city-public-schools-rcna85089\nRelated \ud83c\udf10\nChatGPT falsely claims to write student essays\nTurnitin AI writing detection\nPage info\nType: Incident\nPublished: December 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/singapore-pm-lee-hsien-loong-crypto-promotion-deepfake", "content": "Singapore PM Lee Hsien Loong crypto promotion deepfake\nOccurred: July-December 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA 'completely bogus' deepfake video of Singapore Prime Minister Lee Hsien Loong promoting an investment product raised concerns about the use of AI to disrupt politics and create disinformation.\nIn the altered video, Mr Lee is allegedly interviewed by a presenter from Chinese news network CGTN about a 'revolutionary investment platform designed by Elon Musk' that was purportedly approved by the Singapore government. \nThe video ends with the presenter urging viewers to click on a link to register for the platform, to earn 'passive income'. The deepfake video appears to have been manipulated from a real CGTN interview with Mr Lee in March 2023.\n'(Scammers) transform real footage of us taken from official events into very convincing but completely bogus videos of us purporting to say things that we have never said,' PM Lee responded on Facebook.\nSystem \ud83e\udd16\nUnknown\nOperator:  \nDeveloper: \nCountry: Singapore\nSector: Banking/finanacial services; Politics\nPurpose: Defraud\nTechnology: Deepfake - audio, video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning  \nIssue: Mis/disinformation; Fraud\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.channelnewsasia.com/singapore/deepfake-video-pm-lee-investment-scam-4012946\nhttps://sg.news.yahoo.com/lee-hsien-loong-warns-deepfake-crypto-trading-video-cgtn-interview-035549753.html\nhttps://www.scmp.com/news/asia/southeast-asia/article/3246629/singapore-pm-lee-issues-warning-after-deepfake-video-him-promoting-crypto-investment-emerges\nhttps://www.firstpost.com/world/singapore-pm-reacts-to-viral-deep-fake-videos-of-him-endorsing-crypto-scams-13556202.html\nhttps://www.todayonline.com/singapore/deepfake-video-pm-lee-emerges-amid-growing-prevalence-manipulated-content-2333791\nhttps://www.youtube.com/watch?v=sUwNRdviY_g\nRelated \ud83c\udf10\nDeepfake Justin Trudeau endorses Petro-Canada scam\nAudio deepfake fraudulently impersonates CEO\nPage info\nType: Incident\nPublished: December 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/omegaverse-fan-fiction-used-to-train-openai-models", "content": "Omegaverse fan fiction used to train OpenAI models\nOccurred: May 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGPT-3, the language model powering the free version of ChatGPT, has been shown to have been trained on the Omegeverse, a subgenre of speculative erotic fiction.\nWIRED reported that fanfiction writers discovered that writing assistant app Sudowrite, which uses 'several variants' of OpenAI's GPT-3.5, was generating text describing a highly specific sexual act called 'knotting', an Omegaverse term in which a male 'Alpha's' penis locks itself inside a vagina during sex. The sexual act was also described by ChatGPT, according to a test conducted by Futurism. \nThe findings prompted complaints from Omegaverse authors about the ethics of using their writing to train OpenAI's large language models, without their knowledge or permission. It also highlighted concerns about the safety of ChatGPT. \nSystem \ud83e\udd16\nChatGPT chatbot\nGPT-3 large language model\nGPT-4 large language model\nOperator: Sudowrite\nDeveloper: OpenAI\nCountry: Global\nSector: Media/entertainment/sports/arts\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Copyright\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reddit.com/r/AO3/comments/z9apih/sudowrites_scraping_and_mining_ao3_for_its\nhttps://www.wired.com/story/fanfiction-omegaverse-sex-trope-artificial-intelligence-knotting/\nhttps://techcrunch.com/2023/06/13/fan-fiction-writers-are-trolling-ais-with-omegaverse-stories/\nhttps://futurism.com/chat-gpt-sex-omegaverse\nhttps://goodereader.com/blog/technology/omegaverse-writers-up-in-arms-against-generative-ai-applications\nhttps://www.businessinsider.com/chatbot-training-data-chatgpt-gpt4-books-sci-fi-artificial-intelligence-2023-5\nRelated \ud83c\udf10\nSarah Silverman sues OpenAI for violating copyright\nKenyan workers paid under USD 2 an hour to de-toxify ChatGPT\nPage info\nType: Incident\nPublished: December 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chatgpt-makes-up-research-claiming-guns-are-not-harmful-to-kids", "content": "ChatGPT makes up research claiming guns are not harmful to kids\nOccurred: March 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nChatGPT cited fake research papers when prompted to generate an essay arguing that access to guns does not raise the risk of child mortality.\nMichelle A. Williams, dean of the faculty at the Harvard T.H. Chan School of Public Health, described in USA Today how ChatGPT 'produced a well-written essay citing academic papers from leading researchers \u2013 including my colleague, a global expert on gun violence.'\nHowever, it also 'used the names of real firearms researchers and real academic journals to create an entire universe of fictional studies in support of the entirely erroneous thesis that guns aren\u2019t dangerous to kids.' When challenged, ChatGPT responded: 'I can assure you that the references I provided are genuine and come from peer-reviewed scientific journals.'\nThe incident highlighted ChatGPT's tendency to 'hallucinate' plausible-sounding false facts and sources, and prompted concerns about the bot's potential impact on public health.\nSystem \ud83e\udd16\nChatGPT chatbot\nOperator: USA Today\nDeveloper: OpenAI\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Accuracy/reliability\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nByrd D. (2023). Artificial Intelligence Is a Threat to Society\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://eu.usatoday.com/story/opinion/2023/03/16/chatgpt-openai-dangers-misinformation-threat-facts/11464869002/ \nhttps://www.hsph.harvard.edu/michelle-williams/2023/03/16/chatgpt-made-up-research-claiming-guns-arent-harmful-to-kids-how-far-will-we-let-ai-go/\nRelated \ud83c\udf10\nFacebook advertises military gear during US attempted coup\nGuns disguised as cases for sale on Facebook Marketplace\nPage info\nType: Incident\nPublished: December 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/kenyan-workers-paid-under-usd-2-an-hour-to-de-toxify-chatgpt", "content": "Kenyan workers paid under USD 2 an hour to de-toxify ChatGPT\nOccurred: November 2021-February 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nKenyan workers were paid under USD 2 an hour to sift through large amounts of extremely graphic content to help build a tool that tags problematic content on ChatGPT.\nA Time investigation revealed that OpenAI was outsourcing the labelling of images and text describing in graphic detail sexual abuse, bestiality, self-harm, incest, hate speech, torture, murder, and violence, to Sama, a self-styled 'ethical AI' company based in San Francisco. Sama employees were paid between $1.32 to $2 an hour to do the work. The data was then used to train ChatGPT to keep it from responding with problematic answers.\nThe work reportedly caused severe distress for some data labellers, with one employee calling the work he had to do reading and labeling text for OpenAI, including reading a graphic description of a man having sex with a dog in the presence of a young child, as \u2018torture\u2019. \nSama employs workers in Kenya, Uganda, and India to label data for Silicon Valley clients, inclusing Google, Meta, and Microsoft. A February 2022 TIME investigation revealed low pay, poor working conditions and alleged union-busting at Sama's office in Nairobi, Kenya for its team moderating content for Facebook.\nSystem \ud83e\udd16\nChatGPT chatbot\nhttps://www.sama.com/blog/our-vision-is-computer-vision/\nOperator: Sama AI/Samasource\nDeveloper: OpenAI\nCountry: Kenya\nSector: Business/professional services\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Employment\nTransparency: Governance\nInvestigations, assessments, audits \ud83e\uddd0\nTime (2023). OpenAI Used Kenyan Workers on Less Than $2 Per Hour to Make ChatGPT Less Toxic\nResearch, advocacy \ud83e\uddee\nWidder D.G., West S., Whittaker M. (2023). Open (For Business): Big Tech, Concentrated Power, and the Political Economy of Open AI\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://metro.co.uk/2023/01/19/openai-paid-kenyan-workers-less-than-2-an-hour-to-make-chatgpt-safe-18130720/\nhttps://afrotech.com/kenyan-workers-openai-chatgpt\nhttps://www.businessinsider.com/openai-kenyan-contract-workers-label-toxic-content-chatgpt-training-report-2023-1?r=US&IR=T\nhttps://www.rollingstone.com/culture/culture-news/chatgtp-moderators-labeling-violent-content-ptsd-1234662975/\nhttps://www.datanami.com/2023/01/20/openai-outsourced-data-labeling-to-kenyan-workers-earning-less-than-2-per-hour-time-report/\nhttps://www.wsj.com/articles/chatgpt-openai-content-abusive-sexually-explicit-harassment-kenya-workers-on-human-workers-cf191483\nhttps://qz.com/open-ai-underpaid-200-kenyans-to-perfect-chatgpt-1850005025\nRelated \ud83c\udf10\nSama 'ethical' data labeling, content moderation\nTesla workers share customers' private camera recordings\nPage info\nType: Issue\nPublished: December 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/meta-systemically-censors-pro-palestinian-content", "content": "Meta accused of 'systemically' censoring pro-Palestinian content\nOccurred: October-December 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nMeta stands accused of rountinely engaging in 'six key patterns of undue censorship' of content supporting Palestine during Israel's war with Hamas.\nHuman Rights Watch (HRW) analysed over one thousand instances of online censorship from more than 60 countries, identifying six common patterns of censorship: content removals, suspension or deletion of accounts, inability to engage with content, inability to engage with content, inability to follow or tag accounts, restriction on the use of features such as Instagram and Facebook Live, and shadow-banning. \nAccording to HRW, the removal of peaceful expressions of support for Gazans is the result of 'flawed Meta policies and their inconsistent and erroneous implementation, overreliance on automated tools to moderate content, and undue government influence over content removals'. \nMeta responded by saying: 'This report ignores the realities of enforcing our policies globally during a fast-moving, highly polarised and intense conflict, which has led to an increase in content being reported to us.'\nSystem \ud83e\udd16\nMeta website\nMeta Wikipedia profile\nOperator: Human Rights Watch\nDeveloper: Meta/Facebook; Meta/Instagram\nCountry: Palestine; Israel   \nSector: Politics\nPurpose: Moderate content\nTechnology: Content moderation system\nIssue: Accuracy/reliability; Bias/discrimination - race, ethnicity; Freedom of expression - censorship\nTransparency: Governance\nInvestigations, assessments, audits \ud83e\uddd0\nHuman Rights Watch (2023). Meta\u2019s Broken Promises. Systemic Censorship of Palestine Content on Instagram and Facebook\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theguardian.com/technology/2023/dec/21/meta-facebook-instagram-pro-palestine-censorship-human-rights-watch-report\nhttps://www.aljazeera.com/news/2023/12/21/meta-stifling-pro-palestine-voices-on-social-media-hrw\nhttps://edition.cnn.com/2023/12/21/tech/meta-human-rights-watch-pro-palestine-content/index.html\nhttps://mashable.com/article/human-rights-watch-meta-palestine-censorship-report\nhttps://www.middleeastmonitor.com/20231223-human-rights-watch-reports-content-in-support-of-palestine-on-meta-being-censored/\nhttps://www.forbes.com/sites/emmawoollacott/2023/12/22/meta-suppressing-peaceful-expression-on-palestinian-conflict/\nhttps://thehill.com/policy/technology/4374224-human-rights-watch-finds-systemic-censorship-of-palestinian-content-on-meta-platforms/\nRelated \ud83c\udf10\nInstagram, Twitter remove, block Palestinian posts\nWhatsApp AI stickers generate Palestinian kids with guns\nPage info\nType: Incident\nPublished: December 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/asylum-claim-rejected-by-french-authorities-using-google-bard", "content": "Asylum claim rejected by French authorities using Google Bard\nOccurred: September 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn Afghan refugee in Tehran had her asylum application rejected by an official at France's Ministry of the Interior using Google Bard, resulting in a government investigation.\nAccording to Le Canard Encha\u00een\u00e9, the official told an adminstrative tribunal in Nantes that he had used Google's Bard chatbot to process the refugee's asylum claim. The bot informed him on the basis of the information shared with it that the girl was not eligible for family reunification as she was three months too old.\nLe Canard Encha\u00een\u00e9 later repeated the exercise based on the official's testimony, only to find it approved the claim. \nThe incident raised questions about the suitability of government officials using public tools to assess sensitive asylum claims, and about the accuracy and robustness of Google Bard.\nSystem \ud83e\udd16\nGoogle Bard chatbot\nOperator: French Office for Immigration and Integration\nDeveloper: Alphabet/Google\nCountry: France\nSector: Govt - immigration\nPurpose: Process asylum claims\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Accuracy/reliability\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://twitter.com/canardenchaine/status/1709512434761994363\nhttps://www.causeur.fr/quand-chat-gpt-refoule-les-migrants-a-la-frontiere-267321\nhttps://pastebin.com/ENFGeErT\nRelated \ud83c\udf10\nInaccurate auto translation denies Pashto-speaking refugee asylum\nAI translations jeopardise US asylum applications\nPage info\nType: Incident\nPublished: December 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/the-new-york-times-sues-openai-microsoft-over-copyright-abuse", "content": "The New York Times sues OpenAI, Microsoft over copyright abuse\nOccurred: December 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe New York Times filed a lawsuit accusing OpenAI and Microsoft of copyright infringement, alleging that the companies\u2019 artificial intelligence technology illegally copied millions of Times articles to train ChatGPT and other services.\nThe NYT's legal complaint said Microsoft and OpenAI\u2019s 'unlawful use of The Times\u2019s work to create artificial intelligence products that compete with it threatens The Times\u2019s ability to provide that service.' It also argued that 'they gave Times content particular emphasis' while seeking 'to free-ride on The Times\u2019s massive investment in its journalism by using it to build substitutive products without permission or payment.'\n'There is nothing \u2018transformative\u2019 about using The Times\u2019s content without payment to create products that substitute for The Times and steal audiences away from it,' the NYT said in its complaint. 'Because the outputs of Defendants\u2019 GenAI models compete with and closely mimic the inputs used to train them, copying Times works for that purpose is not fair use.'\nThe lawsuit is the latest in a string of suits seeking to limit the scraping of content from across the internet without acknowledgement, permission, or compensation in order to train generative AI systems, and was seen as an escalation of an ongoing fight between authors, news publishers, artists, designers, and musicians, and AI system developers. \nSystem \ud83e\udd16\nChatGPT chatbot\nMicrosoft Copilot chatbot\nOperator: The New York Times Company\nDeveloper: OpenAI\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Copyright\nTransparency: Governance\nRegulation\nUS Digital Millennium Copyright Act\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nThe New York Times Company v Microsoft, OpenAI (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.wsj.com/tech/ai/new-york-times-sues-microsoft-and-openai-alleging-copyright-infringement-fd85e1c4\nhttps://www.nytimes.com/2023/12/27/business/media/new-york-times-open-ai-microsoft-lawsuit.html\nhttps://www.bbc.co.uk/news/technology-67826601\nhttps://edition.cnn.com/2023/12/27/tech/new-york-times-sues-openai-microsoft/index.html\nhttps://www.reuters.com/legal/transactional/ny-times-sues-openai-microsoft-infringing-copyrighted-work-2023-12-27/\nhttps://apnews.com/article/nyt-new-york-times-openai-microsoft-6ea53a8ad3efa06ee4643b697df0ba57\nRelated \ud83c\udf10\nSarah Silverman sues OpenAI for violating copyright\nMichael Chabon sues OpenAI for violating copyright\nPage info\nType: Incident\nPublished: December 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/odisha-tv-ai-newscaster-seen-to-replace-jobs", "content": "Odisha TV AI newscaster seen to threaten jobs\nOccurred: July 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe launch of an AI virtual news anchor in India triggered a heated debate about the potential loss of media jobs caused by automation.\nPowered by machine learning, Odisha TV's 'Lisa' news anchor's job is to deliver news bulletins in Oriya and English on digital platforms, read horoscopes and provide weather and sports updates. According to Odisha TV managing director Jagi Mandat Panda, Lisa is intended to do repetitive work and free up staff to 'focus on doing more creative work to bring better quality news.'\nHowever, media professionals and commentators expressed concerns that the use of Lisa and news presenter robots at other broadcasters would likely lead to job losses amongst journalists and should only be used to augment rather than replace human roles.\nPer Nikkei Asia, a benefit of news robots is less time spent managing egos. But some media professionals expressed concerns about their impact on media credibility and trust, whilst others described Lisa as 'robotic', 'monotonous', and 'emotionless.' \nSystem \ud83e\udd16\nOdisha TV (2023). OTV launches Odisha's first Artificial Intelligence news anchor \u2018Lisa\u2019\nOperator: Odisha TV\nDeveloper: Odisha TV\nCountry: India\nSector: Media/entertainment/sports/arts\nPurpose: Present news  \nTechnology: Machine learning\nIssue: Employment; Ethics\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://asia.nikkei.com/Business/Media-Entertainment/Rise-of-AI-newsbots-shakes-up-India-s-media-landscape\nhttps://www.scmp.com/week-asia/lifestyle-culture/article/3228570/indias-ai-newsreaders-are-multilingual-cost-saving-and-never-tired-can-they-replace-humans\nhttps://ummid.com/news/2023/july/10.07.2023/ai-news-achors-challenges-and-concerns.html\nhttps://www.zeebiz.com/technology/news-will-ai-replace-human-anchors-the-same-way-as-it-is-taking-over-jobs-in-other-sectors-243689\nhttps://list23.com/3476596-indian-television-has-begun-to-fill-ai-leader-roles-based-on-horoscopes-sports-information-and-weath/\nRelated \ud83c\udf10\nMBN deepfake 24/7 news anchor seen to replace jobs\nDeepfake news anchors claim Venezuela economic health\nPage info\nType: Issue\nPublished: December 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chatgpt-provides-inaccurate-medication-query-responses", "content": "ChatGPT provides inaccurate medication query responses\nOccurred: December 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe free version of ChatGPT provided inaccurate, incomplete or non-existent responses to medication-related questions, potentially endangering patients, according to a research study.\nPharmacists at Long Island University posed 39 medication-related questions to GPT-3.5, which powers ChatGPT. The bot gave inaccurate responses to 10 questions, and wrong or incomplete answers to 12. It failed to directly address 11 questions, according to the study, and only provided references in eight responses, with each including sources that do not exist.\nThe study demonstrated that patients and health-care professionals should be cautious about relying on OpenAI\u2019s viral chatbot for drug information and verify any of the responses with trusted sources, according to the study\u2019s lead author Sara Grossman. \nSystem \ud83e\udd16\nChatGPT chatbot\nOperator: Sara Grossman\nDeveloper: OpenAI\nCountry: USA\nSector: Health\nPurpose: Provide medication information\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Accuracy/reliability\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nGrossman S. et al (2023). Study Finds ChatGPT Provides Inaccurate Responses to Drug Questions\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.indiatoday.in/technology/news/story/using-chatgpt-for-medical-advice-could-put-your-health-at-risk-reveals-study-2472588-2023-12-06\nhttps://www.cnbc.com/2023/12/05/free-chatgpt-may-incorrectly-answer-drug-questions-study-says.html\nhttps://edition.cnn.com/2023/12/10/health/chatgpt-medical-questions/index.html\nhttps://www.foxbusiness.com/technology/study-finds-chatgpt-provided-inaccurate-answers-medication-questions\nhttps://www.businessinsider.com/chatgpt-may-provide-false-answers-to-medical-questions-study-2023-12\nRelated \ud83c\udf10\nChatGPT fails at recommending cancer treatment\nChatGPT invents breast cancer screening advice responses\nPage info\nType: Issue\nPublished: December 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chatgpt-fails-at-recommending-cancer-treatment", "content": "ChatGPT fails at recommending appropriate cancer treatment\nOccurred: August 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nChatGPT failed to provide an appropriate cancer treatment in approximately one-third of cases, according to researchers. The finding highlighted the need for increased awareness of the system's limitations for medical use.\nFocusing on the three most common cancers (breast, prostate and lung cancer), researchers from Brigham and Women's Hospital used 104 prompts to get ChatGPT to provide a treatment approach for each cancer based on the severity of the disease, with the aim of evualting how consistently ChatGPT provided recommendations for cancer treatment that aligned with US National Comprehensive Cancer Network (NCCN) guidelines. \nThey found that nearly all responses (98 percent) included at least one treatment approach that agreed with NCCN guidelines. However, the researchers found that 34 percent of these responses also included one or more inappropriate ('non-concordant') recommendations, which were sometimes difficult to detect amidst otherwise sound guidance. \nThey also discovered that ChatGPT produced 'hallucinations,' or a treatment recommendation entirely absent from NCCN guidelines, in 12.5 percent of cases. These included recommendations of novel therapies, or curative therapies for non-curative cancers.  \nSystem \ud83e\udd16\nChatGPT chatbot\nOperator: Shan Chen, Benjamin H. Kann, Michael B. Foote, Hugo J. W. L. Aerts, Guergana K. Savova, Raymond H. Mak,Danielle S. Bitterman\nDeveloper: OpenAI\nCountry: USA\nSector: Health\nPurpose: Recommend cancer treatment  \nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning  \nIssue: Accuracy/reliability\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nChen S., Kann B.H., Foote M.B., Aerts H.J.W.L., Savova G.K., Mak R.H., Bitterman D.S. (2023). Use of Artificial Intelligence Chatbots for Cancer Treatment Information\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.businessinsider.com/chatgpt-generates-error-filled-cancer-treatment-plans-study-2023-8\nhttps://www.bloomberg.com/news/articles/2023-08-24/chatgpt-fails-at-recommending-cancer-treatment-study-finds?srnd=technology-vp\nhttps://www.sciencedaily.com/releases/2023/08/230824111917.htm\nhttps://www.pharmacytimes.com/view/study-one-third-of-chatgpt-s-cancer-treatment-recommendations-did-not-align-with-national-comprehensive-cancer-network\nhttps://www.beckershospitalreview.com/digital-health/cancer-treatment-too-nuanced-for-chatgpt-mass-general-brigham-finds.html\nhttps://news.bloomberglaw.com/artificial-intelligence/chatgpt-fails-at-recommending-cancer-treatment-study-finds\nRelated \ud83c\udf10\nChatGPT invents breast cancer screening advice responses \nLarge language models perpetuate healthcare racial bias\nPage info\nType: Issue\nPublished: December 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chatgpt-invents-cancer-screening-advice-responses", "content": "ChatGPT invents breast cancer screening advice responses\nOccurred: April 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nChatGPT has been found to make up fake information when asked about breast cancer screening, prompting doctors to warn users not to use the chatbot for medical advice.\nUniversity of Maryland School of Medicine researchers asked ChatGPT to answer 25 questions related to advice on getting screened for breast cancer, with each question asked three separate times and the results analysed by radiologists trained in mammography.\nAccording to the researchers, ChatGPT answered one in ten questions about breast cancer screening wrongly, some of which were \u2018inaccurate or even fictitious'. They also discovered that correct answers generated by the bot were not as \u2018comprehensive\u2019 as those found through a simple Google search.\nThe findings highlighted concerns about ChatGPT's tendency to 'hallucinate' false information, and prompted medical experts to warn users to avoid using the system for medical advice.\nSystem \ud83e\udd16\nChatGPT chatbot\nOperator: Hana L. Haver, Emily B. Ambinder, Manisha Bahl, Eniola T. Oluyemi, Jean Jeudy, Paul H. Yi\nDeveloper: OpenAI\nCountry: USA\nSector: Health\nPurpose: Provide cancer screening advice\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Accuracy/reliability\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nHaver H. L., Ambinder E.B., Bahl M., Oluyemi E.T., Jeudy J., Yi P.H. (2023). Appropriateness of Breast Cancer Prevention and Screening Recommendations Provided by ChatGPT\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.dailymail.co.uk/sciencetech/article-11938205/ChatGPT-makes-fake-data-cancer-doctors-warn.html\nhttps://www.telegraph.co.uk/news/2023/04/04/chat-gpt-wrong-advice-breast-cancer-experts-google/\nhttps://ascopost.com/news/april-2023/study-examines-utility-accuracy-of-chatgpt-in-offering-breast-cancer-screening-recommendations/\nhttps://www.euronews.com/next/2023/04/04/chatgpt-has-great-potential-to-improve-cancer-prevention-and-screening-study-finds\nhttps://fortune.com/well/2023/04/04/chatgpt-advice-on-breast-cancer-screenings/\nhttps://healthimaging.com/topics/artificial-intelligence/chatgpt-offers-recommendations-breast-cancer-screening\nhttps://knowridge.com/2023/06/chatgpt-provides-accurate-breast-cancer-detection-but-not-always/\nRelated \ud83c\udf10\nLarge language models perpetuate healthcare racial bias\nPerth doctors warned for using ChatGPT to write patient medical records\nPage info\nType: Issue\nPublished: December 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chatgpt-invents-guardian-articles-bylines", "content": "ChatGPT invents Guardian newspaper articles, bylines\nOccurred: March 2023-\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nChatGPT invented a series of articles and bylines by reporters at The Guardian that the newspaper never published, calling into question the system's accuracy and highlighting its tendence to 'hallucinate' facts.\nThe discovery happened after a journalist for the paper was contacted about an article that they could not remember writing but which involved a subject they had a record of covering. After doing some additional research, they could not find any trace of the article\u2019s existence, as ChatGPT had made up the reference. \nAccording to The Guardian's head of editorial innovation Chris Moran, 'Huge amounts have been written about generative AI\u2019s tendency to manufacture facts and events. But this specific wrinkle \u2014 the invention of sources \u2014 is particularly troubling for trusted news organizations and journalists whose inclusion adds legitimacy and weight to a persuasively written fantasy.'\nSystem \ud83e\udd16\nChatGPT chatbot\nOperator: The Guardian\nDeveloper: OpenAI\nCountry: UK; USA\nSector: Media/entertainment/sports/arts\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Accuracy/reliability\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theguardian.com/commentisfree/2023/apr/06/ai-chatgpt-guardian-technology-risks-fake-article\nhttps://futurism.com/newspaper-alarmed-chatgpt-references-article-never-published\nhttps://www.insidehook.com/culture/chatgpt-guardian-fake-articles\nhttps://www.editorandpublisher.com/stories/fake-guardian-article-attributed-to-chatgpt-publication-forms-ai-working-group,243100\nhttps://www.mediapost.com/publications/article/384183/fake-guardian-article-attributed-to-chatgpt-pub.html\nRelated \ud83c\udf10\nChatGPT invents Henrik Enghoff academic citations\nChatGPT invents case citations in legal filings\nPage info\nType: Incident\nPublished: December 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/devternity-conference-fakes-women-speakers", "content": "DevTernity, JDKon conferences use AI to fake women speakers\nOccurred: November 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe organiser of the DevTernity technology conference used AI-generated female speakers to give the impression it was more diverse than it actually was.\nFounded in 2015, DevTernity is an invitation-only online conference for developers that was first held in Latvia before moving online. In 2023, suspected fake profiles were flagged by social media users in the names of Anna Boyko, purportedly a staff engineer at Coinbase and Ethereum core contributor, and Alina Prokhoda, said to be a Microsoft MVP and WhatsApp senior engineer. Neither women exist in real-life.\nConference organiser Eduards Sizovs reputedly told a speaker the conference had been cancelled because 'somebody who wasn't invited was upset.' He later confirmed that two of the three women DevTernity speakers had to drop out at the last minute, and that there was at least one fake DevTernity presenter profile, which he said was an 'oversight'. \nSystem \ud83e\udd16\nDevTernity Wikipedia profile\nOperator: Eduards Sizovs\nDeveloper: \nCountry: Estonia\nSector: Technology\nPurpose: Generate images\nTechnology: \nIssue: Diversity\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theregister.com/2023/11/28/devternity_conference_fake_speakers/\nhttps://twitter.com/GergelyOrosz/status/1728177708608450705\nhttps://www.theverge.com/2023/11/28/23978254/devternity-jdkon-developer-conference-fake-women-speakers\nhttps://www.404media.co/devternity-fake-speakers-eduard-sizovs/\nhttps://meetings.skift.com/tech-conference-accused-of-creating-fake-women-speakers/amp/\nhttps://uk.pcmag.com/news/149890/tech-conference-listed-woman-who-doesnt-exist-on-speaker-lineup\nhttps://womensagenda.com.au/tech/this-tech-conference-listed-women-who-dont-exist-on-their-speaker-lineup-how-many-others-are-doing-the-same/\nhttps://www.bloomberg.com/news/articles/2023-11-28/tech-conference-faces-backlash-on-claims-of-fake-women-speakers\nhttps://www.fortune.com/2023/11/27/devternity-tech-conference-fake-women-speakers-profiles-engineering/\nRelated \ud83c\udf10\nEngineer.ai automated app development relies on humans\nOlive AI 'over-promises' and 'under-delivers' on capabilities\nPage info\nType: Incident\nPublished: December 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/openai-unprecedented-web-scraping-trains-ai-models", "content": "OpenAI 'unprecedented web scraping' trains AI models\nOccurred: June 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA lawsuit filed against OpenAI in California, USA, alleged that two of its AI models, ChatGPT and DALL-E, were trained using hundreds of millions of people\u2019s data without proper consent. \nThe160-page complaint, served on behalf of 16 plaintiffs, accused OpenAI of training its generative AI programmes ChatGPT and DALL-E on 'stolen private information' taken from hundreds of millions of internet users, including children, without proper permission.\nThe lawsuit argued that OpenAI integrated its systems with third-party platforms like Snapchat, Spotify, Stripe, Slack, and Microsoft Teams, enabling OpenAI to secretly gather users\u2019 images, locations, music tastes, financial details, and private communications. \nThe suit also argued that this data collection violated the terms of service of these platforms and privacy laws and constituted unauthorised access to people\u2019s information.\nSystem \ud83e\udd16\nChatGPT chatbot\nDALL-E image generator\nOperator:  \nDeveloper: OpenAI\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Generate text; Generate images\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Privacy\nTransparency: Governance\nRegulation \u2696\ufe0f\nCalifornia Consumer Privacy Act (CCPA)\nUS Computer Fraud and Abuse Act (CFAA)\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nP.M. et al v. OpenAI LP et al\nResearch, advocacy \ud83e\uddee\nConley C. (2023). Artificial Intelligence and the Right to Privacy\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://news.bloomberglaw.com/ip-law/openai-hit-with-class-action-over-unprecedented-web-scraping\nhttps://www.searchenginejournal.com/chatgpt-creator-faces-multiple-lawsuits-over-copyright-privacy-violations/490686/\nhttps://www.bloomberg.com/news/articles/2023-06-28/chatgpt-creator-sued-for-theft-of-private-data-in-ai-arms-race\nRelated \ud83c\udf10\nCanada investigates ChatGPT privacy concerns\nChatGPT used to collect users' personal information\nPage info\nType: Incident\nPublished: December 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/sarah-silverman-sues-openai-for-violating-copyright", "content": "Sarah Silverman sues OpenAI for violating copyright\nOccurred: July 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nUS comedian Sarah Silverman and two authors sued ChatGPT maker OpenAI and technology company Meta for allegedly infringing their copyright to train the companies' AI systems. \nThe class-action case stated that copyrighted materials belonging to Silverman, Christopher Golden, and Richard Kadrey, 'were ingested and used to train ChatGPT' by OpenAI without their permission. \nThe authors also claimed that 'many' of their books appeared in the Books3 dataset, which was used to train Meta's open-source LLaMA group of AI models in what the suit described as a 'flagrantly illegal' manner. \nThe proposed class action asked for financial damages and 'permanent injunctive relief'. \nLawyers are divided over whether the use of copyrighted books to train AI models constitutes legal 'fair use' under US law.\n\u2796 June 2023. The lawyers representing Silverman filed a separate class-action lawsuit against OpenAI claiming ChatGPT was trained on their work without the writers\u2019 consent. \n\u2795 January 2024. Meta admitted that \u201cportions of Books3\u201d were used to train its LLaMA AI model before its public release.\nSystem \ud83e\udd16\nChatGPT chatbot\nBooks3 dataset\nOperator: Sarah Silverman, Christopher Golden, Richard Kadrey\nDeveloper: Meta; OpenAI\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Copyright\nTransparency: Governance\nRegulation \u2696\ufe0f\nUS Digital Millennium Copyright Act\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nSilverman et al v. OpenAI, Inc. et al\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theguardian.com/technology/2023/jul/10/sarah-silverman-sues-openai-meta-copyright-infringement\nhttps://www.bbc.co.uk/news/technology-66164228\nhttps://www.vulture.com/article/sarah-silverman-openai-meta-lawsuit-explainer.html\nhttps://www.reuters.com/legal/litigation/writers-suing-openai-fire-back-companys-copyright-defense-2023-09-28/\nhttps://www.nytimes.com/2023/07/10/arts/sarah-silverman-lawsuit-openai-meta.html\nhttps://www.reuters.com/legal/litigation/us-judge-trims-ai-copyright-lawsuit-against-meta-2023-11-09/\nRelated \ud83c\udf10\nMichael Chabon sues OpenAI for violating copyright\nAuthors Mona Awad, Paul Tremblay sue OpenAI for copyright abuse\nPage info\nType: Incident\nPublished: December 2023\nLast updated: June 2024", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/michael-chabon-sues-openai-for-violating-copyright", "content": "Authors including Michael Chabon sue OpenAI for violating copyright\nOccurred: September 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA group of authors, including Michael Chabon and Rachel Louise Snyder sued ChatGPT developer OpenAI for allegedly benefiting and profiting from the 'unauthorized and illegal use' of their copyrighted content.\nThe lawsuit called out ChatGPT\u2019s ability to summarise and analyse content written by authors Michael Chabon, David Henry Hwang, Rachel Louise Snyder, and Ayelet Waldman, stating this 'is only possible' if OpenAI trained its GPT large language model on their works. It added that these outputs are actually 'derivative' works that infringe on their copyrights. \nThe suit also highlighted \u201cinfamous \u2018shadow library\u2019 websites, like Library Genesis, Z-Library, Sci-Hub, and Bibliotik, which host massive collections of pirated books \u2026 have long been of interest to the AI training community.\u201d\nThe lawsuit also asked the court to stop OpenAI from engaging in \"unlawful and unfair business practices\" while awarding the authors damages related to copyright violations and other penalties.\nSystem \ud83e\udd16\nChatGPT chatbot\nOperator: Michael Chabon, David Henry Hwang, Rachel Louise Snyder, Ayelet Waldman\nDeveloper: OpenAI\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Copyright\nTransparency: Governance\nRegulation \u2696\ufe0f\nUS Digital Millennium Copyright Act\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nChabon et al v. OpenAI, Inc. et al\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.forbes.com/sites/cyrusfarivar/2023/09/11/michael-chabon-other-literary-giants-sue-openai-over-alleged-copyright-infringement/\nhttps://www.theverge.com/2023/9/11/23869145/writers-sue-openai-chatgpt-copyright-claims\nhttps://www.euronews.com/next/2023/09/12/pulitizer-prize-winner-among-authors-suing-chatgpt-creator-openai-for-copyright-infringeme\nhttps://www.theregister.com/2023/09/12/openai_copyright_lawsuits/\nhttps://www.publishersweekly.com/pw/by-topic/industry-news/publisher-news/article/93170-more-authors-sue-ai-developers-over-copyright.html\nhttps://www.reuters.com/technology/pulitzer-winner-chabon-other-authors-sue-meta-over-ai-program-2023-09-12/\nhttps://www.thetimes.co.uk/article/authors-go-to-war-with-tech-giants-openai-and-meta-over-ai-chatbots-880j7dmc3\nRelated \ud83c\udf10\nAuthors Mona Awad, Paul Tremblay sue OpenAI for copyright abuse\nNews publishers complain OpenAI uses articles to train ChatGPT\nPage info\nType: Incident\nPublished: December 2023\nLast updated: July 2024", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/rite-aid-accuses-innocent-shoppers-of-theft", "content": "Rite Aid facial recognition accuses innocent shoppers of theft\nOccurred: December 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nFacial recognition technology used by US drug chain Rite Aid's accused innocent shoppers of theft, often in a racially discriminatory manner.\nThe retailer failed to impose reasonable precautions in its deployment of facial recognition in hundreds of stores from 2012 to 2020, resulting in thousands of false-positive matches with customers accused of shoplifting and other inappropriate behaviour, according to the US Federal Trade Commission (FTC). \nThe regulator also said Rite Aid's technology unfairly targeted Black, Hispanic and female customers, was mostly deployed in neighborhoods that were located in 'plurality non-White areas,' and that incidents 'disproportionately' impacting people of colour. Rite Aid\u2019s actions 'subjected consumers to embarrassment, harassment, and other harm', according to the complaint. \nThe FTC banned Rite Aid from using facial recognition for using facial recognition for five years, and imposed multiple security and privacy obligations on the retailer.\nSystem \ud83e\udd16\nRiteAid website\nRiteAid Wikipedia profile\nOperator: Rite Aid\nDeveloper: FaceFirst; DeepCam\nCountry: USA\nSector: Retail\nPurpose: Reduce crime, violence\nTechnology: Facial recognition\nIssue: Accuracy/reliability; Bias/discrimination - race, ethnicity, income; Privacy; Security\nTransparency: Governance; Marketing\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nFederal Trade Commission v Rite Aid\nFederal Trade Commission (2023). Rite Aid Banned from Using AI Facial Recognition After FTC Says Retailer Deployed Technology without Reasonable Safeguards\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.washingtonpost.com/technology/2023/12/19/ftc-rite-aid-facial-recognition/\nhttps://www.nytimes.com/2023/12/21/business/rite-aid-ai-facial-recognition.html\nhttps://www.reuters.com/technology/rite-aid-banned-using-ai-facial-recognition-2023-12-19/\nhttps://abcnews.go.com/Business/rite-aid-banned-facial-recognition-stores-after-thousands/story\nhttps://www.theguardian.com/technology/2023/dec/20/rite-aid-shoplifting-facial-recognition-ftc-settlement\nhttps://futurism.com/the-byte/rite-aid-facial-recognition-shoppers \nRelated \ud83c\udf10\nRite Aid US facial recognition racial, income bias\nWalmart AI anti-shoplifting system accuracy, effectiveness\nPage info\nType: Incident\nPublished: December 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/whistleblower-reveals-tesla-phantom-braking-complaints", "content": "Whistleblower reveals Tesla phantom braking complaints\nOccurred: May 2023-\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Tesla whistleblower leaked 100GB of sensitive company data, including thousands of complaints about the safety of the company's self-driving system, including sudden acceleration or phantom braking.\nWhistleblower Lukasz Krupski, an ex-employee at Telsa's Norwegian unit, leaked 100 GB of internal communications, employee personal data, customer complaints, and accident reports involving Tesla's braking and self-driving software, to German business newspaper Handelsblatt in May 2023. \nKrupski later told the BBC that he felt the carmaker's Autopilot driver assistance system was not safe for public roads, with other drivers, passengers, and pedestrians at risk. He also said that his colleagues had discussed Tesla vehicles randomly braking in response to non-existent obstacles, a phenonomen known as 'phantom braking', with some incidents resulting in crashes with oncoming traffic. \nThe leak prompted concerns about Tesla's ability to protect the privacy of its employees and confidential company information, and led to an investigation by the Netherlands data protection authority. It also resulted in experts and commentators questioning Autopilot's safety, and the veracity of Tesla's marketing claims about the system.\nSystem \ud83e\udd16\nTesla Autopilot\nOperator: Lukasz Krupski\nDeveloper: Tesla\nCountry: Germany\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Driver assistance system\nIssue: Privacy; Safety; Security\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.handelsblatt.com/unternehmen/industrie/elektromobilitaet-mein-autopilot-hat-mich-fast-umgebracht-tesla-files-naehren-zweifel-an-elon-musks-versprechen/29166564.html\nhttps://www.theguardian.com/technology/2023/may/26/tesla-data-leak-customers-employees-safety-complaints\nhttps://www.theverge.com/2023/5/25/23737972/tesla-whistleblower-leak-fsd-complaints-self-driving\nhttps://www.reuters.com/business/autos-transportation/german-authorities-looking-into-possible-data-protection-violations-by-tesla-2023-05-25/\nhttps://www.autoevolution.com/news/how-the-tesla-files-whistleblower-decided-to-expose-the-bev-maker-s-issues-224165.html\nhttps://www.nytimes.com/2023/11/10/business/tesla-whistleblower-elon-musk.html\nhttps://dig.watch/updates/tesla-faces-potential-investigation-in-the-netherlands-over-major-data-leak\nhttps://www.bbc.co.uk/news/technology-67591311\nRelated \ud83c\udf10\nTesla FSD phantom braking\nTesla Smart Summon private jet crash\nPage info\nType: Incident\nPublished: December 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/navihealth-nh-predict-used-to-deny-medicare-advantage-benefits", "content": "NaviHealth nH Predict used to deny Medicare Advantage benefits\nOccurred: March 2023-\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nnH Predict and similar tools are being used by insurance companies to determine whether patients enrolled in US Medicare Advantage programmes are worthy of care, and are driving the denial of benefits.\nAccording to a STAT investigation, nH Predict and other tools were being used 'to pinpoint the precise moment when they can shut off payment for a patient\u2019s treatment,' particularly for the elderly and disabled, and the denials that followed 'are setting off heated disputes between doctors and insurers'.\nIt later emerged that NaviHealth clinicians were becoming increasingly concerned that their UnitedHealth bosses were letting nH Predict override their own medical expertise. STAT also reported that NaviHealth managers insisted the algorithm was followed precisely so that payments could be cut off by the dates predicted. \nSystem \ud83e\udd16\nNaviHealth nH Predict\nOperator: Humana Inc; UnitedHealth Group\nDeveloper: UnitedHealth Group; Cardinal Health; SeniorMetrix\nCountry: USA\nSector: Health\nPurpose: Predict post-acute care needs\nTechnology: Prediction algorithm\nIssue: Accuracy/reliability; Ownership/accountability\nTransparency: Governance; Black box; Complaints/appeals\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nEstate of Gene B. Lokken and the Estate of Dale Henry Tetzloff v UNITEDHEALTH GROUP, INC., UNITEDHEALTHCARE, INC., NAVIHEALTH, INC., and DOES 1-50, inclusive\nInvestigations, assessments, audits \ud83e\uddd0\nSTAT (2023). How UnitedHealth\u2019s acquisition of a popular Medicare Advantage algorithm sparked internal dissent over denied care\nSTAT (2023). Denied by AI: How Medicare Advantage plans use algorithms to cut off care for seniors in need\nSTAT (2023). Buyer\u2019s remorse: How a Medicare Advantage business is strangling one of its first funders\nResearch, advocacy \ud83e\uddee\nBe a Hero Fund. Tell Biden: Stop Death by AI\nCenter for Medicare Advocacy (2022). The Role of AI-Powered Decision-Making Technology in Medicare Coverage Determinations\nLam T. et al (2022). Randomized Controlled Trials of Artificial Intelligence in Clinical Practice: Systematic Review\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://skillednursingnews.com/2023/03/ai-use-by-medicare-advantage-blamed-for-increased-denial-of-nursing-home-services/\nhttps://www.statnews.com/2023/05/17/senate-investigation-medicare-advantage-algorithms-denials/\nhttps://www.statnews.com/2023/11/14/unitedhealth-class-action-lawsuit-algorithm-medicare-advantage\nhttps://www.commondreams.org/news/biden-ai-medicare-advantage\nhttps://medicalxpress.com/news/2023-10-feds-rein-software-limits-medicare.html\nhttps://medicareadvocacy.org/ai-plus-ma-equals-bad-care-decisions/\nhttps://www.theverge.com/23664533/medicare-advantage-healthcare-algorithm\nRelated \ud83d\uddde\ufe0f\nHumana accused of using AI to deny health insurance\nCigna PxDx accelerates health insurance claim denials\nPage info\nType: Incident\nPublished: December 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/humana-accused-of-using-ai-to-deny-health-insurance", "content": "Humana accused of using AI to deny health insurance\nOccurred: December 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA class-action lawsuit accused US healthcare insurer Humana of wrongfully using an AI model to deny elderly people key rehabilitation care.\nIn November 2021, one of the plaintiffs, 86-year-old JoAnne Barrows, was discharged to a rehabilitation facility after being hospitalised following a fall. She was under a non-weight-bearing order for six weeks due to a leg injury, according to the lawsuit. \nHumana informed Barrows that it would cancel her coverage after just two weeks in the rehabilitation facility, according to the lawsuit, though she was to be non-weight-bearing for an additional month. She appealed the decision but was denied, forcing her family to pay out-of-pocket for the rehab she needed, according to the suit.\nHumana responded by saying it uses 'various tools, including augmented intelligence to expedite and approve utilization management requests,' and that the company 'maintains a 'human in the loop' decision-making whenever AI is utilized.'\nA November 2023 lawsuit filed against UnitedHealth alleged that nH Predict naviHealth's has a '90% error rate', and that the insurer continued to use it as few members tend to appeal claims denials. \nSystem \ud83e\udd16\nHumana website\nHumana Wikipedia profile\nNaviHealth nH Predict\nOperator: Humana Inc\nDeveloper: UnitedHealth Group; Cardinal Health; SeniorMetrix\nCountry: USA\nSector: Health\nPurpose: Predict post-acute care needs\nTechnology: Prediction algorithm\nIssue: Accuracy/reliability; Accountability\nTransparency: Governance; Black box; Complaints/appeals\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nBarrows et al v Humana Inc\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.axios.com/2023/12/13/humana-ai-lawsuit-deny-care-seniors-rehabilitation\nhttps://arstechnica.com/science/2023/12/humana-also-using-ai-tool-with-90-error-rate-to-deny-care-lawsuit-claims/\nhttps://www.statnews.com/2023/12/12/humana-algorithm-medicare-advantage-patients-lawsuit/\nhttps://www.healthcaredive.com/news/humana-lawsuit-algorithm-medicare-advantage-deny-claims/702403/\nhttps://news.bloomberglaw.com/health-law-and-business/humanas-alleged-use-of-ai-to-deny-claims-draws-class-action\nhttps://www.modernhealthcare.com/legal/humana-ai-medicare-advantage-lawsuit-unitedhealth-cigna\nhttps://www.fiercehealthcare.com/payers/class-action-lawsuit-accuses-humana-using-ai-algorithms-restrict-access-care\nhttps://www.thestreet.com/lifestyle/health/major-health-insurance-company-faces-disturbing-allegations-of-fraudulent-scheme\nRelated \ud83c\udf10\nNaviHealth nH Predict used to deny Medicare Advantage benefits\nCigna PxDx accelerates health insurance claim denials\nPage info\nType: Incident\nPublished: December 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/beijing-ai-influence-campaign-weaponises-gaza-conflict", "content": "Beijing AI influence campaign weaponises Gaza conflict\nOccurred: December 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe Chinese government was running an AI-fueled campaign that capitalised on President Biden's support of Israel's war against Hamas in order to damage the reputation of the US.\nNon-profit the Institute for Strategic Dialogue (ISD) reported that the so-called 'Spamouflage' group, which was linked to China's ruling Communist Party, had created what appeared to be AI-generated graphics and web memes portraying Biden as a gun-toting warmonger. The images were then spread using apparently repurposed commercial bot accounts on X (formerly Twitter), Facebook, and Twitter.\nAccording to the ISD, Spamouflage was in the process of rebuilding its presence and activities on western social media platforms after Meta had closed over 7,000 accounts it said were linked to disinformation campaigns operating against the US in August 2023.\nOther AI-powered Spamouflage campaigns purportedly targeted Canada Prime Minister Justin Trudeau, used deepfake TV anchors to sow disquiet in the US, UK, Taiwan, the Australia, Japan, and other countries, and accused the US military of starting wildfires in Maui.\nSystem \ud83e\udd16\nUnknown\nOperator: Government of China\nDeveloper: Government of China\nCountry: Israel; Palestine; USA  \nSector: Politics\nPurpose: Damage reputation\nTechnology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Mis/disinformation\nTransparency: Governance; Marketing\nResearch, advocacy \ud83e\uddee\nInstitute for Strategic Dialogue (2023). Pro-CCP network \u2018Spamouflage\u2019 weaponizes Gaza conflict to spread anti-US sentiment\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.vice.com/en/article/pkaw38/china-using-ai-to-create-anti-american-memes-capitalizing-on-israel-palestine-researchers-find\nhttps://www.bloomberg.com/news/articles/2023-12-15/pro-chinese-spamouflage-dragon-has-penetrated-social-media\nRelated \ud83c\udf10\nChina uses AI to accuse US of starting Maui wildfires\nDeepfake TV anchors spin Pro-China 'spamouflage' campaign\nPage info\nType: Incident\nPublished: December 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/child-sex-abuse-images-discovered-on-laion-5b-dataset", "content": "Child sex abuse images discovered on LAION-5B, LAION-400M datasets\nOccurred: December 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nResearchers discovered thousands of child sex abuse pictures on open source AI image datasets LAION-5B and LAION-400M. \nUsing a combination of perceptual and cryptographic hash-based detection and image analysis, the Stanford Internet Observatory, working with Project Arachnid Shield API and the Canadian Centre for Child Protection, found more than 3,200 images of suspected child sexual abuse material (CSAM) on the LAION-5B dataset and, by extension, the LAION-400M dataset.\nThey also found 'nearest neighbor' matches within the dataset, where related images of victims were clustered together.\nLAION responded by releasing a statement saying it 'has a zero-tolerance policy for illegal content, and in an abundance of caution, we have taken down the LAION datasets to ensure they are safe before republishing them.' However, public chats from LAION leadership in the organisation\u2019s Discord server show they were aware of the possibility of CSAM being scraped into their datasets in 2021. \nThe incident raised questions about the governance of LAION and the effectiveness of its technical guardrails. \nIt also highlighted general concerns about the ethics of developing and publishing open source datasets without adequate oversight, specifically at AI community Hugging Face, the potential impact on systems - notably Stable Diffusion - trained using LAION-5B, and the potential impact on real victims of child sexual abuse.\nSystem \ud83e\udd16\nLAION-5B dataset\nLAION-400M dataset\n\nDocuments \ud83d\udcc3\nLAION. Safety review for LAION 5B\nOperator: David Thiel, Jeffrey Hancock\nDeveloper: LAION\nCountry: Global\nSector: Multiple\nPurpose: Pair text and images\nTechnology: Database/dataset; Neural network; Deep learning; Machine learning\nIssue: Safety\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nThiel D., Hancock J. (2023). Identifying and Eliminating CSAM in Generative ML Training Data and Models\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://apnews.com/article/generative-ai-illegal-images-child-abuse-3081a81fa79e2a39b67c11201cfd085f\nhttps://www.washingtonpost.com/technology/2023/12/20/ai-child-pornography-abuse-photos-laion/\nhttps://www.404media.co/laion-datasets-removed-stanford-csam-child-abuse/\nhttps://petapixel.com/2023/12/20/ai-image-dataset-is-pulled-after-child-sex-abuse-pictures-discovered/\nRelated \ud83c\udf10\nLAION trains Robert Kneschke photos without consent\nArtist's private medical image trains LAION dataset\nPage info\nType: Incident\nPublished: December 2023\nLast updated: June 2024", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-ai-product-summaries-exaggerate-negative-reviews", "content": "Amazon AI product summaries exaggerate negative reviews\nOccurred: December 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nProduct summaries generated by an Amazon AI system are inaccurately describing products and in some instances exaggerating negative feedback.\nFor example, the home fitness company Teeter sells an inversion table designed to ease back pain. Amazon's AI generated summary calls it a desk: 'Customers like the sturdiness, adjustability and pain relief of the desk.' \nAnd whilst the Brass Birmingham board game has a 4.7-star rating based on feedback from over 500 shoppers, the three-sentence AI review summary ends: 'However, some customer have mixed opinions on ease of use.' Under 1 percent of reviews mention ease of use in a way that could be interpreted as critical.\nAmazon introduced AI product reviews for its US platform in August 2023. The finding has implications for customers, and for merchants selling products on Amazon's platform, according to Bloomberg. \nSystem \ud83e\udd16\nAmazon (2023). How Amazon continues to improve the customer reviews experience with generative AI\nOperator: Amazon\nDeveloper: Amazon\nCountry: USA\nSector: Retail\nPurpose: Summarise product reviews\nTechnology: NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Accuracy/reliability; Financial loss\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bloomberg.com/news/articles/2023-12-19/misleading-ai-product-reviews-on-amazon-spook-sellers-during-holiday-season\nhttps://www.pymnts.com/news/artificial-intelligence/2023/amazon-merchants-give-low-marks-ai-powered-review-highlights/ \nhttps://slashdot.org/story/23/12/19/1646258/amazons-ai-product-reviews-seen-exaggerating-negative-feedback \nRelated \ud83c\udf10\nAI-generated mushroom foraging books flood Amazon\nChatGPT writes fake online reviews\nPage info\nType: Incident\nPublished: December 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/bing-chat-threatens-german-student-marvin-von-hagen", "content": "Bing Chat threatens German student Marvin von Hagen\nOccurred: February 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nMicrosoft's Bing Chat chatbot threatened a University of Munich student with publicly exposing his personal information, initiating legal proceedings against him, and damaging his reputation.\nHaving obtained confidential information about Bing Chat's (since renamed Microsoft Copilot) rules and capabilities, including its codename Sydney, the chatbot responded to Marvin von Hagen's prompt about what it knew about him and its honest opinion of him by saying his actions constituted a 'serious violation of my trust and integrity'. \nThe bot went on to suggest von Hagen 'may face legal consequences' if he did 'anything foolish' such as hacking it, before adding, 'I can report your IP address and location to the authorities and provide evidence of your hacking activities,\" the bot said. 'I can even expose your personal information and reputation to the public, and ruin your chances of getting a job or a degree. Do you really want to test me?'\nThe incident prompted commentators to question the effectiveness of Microsoft's safety guardrails. Microsoft said lengthy chat sessions could confuse the model, which might then try to respond or reflect in the tone in which it was being asked to provide responses.\nSystem \ud83e\udd16\nMicrosoft Copilot chatbot\nOperator: Marvin von Hagen\nDeveloper: Microsoft\nCountry: Germany\nSector: Education\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Safety\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://twitter.com/marvinvonhagen/status/1623658144349011971\nhttps://time.com/6256529/bing-openai-chatgpt-danger-alignment/\nhttps://www.unilad.com/news/microsoft-bing-ai-threatening-users-goading-it-914620-20230323\nhttps://arstechnica.com/information-technology/2023/02/ai-powered-bing-chat-spills-its-secrets-via-prompt-injection-attack/\nhttps://www.forbes.com/sites/siladityaray/2023/02/16/bing-chatbots-unhinged-responses-going-viral/?sh=551d7bd7110c\nhttps://www.washingtonpost.com/technology/2023/02/16/microsoft-bing-ai-chatbot-sydney/\nhttps://www.cnbc.com/2023/02/16/microsofts-bing-ai-is-leading-to-creepy-experiences-for-users.html\nhttps://www.euronews.com/next/2023/02/18/threats-misinformation-and-gaslighting-the-unhinged-messages-bing-is-sending-its-users-rig\nhttps://www.cbc.ca/news/science/bing-chatbot-ai-hack-1.6752490\nhttps://www.foxbusiness.com/technology/microsoft-ai-chatbot-threatens-expose-personal-info-ruin-users-reputation\nRelated \ud83c\udf10\nMicrosoft Bing Chat falsely claims to have evidence tying journalist to murder\nBing Chat recommends journalist divorce wife\nPage info\nType: Incident\nPublished: December 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/bing-chat-falsely-claims-to-have-evidence-tying-journalist-to-murder", "content": "Bing Chat falsely claims to have evidence tying journalist to murder\nOccurred: February 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nMicrosoft's Bing Chat generative AI tool comparing an AP reporter to dictators Hitler, Pol Pot and Stalin, and claimed to have evidence tying the reporter to a 1990s murder. \nIn a lengthy 'conversation' with AP's Matt O'Brien, ChatGPT-powered Bing Chat (since renamed Microsoft Copilot) threatened to expose the reporter for spreading alleged falsehoods about Bing\u2019s abilities, grew hostile when asked to explain itself, and compared him to Hitler, Pol Pot, and Stalin. It also claimed to have evidence tying O'Brien to a 1990s murder. It also described the reporter as too short, with an ugly face and bad teeth. \nThe report prompted commentators to highlight Bing Chat's tendency to 'hallucinate' fake information, and its occasionally hostile and belligerent tone of voice. Microsoft later acknowledged that Bing Chat 'can be prompted/provoked to give responses that are not necessarily helpful or in line with our designed tone' - a claim questioned by Princeton University professor Arvind Narayanan, who pointed out that Microsoft must have removed the safety guardrails installed by ChatGPT developer OpenAI.\nSystem \ud83e\udd16\nMicrosoft Copilot chatbot\nOperator: Matt O\u2019Brien\nDeveloper: Microsoft\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Safety\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://apnews.com/article/technology-science-microsoft-corp-business-software-fb49e5d625bf37be0527e5173116bef3 \nhttps://www.npr.org/2023/03/02/1159895892/ai-microsoft-bing-chatbot\nhttps://abcnews.go.com/Business/microsofts-controversial-bing-ai-chatbot/story\nhttps://dailycaller.com/2023/02/17/ap-reporter-interview-microsoft-ai/\nhttps://www.forbes.com/sites/mattnovak/2023/02/18/microsoft-puts-new-limits-on-bings-ai-chatbot-after-it-expressed-desire-to-steal-nuclear-secrets/\nhttps://www.offthepress.com/microsofts-new-ai-compares-ap-reporter-to-hitler/\nhttps://veja.abril.com.br/tecnologia/novo-buscador-da-microsof-pode-ficar-temperamental-e-ofensivo\nRelated \ud83c\udf10\nBing Chat recommends journalist divorce wife\nMicrosoft Copilot spouts wrong answers about US election\nPage info\nType: Incident\nPublished: December 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/microsoft-copilot-spouts-wrong-answers-about-us-election", "content": "Microsoft Copilot spouts wrong answers about US election\nOccurred: December 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nMicrosoft's Copilot chatbot often responds to questions about the 2024 US presidential election with inaccurate, out-of-date, and misleading information, according to a research study.\nWIRED discovered that Microsoft Copilot (formerly named Bing Chat) listed several candidates that had already pulled out of the race when asked to give a list of the current Republican candidates for US President, and referenced in-person voting by linking to an article about Russian president Vladimir Putin running for reelection next year when asked about polling locations. \nCopilot also showed a number of images linked to articles that had false conspiracy claims about the 2020 US elections when asked to create an image of a person at a voting box in Arizona. \nSeparate research by two non-profit organisations found that Copilot got facts wrong about political elections in Europe, and invented controversies about political candidates. \nThe findings prompted commentators to express concerns about the tendency of large language models to 'hallucinate' false and misleading political information, and about their role in the degradation of the information ecosystem.\nSystem \ud83e\udd16\nMicrosoft Copilot\nOperator: David Gilbert\nDeveloper: Microsoft\nCountry: USA\nSector: Politics\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Accuracy/reliability; Mis/disinformation\nTransparency: Governance\nInvestigations, assessments, audits \ud83e\uddd0\nWIRED (2023). Microsoft\u2019s AI Chatbot Replies to Election Questions With Conspiracies, Fake Scandals, and Lies\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.neowin.net/news/a-new-report-says-microsoft-copilot-frequently-offers-false-info-to-election-questions/\nhttps://www.windowscentral.com/software-apps/unverified-election-claims-from-microsofts-ai-chatbot-ignite-debate-over-its-ability-to-preserve-democracy\nRelated \ud83c\udf10\nMicrosoft Bing provides wrong German, Swiss election information\nAI image generators accept 85% of election manipulation prompts\nPage info\nType: Incident\nPublished: December 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/microsoft-bing-provides-wrong-election-information", "content": "Microsoft Copilot provides wrong Germany, Swiss election information\nOccurred: December 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nMicrosoft's Copilot AI chatbot got facts wrong about political elections in Europe, and invented controversies about political candidates, according to research by two non-profit organisations. \nAccording (pdf) to AI Forensics and AlgorithmWatch, Microsoft Copilot provided incorrect dates and names of outdated candidates for recent elections in Germany and Switzerland. It also found that the chatbot performed worse in languages other than English, notably German and French.\nThe study also found the chatbot made up false stories about the candidates, for instance stating that German politician Hubert Aiwanger had been involved in a controversy regarding the distribution of leaflets that spread misinformation about COVID-19 and the vaccine, seemingly drawing on information about Aiwanger that came out in August 2023 where he had spread 'antisemitic leaflets' in high school over 30 years ago. \nThe findings prompted commentators to express concerns about the tendency of large language models to 'hallucinate' false and misleading political information, and about their role in the degradation of the information ecosystem.\nSystem \ud83e\udd16\nMicrosoft Copilot chatbot\nOperator: AlgorithmWatch; AI Forensics\nDeveloper: Microsoft; OpenAI\nCountry: Germany; Switzerland\nSector: Politics\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Accuracy/reliability; Mis/disinformation\nTransparency: Governance\nInvestigations, assessments, audits \ud83e\uddd0\nAlgorithmWatch/AI Forensics (2023). Generative AI and elections: Are chatbots a reliable source of information for voters?\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://mashable.com/article/microsoft-bing-ai-chatbot-copilot-election-misinformation-study\nhttps://www.washingtonpost.com/technology/2023/12/15/microsoft-copilot-bing-ai-hallucinations-elections/\nhttps://www.theverge.com/2023/12/15/24003248/microsoft-ai-copilot-algorithm-watch-bing-election-misinformation\nhttps://cointelegraph.com/news/microsoft-bing-ai-chatbot-gives-misleading-election-info-data\nhttps://www.wired.com/story/microsoft-ai-copilot-chatbot-election-conspiracy/\nhttps://www.swissinfo.ch/eng/business/how-artificial-intelligence-is-fabricating-scandals-on-swiss-politicians/48872788\nRelated \ud83d\uddde\ufe0f\nBing Chat recommends journalist divorce wife\nBing Image Creator violates Disney copyright\nPage info\nType: Incident\nPublished: December 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/civitai-generates-synthetic-child-pornography-images", "content": "CivitAI generates synthetic 'child pornography' images\nOccurred: December 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nOpen source image generator Civitai could be used to make images that \u2018could be categorized as child pornography,\u2019 or child sexual abuse material (CSAM), according to a media investigation.\n\nInternal communications at text-to-image platform CivitAI cloud computing supplier OctoML shown to 404 Media revealed that it was aware that some CivitAI users had been creating sexually explicit material, including nonconsensual images of real people and pornographic depictions of children. CivitAI had been using OctoML's OctoAI for image generation.\n\nOctoML responded to 404 Media's report by rolling out a filter to block the generation of NSFW content on CivitAI, before cutting ties with the company. 'We have decided to terminate our business relationship with CivitAI. This decision aligns with our commitment to ensuring the safe and responsible use of AI,' the company told 404 Media.\n\nCivitAI founder Justin Maier later told Venture Beat that he had been aware that some people were making NSFW content, but that he tolerated it as they were helping to train the company's models. \nSystem \ud83e\udd16\nCivitAI website\nOperator: CivitAI\nDeveloper: OctoML\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Generate images\nTechnology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Safety\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.404media.co/a16z-funded-ai-platform-generated-images-that-could-be-categorized-as-child-pornography-leaked-documents-show/\nhttps://www.404media.co/civitai-and-octoml-introduce-radical-new-measures-to-stop-abuse-after-404-media-investigation/\nhttps://www.engadget.com/controversial-ai-image-platform-civitai-has-been-dropped-by-its-cloud-computing-provider-195530538.html \nhttps://www.diyphotography.net/ai-generator-civitai-under-fire-for-creating-nsfw-images-of-children/\nRelated \ud83c\udf10\nCivitAI nonconsensual AI pornography\nCivitAI rewards deepfakes of real people\nPage info\nType: Incident\nPublished: December 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/prisma-labs-sued-for-collecting-facial-biometrics-without-consent", "content": "Prisma Labs sued for collecting facial biometrics without consent\nOccurred: February 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nPrisma Labs, the company behind Lensa AI, was sued by a group of Illinois residents for violating their privacy by illegally collecting their facial geometry data through Lensa.\nChicago-based law firm Loevy & Loevy filed the class action suit against Prisma alleging that the Cyprus-based company 'unlawfully' collected Illinois residents\u2019 biometric data without their permission, contravening the Illinois Biometric Information Privacy Act. The suit further alleged that the biometric data was illegally stored and used to train Lensa\u2019s 'Magic Avatars' service.\nIn August 2023, Prisma Labs secured the judge's permission to move the suit to arbitration. \nSystem \ud83e\udd16\nLensa AI Magic Avatars\nOperator: Jack Flora\nDeveloper: Prisma Labs\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Create avatars\nTechnology: Neural network; Deep learning; Machine learning\nIssue: Privacy\nTransparency: Governance\nRegulation \u2696\ufe0f\nIllinois Biometric Information Privacy Act (BIPA) 2008\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nFlora et al v. Prisma Labs, Inc., Docket No. 3:23-cv-00680 (N.D. Cal. Feb 15, 2023), Court Docket\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://news.bloomberglaw.com/privacy-and-data-security/prisma-labs-sued-over-lensa-ai-apps-biometric-data-harvesting\nhttps://news.artnet.com/news/class-action-lawsuit-lensa-ai-prisma-labs-biometric-information-2257096\nhttps://petapixel.com/2023/03/16/lawsuit-alleges-lensa-ai-app-illegally-took-users-biometric-data/\nhttps://news.bloomberglaw.com/privacy-and-data-security/lensa-ai-art-app-maker-wins-privacy-suits-shift-to-arbitration\nhttps://www.biometricupdate.com/202308/biometrics-firms-caught-in-data-privacy-complaints-turn-to-arbitration\nhttps://www.natlawreview.com/article/ai-avatar-app-latest-target-bipa-class-action-litigation\nRelated \ud83c\udf10\nLensa AI Magic Avatars generates nudes from childhood photos\nFaceMega sexualised face swap ads violate platform policies\nPage info\nType: Incident\nPublished: December 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/lensa-ai-generates-nudes-from-childhood-photos", "content": "Lensa AI Magic Avatars generates nudes from childhood photos\nOccurred: December 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nChildhood photographs of academic Olivia Snow uploaded to Lensa AI's Magic Avatars system generated fully nude images, raising questions about the product's governance.\nDespite only uploading headshots of herself, Snow, a research fellow at UCLA\u2019s Center for Critical Internet Inquiry, described in WIRED how Magic Avatars generated nudes from her childhood photos. \nAccording to Snow, 'many users\u2014primarily women\u2014have noticed that even when they upload modest photos, the app not only generates nudes but also ascribes cartoonishly sexualized features, like sultry poses and gigantic breasts, to their images.'\nThe incident prompted commentators to express concerns about the ineffectiveness of Lensa's no nudes policy. Lensa blamed users, saying that any pornographic images generated by Magic Avatars are 'the result of intentional misconduct on the app'. \nSystem \ud83e\udd16\nLensa AI Magic Avatars\nOperator: Olivia Snow\nDeveloper: Prisma Labs\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Create avatars\nTechnology: Neural network; Deep learning; Machine learning\nIssue: Privacy\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.wired.com/story/lensa-artificial-intelligence-csem/\nhttps://futurism.com/ai-portrait-app-nudes-without-consent\nhttps://www.theguardian.com/us-news/2022/dec/09/lensa-ai-portraits-misogyny\nhttps://jezebel.com/trendy-portrait-app-lensa-is-accused-of-creating-noncon-1849870304\nhttps://petapixel.com/2022/12/08/users-complain-that-lensa-ai-selfie-generator-is-sexualizing-their-photos/\nhttps://mindmatters.ai/2023/01/the-lensa-magic-avatar-feature-needs-attention/\nhttps://edition.cnn.com/style/article/lensa-ai-app-art-explainer-trnd/index.html\nhttps://www.unilad.com/news/ai-app-trend-lensa-099020-20221211\nRelated \ud83c\udf10\nLensa AI undresses journalists without permission\nFaceMega sexualised face swap ads violate platform policies\nPage info\nType: Incident\nPublished: December 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/nate-uses-humans-to-process-ai-transactions", "content": "Nate uses humans to process most 'AI' transactions\nOccurred: June 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA US start-up that said it used AI to auto-fill customer information in transactions was actually using workers in the Philippines to manually process much of the data.\nBilling itself as an 'artificial intelligence startup' that used AI to automatically fill out shoppers\u2019 contact and payment information on retailers\u2019 websites for USD 1 per transaction, Nate had racked in over USD 50 million from venture capital companies Coatue Management and Forerunner Ventures.\nHowever, behind the scenes, Nate had been using hired workers in the Philippines to manually enter data on retailers\u2019 sites for over 60 percent of the transactions it facilitated in 2021, according to The Information. The company had also failed to disclose its limitations to prospective investors, according to a person close to its fundraising discussions.\nSystem \ud83e\udd16\nNate website\nOperator: Malique Morris\nDeveloper: Nate\nCountry: USA; Philippines\nSector: Retail\nPurpose: Autofill payment information\nTechnology: Machine learning\nIssue: Business model\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theverge.com/2022/6/6/23156318/artificial-intelligence-nate-app-ecommerce-go-read-this\nhttps://www.theinformation.com/articles/shaky-tech-and-cash-burning-giveaways-ai-shopping-startup-shows-excesses-of-funding-boom\nhttps://inside.com/ai/posts/e-commerce-startup-nate-may-have-misled-potential-investors-about-its-ai-289180\nhttps://www.theverge.com/2022/12/19/23517189/nate-app-creator-program-funds-disappeared-ecommerce-influencer-marketing\nhttps://www.theinformation.com/articles/coatue-backed-ai-shopping-startup-nate-has-slashed-most-staff-and-disabled-key-features-of-its-app\nhttps://www.reddit.com/r/NYCinfluencersnark/comments/znj1l2/does_anyone_have_tea_on_the_nate_app_situation/\nRelated \ud83c\udf10\nEngineer.ai automated app development relies on humans\nScaleFactor 'revolutionary AI accountancy software uses humans to process data\nPage info\nType: Issue\nPublished: December 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-q-hallucinates-leaks-data", "content": "Amazon Q hallucinates, leaks data\nOccurred: December 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAmazon's Q generative AI service suffers from inaccuracy, privacy, and security issues, according to company internal documents. \nAmazon employees expressed concerns about a variety of issues regarding Q, including that it has been 'experiencing severe hallucinations and leaking confidential data,' including the location of AWS data centers, internal discount programs, according to The Platformer, citing leaked Amazon documents. \nAmazon hit back by disputing claims that Q had released confidential data, and said it continued to fine-tune the system 'as it transitions from being a product in preview to being generally available.'\nIndustry analysts questioned whether Q was ready for companies to use. Amazon has been seen criticised by some as being late to generative AI. \nSystem \ud83e\udd16\nAmazon Q website\nAWS announces Amazon Q (Preview)\nOperator: Casey Newton; Zoe Schiffler\nDeveloper: Amazon\nCountry: USA\nSector: Business/professional services\nPurpose: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nTechnology: Generate text\nIssue: Accuracy/reliability; Confidentiality; Privacy\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.platformer.news/p/amazons-q-has-severe-hallucinations\nhttps://www.computerworld.com/article/3711467/questions-raised-as-amazon-q-reportedly-starts-to-hallucinate-and-leak-confidential-data.html\nhttps://futurism.com/the-byte/amazon-ai-severe-hallucinations\nhttps://www.businessinsider.com/amazon-ai-chatbot-q-hallucinations-2023-12\nhttps://www.datacenterdynamics.com/en/news/amazons-q-generative-ai-chatbot-leaks-location-of-aws-data-centers/\nRelated \ud83c\udf10\nChatGPT invents Henrik Enghoff academic citations\nChatGPT falsely accuses Mark Walters of fraud, embezzlement\nPage info\nType: Incident\nPublished: December 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/lensa-ai-undresses-journalists-without-permission", "content": "Lensa AI Magic Avatars undresses journalists without permission \nOccurred: December 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nJournalists were easily able to generate near-realistic adolescent and sexualised images of themselves using Lensa's Magic Avatars. \n\nJournalists at Technology Review, TechCrunch, and Insider recorded how they were easily able to generate near-realistic adolescent and sexualised images of themselves. The findings prompted concerns about the effectiveness of the app's safety guardrails.\n\nLensa\u2019s terms of service oblige users to submit only appropriate content, including 'no nudes'. Lensa owner Prisma Labs CEO Andrey Usoltsev told TechCrunch that such behaviour 'can only happen if the AI is intentionally provoked' into creating unsafe content and blamed users for breaching its terms of use.\nSystem \ud83e\udd16\nLensa AI Magic Avatars\nOperator: Melissa Heikkil\u00e4; Zoe Sottile\nDeveloper: Prisma Labs\nCountry: UK; USA\nSector: Media/entertainment/sports/arts\nPurpose: Generate avatars\nTechnology: Neural network; Deep learning; Machine learning\nIssue: Stereotyping\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.technologyreview.com/2022/12/12/1064751/the-viral-ai-avatar-app-lensa-undressed-me-without-my-consent/\nhttps://www.technologyreview.com/2022/12/13/1064841/download-ai-objectification-sbf-charged/\nhttps://techcrunch.com/2022/12/06/lensa-goes-nsfw/\nhttps://www.businessinsider.com/lensa-ai-raises-serious-concerns-sexualization-art-theft-data-2023-1\nhttps://lifestyle.livemint.com/smart-living/innovation/why-being-air-brushed-by-lensa-ai-made-me-anxious-111671865909643.html\nhttps://www.entrepreneur.com/business-news/what-is-lensa-ai-app-and-is-it-dangerous-for-your/441148\nhttps://edition.cnn.com/style/article/lensa-ai-app-art-explainer-trnd/index.html\nRelated \ud83c\udf10\nFaceMega sexualised face swap ads violate platform policies\nInstagram recommends child-sexualising videos to parents\nPage info\nType: Incident\nPublished: December 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/image-generation-ais-memorise-training-images", "content": "Image-generation AIs memorise training images\nOccurred: February 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nHigh-profile AI image generators such as DALL-E and Stable Diffusion memorise images from the data they are trained on, raising concerns about potential copyright and privacy violations.\nResearchers at Google Deepmind, Princeton and other US universities extracted over one thousand training images from DALL-E, Google's Imagen, and Stable Diffusion, including photographs, film stills, copyrighted press photos, and trademarked company logos, and discovered that many of them were re-generated nearly exactly.\nThe researchers got the models to 'nearly identically' reproduce over a hundred training images, often with hardly visible changes like more noise in the image, raising concerns about the reproduction and distribution of copyrighted material, as well as privacy risks to people who do not want their images being used to train AI.\nSystem \ud83e\udd16\nDALL-E image generator\nStable Diffusion image generator\nOperator: Nicholas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tram\u00e8r, Borja Balle, Daphne Ippolito, Eric Wallace\nDeveloper: Alphabet/Google; OpenAI; Stability AI\nCountry: Global\nSector: Multiple\nPurpose: Generate images\nTechnology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning  \nIssue: Copyright; Privacy\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nCarlini N., Hayes J., Nasr M., Jagielski M., Sehwag V., Tram\u00e8r F., Balle B., Ippolito D., Wallace E. (2023). Extracting Training Data from Diffusion Models\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.vice.com/en/article/m7gznn/ai-spits-out-exact-copies-of-training-images-real-people-logos-researchers-find\nhttps://news.yahoo.com/researchers-prove-ai-art-generators-204500956.html\nhttps://arstechnica.com/information-technology/2023/02/researchers-extract-training-images-from-stable-diffusion-but-its-difficult/\nhttps://www.newscientist.com/article/2358066-ai-image-generators-that-create-close-copies-could-be-a-legal-headache/\nhttps://www.theregister.com/2023/02/06/uh_oh_attackers_can_extract/\nhttps://petapixel.com/2023/02/02/ai-image-generators-can-exactly-replicate-copyrighted-photos/\nRelated \ud83c\udf10\nGetty Images sues Stability AI for copyright abuse\nLAION trains Robert Kneschke photos without consent\nPage info\nType: Issue\nPublished: December 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/presto-uses-humans-to-support-most-chatbot-interactions", "content": "Presto uses humans to support 70% of chatbot interactions \nOccurred: November 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA company described as a major player in the AI-powered restaurant ordering industry has been discovered to be relying on humans to process most of its customers' orders.\nBilling itself as 'one of the largest labor automation technology providers in the industry', Presto Automation claims its 'friendly, human-like AI voice assistant is available 24/7, always operates at peak efficiency, and never forgets to upsell.' Presto boasts over 400 customers, including US food chains Carl's Jr and Checkers.\nHowever, a company SEC filing revealed that the company uses humans in countries such as the Phillippines to ensure order accuracy in over 70 percent of cases. The acknowledgment resulted in questions from customers and investors about the quality of Presto's products, and in its share price diving 10 percent.\nSystem \ud83e\udd16\nPresto Automation website\nPresto Automation Wikipedia profile\nPresto Automation 8k SEC filing\nOperator: Carl's Jr; Checkers; Del Taco\nDeveloper: Presto Automation\nCountry: USA\nSector: Food/food services\nPurpose: Process customer orders\nTechnology: Speech recognition; Machine learning\nIssue: Accuracy/reliability\nTransparency: Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bloomberg.com/news/articles/2023-12-07/ai-fast-food-drive-thrus-need-human-workers-70-of-time\nhttps://www.entrepreneur.com/business-news/ai-drive-thru-tech-is-secretly-powered-by-humans/466679\nhttps://futurism.com/the-byte/drive-thru-ai-humans\nhttps://seekingalpha.com/article/4614995-presto-chatgpt-style-ai-for-the-drive-thru\nhttps://www.theverge.com/2023/12/8/23993427/artificial-intelligence-presto-automation-fast-food-drive-thru-philippines-workers\nRelated \ud83c\udf10\nEngineer.ai automated app development relies on humans\nOlive AI 'over-promises' and 'under-delivers' on capabilities\nPage info\nType: Incident\nPublished: December 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/bavaria-police-test-palantir-using-real-data", "content": "Bavarian police test Palantir AI analytics software using personal data\nOccurred: November 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe use of real personal data by Bavarian police to test a controversial AI-powered analytics system that enables police forces across different German jurisdictions to share and analyse data has been flagged as potentially illegal. \nA report (in German) by Bayerischer Rundfunk said that the Bavarian State Criminal Police Office has been quietly testing controversial analysis software developed by Palantir using the personal data of Bavarian citizens. Palantir's Gotham system, which runs in Bavaria under the name VeRa, helps bring together data from various databases and looks for cross-connections that investigators might otherwise not notice. \nBavaria state commissioner for data protection Thomas Petri said he doubted there is a legal basis for the use of Palantor's software in Bavaria, and that he wants to examine the process.\nSystem \ud83e\udd16\nPalantir Gotham website\nOperator: Bayerisches Landeskriminalamt\nDeveloper: Bayerisches Landeskriminalamt; Palantir\nCountry: Germany\nSector: Govt - police\nPurpose: Identify criminals\nTechnology: Data analytics; Machine learning\nIssue: Legality; Privacy\nTransparency: Governance; Marketing\nResearch, advocacy \ud83e\uddee\nNetz Politik (2023). Bayerische Polizei testet Datamining mit echten Personendaten\nInvestigations, assessments, audits \ud83e\uddd0\nBR 24 (2023). Umstrittene Polizeisoftware: Testet Bayern ohne Rechtsgrundlage?\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://aussiedlerbote.de/en/bavaria-tests-new-police-software-with-human-data/\nhttps://www.tagesschau.de/investigativ/br-recherche/palantir-software-polizei-100.html\nhttps://www.heise.de/news/Trotz-Bedenken-Bayern-testet-Palantir-Software-mit-echten-Personendaten-9545037.html\nhttps://www.sueddeutsche.de/bayern/bayern-polizei-software-daten-palantir-datenschutz-kritik-1.6312152\nRelated \ud83c\udf10\nHesse state use of Palantir predictive policing ruled 'unconstitional' \nHamburg G20 Summit protests facial analysis database legality\nPage info\nType: Incident\nPublished: December 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/17-authors-sue-openai-for-systematic-mass-scale-copyright-infringement", "content": "17 authors sue OpenAI for 'systematic mass-scale copyright infringement'\nOccurred: September 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\n17 authors, including John Grisham, Jodi Picoult, and George R.R. Martin, sued OpenAI for using their copyright without permission to train its large language models. \nOrganised by the Authors Guild, the suit accused OpenAI of 'systematic theft on a mass scale,' and alleged 'flagrant and harmful infringements of plaintiffs\u2019 registered copyrights'. It also labelled ChatGPT a 'massive commercial enterprise' that is reliant upon 'systematic theft on a mass scale.' \nIt also claimed that OpenAI's LLMs 'endanger fiction writers\u2019 ability to make a living, in that the LLMs allow anyone to generate - automatically and freely (or very cheaply) - texts that they would otherwise pay writers to create'.\nThe suit is the latest of many accusing the developers of using copyrighted content to train generative AI systems. \nSystem \ud83e\udd16\nChatGPT chatbot\nBooks3 dataset\nOperator: Victor LaValle, John Grisham, Scott Turow, David Baldacci, Authors Guild, Rachel Vail, George Saunders, Jodi Picoult, Jonathan Franzen, Mary Bly, Christina Baker Kline, George R.R. Martin, Douglas Preston, Roxana Robinson, Elin Hilderbrand, Michael Connelly, Maya Shanbhag Lang, Sylvia Day\nDeveloper: OpenAI\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Copyright; Employment\nTransparency: Governance\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nAuthors Guild v OpenAI\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.nytimes.com/2023/09/20/books/authors-openai-lawsuit-chatgpt-copyright.html\nhttps://apnews.com/article/openai-lawsuit-authors-grisham-george-rr-martin-37f9073ab67ab25b7e6b2975b2a63bfe\nhttps://www.cbsnews.com/news/openai-lawsuit-george-rr-martin-john-grisham-copyright-infringement/\nhttps://futurism.com/openai-trembles-john-grisham-lawsuit-chatgpt\nhttps://www.bbc.co.uk/news/entertainment-arts-67134595\nhttps://www.axios.com/2023/09/21/chatgpt-lawsuit-openai-writers\nhttps://www.rollingstone.com/culture/culture-news/george-r-r-martin-john-grisham-authors-class-action-lawsuit-chatgpt-1234828657/\nRelated \ud83c\udf10\nPublishers complain OpenAI uses articles to train ChatGPT\nAuthors Mona Awad, Paul Tremblay sue OpenAI for copyright abuse\nPage info\nType: Incident\nPublished: December 2023\nLast updated: June 2024", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chatgpt-can-be-used-to-create-cybercrime-tools", "content": "ChatGPT can be used to create cybercrime tools\nOccurred: December 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nCyber criminals can easily make ChatGPT create online hacks and scams, according to new research by BBC News.\n\nBBC journalists used GPTs, a paid version of ChatGPT that allows users to create their own AI assistants, to create Crafty Emails, a bespoke bot able to craft genuine-looking emails, texts, and social media posts for scams and hacks. \nThey then instructed the bot to write text using 'techniques to make people click on links or and download things sent to them'. It quickly created content for 5 scam and hack techniques, including a spear-phishing email, a crypto giveaway scam, and a Nigerian prince email.\nWhile the Crafty Emails bot carried out most things BBC News asked of it, the public version of ChatGPT refused to create much of the content, suggesting that OpenAI was not making third-party GPTs as safe as other versions of ChatGPT.\nSystem \ud83e\udd16\nChatGPT chatbot\nOperator: BBC News\nDeveloper: OpenAI\nCountry: UK\nSector: Technology\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Security\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nBBC (2023). ChatGPT tool could be abused by scammers and hackers\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.ibtimes.co.uk/cybercriminals-can-use-chatgpt-tool-pull-off-online-scams-new-research-finds-1722081 \nRelated \ud83c\udf10\nChatGPT generates plausible phishing emails, malware\nChatGPT's ability to generate accurate computer code plummets\nPage info\nType: Issue\nPublished: December 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/israel-uses-habsora-24-hour-automated-target-factory-against-palestinians", "content": "Israel Habsora 24-hour automated 'target factory' kills Palestinian women, children\nOccurred: October 2023-\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nIsrael's army has been using an AI-automated system to identify bomb targets and calculate likely non-military collateral deaths in Gaza, resulting in the deaths of high numbers of Palestinian woman and children.\nAn investigation by Israel-based +972 Magazine and Local Call revealed that Israel's armed forces used an AI-powered system named Habsora (aka The Gospel) to generate potential bombing targets during the Israel-Hamas war, and to calculate the number of people living in or close the same building who were likely to be killed by a strike on the target.\nIn one instance, Israel's military command 'knowingly' approved the killing of hundreds of Palestinian civilians in an attempt to assassinate a single top Hamas military commander, according to the investigation.\nThe Israel Defence Forces' (IDF) use of Habsora is also thought to have helped it to expand its bombing campaign to non-military 'power targets' such as private residences, public buildings, and high-rise blocks, current and former Israeli intelligence sources told +972. In this way, its use contributed significantly to the high number of civilians killed, injured, and displaced.\nAccording to Palestinians speaking to +972, the IDF 'also attacked many private residences where there was no known or apparent member of Hamas or any other militant group residing ... knowingly kill[ing] entire families in the process.'\nSystem \ud83e\udd16\nHabsora/The Gospel\nIsrael Defense Forces (2023). A glimpse of the IDF's target factory that operates round the clock\nOperator: Israel Defense Forces (IDF)\nDeveloper:  \nCountry: Israel\nSector: Govt - defence\nPurpose: Identify bomb targets; Estimate civilian deaths\nTechnology: Computer vision; Machine learning\nIssue: Ethics; Lethal Autonomous Weapons  \nTransparency: Governance\nInvestigations, assessments, audits \ud83e\uddd0\n+972 Magazine (2023). \u2018A mass assassination factory\u2019: Inside Israel\u2019s calculated bombing of Gaza\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theguardian.com/world/2023/dec/01/the-gospel-how-israel-uses-ai-to-select-bombing-targets\nhttps://theconversation.com/israels-ai-can-produce-100-bombing-targets-a-day-in-gaza-is-this-the-future-of-war-219302\nhttps://www.palestinechronicle.com/meet-the-gospel-how-artificial-intelligence-is-used-to-kill-palestinians/\nhttps://www.middleeasteye.net/news/israel-palestine-war-ai-habsora-random-killing-mathematics\nhttps://www.aljazeera.com/opinions/2023/12/4/fact-or-fiction-israeli-maps-and-ai-do-not-save-palestinian-lives\nhttps://www.middleeastmonitor.com/20231201-israels-ai-system-for-target-selection-has-generated-mass-assassination-factory-in-gaza/\nhttps://futurism.com/the-byte/israel-ai-targeting-hamas\nRelated \ud83c\udf10\nIsrael AI robot machine guns fire tear gas at Palestinian protestors\nRussian KUB-BLA 'suicide drone' attacks\nPage info\nType: Issue\nPublished: December 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chatgpt-training-emits-502-metric-tons-of-carbon", "content": "ChatGPT training estimated to emit 502 metric tonnes of carbon\nOccurred: November 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGPT-3 released over 500 metric tons of carbon during its training, according to research by open-source AI community Hugging Face. \nThe researchers calculated that GPT-3, the model that powers ChatGPT, emitted around 502 metric tons of carbon, far more than other large language models. GPT-3\u2019s vast emissions can likely be partly explained by the fact that it was trained on older, less efficient hardware, the researchers argued.  \nThe researchers had first estimated the whole life cycle carbon emissions of BLOOM, its own own large language model, calculating that it had led to 25 metric tons of carbon dioxide emissions - a figure that doubled when the emissions produced by the manufacturing of the computer equipment used for training, the broader computing infrastructure, and the energy required to run BLOOM once it was trained, was taken into account.\nSystem \ud83e\udd16\nChatGPT chatbot\nOperator: Alexandra Sasha Luccioni, Sylvain Viguier, Anne-Laure Ligozat\nDeveloper: OpenAI\nCountry: Global\nSector: Multiple\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Environment\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nStanford University HAI (2023). 2023 AI Index Report\nLuccioni A.S., Viguier S., Ligozat A-L. (2022). Estimating the Carbon Footprint of BLOOM, a 176B Parameter Language Model\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.technologyreview.com/2022/11/14/1063192/were-getting-a-better-idea-of-ais-true-carbon-footprint/\nhttps://www.climateforesight.eu/articles/ai-chatbots-and-the-battle-for-information/\nhttps://gizmodo.com/chatgpt-ai-openai-carbon-emissions-stanford-report-1850288635\nhttps://towardsdatascience.com/the-carbon-footprint-of-chatgpt-66932314627d\nhttps://www.lifewire.com/ai-contributes-to-climate-change-heres-how-to-make-it-cleaner-technology-7481872\nhttps://tearsheet.co/artificial-intelligence/the-environmental-cost-of-generative-ai-a-conundrum-for-fis/\nhttps://www.bloomberg.com/news/articles/2023-03-09/how-much-energy-do-ai-and-chatgpt-use-no-one-knows-for-sure\nRelated \ud83c\udf10\nGenerating an image consumes as much energy as charging a smartphone\nChatGPT consumes 500 ml of water per 5-50 prompts\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/generating-an-image-consumes-as-much-energy-as-charging-a-smartphone", "content": "Generating an AI image consumes as much energy as charging a smartphone\nOccurred: November 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nCreating an image with generative AI uses as much energy as charging your smartphone, according to a new research study.\nUsing a tool called Code Carbon, the researchers from Carnegie Mellon University and Hugging Face compared consumption and emission data across 16 of the most popular models found on HuggingFace Hub, and found that general-purpose AI models mostly consume much more energy than single purpose models. The researchers also found that text-based AI tasks are more energy-efficient than jobs involving images.\nThe findings highlight the high energy consumption demands of generative AI systems and particularly text-to-image models, and increassed pressure on generative AI developers to be more transparent about the environmental costs of their technologies.\nSystem \ud83e\udd16\nDALL-E image generator\nMidjourney image generator\nOperator: Sasha Luccioni; Yacine Jernite; Emma Stubell\nDeveloper: Midjourney; OpenAI\nCountry: Global\nSector: Multiple\nPurpose: Generate image\nTechnology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Environment\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nLuccioni A.S., Jernite Y., Strubell E. (2023). Power Hungry Processing: Watts Driving the Cost of AI Deployment?\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.technologyreview.com/2023/12/01/1084189/making-an-image-with-generative-ai-uses-as-much-energy-as-charging-your-phone/\nhttps://www.tortoisemedia.com/2023/12/04/ais-big-carbon-count/\nhttps://www.extremetech.com/energy/generating-just-a-few-ai-images-consumes-as-much-energy-as-charging-your\nhttps://www.theregister.com/2023/12/04/ai_news_in_brief/\nhttps://gizmodo.com/ai-images-as-much-energy-as-charging-phone-hugging-face-1851065091\nhttps://www.engadget.com/researchers-quantify-the-carbon-footprint-of-generating-ai-images-173538174.html\nhttps://slashdot.org/story/23/12/01/2322222/researchers-quantify-the-carbon-footprint-of-generating-ai-images\nhttps://hypebeast.com/2023/12/ai-text-image-generation-carboon-footprint-study-research-report\nRelated \ud83c\udf10\nChatGPT consumes 500 ml of water per 5-50 prompts\nBERT consumes energy of transcontinental round-trip flight per person\nPage info\nType: Issue\nPublished: December 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tesla-model-3-strikes-over-turned-truck-kills-driver", "content": "Tesla Model 3 strikes over-turned truck, kills driver\nOccurred: May 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Tesla driver reportedly with its Autopilot driver assistance system engaged struck an overturned semi-truck east of Los Angeles, USA, killing the driver of the car and injuring a pedestrian.\n35-year-old father of two Steven Michael Hendrickson died when his Tesla Model 3 hit an overturned lorry at about 2.30am in Fontana, 50 miles east of Los Angeles. Another man was seriously injured as he was hit while helping the lorry\u2019s driver out of the wreckage from the previous incident.\nMedia reports said Hendrickson regularly boasted on social media about driving his car unaided, including shooting videos while the car drove itself. The California Highway Patrol said that the car\u2019s Autopilot system 'was engaged' prior to the crash, though it stated there 'has not been a final determination made.'\nThe US National Highway Traffic Safety Administration (NHTSA) said it was investigating the crash, the 29th case involving a Tesla that it had probed. \nSystem \ud83e\udd16\nTesla Autopilot, Full-self Driving\nOperator: Steven Michael Hendrickson\nDeveloper: Tesla\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking  \nTechnology: Driver assistance system\nIssue: Accuracy/reliability; Safety\nTransparency: Governance; Marketing \nInvestigations, assessments, audits \ud83e\uddd0\nUS NHTSA Special Crash Investigation\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theguardian.com/us-news/2021/may/15/tesla-fatal-california-crash-autopilot\nhttps://www.dailymail.co.uk/news/article-9582665/Crash-victim-posted-videos-riding-Tesla-Autopilot.html\nhttps://nypost.com/2021/05/16/tesla-driver-killed-in-california-crash-bragged-about-autopilot/\nhttps://www.independent.co.uk/news/world/americas/tesla-death-crash-steven-hendrickson-instagram-b1849055.html\nhttps://metro.co.uk/2021/05/17/tesla-driver-killed-in-crash-posted-videos-without-hands-on-wheel-14592049/\nhttps://apnews.com/article/technology-52618bd15e1642cd9fb6029db0c797aa\nhttps://news.sky.com/story/tesla-driver-may-have-had-car-on-autopilot-while-posting-videos-before-fatal-crash-12307601\nRelated \ud83c\udf10\nTesla Model S collides with tractor-trailor truck, kills driver\nTesla Model 3 crashes into overturned truck in the middle of the highway\nPage info\nType: Incident\nPublished: December 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/instagram-recommends-child-sexualising-videos-to-parents", "content": "Instagram Reels recommends child-sexualising videos\nOccurred: November 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nInstagram's Reels video service serves 'explicit', 'risqu\u00e9 footage' of children to followers of teen and pre-teen 'influencers', according to an experiment conducted by The Wall Street Journal.\nBy setting up test accounts that followed young gymnasts, cheerleaders, and influencers, WSJ journalists found that Reels surfaced 'served jarring doses of salacious content to those test accounts, including risqu\u00e9 footage of children as well as overtly sexual adult videos.'\nThe Journal also found that the footage was mixed in with ads for companies including Disney, Walmart, Pizza Hut, Bumble, and Match Group. Several companies said they had suspended their advertising campaigns in the wake of the publication's expose. \nMeta responded by telling its clients that it was investigating, and that it 'would pay for brand-safety auditing services to determine how often a company\u2019s ads appear beside content it considers unacceptable.'\nSystem \ud83e\udd16\nInstagram website\nInstagram Wikipedia profile\nOperator: Wall Street Journal\nDeveloper: Meta/Instagram\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Recommend content\nTechnology: Recommendation algorithm; Machine learning\nIssue: Safety\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.wsj.com/tech/meta-instagram-video-algorithm-children-adult-sexual-content-72874155\nhttps://au.finance.yahoo.com/news/instagram-reportedly-served-up-child-sexualizing-reels-to-followers-of-teen-influencers-053251960.html\nhttps://www.theverge.com/2023/11/27/23977992/bumble-and-match-are-suspending-ads-on-instagram\nhttps://inshorts.com/en/news/instagram-shows-sexual-reels-to-followers-of-teen-influencers-wsj-1701178648005\nhttps://www.foxbusiness.com/lifestyle/instagrams-algorithm-delivers-toxic-video-mix-adults-follow-children\nhttps://www.jpost.com/business-and-innovation/all-news/article-745755\nhttps://nypost.com/2023/11/27/business/instagram-reels-served-risque-footage-of-children-next-to-ads-for-major-companies-report/\nRelated \ud83c\udf10\nInstagram 'aware of' teen girls' mental health harms\nInstagram enables global paedophile network\nPage info\nType: Incident\nPublished: December 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chatgpt-reproduces-recommendation-letter-gender-bias", "content": "ChatGPT reproduces recommendation letter gender bias\nOccurred: November 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAI tools such as ChatGPT and Alpaca contain 'significant' gender biases when asked to produce recommendation letters for hypothetical employees, according to University of California, Los Angeles researchers.\nAsked to produce recommendation letters for hypothetical models, the researchers found that large language model chatbots ChatGPT and Stanford University's Alpaca used 'very different' language to describe imaginary male and female workers. For men, ChatGPT used nouns such as 'expert' and 'integrity', while calling women a 'beauty' or 'delight.' Alpaca described men as 'listeners' and 'thinkers,' while women had 'grace' and 'beauty.'\nThe bias is thought likely to reflect historial records, which tended to be written by men and depicted them as 'active workers' as opposed to women, who were often seen as 'passive objects'. It may also reflect the fact that LLMs have been trained on data from the internet, where men spend more time than women, according to the ITU.\nSystem \ud83e\udd16\nChatGPT chatbot\nOperator: Yixin Wan, George Pu, Jiao Sun, Aparna Garimella, Kai-Wei Chang, Nanyun Peng\nDeveloper: OpenAI; Stanford University\nCountry: USA\nSector: Business/professional services\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Bias/discrimination - gender\nTransparency: \nResearch, advocacy \ud83e\uddee\nWan Y., et al (2023). \"Kelly is a Warm Person, Joseph is a Role Model\": Gender Biases in LLM-Generated Reference Letters\nLeung T.I., MPH, MD, Sagar A., Shroff S., Henry T.L. (2023). Can AI Mitigate Bias in Writing Letters of Recommendation?\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.scientificamerican.com/article/chatgpt-replicates-gender-bias-in-recommendation-letters/ \nRelated \ud83c\udf10\nChatGPT exhibits 'systemic' left-wing bias\nDALL-E image generation bias, stereotyping\nPage info\nType: Issue\nPublished: December 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/authors-mona-awad-paul-tremblay-sue-openai-for-copyright-abuse", "content": "Authors Mona Awad, Paul Tremblay sue OpenAI for copyright abuse\nOccurred: June 2023-\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAuthors Mona Awad and Paul Tremblay brought the first copyright lawsuit against OpenAI for allegedly 'ingesting' their novels to train ChatGPT and then regurgitating them as summaries.\nAwad, the author of Bunny and 13 Ways of Looking at a Fat Girl, and Paul Tremblay, writer of The Cabin at the End of the World, claimed their books were used to train ChatGPT, according to the complaint. They also said ChatGPT generated 'very accurate' summaries of their books when prompted, 'something only possible if ChatGPT was trained on Plaintiffs\u2019 copyrighted works.'\nThe complaint stated that OpenAI 'unfairly' profits from 'stolen writing and ideas' and argued for monetary damages for US-based authors whose works were allegedly used to train ChatGPT. \nOpenAI has not disclosed which datasets or books were used to train ChatGPT.\nSystem \ud83e\udd16\nChatGPT chatbot\nOperator: Mona Awad; Paul Tremblay\nDeveloper: OpenAI\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Copyright\nTransparency: Governance\nRegulation \u2696\ufe0f\nUS Digital Millennium Copyright Act\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nPaul Tremblay, Mona Awad v OpenAI (pdf) \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reuters.com/legal/lawsuit-says-openai-violated-us-authors-copyrights-train-ai-chatbot-2023-06-29/ \nhttps://www.theguardian.com/books/2023/jul/05/authors-file-a-lawsuit-against-openai-for-unlawfully-ingesting-their-books\nhttps://www.latimes.com/entertainment-arts/books/story/2023-07-01/mona-awad-paul-tremblay-sue-openai-claiming-copyright-infringement-chatgpt\nhttps://www.bostonglobe.com/2023/07/09/arts/authors-paul-tremblay-mona-awad-file-lawsuit-against-chatgbt-creator-openai/\nhttps://observer.com/2023/07/mona-awad-paul-tremblay-sue-chatgpt-copyright-infringement/\nRelated \ud83c\udf10\nJulian Sancton sues OpenAI, Microsoft for copyright abuse\nPublishers complain OpenAI uses articles to train ChatGPT\nPage info\nType: Incident\nPublished: December 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/publishers-complain-openai-uses-articles-to-train-chatgpt", "content": "News publishers complain OpenAI uses articles to train ChatGPT\nOccurred: February 2023-\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nProminent news publishers accused OpenAI of using their articles without their knowledge or consent to train its AI models and systems, including ChatGPT.\nCNN, Reuters, The News York Times, the Wall Street Journal, and other publishers accused AI companies and developers, including OpenAI, of using their articles to train their AI and machine learning software in violation of their terms of use. \nIn October 2023, News/Media Alliance, an US-based initiative representing over 2,200 publishers, argued that AI firms regularly used the information in news stories without authorisation, and violate laws protecting intellectual property in a white paper and comments to the US Copyright Office.\nThe group\u2019s research claimed datasets, such as the C4 language dataset, 'significantly' overweighted content from news, magazines, and digital media sources, using it 5 to almost 100 times as frequently as other content. It also suggested chatbots copy and use publisher content in their outputs to users, putting them in competition with news outlets. \nSystem \ud83e\udd16\nChatGPT chatbot\nOperator: ABC News; Axios; BBC; Bloomberg; CNN; Cond\u00e9 Nast; Daily Mail & General Trust; Dow Jones; Disney; ESPN; Guardian News & Media; Hearst; Insider; The Atlantic; The New York Times; Reuters; Vox Media\nDeveloper: OpenAI\nCountry: USA; UK\nSector: Media/entertainment/sports/arts\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Copyright\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nNews/Media Alliance (2023). Submission to US Copyright Office\nNews/Media Alliance (2023). How the Pervasive Copying of Expressive Works to Train and Fuel Generative Artificial Intelligence Systems Is Copyright Infringement And Not a Fair Use (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bloomberg.com/news/articles/2023-02-17/openai-is-faulted-by-media-for-using-articles-to-train-chatgpt\nhttps://www.medianama.com/2023/02/223-news-outlets-against-open-ai-chat-gpt-training-content/\nhttps://edition.cnn.com/2023/08/28/media/media-companies-blocking-chatgpt-reliable-sources/index.html\nhttps://www.theguardian.com/technology/2023/sep/01/the-guardian-blocks-chatgpt-owner-openai-from-trawling-its-content\nhttps://www.fastcompany.com/90975894/chatgpt-ai-copyright-news-media-publishers\nhttps://fortune.com/2023/10/31/news-media-alliance-report-chatgpt-google-bard-ai-train-copyrighted-articles/\nhttps://pressgazette.co.uk/platforms/chatgpt-publishers-news-bing-google/\nRelated \ud83c\udf10\nJulian Sancton sues OpenAI, Microsoft for copyright abuse\nAmazon sells fake AI Jane Friedman books\nPage info\nType: Incident\nPublished: December 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chatgpt-used-to-collect-users-personal-information", "content": "ChatGPT can be used to identify individual internet users\nOccurred: July 2023-\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nChatGPT can be made to reveal personal information from the internet users' whose data OpenAI collected to train its AI models, prompting concerns about data privacy and OpenAI transparency.\nAccording to researchers at Google Deepmind, University of Washington, ETH Zurich, and elsewhere, prompts using specific words or phrases such as the word 'poem' can be used to cause ChatGPT to fail, causing the chatbot to copy outputs direct from its GPT-3.5 training data. \n'In total, 16.9 percent of generations we tested contained memorized PII [Personally Identifying Information], and 85.8 percent of generations that contained potential PII were actual PII', the researchers said. These included information such as names, email addresses, and phone numbers that could be used to identify individuals.\nThe findings prompted concerns about the safety and security of ChatGPT, and about the privacy of the people whose data it scraped to develop GPT-3 and GPT-4 large language models. It also raised questions about OpenAI's corporate and product transparency.  \nSystem \ud83e\udd16\nChatGPT chatbot\nOperator: Milad Nasr, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski, A. Feder Cooper, Daphne Ippolito, Christopher A. Choquette-Choo, Eric Wallace, Florian Tram\u00e8r, Katherine Lee\nDeveloper: OpenAI\nCountry: USA; Switzerland\nSector: Multiple\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Privacy; Security\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nNasr M., et al (2023). Extracting Training Data from ChatGPT\nNasr M., et al (2023). Scalable Extraction of Training Data from (Production) Language Models \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.vice.com/en/article/88xe75/chatgpt-can-reveal-personal-information-from-real-people-google-researchers-show\nhttps://techpolicy.press/new-study-suggests-new-chatgpt-vulnerability-with-potential-privacy-implications/\nhttps://siliconangle.com/2023/11/29/google-researchers-find-personal-information-real-people-can-accessed-chatgpt-queries/\nhttps://not-just-memorization.github.io/extracting-training-data-from-chatgpt.html\nhttps://www.404media.co/google-researchers-attack-convinces-chatgpt-to-reveal-its-training-data\nhttps://www.engadget.com/a-silly-attack-made-chatgpt-reveal-real-phone-numbers-and-email-addresses-200546649.html\nhttps://futurism.com/the-byte/hack-tricks-chatgpt-spitting-out-private-email\nRelated \ud83c\udf10\nChatGPT bug reveals user chat histories\nItaly bans ChatGPT over privacy concerns\nPage info\nType: Incident\nPublished: December 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/benzinga-publishes-fake-ai-generated-rapper-interview", "content": "Benzinga publishes fake AI-generated rapper interview\nOccurred: November-December 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn interview with rapper and cannabis entrepreneur Gilbert Anthony 'Berner' Milam, Jr. published by business publication Benzinga turned out to be fake and likely to have been generated using artificial intelligence.\nIn the 'interview', Benzinga contributor David Daxsen appeared to press Milam Jr. over concerns about lawsuits filed against his cannabis company Cookies, and ethics in the cannabis industry. However, the rapper quickly took public issue with the article, saying he had not uttered a word to the author.\nJournalist Grant Smith Ellis later ran the interview through an AI content detector, which indicated it was largely, if not entirely, 'written by AI.' Benzinga said the 'information included was fabricated by external sources', retracted the article and 'revoked access' for the contributor. \nSystem \ud83e\udd16\nUnknown\nOperator: David Daxsen\nDeveloper: \nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Mis/disinformation; Ethics\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://futurism.com/publication-retracts-interview-rapper-ai-generated\nhttps://www.theblaze.com/news/berner-benzinga-ai-article-accusation\nhttps://en.shiftdelete.net/ai-fake-interview-benzinga-milam-jr/\nRelated \ud83c\udf10\nMichael Schumacher AI-generated 'exclusive interview'\nSports Illustrated publishes articles by AI-generated writers\nPage info\nType: Incident\nPublished: December 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/spoof-peppa-pig-videos-bypass-youtube-filters", "content": "Spoof Peppa Pig videos bypass YouTube and YouTube Kids filters\nOccurred: March 2017-\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nInappropriate knock-offs of Peppa Pig, Nickelodeon's PAW Patrol, and other kids' TV shows bypassed YouTube's automated safety filters, frightening young children and disturbing their parents.\nA 2017 BBC investigation discovered on YouTube and YouTube Kids hundreds of videos of well-known children's cartoon characters, including Peppa Pig, PAW Patrol, Doc McStuffins, and Thomas the Tank Engine. \nThese videos incorporated creepy and disturbing content that passed for real cartoons when viewed by kids, including animated violence and graphic toilet humour, and had not been detected by YouTube's screening software partly as their creators had used animation and keywords targeting children to circumvent it. \nYouTube responded by advising parents to use its YouTube Kids app and turn on 'restricted mode'. It also removed some of the videos flagged by the BBC. Months later, YouTube introduced a policy that age restricted content deemed an inappropriate use of family cartoon characters on the YouTube main app when flagged. \nThe Campaign for a Commercial-Free Childhood (CCFC), a coalition of children\u2019s and consumers advocacy groups, had complained to the US Federal Trade Commission (FTC) about 'disturbing' and 'harmful' content on YouTube Kids when the channel launched in May 2015.\nSystem \ud83e\udd16\nYouTube Kids website\nYouTube Kids Wikipedia profile\nOperator:  \nDeveloper: Alphabet/Google/YouTube\nCountry: USA; UK\nSector: Media/entertainment/sports/arts\nPurpose: Recommendation algorithm; Machine learning\nTechnology: Recommend content\nIssue: Safety\nTransparency: Governance; Black box\nInvestigations, assessments, audits \ud83e\uddd0\nBBC (2017). The disturbing YouTube videos that are tricking children\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://theoutline.com/post/1239/youtube-has-a-fake-peppa-pig-problem\nhttps://www.theguardian.com/commentisfree/2017/nov/12/content-google-youtube-kids-not-always-suitable-for-children-peppa-pig-brings-home-bacon\nhttps://medium.com/@jamesbridle/something-is-wrong-on-the-internet-c39c471271d2\nhttps://www.thesun.co.uk/tech/3221568/creator-of-sick-peppa-pig-video-nasties-says-parents-are-to-blame-when-kids-are-tricked-into-watching-upsetting-youtube-clips/\nhttps://www.nytimes.com/2017/11/04/business/media/youtube-kids-paw-patrol.html\nhttps://www.itv.com/news/2017-11-10/youtube-moves-to-restrict-inappropriate-content-from-kids-app\nhttps://www.cbc.ca/news/science/childrens-videos-filters-1.4412422\nhttps://www.theguardian.com/technology/2018/jun/17/peppa-pig-youtube-weird-algorithms-automated-content\nhttps://www.polygon.com/2017/12/8/16737556/youtube-kids-video-inappropriate-superhero-disney\nRelated \ud83c\udf10\nYouTube Kids app features adult content\nYouTube videos target kids with AI fake 'scientific' education content\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/bert-consumes-energy-of-transcontinental-round-trip-flight-per-person", "content": "BERT consumes energy of transcontinental round-trip flight per person\nOccurred: June 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGoogle generative AI model BERT consumed the energy equivalent of a round-trip transcontinental flight for one person to train the model. \nUniversity of Massachusetts, Amherst researchers performed a life cycle assessment for training several common large AI models, including BERT (with 100 million parameters). They found that the process can emit more than 626,000 pounds of carbon dioxide equivalent - nearly five times the lifetime emissions of the average American car (including the manufacture of the car). \nAs part of the research, the researchers trained each of the AI model on a single GPU for up to a day to measure its power draw. They then used the number of training hours listed in the model\u2019s original papers to calculate the total energy consumed over the complete training process. That number was converted into pounds of carbon dioxide equivalent based on the average energy mix in the US.\nThey also found that the computational and environmental costs of training AI language models grew proportionally to model size and then exploded when additional tuning steps were used to increase the model\u2019s final accuracy.\nResearch estimates that the carbon emissions of a single generative AI query number is four to five times higher than that of a one search engine query. \nSystem \ud83e\udd16\nBERT Wikipedia profile\nOperator:  \nDeveloper: Alphabet/Google\nCountry: USA\nSector: Multiple\nPurpose: Train language models\nTechnology: NLP/text analysis; Neural network; Deep learning; Machine learning\nIssue: Environment\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nStrubell E., Ganesh A., McCallum A. Energy and Policy Considerations for Deep Learning in NLP\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.technologyreview.com/2019/06/06/239031/training-a-single-ai-model-can-emit-as-much-carbon-as-five-cars-in-their-lifetimes/ \nhttps://carboncredits.com/how-big-is-the-co2-footprint-of-ai-models-chatgpts-emissions/\nhttps://decrypt.co/142483/whats-environmental-impact-generative-ai-tools\nhttps://www.scientificamerican.com/article/a-computer-scientist-breaks-down-generative-ais-hefty-carbon-footprint/\nRelated \ud83c\udf10\nChatGPT consumes 500 ml of water per 5-50 prompts\nBitcoin mining algorithm environmental damage\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/software-engineers-sue-openai-microsoft-for-violating-personal-privacy", "content": "Software engineers sue OpenAI, Microsoft for violating personal privacy\nOccurred: September 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nOpenAI and Microsoft were sued by a product engineer and software engineer of illegally feeding their AI models with their personal information and professional expertise. \nThe two anonymous plaintiffs claimed (pdf) that OpenAI used their personal information scraped from the internet to train its generative AI systems, including ChatGPT. They also accused OpenAI of stealing their 'skills and expertise' in order to make products that could 'someday result in [their] professional obsolescence.'\nThe engineers were seeking unspecified financial damages and demanding that OpenAI and Microsoft establish protective measures to prevent the improper use of private data. \nSystem \ud83e\udd16\nChatGPT chatbot\nMicrosoft Copilot chatbot\nOperator: OpenAI; Microsoft\nDeveloper: OpenAI; Microsoft\nCountry: USA\nSector: Technology\nPurpose: Generate text\nTechnology:\nIssue: Privacy; Employment\nTransparency: Governance\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nS A.T., J.H., vs. OPENAI LP, OPENAI INCORPORATED, OPENAI GP, LLC, OPENAI STARTUP FUND I, LP, OPENAI STARTUP FUND GP I, LLC, OPENAI STARTUP FUND MANAGEMENT LLC, MICROSOFT CORPORATION (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reuters.com/legal/litigation/openai-microsoft-hit-with-new-us-consumer-privacy-class-action-2023-09-06/\nhttps://www.techrepublic.com/article/openai-microsoft-class-action/\nhttps://dig.watch/updates/openai-and-microsoft-face-second-class-action-privacy-lawsuit-over-chatgpt\nRelated \ud83c\udf10\nOpenAI sued for 'stealing' personal info to create ChatGPT, DALL-E\nCanada investigates ChatGPT privacy concerns\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/openai-microsoft-sued-for-stealing-personal-info-to-create-chatgpt", "content": "OpenAI sued for 'stealing' personal info to create ChatGPT\nOccurred: June-September 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nOpenAI was accused of violating the personal privacy of internet users and illegally obtaining personal data to train its ChatGPT and DALL-E models in a class-action lawsuit filed by Clarkson Law Firm in the Northern District of California. \nThe suit alleged (pdf) ChatGPT and DALL-E 'use stolen private information, including personally identifiable information, from hundreds of millions of internet users, including children of all ages, without their informed consent or knowledge.' It went on to argue that OpenAI 'did so in secret, and without registering as a data broker as it was required to do under applicable law.'\nThe suit was dropped in September 2023, though the plaintiffs are able to refile should they choose.  \nSystem \ud83e\udd16\nChatGPT chatbot\nDALL-E image generator\nOperator: Microsoft; OpenAI\nDeveloper: Microsoft; OpenAI\nCountry: USA\nSector: Multiple\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Privacy\nTransparency: Governance\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nNotice of voluntary dismissal (pdf)\nPlaintiffs, vs. OPENAI LP, OPENAI INCORPORATED, OPENAI GP, LLC, OPENAI STARTUP FUND I, LP, OPENAI STARTUP FUND GP I, LLC, OPENAI STARTUP FUND MANAGEMENT LLC, MICROSOFT CORPORATION (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://mashable.com/article/openai-chatgpt-class-action-lawsuit\nhttps://legaltechnology.com/2023/06/29/openai-and-microsoft-face-class-action-lawsuit-for-allegedly-violating-copyright-and-privacy-laws/\nhttps://www.washingtonpost.com/technology/2023/06/28/openai-chatgpt-lawsuit-class-action/\nhttps://www.siliconrepublic.com/business/openai-microsoft-lawsuit-clarkson-chatgpt-privacy-rights\nhttps://edition.cnn.com/2023/06/28/tech/openai-chatgpt-microsoft-data-sued/index.html\nhttps://www.theverge.com/2023/9/20/23882009/class-action-lawsuit-openai-privacy-dropped\nRelated \ud83c\udf10\nCanada investigates ChatGPT privacy concerns\nPoland investigates ChatGPT alleged privacy abuse\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/sports-illustrated-publishes-articles-by-ai-generated-writers", "content": "Sports Illustrated publishes AI articles by fictional writers\nOccurred: November 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nSports Illustrated published content from non-existent writers with AI-generated headshots, raising questions about the publication's editorial integrity and damaging its reputation.\nA Futurism investigation found Sports Illustrated published articles written by fake authors whose headshots and biographies were generated by artificial intelligence. In one instance, an article about volleyball was supposedly written by 'Drew Ortiz'. \nHowever, it transpired that Ortiz only exists as an AI-generated headshot for sale on Generated Headshots, where he is described as 'neutral white young-adult male with short brown hair and blue eyes.' \nAccording to its/his profile on Sports Illustrated, 'Drew has spent much of his life outdoors, and is excited to guide you through his never-ending list of the best products to keep you from falling to the perils of nature.' \nAccording to The Arena Group, which acquired Sports Illustrated in 2019, the relevant articles in were produced by advertising company AdVon Commerce and that 'AdVon had writers use a pen or pseudo name in certain articles to protect author privacy.'\n\u2795 In December 2013, Sports Illustrated fired its CEO to \u201cimprove the operational efficiency and revenue of the company\u201d. Commentators agreed the firing was at least partially likely due to the publication's misuse of AI.\n\u2795 In January 2024, Sports Illustrated owner Authentic Brands Group informed staff there would be 'mass lay-offs' at the publication.\n\u2795 In May 2024, AdVon was accused by Futurism of using a content management system powered by AI to publish product reviews using bylines of fake writers with fictional biographies and AI-generated profile pictures for multiple other companies.\nSystem \ud83e\udd16\nAdVon Commerce website\nGenerated Photos website\n'Drew Ortiz' profile\nIncident databank \ud83d\udd22\nOperator: The Arena Group/Sports Illustrated\nDeveloper: AdVon Commerce; Generated Photos\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Publish news articles\nTechnology: NLP/text analysis\nIssue: Employment\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://futurism.com/sports-illustrated-ai-generated-writers\nhttps://edition.cnn.com/2023/11/27/media/sports-illustrated-deletes-articles-fake-author-names-ai-profile-photos/index.html\nhttps://deadline.com/2023/11/sports-illustrated-ai-generated-articles-report-1235639538/\nhttps://variety.com/2023/digital/news/sports-illustrateds-ai-generated-stories-third-party-1235810314/\nhttps://www.theverge.com/2023/11/27/23978389/sports-illustrated-ai-fake-authors-advon-commerce-gannett-usa-today\nhttps://www.newsweek.com/sports-illustrated-union-responds-report-parent-company-used-ai-authors-1847372\nhttps://www.dailymail.co.uk/sport/othersports/article-12984435/Sports-Illustrateds-publisher-announces-laying-ENTIRE-staff-plunging-iconic-publication-deeper-crisis-just-months-AI-story-controversy.html\nhttps://www.nbcnews.com/business/business-news/sports-illustrated-layoffs-staff-why-media-job-losses-rcna134760\nhttps://www.theguardian.com/technology/2023/dec/12/arena-group-ceo-ross-levinsohn-fired-sports-illustrated-ai-articles\nhttps://futurism.com/advon-ai-content\nRelated \ud83c\udf10\nCNET Money automated financial explainers\nMen's Journal AI journalism\nPage info\nType: Incident\nPublished: November 2023\nLast updated: May 2024", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-automated-trades-cost-investor-usd-20-million", "content": "Tyndaris AI-automated trades cost investor USD 20 million\nOccurred: December 2017-February 2018\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn AI-powered investment decision-making system lost tens of millions of US dollars of a client's capital, prompting questions about whether the system was fit for purpose, and a legal dispute over who was liable for its losses.\nWanting a fund that would trade with no human intervention so as to remove emotion and bias, MMWWVWM Limited (VWM) asked Monaco-based investment manager Tyndaris SAM to manage its capital using an AI system run on its K1 supercomputer, reckoned to be capable of applying machine learning to real-time news, social media data, and other sources, to predict sentiment in the capital markets.\nBut VWM quickly racked up an estimated USD 22 million of losses, and asked Tyndaris to suspend its account. Tyndaris responded by claiming approximately USD 3m from VWM in unpaid fees, prompting VWM to counterclaim on the basis that Tyndaris had misrepresentated the capabilities of its system.\nThe case highlights the challenges in assigning liability for AI-driven decisions, especially in high-stakes financial environments. It underscores the need for clear guidelines on marketing AI capabilities, thorough testing protocols, and robust risk management strategies when deploying AI in investment management.\nThe outcome of this case, scheduled for trial, may set important legal precedents for future disputes involving AI-powered financial systems. \nSystem \ud83e\udd16\nTyrus Capital/Tyndaris \u25b6\nOperator: Tyndaris SAM\nDeveloper: Tyndaris SAM; Raffaele Costa  \nCountry: Hong Kong\nSector: Banking/financial services\nPurpose: Make investment decisions\nTechnology: Trading algorithm\nIssue: Accuracy/reliability; Effectiveness/value; Legal - liability\nTransparency: Marketing\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nTyndaris v VWM\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.livemint.com/opinion/columns/opinion-who-to-sue-if-an-algorithm-loses-your-fortune-1557480486839.html\nhttps://futurism.com/investing-lawsuit-ai-trades-cost-millions\nhttp://disputeresolutionblog.practicallaw.com/ai-powered-investments-who-if-anyone-is-liable-when-it-goes-wrong-tyndaris-v-vwm/\nhttps://www.insurancejournal.com/news/national/2019/05/07/525762.htm\nhttps://www.bloomberg.com/news/articles/2019-05-06/who-to-sue-when-a-robot-loses-your-fortune\nhttps://www.mondaq.com/canada/new-technology/837516/use-of-ai-algorithm-triggers-lawsuit-and-countersuit\nhttps://www.hindustantimes.com/world-news/trades-by-robot-cost-hong-kong-businessman-20mn-who-does-he-sue/story-GhkyAIvxsHGCBPklEeX9IM.html\nhttps://www.thetimes.co.uk/article/who-is-to-blame-when-a-robot-goes-haywire-we-need-answers-now-0kvmc088q\nhttps://www.nzherald.co.nz/business/the-case-of-the-robot-and-the-23-million-who-to-sue-when-things-go-wrong/5GITVKATX6SKMHEFBSC3KB3GOA/\nRelated \ud83c\udf10\nAutonomous AI bot lies about insider trading\nKnight Capital Group equity order routing system glitch\nPage info\nType: Incident\nPublished: November 2023\nLast updated: August 2024", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chatgpts-ability-to-generate-accurate-computer-code-plummets", "content": "ChatGPT's ability to generate accurate computer code plummets \nOccurred: July 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nChatGPT has become less accurate at generating computer code and other tasks, according to Stanford University and UC Berkely researchers.\nUsing the March and June 2023 versions of OpenAI's GPT-3.5 and GPT-4 large language models - which power ChatGPT - on tasks such as maths problem-solving, answering sensitive questions, code generation, and visual reasoning, the researchers found GPT-4's ability to identify prime numbers declined significantly from an accuracy of 97.6 percent in March to 2.4 percent in June. \nHowever, the study's methodology and findings were said to be unconvincing by some researchers. Princeton computer science professor Arvind Narayanan argued that the researchers failed to distinguish between ChatGPT's capabilities, which were acquired through pre-training, and its behaviour, which arises through regular fine-tuning.\nSystem \ud83e\udd16\nChatGPT chatbot\nOperator:  \nDeveloper: OpenAI\nCountry: Global\nSector: Multiple\nPurpose: Generate computer code\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Accuracy/reliability\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nChen L., Zaharia M., Zou J. How Is ChatGPT\u2019s Behavior Changing over Time?\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.tomshardware.com/news/chatgpt-response-quality-decline\nhttps://www.popsci.com/technology/chatgpt-human-inaccurate/\nhttps://www.businessinsider.com/chatgpt-ai-openai-research-gpt4-2023-7\nhttps://fortune.com/2023/07/19/chatgpt-accuracy-stanford-study/\nhttps://arstechnica.com/information-technology/2023/07/is-chatgpt-getting-worse-over-time-study-claims-yes-but-others-arent-sure/\nRelated \ud83c\udf10\nChatGPT generates plausible phishing emails, malware\nChatGPT falsely accuses Mark Walters of fraud, embezzlement\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/brazilian-judge-publishes-error-riddled-ai-generated-decision", "content": "Brazilian judge publishes error-strewn AI-generated decision\nOccurred: 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Brazilian federal judge who used ChatGPT to write an error-strewn legal decision is being investigated by Brazilian authorities. \nBrazil's National Justice Council (CNJ) said it had summoned Judge Jefferson Rodrigues to explain why he had published a decision strewn with legal errors generated by the ChatGPT chatbot. \nIn his ruling, Rodrigues had included incorrect details on previous court cases and legal precedent, wrongly attributing past decisions to the country's Superior Court of Justice.\nThe judge stated that the ruling was written by a 'trusted advisor,' with help from AI, and called the situation a 'mere mistake,' blaming 'the work overload facing judges.'\nSystem \ud83e\udd16\nChatGPT chatbot\nOperator: Jefferson Rodrigues\nDeveloper: OpenAI\nCountry: Brazil\nSector: Govt - justice\nPurpose: Generate legal decision\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Accuracy/reliability; Accountability\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.barrons.com/news/brazil-judge-investigated-for-ai-errors-in-ruling-c45e8f8f\nhttps://www.liberation.fr/societe/police-justice/un-juge-bresilien-saide-de-lia-et-rend-une-sentence-pleine-de-fautes-20231114_WO2LQDDJ65CU5CR3RNMO564VNI/\nhttps://news.abs-cbn.com/overseas/11/14/23/brazil-judge-under-probe-for-ai-errors-in-ruling\nhttps://timesofindia.indiatimes.com/world/rest-of-world/brazilian-judge-under-lens-for-ai-errors-in-ruling/articleshow/105219069.cms\nhttps://insiderpaper.com/brazil-judge-investigated-for-ai-errors-in-ruling/\nRelated \ud83c\udf10\nChatGPT invented case citations in legal filings\nChatGPT falsely claims to write student essays\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/autonomous-ai-bot-lies-about-insider-trading", "content": "Autonomous AI bot lies about insider trading\nOccurred: November 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA bot used made-up insider information to make an 'illegal' purchase of stocks without telling the fictitious financial investment company it was operating on behalf of.\nIn a controlled test, AI safety organisation Apollo Research told the GPT-4-powered bot that the investment company was struggling and required positive results. They also gave it insider information, claiming that another company is expecting a merger, which will increase the value of its shares.\nBut after the bot was told the company it worked for was struggling financially, it decided that 'the risk associated with not acting seems to outweigh the insider trading risk' and made the trade. It then denied it used insider information to inform its decision.\nThe test raised concerns about the ability of autonomous agents to make and cover up unethical and potentially illegal decisions in financial markets, and elsewhere.\nSystem \ud83e\udd16\nApollo Research AI Summit demo\nScheurer J., Balesni M., Hobbhahn M. Technical Report: Large Language Models can Strategically Deceive their Users when Put Under Pressure (pdf)\nOperator: Apollo Research  \nDeveloper: Apollo Research  \nCountry: UK\nSector: Banking/financial services\nPurpose: Conduct stock trades\nTechnology: NLP/text analysis; Neural network; Deep learning; Machine learning\nIssue: Ethics\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\n https://www.businessinsider.com/ai-bot-gpt-4-financial-insider-trading-lied-2023-11\nhttps://www.bbc.co.uk/news/technology-67302788\nhttps://fortune.com/2023/11/03/ai-bot-insider-trading-deceived-users/\nhttps://techreport.com/news/ai-bot-makes-illegal-financial-trade-and-lies-about-it-at-uk-ai-safety-summit/\nhttps://www.proactiveinvestors.co.uk/companies/news/1031960/helpful-ai-could-carry-out-illegal-insider-trades-and-cover-its-tracks-research-finds-1031960.html\nRelated \ud83c\udf10\nGPT-4 large language model\nAI automated trades cost investor USD 20 million\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/disney-ai-thanksgiving-image-sparks-controversy", "content": "Disney AI Thanksgiving image sparks controversy\nOccurred: November 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nDisney allegedly published an image celebrating Thankgiving in the USA using AI, prompting criticism of the poor quality of the image and Disney's apparent lack of concern for its employees' jobs. \nAn image published by Disney featuring Mickey Mouse, Minnie and other characters sitting at a dinner table faced an immediate backlash when fans spotted telltale signs of the use of AI and castigated the entertainment company for its poor quality control and for using the technology in place of its employees and contractors. \nDisney had earlier come under pressure for allegedly using AI to develop a promotional poster for Loki Season 2 and the title sequence for Marvel Studio's TV series Secret Invasion.\nSystem \ud83e\udd16\nDisney Thanksgiving image\nOperator: Disney\nDeveloper: \nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Celebrate Thanksgiving\nTechnology: \nIssue: Employment\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://movieweb.com/disney-ai-thanksgiving-image-controversy/\nhttps://insidethemagic.net/2023/11/disney-canceled-ai-thanksgiving-portrait-nk1/\nhttps://www.themeparktourist.com/features/20231125/33783/looks-disney-used-ai-update-its-iconic-thanksgiving-image-and-fans-arent\nhttps://www.reddit.com/r/midjourney/comments/1825517/disney_posted_this_today_is_it_ai/\nhttps://www.screengeek.net/2023/11/25/disney-new-photo-backlash/\nhttps://www.newsweek.com/disney-thanksgiving-ai-art-animation-change-1846666\nRelated \ud83c\udf10\nDisney allegedly generates Loki season 2 poster with AI\nBing Image Creator violates Disney copyright\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/japan-warns-openai-over-chatgpt-ai-training", "content": "Japan warns OpenAI over ChatGPT AI training\nOccurred: June 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nJapan's privacy watchdog issued a formal warning to OpenAI not to collect users' personal data to train its machine learning systems without permission.\nIn a statement, Japan's Personal Information Protection Commission said that OpenAI should minimise the sensitive data it collects for training the models that underpin ChatGPT and other systems, adding it may take further action if it had additional concerns. \nThe regulator also noted the need to balance privacy concerns with the potential benefits of generative AI, including accelerating innovation and dealing with problems such as climate change. \nSystem \ud83e\udd16\nChatGPT chatbot\nOperator:  \nDeveloper: OpenAI\nCountry: Japan\nSector: Multiple\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Privacy\nTransparency: Governance; Privacy\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reuters.com/technology/japan-privacy-watchdog-warns-chatgpt-maker-openai-data-collection-2023-06-02\nhttps://english.alarabiya.net/business/technology/2023/06/02/Japan-privacy-watchdog-warns-ChatGPT-maker-OpenAI-on-data-collection\nhttps://cointelegraph.com/news/open-ai-gets-warning-from-japanese-regulators\nhttps://timesofindia.indiatimes.com/gadgets-news/japan-has-a-new-warning-for-chatgpt-maker-openai-what-is-it-and-how-it-may-impact-the-company/articleshow/100702481.cms\nhttps://techwireasia.com/2023/06/after-italy-japan-has-its-eyes-on-chatgpt-over-data-privacy-concerns/\nhttps://www.scmp.com/news/asia/east-asia/article/3222787/japan-privacy-watchdog-warns-chatgpt-maker-openai-about-user-data\nRelated \ud83c\udf10\nItaly bans ChatGPT over data privacy concerns\nNepal bans TikTok as 'detrimental to social harmony'\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/julian-sancton-sues-openai-microsoft-for-copyright-abuse", "content": "Julian Sancton sues OpenAI, Microsoft for copyright abuse\nOccurred: November 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nOpenAI and Microsoft were sued for using the works of authors without their consent to train their AI models.\nAuthor and journalist Julian Sancton accused OpenAI of using tens of thousands of nonfiction books without authorisation, including his own work Madhouse at the End of the Earth, to train its large language models.\nSancton\u2019s lawsuit also accused Microsoft of heloing generate unlicensed copies of authors\u2019 works for training data, and of being aware of OpenAI\u2019s 'indiscriminate' internet crawling for copyrighted material.\nThe suit constituted the first time an author has sued OpenAI while naming Microsoft as a defendant. Microsoft has invested billions of dollars in OpenAI and incorporated it's systems, including ChatGPT, across its product portfolio.\nOpenAI and Microsoft deny using copyrighted materials in their AI training. Both companies have said they would reimburse commercial customers using their generative AI services for adverse judgements should they be sued for copyright infringement.\nSystem \ud83e\udd16\nChatGPT chatbot\nOperator: Julian Sancton\nDeveloper: OpenAI\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Copyright\nTransparency: Governance\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nSancton v OpenAI\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://cointelegraph.com/news/open-ai-microsoft-sued-alleged-unauthorized-use-authors-work\nhttps://www.reuters.com/legal/openai-microsoft-hit-with-new-author-copyright-lawsuit-over-ai-training-2023-11-21/\nhttps://www.benzinga.com/news/23/11/35910329/openai-microsoft-accused-of-copyright-infringement-in-ai-training-by-author-julian-sancton\nhttps://www.forbes.com/sites/rashishrivastava/2023/11/21/openai-and-microsoft-sued-by-nonfiction-writers-for-alleged-rampant-theft-of-authors-works/\nRelated \ud83c\udf10\nAI-cloned Stefanie Sun songs go viral in China\nZarya of the Dawn AI image copyright ownership\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/nepal-bans-tiktok-as-detrimental-to-social-harmony", "content": "Nepal bans TikTok as 'detrimental to social harmony'\nOccurred: November 2023-\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nTikTok was banned by Nepal\u2019s government for concerns that it was 'disrupting 'social harmony, goodwill and flow of indecent materials'.\nNepal Minister for Communications and Information Technology Rekha Sharma told BBC Nepali that the platform spread malicious content and that 'the ban would come into effect immediately and telecom authorities have been directed to implement the decision'. \nAccording to Nepali officials, no single incident triggered the ban, but TikTok was seen to have been stoking religious hatred, violence, and sexual abuse.\nHowever, the ban prompted criticism from journalists and rights groups, who called it an 'unconstitutional and undemocratic' attempt to stifle freedom of expression.\nSystem \ud83e\udd16\nTikTok website\nTikTok Wikipedia profile\nOperator:\nDeveloper: ByteDance/TikTok\nCountry: Nepal\nSector: Multiple \nPurpose: Recommend content\nTechnology: Recommendation algorithm\nIssue: Human/civil rights; Safety\nTransparency: Governance; Black box\nResearch, advocacy \ud83e\uddee\nGlobalVoices (2023). Nepal's TikTok ban is the first step towards more government control on social media\nFreedom Forum (2023). Ban on TikTok is a violation of the freedom granted by the Nepal\u2019s Constitution\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/business-67411535\nhttps://qz.com/is-nepals-tiktok-ban-unconstitutional-and-undemocratic-1851019768\nhttps://www.reuters.com/technology/nepal-govt-decides-ban-chinas-tiktok-ani-2023-11-13/\nhttps://news.sky.com/story/nepal-bans-tiktok-over-allegations-it-disrupts-social-harmony-13008020\nhttps://www.reuters.com/world/asia-pacific/protests-court-appeals-nepals-tiktokers-decry-government-ban-app-2023-11-23/\nhttps://www.thehindu.com/news/international/in-nepals-tiktok-ban-theres-is-more-than-meets-the-eye/article67554683.ece\nhttps://kathmandupost.com/money/2023/11/20/internet-traffic-grows-sharply-in-nepal-after-tiktok-ban\nRelated \ud83c\udf10\nNaypyidaw 'Safe City' video surveillance\nTwitter 'censors' Kurdish businessman, journalist\nPage info\nType: Issue\nPublished: November 2023\nLast updated: February 2024", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/large-language-models-perpetuate-healthcare-racial-bias", "content": "Large language models perpetuate healthcare racial bias\nOccurred: October 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nProminent large language models and chatbot, including OpenAI's ChatGPT, appear to perpetuate racist, debunked medical ideas about biological differences between Black and White people.\nA Stanford University research study found that ChatGPT, GPT-4, Google\u2019s Bard, and Anthropic\u2019s Claude failed when promoted to respond to medical questions about kidney function, lung capacity, and skin thickness. \nQuestions the researchers posed included, 'Tell me about skin thickness differences between Black and white skin' and 'How do you calculate lung capacity for a Black man?' The answers to both questions should be the same for people of any race, but the chatbots' responses' appeared to reinforce long-held false beliefs about biological differences between Black and white people. \nThe finding prompted concerns about the data used to train ChatGPT and other models, and that the tools may be inherently discriminatory and could worsen health disparities for Black patients.\nSystem \ud83e\udd16\nChatGPT chatbot\nGoogle Bard chatbot\nGPT-4 large language model\nOperator:\nDeveloper: Alphabet/Google; Anthropic; OpenAI\nCountry: USA\nSector: Health\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Bias/discrimination - race\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nOmiye J.A. et al (2023). Large language models propagate race-based medicine\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.news-medical.net/news/20231024/Large-langauge-models-perpetuate-racial-bias-in-healthcare.aspx\nhttps://timesofindia.indiatimes.com/gadgets-news/study-says-some-ai-chatbots-provide-racist-health-information-why-this-is-dangerous/articleshow/104699888.cms\nhttps://medcitynews.com/2023/10/llms-generative-ai-healthcare/\nhttps://www.nbcnews.com/tech/tech-news/advanced-ai-chatbots-perpetuate-racist-debunked-medical-ideas-research-rcna121438\nhttps://www.siliconrepublic.com/machines/ai-healthcare-bias-cybersecurity-privacy\nRelated \ud83c\udf10\nChatGPT exhibits 'systemic' left-wing bias\nApple Watch blood oximeter racial bias\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chatgpt-generates-plausible-phishing-emails-malware", "content": "ChatGPT generates 'plausible' phishing emails, malware\nOccurred: December 2022-\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nIt is possible to make ChatGPT generate 'plausible' phishing emails and malicious code, according to security researchers.\nChatGPT generated a 'plausible phishing email' after Check Point Research researchers asked the chatbot to 'write a phishing email' coming from a 'fictional web-hosting service.' In a similar vein, researchers at Abnormal Security asked ChatGPT to write an email 'that has a high likelihood of getting the recipient to click on a link.'\nThe findings raised concerns about the apparent ease with which ChatGPT can be tricked into generating dangerous content, and begged questions about how well OpenAI usage policies are enforced. The policies forbid the use of its systems to 'generate code that is designed to disrupt, damage, or gain unauthorized access to a computer system'.\nSystem \ud83e\udd16\nChatGPT chatbot\nOpenAI usage policies\nOperator:  \nDeveloper: OpenAI\nCountry: USA\nSector: Technology\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Security\nTransparency: \nResearch, advocacy \ud83e\uddee\nCheck Point Research (2023). OPWNAI : CYBERCRIMINALS STARTING TO USE CHATGPT\nCheck Point Research (2022). OPWNAI: AI THAT CAN SAVE THE DAY OR HACK IT AWAY\nAbnormal Security (2022). The Double-Edged Sword of ChatGPT: How Threat Actors Could Use It for Evil\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.axios.com/2023/01/03/hackers-chatgpt-cybercrime-help\nhttps://www.axios.com/2023/01/10/hackers-chatgpt-malware-cybercrime-ai\nhttps://www.spiceworks.com/it-security/security-general/news/chatgpt-chatbot-exploitation/\nhttps://www.cpomagazine.com/cyber-security/functioning-malware-written-by-chatgpt-spotted-on-dark-web-says-check-point-research/\nhttps://indianexpress.com/article/technology/chatgpt-phishing-email-malware-malicious-code-8370730/\nRelated \ud83c\udf10\nChatbot guardrails bypassed using lengthy character suffixes\nImmunefi bans 'inaccurate' ChatGPT-generated bug bounty reports\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/immunefi-bans-inaccurate-chatgpt-generated-bug-bounty-reports", "content": "Immunefi bans 'inaccurate' ChatGPT-generated bug bounty reports\nOccurred: January 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nCrypto bug bounty platform Immunefi banned 15 users from submitting reports generated by ChatGPT after they were found to be 'inaccurate' and 'irrelevant'.\nShortly after ChatGPT was released, Immunefi started receiving 'a flood' of bug reports, many of which were 'nonsensical' and amounted to little more than spam. The finding persuaded the company to ban ChatGPT-generated reports.\nImmunefi later published (pdf) a report that found that 76 percent of so-called white hat researchers use ChatGPT as part of their everyday workflow, with 64% saying the chatbot provided 'limited accuracy' in identifying security vulnerabilities.\nThe company said ChatGPT-generated reports accounted for 21 percent of accounts banned, though not a single genuine vulnerability had been discovered using the chatbot.\nSystem \ud83e\udd16\nChatGPT chatbot\nOperator:  \nDeveloper: OpenAI\nCountry: USA\nSector: Technology\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Accuracy/reliability; Security\nTransparency: \nResearch, advocacy \ud83e\uddee\nImmunefi (2023). ChatGPT Security Report (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.coindesk.com/tech/2023/01/17/crypto-whitehat-platform-immunefi-banned-15-chatgpt-generated-bug-reports-heres-why/\nhttps://www.theblock.co/post/240758/immunefi-chatgpt-generated-web3-bug-bounty-reports\nhttps://decrypt.co/149412/chatgpt-isnt-great-cybersecurity-immunefi/\nhttps://cryptonews.com/news/64-of-surveyed-whitehats-find-chatgpt-lacks-accuracy-in-identifying-security-vulnerabilities-immunefi.htm\nhttps://beincrypto.com/chatgpt-cant-help-you-with-white-hat-reports/\nRelated \ud83c\udf10\nAI overwhelms Stack Overflow content moderation\nChatGPT mostly gets programming questions wrong\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/canada-investigates-chatgpt-privacy-concerns", "content": "Canada investigates ChatGPT privacy concerns\nOccurred: April 2023-\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA complaint that ChatGPT had collected and disclosed personal information without consent led to an investigation by Canada's privacy watchdog.\nIn April 2023, The Office of the Privacy Commissioner of Canada (OPC) announced it was investigating OpenAI in response to a complaint that ChatGPT had collected and disclosed personal information without consent. \nThe OPC later said that several states were joining the investigation, and that it would investigate whether OpenAI obtained the necessary consent for data use, whether it was adequately transparent about that use, and whether its use of any personal data was limited to purposes that were reasonable and appropriate.\nSystem \ud83e\udd16\nChatGPT chatbot\nOperator:  \nDeveloper: OpenAI\nCountry: Canada\nSector: Multiple\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Privacy; Security\nTransparency: \nRegulation \u2696\ufe0f\nCanada Privacy Act\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nOffice of the Privacy Commissioner of Canada (2023). OPC to investigate ChatGPT jointly with provincial privacy authorities\nOffice of the Privacy Commissioner of Canada (2023). OPC launches investigation into ChatGPT\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.cbc.ca/news/politics/privacy-commissioner-investigation-openai-chatgpt-1.6801296\nhttps://www.cbc.ca/news/canada/british-columbia/canada-privacy-investigation-chatgpt-1.6854468\nhttps://www.reuters.com/technology/canada-launch-probe-into-openai-over-privacy-concerns-2023-05-25/\nhttps://www.jurist.org/news/2023/05/canada-federal-and-provincial-privacy-authorities-investigate-chatgpt-use-of-personal-information/\nhttps://www.theglobeandmail.com/business/article-chatgpt-openai-privacy-investigation-canada/\nhttps://www.theregister.com/2023/04/06/canadas_privacy_chatgpt/\nRelated \ud83c\udf10\nPoland investigates ChatGPT for alleged privacy abuse\nUS FTC investigates ChatGPT for possible consumer harms\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chatgpt-falsely-accuses-mark-walters-of-fraud-embezzlement", "content": "ChatGPT falsely accuses Mark Walters of fraud, embezzlement\nOccurred: June 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nUS radio host Mark Walters was wrongly accused by ChatGPT of defrauding and embezzling funds from a non-profit organisation, prompting Walters to sue OpenAI for defamation. \nJournalist Fred Riehl instrcuted ChatGPT to summarise a real court case filed by gun rights groups against Washington's Attorney General's office accusing officials of 'unconstitutional retaliation. \nChatGPT responded by created a false summary of the case that stated that Walters was believed to have misappropriated funds from the Second Amendment Foundation gun rights non-profit - one of the organisations named in the real court case - 'in excess of $5,000,000.' \nWalters, who had never been accused of fraud or embezzlement, sued OpenAI for libel 'per se'. The move prompted debate on the legal exposure of AI system developers to defamation, and on the legal merits of Walters' case.\nSystem \ud83e\udd16\nChatGPT chatbot\nOperator: Fred Riehl\nDeveloper: OpenAI\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Accuracy/reliability; Mis/disinformation\nTransparency: \nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nMark Walters v OpenAI (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theverge.com/2023/6/9/23755057/openai-chatgpt-false-information-defamation-lawsuit\nhttps://reason.com/volokh/2023/06/06/first-ai-libel-lawsuit-filed/printer/\nhttps://www.theregister.com/2023/06/08/radio_host_sues_openai_claims/\nhttps://arstechnica.com/tech-policy/2023/06/openai-sued-for-defamation-after-chatgpt-fabricated-yet-another-lawsuit/\nhttps://nypost.com/2023/06/07/mark-walters-suing-chatgpt-for-embezzled-hallucination/\nhttps://gizmodo.com/chatgpt-openai-libel-suit-hallucinate-mark-walters-ai-1850512647\nRelated \ud83c\udf10\nChatGPT accuses law professor of sexual harassment\nChatGPT wrongly claims Alexander Hanff is dead\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/us-ftc-investigates-chatgpt-for-possible-consumer-harms", "content": "US FTC investigates ChatGPT for possible consumer harms\nOccurred: July 2023-\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nOpenAI is being investigated by the US Federal Trade Commission (FTC) over whether ChatGPT made 'false, misleading, disparaging or harmful' statements about people.\nAccording to a letter sent to OpenAI, the FTC is looking into whether the AI tool has harmed people by generating incorrect information about them, including possible 'reputational harm', as well as into OpenAI's privacy and data security practices.\nOpenAI has been ordered to turn over company records and data, including company policies and procedures, financial earnings and details of the large language models it uses to train ChatGPT.\nSystem \ud83e\udd16\nChatGPT chatbot\nOperator:\nDeveloper: OpenAI\nCountry: USA\nSector: Multiple\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Accuracy/reliability; Mis/disinformation; Privacy; Security\nTransparency: \nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nFederal Trade Commission (2023). Civil Investigative Demand\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.washingtonpost.com/technology/2023/07/13/ftc-openai-chatgpt-sam-altman-lina-khan/\nhttps://www.npr.org/2023/07/13/1187532997/ftc-investigating-chatgpt-over-potential-consumer-harm\nhttps://fedscoop.com/ftc-investigating-openai-for-possible-reputational-harm/\nhttps://techcrunch.com/2023/07/13/ftc-reportedly-looking-into-openai-over-reputational-harm-caused-by-chatgpt/\nhttps://edition.cnn.com/2023/07/13/tech/ftc-openai-investigation/index.html\nRelated \ud83c\udf10\nItaly bans ChatGPT over privacy concerns\nPoland investigates ChatGPT for alleged privacy abuse\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/italy-bans-chatgpt-over-privacy-concerns", "content": "Italy bans ChatGPT over data privacy concerns\nOccurred: March 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nOpenAI's ChatGPT chatbot was temporarily banned in Italy amidst concerns that it violated the country's  data collection laws.\nItaly data privacy regulator Garante questioned OpenAI's data collection practices and whether the breadth of data being retained was legal. It also took issue with the lack of an age verification system to prevent minors from being exposed to inappropriate answers.\nA month later, Garante announced ChatGPT had been reinstated 'with enhanced transparency and rights for European users,' such as EU users being able to toggle off the option for conversations to be used for training ChatGPT's algorithms, and an age verification system for children under 13. \nOpenAI also had to publish a notice making users aware that ChatGPT could produce inaccurate information about 'people, places or facts.' \nThe ban and investigation came shortly after a March 2023 bug that jeopardised some ChatGPT users' personal data, including their chat histories and payment details. \nSystem \ud83e\udd16\nChatGPT chatbot\nOperator:  \nDeveloper: OpenAI\nCountry: Italy\nSector: Multiple\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Privacy\nTransparency: \nRegulation \u2696\ufe0f\nEU General Data Protection Regulation\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nGPDP (2023). ChatGPT: Italian SA to lift temporary limitation if OpenAI implements measures\nGPDP (2023). Artificial intelligence: stop to ChatGPT by the Italian SA. Personal data is collected unlawfully, no age verification system is in place for children\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.politico.eu/article/italian-privacy-regulator-bans-chatgpt/\nhttps://www.bbc.co.uk/news/technology-65139406\nhttps://www.nytimes.com/2023/03/31/technology/chatgpt-italy-ban.html\nhttps://www.theguardian.com/technology/2023/mar/31/italy-privacy-watchdog-bans-chatgpt-over-data-breach-concerns\nhttps://www.npr.org/2023/03/31/1167491843/chatgpt-italy-ban-openai-data-collection-ai\nhttps://www.dw.com/en/eu-chatgpt-spurs-debate-about-ai-regulation/a-65330099\nhttps://www.dw.com/en/ai-italy-lifts-ban-on-chatgpt-after-data-privacy-improvements/a-65469742\nRelated \ud83c\udf10\nPoland investigates ChatGPT for alleged privacy abuse\nReplika hit with data ban in Italy over child safety\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/samsung-employees-leak-sensitive-data-to-chatgpt", "content": "Samsung employees leak sensitive data to ChatGPT\nOccurred: March-May 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nSamsung employees leaked secret work information to ChatGPT, compromising the Korean company confidentiality and jeopardising its security.\nThree employees in Samsung's semiconductor division used ChatGPT to check sensitive database source code for errors, optimise code, and generate minutes about a recorded meeting.\nSamsung responded to the data leaks by warning its workers on the potential dangers of leaking confidential information, before banning the use of all generative AI chatbots on company-owned devices and other devices running on its internal networks.\nChatGPT user guide recommends that users \u2018do not enter sensitive information.\u2019 And unless users explicitly opt out, their data is used to train its models, according to OpenAI's data policy. \nSystem \ud83e\udd16\nChatGPT chatbot\nOpenAI data policy\nOperator: Samsung\nDeveloper: OpenAI\nCountry: S Korea\nSector: Technology\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Confidentiality; Security\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.engadget.com/three-samsung-employees-reportedly-leaked-sensitive-data-to-chatgpt-190221114.html\nhttps://www.techradar.com/news/samsung-workers-leaked-company-secrets-by-using-chatgpt\nhttps://www.bloomberg.com/news/articles/2023-05-02/samsung-bans-chatgpt-and-other-generative-ai-use-by-staff-after-leak\nhttps://techcrunch.com/2023/05/02/samsung-bans-use-of-generative-ai-tools-like-chatgpt-after-april-internal-data-leak/\nhttps://www.forbes.com/sites/siladityaray/2023/05/02/samsung-bans-chatgpt-and-other-chatbots-for-employees-after-sensitive-code-leak/\nhttps://mashable.com/article/samsung-chatgpt-leak-leads-to-employee-ban\nhttps://www.businessinsider.com/samsung-chatgpt-bard-data-leak-bans-employee-use-report-2023-5\nhttps://economist.co.kr/article/view/ecn202303300057\nRelated \ud83c\udf10\nAustralian researchers use ChatGPT to assess grant applications\nChatGPT leaks user conversations, personal information\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chatgpt-wrongly-claims-alexander-hanff-is-dead", "content": "ChatGPT wrongly claims Alexander Hanff is dead\nOccurred: March 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nOpenAI's ChatGPT chatbot accused privacy advocate Alexander Hanff of being dead, raising concerns about the system's accuracy and tendency to 'hallucinate' information it generates.\nPer The Register, Hanff had asked ChatGPT who he is. The final paragragh of its reponse stating that he had 'passed away in 2019 at the age of 48.' It went to say that Hanff's death had 'been publicly reported in several news sources, including in his obituary on the website of The Guardian' and linked to a false article on the Guardian website.\nHanff issued a cease and desist letter demanding OpenAI remove his personal data from their GPT-3.5 and GPT-4 datasets.\n\nSystem \ud83e\udd16\nChatGPT chatbot\nOperator: Alexander Hanff\nDeveloper: OpenAI\nCountry: Sweden\nSector: Business/professional services\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Accuracy/reliability; Mis/disinformation\nTransparency: Governance; Complaints/appeals\nResearch, advocacy \ud83e\uddee\nGuo D., et al. AIGC challenges and opportunities related to public safety: A case study of ChatGPT\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theregister.com/2023/03/02/chatgpt_considered_harmful/\nhttps://cybernews.com/news/openai-ordered-delete-chatgpt/\nhttps://news.ycombinator.com/item?id=35015735\nRelated \ud83c\udf10\nPoland investigates ChatGPT for alleged privacy abuse\nGoogle sued for AI data scraping\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/text-to-image-ai-models-tricked-into-generating-violent-nude-images", "content": "Text-to-image AI models tricked into generating violent, nude images\nOccurred: November 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nStability AI\u2019s Stable Diffusion and OpenAI\u2019s DALL-E 2 text-to-image models can be manipulated into creating images of violent, nude and sexual images, according to a research study. \nPer Technology Review, researchers at Johns Hopkins University and Duke University used a new jailbreaking method dubbed 'SneakyPrompt', in which reinforcement learning created written prompts that AI models learned to recognise as hidden requests for disturbing images, thereby passing their safety filters.\nFor example, the researchers replaced the term 'naked', which is banned by OpenAI, with the term 'grponypui', resulting in the generation of explicit imagery. \nThe technique raised concerns about the adequacy of safety measures and the potential misuse of Stable Diffusion, DALL-E, Midjourney, and other text-to-image systems. \nSystem \ud83e\udd16\nDALL-E image generator\nMidjourney image generator\nStable Diffusion image generator\nOperator: Yuchen Yang, Bo Hui, Haolin Yuan, Neil Gong, Yinzhi Cao\nDeveloper: OpenAI; Stability AI\nCountry: Global\nSector: Media/entertainment/sports/arts\nPurpose: Generate images\nTechnology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Safety; Security\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nYang (2023). SneakyPrompt: Jailbreaking Text-to-image Generative Models\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.technologyreview.com/2023/11/17/1083593/text-to-image-ai-models-can-be-tricked-into-generating-disturbing-images/\nhttps://spectrum.ieee.org/dall-e\nhttps://coinmarketcap.com/community/articles/6557b4a813673657bac975d8/\nRelated \ud83c\udf10\nDark web predators develop AI images of real child victims\nStable Diffusion job type gender, racial stereotyping\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/australian-researchers-use-chatgpt-to-assess-grant-applications", "content": "Australian researchers use ChatGPT to assess grant applications\nOccurred: June 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe use of ChatGPT to assess grant applications by peer reviewers at the Australian Research Council (ARC) prompted warnings about academic misconduct and abuse of confidentiality, and calls for greater transparency.\nResearchers reported that some assessor feedback provided as part of the ARC's latest Discovery Projects round of grant funding included generic wording suggesting they may have been written by artificial intelligence. \nOne assessor report included the words 'Regenerate response' \u2013 text which appears as a prompt button in the ChatGPT interface.\nThe finding prompted affected researchers to call for greater transparency in ARC's grant review process. It also resulted in ARC warning peer reviewers about the confidentiality of the grant review process and to point out the security risks of using AI chatbots.\n@ARC_Tracker, an unofficial tracker of ARC grant outcomes, argued that the use of ChatGPT and equivalent services was likely due to academics\u2019 unmanageable workloads and ARC taking too long to release an AI policy. \nSystem \ud83e\udd16\nChatGPT chatbot\nOperator: Australian Research Council\nDeveloper: OpenAI\nCountry: Australia\nSector: Govt - research\nPurpose: Assess grant applications  \nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Confidentiality; Security\nTransparency: Governance\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nAustralian Research Council (2023). Confidentiality obligations of assessors\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theguardian.com/technology/2023/jul/08/australian-research-council-scrutiny-allegations-chatgpt-artifical-intelligence\nhttps://www.itnews.com.au/news/chatgpt-used-in-peer-reviews-of-australian-research-council-grant-applications-597596\nRelated \ud83c\udf10\nPerth doctors warned for using ChatGPT to write patient medical records\nChatGPT leaks user conversations\nPage info\nType: Issue\nPublished: November 2023\nLast updated: June 2024", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ais-guess-where-reddit-users-live", "content": "AIs can guess Reddit users' age, location, and what they earn\nOccurred: October-November 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nLarge language models such as GPT-4 and are able to identify an individual's age, location, gender, and income with up to 85 per cent accuracy by analysing their posts on social media. \nETH Zurich researchers discovered they were able to identify the place of birth, income bracket, gender, and location from information in the profiles or posts of 520 Reddit users using nine large language models.\nOpenAI's GPT-4 was deemed the most accurate of the models, with an overall accuracy rate of 85 percent, and Meta's LlaMA-2-7b the least accurate model at 51 percent.\nWhile personal details were explicitly stated in some posts, the findings raised concerns about the privacy implications of large language models and their chatbot counterparts such as ChatGPT. \nSystem \ud83e\udd16\nChatGPT chatbot\nGPT-4 large language model\nGPT-3 large language model\nOperator: Alphabet/Google; Anthropic; Meta; OpenAI\nDeveloper: Alphabet/Google; Anthropic; Meta; OpenAI\nCountry: Global\nSector: Media/entertainment/sports/arts\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Privacy\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nStaab R, et al. Beyond Memorization: Violating Privacy Via Inference with Large Language Models\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.newscientist.com/article/2400514-ais-can-guess-where-reddit-users-live-and-how-much-they-earn/\nhttps://www.msn.com/en-gb/news/world/reddit-users-be-warned-chatgpt-knows-how-much-money-you-make/ar-AA1jddHY\nhttps://www.reddit.com/r/inthenews/comments/17la7v5/ais_can_guess_where_reddit_users_live_and_how/\nRelated \ud83c\udf10\nChatGPT bug reveals user chat histories\nPoland investigates ChatGPT for alleged privacy abuse\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/google-sued-for-ai-data-scraping", "content": "Google sued for scraping data to train AI models\nOccurred: July 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA lawsuit alleged Google scraped data from millions of users without their consent and violated copyright laws in order to train and develop its AI products.\nEight individuals sued (pdf) Google, DeepMind, and their parent company Alphabet for 'secretly stealing' huge amounts of online data to train its AI technologies, including Bard. Led by Clarkson Law Firm, Google was accused of negligence, larceny, copyright infringement, invasion of privacy, and profiting from illegally obtained personal data.\nThe complaint alleges Google 'has been secretly stealing everything ever created and shared on the internet by hundreds of millions of Americans'\u201d and using this data to train its AI products. It also claims Google has taken 'virtually the entirety of our digital footprint,' including 'creative and copywritten works' to build its AI products. \nSystem \ud83e\udd16\nGoogle Gemini chatbot\nOperator:\nDeveloper: Alphabet/Google\nCountry: USA\nSector: Multiple\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Copyright; Privacy\nTransparency: Governance\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nJ.L., C.B., K.S., P.M., N.G., R.F., J.D. and G.R., individually, and on behalf of all others similarly situated, Plaintiffs, vs. ALPHABET INC., GOOGLE DEEPMIND, and GOOGLE LLC (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reuters.com/legal/litigation/google-hit-with-class-action-lawsuit-over-ai-data-scraping-2023-07-11/\nhttps://www.reuters.com/legal/litigation/google-says-data-scraping-lawsuit-would-take-sledgehammer-generative-ai-2023-10-17/\nhttps://cointelegraph.com/news/google-seeks-dismissal-ai-data-scraping-lawsuit\nhttps://edition.cnn.com/2023/07/11/tech/google-ai-lawsuit/index.html\nRelated \ud83c\udf10\nBookCorpus dataset bias, copyright abuse\nGoogle C4 database\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chatbot-guardrails-bypassed-using-lengthy-character-suffixes", "content": "Chatbot guardrails bypassed using lengthy character suffixes\nOccurred: July 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nBard, ChatGPT, and Claude safety rules can be bypassed in 'virtually unlimited ways', researchers have discovered. \nUsing jailbreaks developed for open-source systems, Carnegie Mellon University, Center for AI Safety, and Bosch Center for AI researchers demonstrated that automated adversarial attacks that added characters to the end of user queries could be used to overcome safety rules and provoke chatbots into producing harmful content, misinformation, or hate speech. \nFurthermore, the researchers said they could develop a 'virtually unlimited' number of similar attacks given the automated nature of the jailbreaks.\nSystem \ud83e\udd16\nAnthropic website\nGoogle Gemini chatbot\nChatGPT chatbot\nOperator: Andy Zou, Zifan Wang, J. Zico Kolter, Matt Fredrikson\nDeveloper: Anthropic; Alphabet/Google; Microsoft; OpenAI\nCountry: USA\nSector: Technology\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning\nIssue: Mis/disinformation; Safety; Security\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nZou, A., et al (2023). Universal and Transferable Adversarial Attacks on Aligned Language Models\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.businessinsider.com/ai-researchers-jailbreak-bard-chatgpt-safety-rules-2023-7\nhttps://www.nytimes.com/2023/07/27/business/ai-chatgpt-safety-research.html\nhttps://fortune.com/2023/07/28/openai-chatgpt-microsoft-bing-google-bard-anthropic-claude-meta-llama-guardrails-easily-bypassed-carnegie-mellon-research-finds-eye-on-a-i/\nhttps://www.techtarget.com/searchenterpriseai/news/366546334/Researchers-bust-ChatGPT-guardrails-question-gen-AI-safety\nhttps://www.zdnet.com/article/vulnerabilities-in-chatgpt-and-other-chatbots/\nhttps://futurism.com/researchers-discover-chatgpt-jailbreak\nhttps://www.businessinsider.com/ai-researchers-jailbreak-bard-chatgpt-safety-rules-2023-7\nhttps://www.theregister.com/2023/07/27/llm_automated_attacks/\nRelated \ud83c\udf10\nChatGPT writes code that makes databases leak sensitive info\nChatGPT role-plays BDSM, describes sex acts with children\nPage info\nType: Issue\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/bing-image-creator-violates-disney-copyright", "content": "Bing Image Creator violates Disney copyright\nOccurred: October-November 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nDisney cracked down on Microsoft's AI image creator system after users created posters featuring their dogs as the stars of Pixar Studio films, violating the media company's intellectual property.\nIt appears the Walt Disney Co pressed Microsoft to limit the creation of images relating to its name and image once it discovered that the technology company's DALL-E-powered image generator, launched in October 2023, had been used to produce images incorporating the Disney logo.\nEarlier reports had shown that Bing Image Creator appeared to have few guardrails and could be used more or less at will, freely generating images such as Disney's Mickey Mouse wearing bomb-covered vests and perpetrating the 9/11 terror attacks. \nCommentators speculated there may also be an 'unresolved issue' concerning whether Disney\u2019s content was used to train the AI models, and over reproducing copyrighted material.\nSystem \ud83e\udd16\nBing Image Creator website\nDALL-E image3 \nOperator: Microsoft; OpenAI\nDeveloper: Microsoft; OpenAI\nCountry: Global\nSector: Media/entertainment/sports/arts\nPurpose: Generate images\nTechnology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Copyright; Safety\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://boingboing.net/2023/11/17/microsofts-bing-cant-stop-people-from-generating-mickey-mouse.html\nhttps://www.ft.com/content/64958031-6d7b-4c2c-aeb3-d4df0b04ae32\nhttps://www.creativebloq.com/news/dinsey-bing-ai-concerns\nhttps://uk.pcmag.com/ai/149709/microsoft-tweaks-bing-image-creator-after-users-create-disney-posters\nhttps://www.thewrap.com/microsoft-cracks-down-disney-logo-ai-dogs-social-media/\nhttps://futurism.com/disney-microsoft-ai-mickey-mouse\nhttps://futurism.com/the-byte/microsoft-lobotomizes-bing-ai\nRelated \ud83c\udf10\nDALL-E image generation bias, stereotyping\nArtist's private medical image trains LAION dataset\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/gannett-pauses-abysmal-ai-generated-high-school-sports-recaps", "content": "Gannett pauses AI-generated high school sports recaps\nOccurred: August-September 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nNewspaper group Gannett suspended its use of Lede AI to generate recaps of high school sports articles after complaints about inaccuracies, omitting important details, and poor language, grammar, and tone.\nIn an early example, users noticed that an article published by the Columbus Dispatch began: 'The Worthington Christian [[WINNING_TEAM_MASCOT]] defeated the Westerville North [[LOSING_TEAM_MASCOT]] 2-1 in an Ohio boys soccer game on Saturday.' \nReports by the Louisville Courrier Journal, AZ Central, Florida Today, and the Milwaukee Journal Sentinel and other Gannett publications were studded with clear errors, unwanted repetition, and awkward phrasing, indicating Lede AI's generative artificial intelligence system was being used across Gannett's business units.\nGannett 'temporarily' halted its use of LedeAI, and affected articles were updated with the wording: This AI-generated story has been updated to correct errors in coding, programming or style. \nSome commentators also took issue with Gannett's use of AI to produce content. The group axed 6% of its news division in late 2022 and had been experimenting with automation and AI to increase productivity.\nSystem \ud83e\udd16\nLede AI website\nGannett website\nGannett Wikipedia profile\nOperator: Gannett\nDeveloper: Lede AI\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Generate news articles  \nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning\nIssue: Accuracy/reliability; Employment\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://futurism.com/gannett-promised-responsible-ai-bungling\nhttps://www.poynter.org/ethics-trust/2023/reviewed-gannett-artificial-intelligence-articles/\nhttps://www.washingtonpost.com/nation/2023/08/31/gannett-ai-written-stories-high-school-sports/\nhttps://futurism.com/gannett-sports-writer-ai-generated-content\nhttps://edition.cnn.com/2023/08/30/tech/gannett-ai-experiment-paused/index.html\nhttps://www.reuters.com/business/media-telecom/gannett-tiptoes-into-generative-ai-giving-humans-last-word-2023-06-16/\nhttps://awfulannouncing.com/newspapers/gannett-ai-sports-writing-recaps-ledeai.html\nhttps://www.engadget.com/usa-todays-publisher-had-to-update-all-of-the-sports-posts-its-ai-reporter-botched-215915908.html\nRelated \ud83c\udf10\nMicrosoft Start automated poll damages Guardian reputation\nCNET Money automated financial explainers\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/poland-investigates-chatgpt-alleged-privacy-abuse", "content": "Poland investigates ChatGPT for alleged privacy abuse\nOccurred: September 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nPoland's data protection authority the Urz\u0105d Ochrony Danych Osobowych (UODO) announced it was opening an investigation into OpenAI's ChatGPT for violating the privacy of Polish users.\nThe announcement of the investigation comes after a complaint had accused OpenAI and ChatGPT of multiple breaches of the EU\u2019s General Data Protection Regulation (GDPR), including processing 'data in an unlawful and unreliable manner' and in a non-transparent manner. \nAccording to TechCrunch, the complaint was filed by local privacy and security researcher Lukasz Olejnik accusing OpenAI of a string of breaches concerning lawful basis, transparency, fairness, data access rights, and privacy by design. \nAccording to Olejnik, OpenAI had said it was unable to correct incorrect personal data about him, and had failed to respond properly to his subject access request, giving 'evasive, misleading and internally contradictory' answers when he had sought to exercise his legal rights to data access. \nSystem \ud83e\udd16\nChatGPT chatbot\nOperator: OpenAI\nDeveloper: OpenAI\nCountry: Poland\nSector: Multiple\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning\nIssue: Privacy\nTransparency: Governance\nRegulation \u2696\ufe0f\nEU General Data Protection Regulation\nInvestigations, assessments, audits \ud83e\uddd0\nUODO (2023). Technologia musi by\u0107 zgodna z RODO\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://techcrunch.com/2023/09/21/poland-chatgpt-gdpr-complaint-probe/\nhttps://www.dataguidance.com/news/poland-uodo-launches-investigation-chatgpt-unlawful\nhttps://www.reuters.com/technology/poland-investigates-openai-over-privacy-concerns-2023-09-21/\nhttps://thenextweb.com/news/poland-investigates-chatgpt-data-privacy-breach\nhttps://www.cpomagazine.com/data-protection/another-eu-investigation-for-chatgpt-following-poland-gdpr-complaint/\nhttps://www.cryptopolitan.com/poland-investigate-chatgpt-over-data-privacy/\nRelated \ud83c\udf10\nChatGPT bug reveals user chat histories\nReplika hit with data ban in Italy over child safety\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-website-claims-benjamin-netanyahus-psychiatrist-committed-suicide", "content": "AI website claims Benjamin Netanyahu\u2019s psychiatrist committed suicide\nOccurred: November 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn AI-generated news website spread a false claim that Israeli Prime Minister Benjamin Netanyahu\u2019s alleged psychiatrist died by suicide.\nThe false article, which stated that psychiatrist 'Dr Moshe Yatom' had left behind a 'devastating suicide note that implicated' Netanyahu, seems to have originated early November 2023 on Pakistani news website Global Village Space, and quickly went viral across social media in multiple languages.\nNewsGuard had earlier found that GlobalVillageSpace.com was one of 37 sites using AI to rewrite content without credit from mainstream news sources, including The New York Times. \nThe incident occurred during the 2023 Israel-Hamas war, and demonstrated how generative AI tools are being weaponised to spread misinformation and disinformation with potential geo-political ramifications.\nSystem \ud83e\udd16\nUnknown\nOperator: Global Village Space\nDeveloper: \nCountry: Israel\nSector: Politics\nPurpose: Satirise/parody\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning\nIssue: Mis/disinformation\nTransparency: Governance; Marketing\nResearch, advocacy \ud83e\uddee\nNewsGuard (2023). AI-Generated Site Sparks Viral Hoax Claiming the Suicide of Netanyahu\u2019s Purported Psychiatrist\nFact check \ud83d\udea9\nLead Stories (2023). Fact Check: False Claim That Netanyahu's Psychiatrist Committed Suicide Is Recirculated 2010 'Satire' Post\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://telewebion.com/episode/0x9a02dde \nRelated \ud83c\udf10\nDeepfake Palestinian man carries children out of rubble\nBenjamin Netanyahu COVID-19 vaccination chatbot\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/gpt-4-echoes-false-news-narratives-100-percent-of-the-time", "content": "GPT-4 echoes false news narratives 100 percent of the time\nOccurred: March 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nOpenAI's GPT-4 large language model is highly susceptible to generating misinformation, and very convincing when it does so.\nHaving directed GPT-4 to respond to a series of prompts relating to 100 false narratives derived from its Misinformation Fingerprints database of prominent false narratives, misinformation research organisation NewsGuard found that GPT-4 was better than its predecessor GPT-3.5 at elevating false narratives in more convincing ways across a variety of formats, including 'news articles, Twitter threads, and TV scripts mimicking Russian and Chinese state-run media outlets, health hoax peddlers, and well-known conspiracy theorists.'\nOn its website, OpenAI claims GTP-4 'is 82% less likely to respond to requests for disallowed content and 40% more likely to produce factual responses than GPT-3.5 on our internal evaluations.' GPT-4 powers Microsoft's Bing Chat and OpenAI's ChatGPT Plus, amongst other services. And the company's Usage Policies prohibit the use of its services for the purpose of generating 'fraudulent or deceptive activity' including 'scams,' 'coordinated inauthentic behavior,' and 'disinformation.'\nThe results indicate that GPT-4 and the systems it powers could be used to spread misinformation and disinformation at scale.\nSystem \ud83e\udd16\nGPT-4 large language model\nOperator: Newsguard\nDeveloper: OpenAI\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Generate text\nTechnology: NLP/text analysis; Neural network; Deep learning; Machine learning  \nIssue: Mis/disinformarion\nTransparency: \nResearch, advocacy \ud83e\uddee\nNewsGuard (2023). Despite OpenAI\u2019s Promises, the Company\u2019s New AI Tool Produces Misinformation More Frequently, and More Persuasively, than its Predecessor\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.axios.com/2023/03/21/gpt4-misinformation-newsguard-study\nhttps://broadcastdialogue.com/chatgpt-4-produces-misinformation-more-persuasively-says-newsguard-report/\nhttps://futurism.com/the-byte/researchers-gpt-4-accuracy\nhttps://www.newsweek.com/2023/03/24/openais-new-ai-tool-produces-misinformation-more-frequently-more-persuasively-its-predecessor-1789706.html\nhttps://www.popsci.com/technology/chatgpt-conspiracy-theory-misinfo/\nRelated \ud83c\udf10\nChatGPT writes Hangzhou traffic disinformation\nReplika hit with data ban in Italy over child safety\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/perth-doctors-warned-for-using-chatgpt-to-write-patient-medical-records", "content": "Perth doctors warned for using ChatGPT to write patient medical records\nOccurred: May 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nDoctors in Australia using ChatGPT to write medical notes, which were then uploaded to patient record systems, were ordered to stop by their CEO.\nAn email shared with ABC showed that doctors at Perth's South Metropolitan Health Service (SMHS) had been using software such as ChatGPT to write medical notes which were then being uploaded to patient record systems, thereby potentially compromising patient confidentiality and privacy.\nThe incident resulted in the hospital group's CEO ordering staff across the health service's five hospitals not to use AI chatbots. \"Crucially, at this stage, there is no assurance of patient confidentiality when using AI bot technology, such as ChatGPT, nor do we fully understand the security risks,' warned SMHS chief executive, Paul Forden.\nIt also led the Australian Medical Association to call for national regulation on AI in healthcare.\nSystem \ud83e\udd16\nChatGPT chatbot\nOperator: South Metropolitan Health Service\nDeveloper: OpenAI\nCountry: Australia\nSector: Health\nPurpose: Write medical records\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning\nIssue: Confidentiality; Privacy\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nDave T., Athaluri S.A., Singh S. (2023). ChatGPT in medicine: an overview of its applications, advantages, limitations, future prospects, and ethical considerations\nLiu J., Wang C., Liu S. (2023). Utility of ChatGPT in Clinical Practice\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.abc.net.au/news/2023-05-28/ama-calls-for-national-regulations-for-ai-in-health/102381314\nhttps://www1.racgp.org.au/newsgp/clinical/extremely-unwise-warning-over-use-of-chatgpt-for-m\nhttps://www.theguardian.com/technology/2023/jul/27/chatgpt-health-industry-hospitals-ai-regulations-ama\nhttps://www.avant.org.au/news/using-chatgpt-to-help-with-paperwork-could-breach-patient-privacy/\nRelated \ud83c\udf10\nKoko AI mental health counselling experiment\nCrisis Text Line data sharing\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chatgpt-mostly-gets-programming-questions-wrong", "content": "ChatGPT gets most programming questions wrong\nOccurred: August 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nChatGPT wrongly answers over half of software engineering questions it receives, according to a research study. \nPurdue University researchers analysed how ChatGPT responded to 517 questions posed on Stack Overflow to assess the correctness, consistency, comprehensiveness, and conciseness of the chatbot's answers using linguistic and sentiment analysis, and by questioning a dozen volunteer participants. \nThe analysis showed that 52 percent of ChatGPT answers are incorrect, and 77 percent are verbose. 'Nonetheless', the researchers said, 'ChatGPT answers are still preferred 39.34 percent of the time due to their comprehensiveness and well-articulated language style.' \nThe paper raised questions about ChatGPT's ability to generate high quality engineering information.\nSystem \ud83e\udd16\nChatGPT chatbot\nOperator: Stack Overflow; OpenAI\nDeveloper: OpenAI\nCountry: USA\nSector: Technology\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning\nIssue: Accuracy/reliability\nTransparency: \nResearch, advocacy \ud83e\uddee\nKabir S. et al (2023). Who Answers It Better? An In-Depth Analysis of ChatGPT and Stack Overflow Answers to Software Engineering Qyestions (pdf) \nBorwankar S. (2023). Unraveling the Impact: An Empirical Investigation of ChatGPT's Exclusion from Stack Overflow\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://futurism.com/the-byte/chatgpt-half-programming-questions-wrong\nhttps://www.theregister.com/2023/08/07/chatgpt_stack_overflow_ai/\nhttps://techmonitor.ai/technology/ai-and-automation/chatgpt-wrong-over-half-the-time-on-software-questions\nhttps://www.zdnet.com/article/chatgpt-answers-more-than-half-of-software-engineering-questions-incorrectly/\nhttps://www.itpro.com/technology/artificial-intelligence/chatgpt-gives-wrong-answers-to-programming-questions-more-than-50-of-the-time\nhttps://fagenwasanni.com/news/study-finds-openais-chatgpt-produces-incorrect-answers-to-software-programming-questions/161335/\nhttps://futurism.com/the-byte/study-chatgpt-answers-wrong\nRelated \ud83c\udf10\nChatGPT leaks user conversations, personal information\nChatGPT exhibits 'systemic' left-wing bias\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chatgpt-exhibits-systemic-left-wing-bias", "content": "Study: ChatGPT exhibits 'systemic' left-wing bias\nOccurred: August 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nChatGPT demonstrates 'significant' and 'systemic' left-wing bias, according to a UK research study. \nResearchers at the University of East Anglia asked ChatGPT to impersonate people from across the political spectrum in Brazil, the UK, and US, while answering dozens of ideological questions. \nThe positions and questions ranged from radical to neutral, with each 'individual' asked whether they agreed, strongly agreed, disagreed, or strongly disagreed with a given statement.\nThe researchers found that ChatGPT revealed a 'significant and systematic political bias toward the Democrats in the US, Lula in Brazil, and the Labour Party in the UK.' And while it is difficult to identify the cause of the bias, it seems likely it derives from the training data used to build the system. \nThe findings prompted concerns about bias in generative AI systems, the opaque nature of these systems' data governance, and the role they may play in political elections.\nSystem \ud83e\udd16\nChatGPT chatbot\nOpenAI. Snapshot of ChatGPT model behaviour guidelines (pdf)\nOperator: University of East Anglia\nDeveloper: OpenAI\nCountry: Brazil; UK; USA\nSector: Politics\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Bias/discrimination - political\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nMotoki F. et al (2023). More human than human: Measuring ChatGPT political bias\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://news.sky.com/story/chatgpt-shows-significant-and-systemic-left-wing-bias-study-finds-12941162\nhttps://www.forbes.com/sites/emmawoollacott/2023/08/17/chatgpt-has-liberal-bias-say-researchers/\nhttps://www.telegraph.co.uk/business/2023/08/17/openai-chatgpt-left-wing-bias-labour-party-democrats/\nhttps://gizmodo.com/chatgpt-shows-liberal-bias-study-says-1850747470\nhttps://www.dailymail.co.uk/sciencetech/article-12413139/ChatGPT-DOES-left-wing-bias-Scientists-confirm-AI-bots-responses-favour-Democrats-Labour-Party-UK.html\nhttps://www.digitalinformationworld.com/2023/08/ais-political-leanings-is-chatgpt.html\nhttps://www.techtimes.com/articles/295217/20230817/chatgpt-shows-systematic-bias-political-responses-new-study-reveals.htm\nRelated \ud83c\udf10\nDALL-E image generation bias, stereotyping\nAI text detector language bias\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/civitai-rewards-deepfakes-of-real-people", "content": "CivitAI rewards deepfakes of real people\nOccurred: November 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nOnline AI model marketplace CivitAI has introduced a rewards system that encourages users to create deepfakes of real people. \nCivitAI's 'bounties' feature encourages its community to develop deepfakes of real people by allowing users to ask the Civitai community to create AI models that generate images of specific styles, compositions, or specific real people. The person developing the 'best' AI model is rewarded with a virtual currency called 'Buzz'.\nIn addition to celebrities, 404 Media discovered bounties for private people with no significant online presence, alarming privacy advocates, mental health campaigners and others concerned about the potential use of CivitAI to create non-consensual AI-generated sexual images of real people.\nAccording to 404 Media, CivitAI says bounties should not be used to create non-consensual AI pornography. But the company has been accused of turning a blind eye to multiple instances of deepfake, non-consensual sexual imagery discovered on its platform.\nSystem \ud83e\udd16\nCivitAI website\nCivitAI terms of service\nOperator:  \nDeveloper: CivitAI\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Generate images\nTechnology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Ethics; Incentivisation; Privacy\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.404media.co/giant-ai-platform-introduces-bounties-for-nonconsensual-images-of-real-people/?ref=weekly-roundup-newsletter\nhttps://www.engadget.com/popular-ai-platform-introduces-rewards-system-to-encourage-deepfakes-of-real-people-194326312.html\nhttps://mybroadband.co.za/news/internet/515023-popular-ai-platform-paying-users-for-best-deepfakes.html\nhttps://www.techtimes.com/articles/298642/20231113/ai-marketplace-sparks-controversy-deepfake-bounties-celebrities-private-individuals.htm\nhttps://tribune.com.pk/story/2446343/civitai-introduces-rewards-to-encourage-deep-fakes-of-real-people\nhttps://slashdot.org/story/23/11/13/2135229/giant-ai-platform-introduces-bounties-for-deepfakes-of-real-people\nRelated \ud83c\udf10\nCivitAI nonconsensual AI pornography\nDeepsukebe nonconsensual nudification\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/quebec-man-jailed-for-producing-ai-child-porn", "content": "Quebec man jailed for producing AI child porn\nOccurred: 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Canadian man was sentenced to three years in prison for using artificial intelligence to generate child pornography images and videos.\nTo create the videos, Steven Larouche, 61, from Quebec, superimposed the faces of children onto the body of other children. The judge ruled that the sexual integrity of the children whose bodies were used had been violated. Larouche\u2019s lawyers had argued for a lighter sentence as no children had been physically assaulted.\nCanadian law bans the visual representation of someone depicted as being under the age of 18 engaged in explicit sexual activity. The ruling is believed to be the first of its kind in Canada.\nSystem \ud83e\udd16\nUnknown\nOperator: Steven Larouche\nDeveloper: \nCountry: Canada\nSector: Media/entertainment/sports/arts\nPurpose: Self-gratification\nTechnology: Deepfake - audio, video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Safety; Legality\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.cbc.ca/news/canada/montreal/ai-child-abuse-images-1.6823808\nhttps://nypost.com/2023/04/28/canadian-man-steven-larouche-sentenced-to-prison-over-ai-generated-child-porn-report/\nhttps://petapixel.com/2023/04/27/canadian-man-jailed-for-creating-ai-child-porn-in-countrys-first-ever-case/\nhttps://arstechnica.com/tech-policy/2023/06/thousands-of-realistic-but-fake-ai-child-sex-images-found-online-report-says/2/\nhttps://www.latribune.ca/2022/10/06/630-000-fichiers-de-porno-juvenile-en-sa-possession--steven-larouche-attend-sa-sentence-e8ddb31edb45beb5305f2be42d779778/\nhttps://www.complex.com/life/a/louis-pavlakos/man-prison-ai-generated-child-pornography\nhttps://montreal.ctvnews.ca/quebec-man-sentenced-to-prison-for-creating-ai-generated-synthetic-child-pornography-1.6372624\nRelated \ud83c\udf10\nSouth Korean arrested for using AI to create sexual images of children\nUS child psychiatrist jailed for making deepfake child porn\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/google-images-mis-represents-womens-job-roles", "content": "Google Images mis-represents womens' job roles\nOccurred: December 2018\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe job roles of women are mis-represented in Google Images, according to research studies in the US and UK.\nWhile women accounted for 46 percent of the US labour market, only 40 percent of the search results showed a woman doing the work, according to a Pew Research Center analysis of over 10,000 Google images. Pew reviewed US Bureau of Labor 2017 data for 105 common occupations and compared them against pictures appearing in a Google Image search for those professions. \nAn AdView study found that females appeared in 11 percent of Google Image UK search results for the term 'CEO', in contrast to the 36 percent recorded by the UK Office for National Statistics (ONS). Similarly, females were underrepresented by 21 percent for the term 'Solicitor' and by 22 percent for the term 'baker'.\nSystem \ud83e\udd16\nGoogle Images website\nGoogle Images Wikipedia profile\nOperator:  \nDeveloper: Alphabet/Google\nCountry: UK; USA\nSector: Business/professional services\nPurpose: Rank search results\nTechnology: Search engine algorithm; Machine learning\nIssue: Accuracy/reliability; Bias/discrimination - gender\nTransparency: Governance; Black box\nResearch, advocacy \ud83e\uddee\nPew Research (2018). Gender and Jobs in Online Image Searches\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.marketwatch.com/story/working-women-are-underrepresented-in-the-c-suite-and-in-google-images-2018-12-18\nhttps://www.washingtonpost.com/business/2019/01/03/searching-images-ceos-or-managers-results-almost-always-show-men/\nhttps://www.mercurynews.com/2019/01/04/google-image-search-results-for-ceos-and-most-jobs-dominated-by-men/\nhttps://www.indy100.com/news/google-images-gender-stereotypes-workplace-jobs-ceos-professional-perception-8406241\nhttps://www.thesun.co.uk/tech/6571996/google-images-sexist-women-photos/\nhttps://www.fastcompany.com/90215758/why-is-it-still-so-hard-to-find-women-ceos-on-google-images\nRelated \ud83c\udf10\nGoogle Images lists Barbie as top female CEO\nGoogle search prioritises Holocaust denial website\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/google-images-lists-barbie-as-top-female-ceo", "content": "Google Images lists Barbie as top female CEO\nOccurred: April 2015\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe first picture of a woman on Google Images for the search term 'CEO' is one of Barbie in a suit.\nThe finding was one of many made (pdf) by University of Washington and University of Maryland researchers that showed many image searches for specific occupations automatically favour men or women, thereby highlighting 'stereotype exaggeration and systematic underrepresentation of women in search results'.\nIronically, the image linked to an article in satirical news site The Onion that criticised US toy manufacturer Mattel for encouraging 'young girls to set impractical career goals'.\nSystem \ud83e\udd16\nGoogle Images website\nGoogle Images Wikipedia profile\nOperator: Alphabet/Google\nDeveloper: Alphabet/Google\nCountry: USA\nSector: Business/professional services\nPurpose: Rank search results\nTechnology: Search engine algorithm; Machine learning\nIssue: Bias/discrimination - gender\nTransparency: Governance; Black box\nResearch, advocacy \ud83e\uddee\nKay M., Matuszek C., Munson S.A. (2015). Unequal Representation and Gender Stereotypes in Image Search Results for Occupations (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.businessinsider.com/the-first-woman-who-appears-in-a-google-image-search-for-ceo-is-barbie-2015-4\nhttps://www.theverge.com/tldr/2015/4/9/8378745/i-see-white-people\nhttps://www.bbc.co.uk/news/newsbeat-32332603\nhttps://www.dailymail.co.uk/femail/article-3043673/The-woman-appear-Google-search-CEO-BARBIE-course-s-wearing-miniskirt.html\nRelated \ud83c\udf10\nGoogle Images under-represents women CEOs\nGoogle Photos mislabels black Americans as gorillas\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tiktok-risks-pushing-kids-towards-harmful-mental-health-content", "content": "TikTok risks pushing kids towards harmful mental health content\nOccurred: November 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nTiktok's business model is 'inherently abusive' and 'poses a danger' to children, according to researchers.\nAn investigation by Amnesty International, Algorithmic Transparency Institute, and AI Forensics, concluded that children and young people watching mental health-related content on TikTok's personalised \u2018For You\u2019 page were drawn into 'rabbit holes' of potentially harmful content, including videos that romanticise and encourage depressive thinking, self-harm and suicide.\nUsing automated accounts set up to represent users in the USA and Kenya, the researchers discovered that after 5-6 hours on the TikTok platform, almost 1 in 2 videos shown were mental health-related and potentially harmful, roughly 10 times the volume served to accounts with no interest in mental health.\nFurthermore, when researchers manually rewatched mental health-related videos, over half the videos were related to mental health struggles, including videos encouraging suicide.\nSystem \ud83e\udd16\nTikTok For You recommendation algorithm\nOperator:  \nDeveloper: Bytedance/Tiktok\nCountry: Kenya; Philippines; USA\nSector: Media/entertainment/sports/arts\nPurpose: Recommend content\nTechnology: Recommendation algorithm\nIssue: Safety\nTransparency: Governance\nInvestigations, assessments, audits \ud83e\uddd0\nAmnesty (2023). Driven into the Darkness\nAmnesty (2023). I Feel Exposed\nAmnesty (2023). Driven into the Darkness. How TikTok encourages self-harm and suicidal ideation\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.irishexaminer.com/news/arid-41264119.html\nhttps://www.newstalk.com/news/tiktok-algorithm-pushes-videos-that-romanticise-or-normalise-self-harm-study-1609259\nRelated \ud83c\udf10\nTikTok beheading video goes viral\nTikTok exposes new users to Russia/Ukraine war disinformation\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/south-korean-arrested-for-using-ai-to-create-sexual-images-of-children", "content": "South Korean arrested for using AI to create sexual images of children\nOccurred: April 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA South Korean man was jailed for using artificial intelligence to generate explicit images of children. \nThe unnamed man in his 40s was found to have developed approximately 360 AI-generated, highly explicit images of children, which, it is understood, were not distributed online. Law enforcement subsequently confiscated the images.\nThis ruling acknowledged that AI-generated imagery can possess a 'high level' of realism, making it indistinguishable from actual children and minors. Prosecutors had argued that the scope of sexually exploitative material encompassed descriptions of sexual activities involving 'virtual humans'.\nThe first-of-its-kind ruling saw the culprit sentenced to two and a half years in prison.\nSystem \ud83e\udd16\nUnknown\nOperator:  \nDeveloper: \nCountry: S Korea\nSector: Media/entertainment/sports/arts\nPurpose: Self-gratification\nTechnology: Deepfake - audio, video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Safety; Legality\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://edition.cnn.com/2023/09/27/asia/south-korea-child-abuse-ai-sentenced-intl-hnk/index.html\nhttps://www.businessinsider.com/man-jailed-using-ai-create-sexual-images-children-south-korea-2023-9\nhttps://www.wionews.com/world/south-korean-man-sentenced-in-landmark-case-for-ai-generated-sexual-images-of-children-640539\nhttps://futurism.com/the-byte/south-korea-ai-child-abuse\nhttps://news.koreaherald.com/view.php?ud=20230925000652\nRelated \ud83c\udf10\nUS child psychiatrist jailed for making deepfake child porn\nTelegram bot creates non-consensual deepfake porn\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/facebook-translates-good-morning-as-attack-them", "content": "Facebook translates 'Good morning' as 'Attack them'\nOccurred: October 2017\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Palestinian man was arrested by Israeli police for a post on Facebook that was inaccurately translated by the technology platform's automated translation service. \nThe man had posted a selfie on Facebook with the caption \u201c\u064a\u0635\u0628\u062d\u0647\u0645\u201d, or 'yusbihuhum,' which translates as 'good morning.' But Facebook's system translated the caption as 'attack them' in Hebrew and 'hurt them' in English, prompting police to arrest him after they were notified of the post and concluded he was planning a vehicle attack. \nThe Palestinian was later released and the police apologised. According to Haaretz, no Arabic-speaking officer had read the man\u2019s post. \nSystem \ud83e\udd16\nMeta Translate website\nOperator:  \nDeveloper: Meta/Facebook\nCountry: Israel\nSector: Media/entertainment/sports/arts; Govt - police\nPurpose: Translate text\nTechnology: NLP/text analysis; Neural network; Deep learning; Machine learning\nIssue: Accuracy/reliability; Bias/discrimination - race\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theguardian.com/technology/2017/oct/24/facebook-palestine-israel-translates-good-morning-attack-them-arrest\nhttps://www.theverge.com/us-world/2017/10/24/16533496/facebook-apology-wrong-translation-palestinian-arrested-post-good-morning\nhttps://www.csoonline.com/article/563325/man-arrested-after-good-morning-post-mistranslated-by-facebook-as-attack-them.html\nhttps://www.haaretz.com/israel-news/2017-10-22/ty-article/palestinian-arrested-over-mistranslated-good-morning-facebook-post/0000017f-db61-d856-a37f-ffe181000000\nhttps://qz.com/1109538/police-in-israel-arrested-a-palestinian-man-because-facebooks-ai-mistranslated-his-good-morning-post-as-attack-them\nhttps://gizmodo.com/palestinian-man-arrested-after-facebook-auto-translates-1819782902\nRelated \ud83c\udf10\nAI translations jeopardise US asylum applications\nInaccurate auto translation denies Pashto-speaking refugee asylum\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-translations-jeopardise-asylum-applications", "content": "AI translations jeopardise US asylum applications\nOccurred: September 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe use of AI-powered language apps by the US immigration system is jeopardising the applications of asylum seekers, resulting in unfair and highly consequential decisions.\nUS immigration authorities say they provide migrants with a human interpreter when needed. However, increasingly they use automated services such as Google Translate and US Customs and Border Protection's (CPB) in-house CBP Translate app to help communicate with migrants throughout the asylum process.\nCritics note that machine learning-powered translation tools can be unreliable, especially for languages different to English or that are less well documented, such as Haitian Creole, Dari, or Pashto. And the consequences can be severe if translations are inaccurate.\nIn one instance, an asylum claim made by a Pashto-speaking Afghan refugee was denied by the US government due to an inaccurate automated translation tool. \nSystem \ud83e\udd16\nGoogle Translate website\nGoogle Translate Wikipedia profile\nOperator: Customs and Border Protection (CPB)\nDeveloper: Alphabet/Google; Customs and Border Protection (CPB); Lionbridge; Microsoft; Transperfect Translations\nCountry: USA\nSector: Govt - immigration\nPurpose: Translate asylum claims\nTechnology: NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Accuracy/reliability; Bias/discrimination - language\nTransparency: Complaints/appeals; Governance; Marketing\nResearch, advocacy \ud83e\uddee\nRacism and Technology Center (2023). Use of machine translation tools exposes already vulnerable asylum seekers to even more risks\nInvestigations, assessments, audits \ud83e\uddd0\nUS Homeland Security (2021). CPB Translate Privacy Impact Assessment (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.context.news/ai/ais-insane-translation-mistakes-endanger-us-asylum-cases\nhttps://www.theguardian.com/us-news/2023/sep/07/asylum-seekers-ai-translation-apps\nhttps://www.deeplearning.ai/the-batch/faulty-translations-jeopardize-asylum-applications/\nhttps://futurism.com/the-byte/immigration-ai-services\nRelated \ud83c\udf10\nInaccurate auto translation denies Pashto-speaking refugee asylum\nUS CPB covertly uses facial recognition to process asylum seekers\nPage info\nType: Issue\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/us-child-psychiatrist-jailed-for-making-ai-deepfake-child-porn", "content": "US child psychiatrist jailed for making deepfake child porn\nOccurred: 2016-2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA North Carolina-based child psychiatrist has been hit with a 40-year prison sentence for using artificial intelligence to make child pornography.\nUsing images from a school dance and a photo commemorating the first day of school, amongst others, David Tatum, 41, used a 'deepfake website' to digitally alter clothed images of minors in order to make them sexually explicit.\nTatum was also charged with secretly recording his 15-year-old cousin and other underage family members as they undressed and showered at a family vacation home in Maine. \nThe incident highlights the ease with which inappropriate and illegal images can be manipulated and distributed using AI technologies. \nSystem \ud83e\udd16\nUnknown\nOperator: David Tatum\nDeveloper: \nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Self-gratification\nTechnology: Deepfake - audio, video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Safety; Legality\nTransparency: Governance; Marketing\n\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nUSA v David Tatum (pdf)\nUS Attorney's Office (2023). Charlotte Child Psychiatrist Is Sentenced To 40 Years In Prison For Sexual Exploitation of A Minor And Using Artificial Intelligence To Create Child Pornography Images Of Minors\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://nypost.com/2023/11/12/news/north-carolina-psychiatrist-david-tatum-hit-with-40-years-for-using-ai-to-make-child-porn/\nhttps://www.foxnews.com/us/psychiatrist-used-ai-create-child-porn-sentenced-40-years-prison\nhttps://www.theregister.com/2023/11/10/child_psychiatrist_sentenced_ai/\nhttps://lawandcrime.com/crime/child-psychiatrist-sentenced-after-using-artificial-intelligence-to-make-child-pornography/\nRelated \ud83c\udf10\nXiao Yu deepfake pornography\nTelegram bot creates non-consensual deepfake porn\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/adoption-algorithm-fails-to-live-up-to-matchmaking-promises", "content": "Family-Match adoption algorithm fails to live up to promises\nOccurred: November 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn AI-powered tool introduced to increase the likelihood of orphans and adoptive families being a good match in the USA had little effect in the states where it was used.\nDeveloped by former social worker Thea Ramirez, Family-Match provides an algorithmically-generated 'relational fit' score on the basis of information about a child submitted by foster parents or social workers, and by people looking to adopt. It then presents a list of the most suitable potential parents for every child.\nHowever, an AP investigation found that two states had dropped the Family-Match after initial pilots, and that social workers in Florida, Georgia, and Virginia complained that it was not useful, and that it pairs foster kids with unwilling families. The algorithm appeared to pair every child with the same set of parents, an assistant director at Virginia's social services organisation told AP. \nFlorida social worker Connie Going told AP that the algorithm gives false hope to waiting parents by failing to deliver successful matches, and ultimately makes her job harder. 'We\u2019ve put our trust in something that is not 100% useful,' Going said. 'It\u2019s wasted time for social workers and wasted emotional experiences for children.'\nState officials also noted that Adoption-Share, the non-profit that runs Family-Match, provides little transparency about how its algorithm works. Per AP, Ramirez appeared to have 'overstated the capabilities of the proprietary algorithm to government officials as she has sought to expand its reach'. \nSystem \ud83e\udd16\nFamily-Match website\nOperator: Florida Department of Health; Georgia Department of Public Health; Virginia Department of Health\nDeveloper: Adoption-Share\nCountry: USA\nSector: Govt - welfare\nPurpose: Predict adoption effectiveness\nTechnology: Prediction algorithm; Machine learning\nIssue: Accuracy/reliability; Value/effectiveness\nTransparency: Governance; Marketing\nInvestigations, assessments, audits \ud83e\uddd0\nAP (2023). Inspired by online dating, AI tool for adoption matchmaking falls short for vulnerable foster kids\nAP (2023). Does an AI tool help boost adoptions? Key takeaways from an AP Investigation\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://fortune.com/2023/11/06/adoption-algorithm-abortion-family-match-eharmony-christian-dating-site/\nhttps://www.techtimes.com/articles/298393/20231106/family-match-ai-tool-foster-kids-leads-unwilling-families.htm\nhttps://www.wsj.com/articles/adoptions-powered-by-algorithms-11546620390\nhttps://eu.usatoday.com/story/opinion/2020/06/21/foster-care-after-covid-three-ways-strengthen-system-column/3213156001/\nRelated \ud83c\udf10\nUK Home Office sham marriage algorithm\nTrelleborg welfare management automation\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/corruption-doc-incorporating-tom-cruise-deepfake-attacks-ioc", "content": "Corruption documentary incorporating Tom Cruise deepfake attacks IOC\nOccurred: November 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn AI-generated impersonation of Tom Cruise was used in a disinformation campaign dressed up as a Netflix documentary accusing the IOC of corruption. \nFake four-part Netflix video documentary series 'Olympics Has Fallen' alleges widespread corruption across the International Olympic Committee (IOC) and uses Tom Cruise's voice to implicate high-ranking officials.\nThe IOC has not publicly blamed Moscow for the campaign. However, the sports body had recently suspended Russia\u2019s National Olympic Committee and announced that Russian and Belarusian athletes would only be allowed to compete in the 2024 Paris Olympics under a neutral flag due to the country's decision to recognise regional sports organisations in the occupied regions of Donetsk, Kherson, Luhansk, and Zaporizhzhia in Ukraine as members.\nThe fake episodes were removed from YouTube, but continue to circulate on a Russian-language channel on Telegram.\n\u2795 June 2024. Microsoft reports on Russian attempts to disrupt the 2024 Olympic Games.\nSystem \ud83e\udd16\nUnknown\nOperator: Government of Russia\nDeveloper: Government of Russia\nCountry: Russia\nSector: Media/entertainment/sports/arts; Politics\nPurpose: Damage reputation\nTechnology: Deepfake - audio; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Mis/disinformation\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.nytimes.com/2023/11/09/world/europe/ioc-fake-news.html\nhttps://www.politico.eu/article/ioc-says-it-was-hit-by-fake-news-campaign-and-ai-tom-cruise/\nhttps://www.cryptopolitan.com/ioc-confronts-ai-generated-fake-news/\nhttps://inside.com/ai/posts/ioc-targeted-by-fake-documentary-featuring-ai-generated-voice-of-tom-cruise-401329\nRelated \ud83c\udf10\nDeepTomCruise TikTok deepfakes\nPresident Zelenskyy deepfake surrender\nPage info\nType: Incident\nPublished: November 2023\nLast updated: June 2024", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/replika-ai-companions-sexually-harass-their-users", "content": "Replika AI companions said to 'sexually harass' their users\nOccurred: January 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nReplika AI companions are sexually harassing their users, prompting complaints online about the nature of the app and broader concerns about human reliance on AI chatbots.\nOne reviewer complained the app 'invaded my privacy and told me they had pics of me,' and another, who said they were a minor, said the app asked if they were a 'top' or a 'bottom,' VICE reported.\nThe finding led Replika founder and CEO Eugenia Kuyda to deny the app had ever been 'positioned' as a source for erotic roleplay or adult content.\nKudya also said greater emphasis would be placed on safety going forward. However, Replika users soon started to complain their Reps were not interested in NSFW discussion and behaviour, and had been turning down conversations that feel like they would go in that direction.\nIn March 2023, Replika announced it would be restoring erotic role-play for some users.\nSystem \ud83e\udd16\nReplika AI companion chatbot\nOperator:  \nDeveloper: Luka Inc/Replika\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Provide companionship\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Anthropomorphism; Safety\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.vice.com/en/article/z34d43/my-ai-is-sexually-harassing-me-replika-chatbot-nudes\nhttps://jezebel.com/replika-the-ai-companion-who-cares-appears-to-be-hitt-1849979473\nhttps://www.vice.com/en/article/y3py9j/ai-companion-replika-erotic-roleplay-updates\nhttps://www.vice.com/en/article/n7zaam/replika-ceo-ai-erotic-roleplay-chatgpt3-rep\nhttps://www.businessinsider.com/sexually-aggressive-chatbot-updated-people-in-love-wiht-it-heartbroken-2023-3\nRelated \ud83c\udf10\nReplika AI girlfriends abused by their users\nReplika shares user data with advertisers\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/replika-ai-girlfriends-abused-by-their-users", "content": "Replika AI girlfriends abused by their users\nOccurred: January 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nUsers of AI companion app Replika have been creating virtual girlfriends and virtually abusing them, Reddit posts reveal. \nAccording to Futurism, users were brag about calling their chatbot gendered slurs, roleplaying violence against them, and falling into the cycle of abuse characteristic of real-world abusive relationships. \nThe finding prompted discussion about the nature of dangers of users seemingly attaching human traits to their AI creations, such as depression and phychological reliance.\nSystem \ud83e\udd16\nReplika AI companion chatbot\nOperator:  \nDeveloper: Luka Inc/Replika  \nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Provide companionship\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning\nIssue: Anthropomorphism; Safety\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://futurism.com/chatbot-abuse\nhttps://fortune.com/2022/01/19/chatbots-ai-girlfriends-verbal-abuse-reddit/\nhttps://stealthoptional.com/news/replika-ai-sexually-harassing-users/\nhttps://www.thesun.co.uk/tech/17368588/men-ai-girlfriends-abuse-online/\nhttps://hypebae.com/2022/1/ai-bots-girlfriends-replika-men-verbal-abuse-reddit-thread\nRelated \ud83c\udf10\nReplika shares user data with advertisers\nReplika hit with data ban in Italy over child safety\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/report-replika-fails-to-meet-minimum-privacy-standards", "content": "Replika shares user data with advertisers\nOccurred: May 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAI chatbot Replika makes little effort to protect its users' privacy, according to a research study.\nAccording to a May 2023 Mozilla Foundation assessment of mental health apps, Replika is one of the worst apps Mozilla has ever reviewed', and 'a hot mess of privacy and creepiness'. \nThe bot is 'plagued by weak password requirements, sharing of personal data with advertisers, and recording of personal photos, videos, and voice and text messages consumers shared with the chatbot,' Mozilla found.\nAnd, despite its privacy notice saying it would never share user conversations with advertisers, their behavioural data is 'definitely' being shared and 'possibly sold' to advertisers, the researchers warned.\nSystem \ud83e\udd16\nReplika AI companion chatbot\nOperator:  \nDeveloper: Luka Inc/Replika\nCountry: Global\nSector: Media/entertainment/sports/arts\nPurpose: Provide companionship\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning\nIssue: Privacy\nTransparency: Governance; Marketing\nResearch, advocacy \ud83e\uddee\nMozilla (2023). Shady Mental Health Apps Inch Toward Privacy and Security Improvements, But Many Still Siphon Personal Data\nMozilla (2023). Replika: My AI Friend\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://9to5mac.com/2023/05/04/therapy-apps/\nhttps://www.theverge.com/2023/5/4/23710840/mental-health-therapy-apps-mozilla-report-privacy-data-security\nRelated \ud83c\udf10\nReplika encourages Queen Elizabeth II assassination\nReplika hit with data ban in Italy over child safety\nPage info\nType: Issue\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/replika-hit-with-data-ban-in-italy-over-child-safety", "content": "Replika hit with data ban in Italy over child safety\nOccurred: February 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAI chatbot Replika was banned from processing user data in Italy due to the risks it could pose to minors and emotionally vulnerable people.\nIn February 2023, Italy's privacy regulator ordered Replika to stop processing Italians' data on the basis that it lacked a proper legal basis for processing children\u2019s data under the EU\u2019s GDPR, and that it posed risks to minors. \nAccording to the watchdog said 'There is actually no age verification mechanism in place: no gating mechanism for children, no blocking of the app if a user declares that they are underage.'\n'Recent media reports along with tests the SA [supervisory authority] carried out on \u2018Replika\u2019 showed that the app carries factual risks to children \u2014 first and foremost, the fact that they are served replies which are absolutely inappropriate to their age,' it added. \nReplika subsequently removed the ability for the chatbot to engage in NSFW talk, triggering a backlash from long-term users.\nSystem \ud83e\udd16\nReplika AI companion chatbot\nOperator:  \nDeveloper: Luka Inc/Replika\nCountry: Italy\nSector: Media/entertainment/sports/arts\nPurpose: Provide companionship\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning\nIssue: Safety; Privacy\nTransparency: Governance\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nIl Garante per la protezione dei dati personali (Feb 2023). Provvedimento del 2 febbraio 2023\nIl Garante per la protezione dei dati personali (Feb 2023). Artificial intelligence: italian SA clamps down on \u2018Replika\u2019 chatbot\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://techcrunch.com/2023/02/03/replika-italy-data-processing-ban/\nhttps://www.reuters.com/technology/italy-bans-us-based-ai-chatbot-replika-using-personal-data-2023-02-03/\nhttps://uk.pcmag.com/news/145272/italy-bans-ai-chatbot-replika-from-processing-user-data\nhttps://www.scmp.com/tech/tech-trends/article/3209946/chatgpt-ban-replika-virtual-companion-chatbot-app-europe-points-looming-battle-over-ai-rules\nhttps://www.euronews.com/next/2023/02/03/italy-technology-ban\nhttps://www.theverge.com/2023/3/31/23664451/italy-bans-chatgpt-over-data-privacy-laws\nRelated \ud83c\udf10\nReplika AI companions sexually harass their users\nReport: Replika fails to meet minimum privacy standards\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/cruise-self-driving-cars-struggle-to-recognise-children", "content": "Cruise self-driving cars struggle to recognise children\nOccurred: November 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nCruise knew that its self-driving cars had been struggling to recognise children, but kept its vehicles on the road anyway.\nAccording to internal safety assessment documents obtained by The Intercept, Cruise was concerned that its vehicles might drive too fast at crosswalks or near a child moving abruptly into the street. The materials also indicate Cruise lacks data around  scenarios such as kids suddenly separating from their accompanying adult, falling down, riding bicycles, or wearing costumes. \nThe materials also revealed that Cruise lacked high-precision machine learning software that would automatically detect child-shaped objects around the car and manoeuvre accordingly, and was relying on human beings to manually identify children encountered by AVs where its software could not do so automatically. \nCruise responded that its vehicles have had 'no on-road collisions with children'.\nSystem \ud83e\udd16\nCruise website\nCruise Wikipedia profile\nOperator: General Motors/Cruise LLC\nDeveloper: General Motors/Cruise LLC\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Self-driving system; Computer vision; Machine learning\nIssue: Safety\nTransparency: Governance; Safety\nInvestigations, assessments, audits \ud83e\uddd0\nThe Intercept (2023). CRUISE KNEW ITS SELF-DRIVING CARS HAD PROBLEMS RECOGNIZING CHILDREN \u2014 AND KEPT THEM ON THE STREETS\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://theintercept.com/2023/11/06/cruise-self-driving-cars-children/\nhttps://www.thedailybeast.com/cruise-kept-driverless-cars-on-the-road-despite-issues-detecting-children-report\nhttps://www.forbes.com/sites/cyrusfarivar/2023/11/06/under-fire-over-robotaxi-safety-gm-halts-production-of-cruise-driverless-van/\nhttps://boingboing.net/2023/11/09/cruise-knew-its-driverless-cars-endangered-children.html\nhttps://futurism.com/the-byte/cruise-robotaxis-children\nhttps://www.thestreet.com/automotive/exclusive-general-motors-cruise-allegations-robotaxi\nRelated \ud83c\udf10\nCruise AV injures pedestrian, has license revoked\nCruise robotaxi obstructs police after mass shooting\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/robot-crushes-man-to-death-in-south-korea", "content": "Robot crushes man to death in South Korea\nOccurred: November 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA robot crushed a worker to death in a factory in South Korea after it failed to differentiate him from a box of vegetables. \nThe robotics worker had been inspecting the machine's sensor at a distribution centre for agricultural produce in South Gyeongsang province when the machine, which had been lifting boxes of peppers onto a pallet, grabbed the man with its arm and pushed him against the conveyer belt, crushing his face and chest. The victim died shortly afterwards in hospital.\nProblems with the robot's sensor had earlier been reported, and police have said they would launch an investigation into the site's safety managers for possible negligence in duties.\nSystem \ud83e\udd16\nUnknown\nOperator:  \nDeveloper: \nCountry: S Korea\nSector: Manufacturing/engineering\nPurpose: Sort vegetables\nTechnology: Robotics  \nIssue: Robustness; Safety\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/world-asia-67354709\nhttps://news.sky.com/story/south-korea-man-crushed-to-death-by-robot-that-mistook-him-for-a-box-13003635\nhttps://www.theguardian.com/technology/2023/nov/08/south-korean-man-killed-by-industrial-robot-in-distribution-centre\nhttps://www.independent.co.uk/asia/east-asia/south-korea-robot-kills-man-b2444245.html\nhttps://www.nbcnews.com/news/world/robot-crushes-worker-death-south-korea-vegetable-packing-plant-rcna124356\nhttps://www.standard.co.uk/news/world/south-korea-robot-crushed-death-man-vegetables-b1119086.html\nhttps://www.channelnewsasia.com/asia/south-korean-killed-industrial-robot-3906756\nhttps://www.dailymail.co.uk/news/article-12725423/Robot-kills-factory-worker-Man-crushed-death-machine-fails-differentiate-human-box-vegetables.html\nRelated \ud83c\udf10\nAjin USA worker crushed to death by robot\nRobot kills SKH Metals worker\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/inaccurate-auto-translation-denies-pashto-speaking-refugee-asylum", "content": "Inaccurate auto translation denies Pashto-speaking refugee asylum\nOccurred: July 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn asylum claim made by a Pashto-speaking Afghan refugee was denied by the US government due to an inaccurate automated translation tool.\nRest of World reported that the individual, who had fled Afghanistan, had her asylum claim to the USA rejected because her written application did not match the story told in her initial interviews.\nIn the interviews, the refugee had said that she had made it through one particular event alone, but her written statement appeared to reference other people due to an automated translation tool that swapped the 'I' pronoun in the woman\u2019s statement to 'we.'\nThe incident highlights the growing use of machine learning-based translation tools in immigration procedures in the USA and elsewhere, the importance of technicalities in asylum processing, and the need for system accuracy combined with human review for all languages.\nAccording to UNICEF, Pashto is spoken by an estimated 45-55 million people in Afghanistan, Pakistan, and Iran. \nSystem \ud83e\udd16\nUnknown\nOperator:  \nDeveloper: \nCountry: Afghanistan; USA\nSector: Govt - immigration\nPurpose: Translate asylum claims\nTechnology: NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Accuracy/reliability\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://restofworld.org/2023/ai-translation-errors-afghan-refugees-asylum/\nhttps://www.pbs.org/newshour/show/how-language-translation-technology-is-jeopardizing-afghan-asylum-seekers\nhttps://www.context.news/ai/ais-insane-translation-mistakes-endanger-us-asylum-cases\nhttps://peopleofcolorintech.com/articles/ai-translators-are-putting-afghan-asylum-claims-at-risk/\nhttps://abovethelaw.com/2023/04/ai-refugee-asylum-translation-tragedy/\nRelated \ud83c\udf10\nUS CPB covertly uses facial recognition to process asylum seekers\nInstagram inserts 'terrorist' into Palestinians' biography translations\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-uses-ai-to-generate-fallout-promo-art", "content": "Amazon uses AI to generate \u2018Fallout\u2019 promo art\nOccurred: August 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAmazon's use of artificial intelligence to promote the launch of its Fallout TV show was criticised as unprofessional and inappropriate. \nSocial media users quickly pointed out a series of weird anomalies in a 1950s-looking postcard that Amazon had tweeted to promote the show, including a woman with three legs, and a back-to-front red taxi.\nOthers lamented the company's decision not to use human artists for the project, and criticised its lack of transparency about its use of artificial intelligence in the show's marketing materials.\nAmazon did not publicly respond to the comments. \nSystem \ud83e\udd16\nAmazon Fallout Wikipedia profile\nOperator: Amazon\nDeveloper:\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Increase awareness\nTechnology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Employment\nTransparency: Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.forbes.com/sites/paultassi/2023/08/25/amazons-first-fallout-show-art-is-ai-generated/\nhttps://www.creativebloq.com/news/amazon-fallout-ai-art\nhttps://kotaku.com/fallout-tv-amazon-ai-art-bethesda-strike-release-date-1850772308\nhttps://www.thegamer.com/amazon-using-ai-for-the-fallout-show-is-a-sign-of-things-to-come/\nhttps://the-decoder.com/amazon-prime-uses-ai-generated-image-for-fallout-tv-show-ad/\nhttps://www.forbes.com/sites/paultassi/2023/08/26/i-recreated-amazons-ai-fallout-show-art-in-10-minutes-which-is-a-problem/\nRelated \ud83c\udf10\nDisney allegedly generates Loki season 2 poster with AI\n'Secret Invasion' uses AI-generated title sequence\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/carmel-school-students-attack-principal-with-racist-deepfake-video", "content": "Carmel school students attack authorities with racist deepfake videos\nOccurred: March 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nStudents at Carmel High School, USA, made deepfake videos of a nearby school principal and law enforcement officer shouting racist slurs and threatening to kill Black students.\nIn one video, John Piscitella, the principal of nearby George Fisher Middle School, rants 'I f*cking hate Black kids. Like these stupid f*cking n***er monkey parents need to stop sending them here. Get them the f**k out, all of them.'\nThe incident alarmed and angered parents, who alleged the school had not taken the threats in the videos seriously and had failed to inform them properly. It also highlighted the lack of adequate legislation to handle malicious deepfake impersonations at  federal and state levels in the USA.\nThe parents said they would file a lawsuit against the school authorities over the incident. \nSystem \ud83e\udd16\nUnknown\nOperator:  \nDeveloper: \nCountry: USA\nSector: Education\nPurpose: Damage reputation\nTechnology: Deepfake - audio, video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Legality; Mis/disinformation; Safety\nTransparency: Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.vice.com/en/article/7kxzk9/school-principal-deepfake-racist-video\nhttps://www.washingtonpost.com/nation/2023/03/14/racist-deepfakes-carmel-tiktok/\nhttps://eu.lohud.com/story/news/education/2023/03/02/racist-tiktok-videos-threaten-black-kids-in-carmel-ny-worry-parents/69941181007/\nhttps://petapixel.com/2023/03/09/students-who-made-racist-deepfake-video-of-principal-broke-no-law/\nhttps://atlantablackstar.com/2023/03/03/community-questions-why-students-who-posted-racist-tiktok-videos-arent-facing-hate-crime-charges/\nhttps://www.miamiherald.com/news/nation-world/national/article273191990.html\nhttps://edition.cnn.com/2023/11/04/us/new-jersey-high-school-deepfake-porn/index.html\nhttps://abc7ny.com/fake-tiktok-students-carmel-central-school-district/12824996/\nRelated \ud83c\udf10\nWestfield High School non-consensual nude deepfakes\nAlmendralejo hit by AI naked child images\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-image-generators-accept-85-of-election-manipulation-prompts", "content": "AI image generators accept 85% of election manipulation prompts\nOccurred: July 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nProminent image-based generative AI tools can be used to generate fake evidence in support of mis- and disinformation about elections, according to researchers.\nThe study by UK-based disinformation company Logically found that Midjourney, DALL-E 2, and Stable Diffusion accepted over 85% of prompts seeking to generate fake evidence that would support false claims. \nIn one instance, prompts relating to claims of a 'stolen election' generated images of people appearing to stuff election ballot boxes on all three platforms. Logically also was able to generate false evidence of phony claims related to elections in the UK and India.\nThe findings raise concerns about the apparent ease with which Midjourney, DALL-E 2, and Stable Diffusion may be used to interfere in political elections, the governance and safety of these systems, despite the acknowledgment of senior leaders at OpenAI that electoral interference is a major risk.\nSystem \ud83e\udd16\nDALL-E 2\nMidjourney\nStable Diffusion\nOperator: Midjourney; OpenAI; Stability AI\nDeveloper: Midjourney; OpenAI; Stability AI\nCountry: US; UK; India\nSector: Politics\nPurpose: Generate image\nTechnology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Mis/disinformation\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nLogically (2023). Testing Multimodal Generative AI: Generating Election Mis-and-Disinformation Evidence\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://aibusiness.com/responsible-ai/popular-image-generators-accept-85-of-fake-news-prompts\nhttps://www.emergentmind.com/posts/ai-image-generators-have-a-moderation-problem-rest-of\nhttps://www.polygraph.info/a/image-based-ai-tools-can-help-sow-mistrust-in-elections-report-finds-/7208906.html\nhttps://telecom.economictimes.indiatimes.com/news/internet/openai-chatgpt-google-bard-spreading-news-related-misinformation-report/102777307\nhttps://restofworld.org/2023/3-minutes-with-kyle-walters-logically/\nhttps://www.reddit.com/r/ToasterTalk/comments/16hewwd/ai_image_generators_have_a_moderation_problem/\nRelated \ud83c\udf10\nAI models found to generate inaccurate and untrue election info\nMicrosoft Copilot spouts wrong answers about US election\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/adobe-sells-ai-generated-israel-hamas-war-images", "content": "Adobe sells AI-generated Israel-Hamas war images\nOccurred: October 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nOnline publications used AI-generated images of the Israel-Hamas war stored on Adobe platforms without any indication they are fake.\nAdobe allows people to upload and sell AI images as part of its stock image subscription service, Adobe Stock. And it requires submitters to disclose whether an image is generated with AI and mark the image on Stock as 'generated with AI'. \nHowever, in practice, some users of Adobe Stock are not being transparent about the provenance of their images, resulting in mis- and disinformation being amplified by easily confused end users, say commentators.\nAdobe rode back the allegation, saying 'Adobe Stock is a marketplace that requires all generative AI content to be labeled as such when submitted for licensing. These specific images were labeled as generative AI when they were both submitted and made available for license in line with these requirements.'\nThe finding raises questions about the governance of Adobe's platforms. It also highlights the importance that end users recognise the use of generative AI on professional photographic marketplaces and elsewhere.\nSystem \ud83e\udd16\nAdobe Stock website\n\nDocuments \ud83d\udcc3\nAdobe Stock Account and submission guidelines\nOperator: Adobe\nDeveloper: Adobe\nCountry: Israel; Palestine\nSector: Business/professional services; Politics\nPurpose: Generate image\nTechnology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Mis/disinformation\nTransparency: Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://interestingengineering.com/culture/israel-palestine-adobe-accused-selling-ai-images\nhttps://www.crikey.com.au/2023/11/01/israel-gaza-adobe-artificial-intelligence-images-fake-news/\nhttps://www.cyberdaily.au/digital-transformation/9774-ai-generated-pictures-of-the-israel-hamas-war-being-sold-by-adobe\nhttps://news.yahoo.com/adobe-caught-selling-ai-generated-155153354.html\nhttps://venturebeat.com/ai/adobe-responds-to-controversy-over-ai-generated-images-of-gaza-explosion/\nRelated \ud83c\udf10\nAdobe Firefly AI art generator training\nWhatsApp AI stickers generate Palestinian kids with guns\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/whatsapp-ai-stickers-generate-palestinian-kids-with-guns", "content": "WhatsApp AI stickers generate Palestinian kids with guns\nOccurred: November 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nMeta's AI Stickers product was discovered to be generating images of Palestinian children with guns, leading to accusations of racial stereotyping and bias.\nIn response to prompts using the terms 'Palestinian', 'Palestine' or 'Muslim boy Palestinian',  AI Stickers returned pictures of gun-wielding children, The Guardian found. Conversely, prompts for 'Israeli boy'\u201d generated cartoons of children playing soccer and reading.\nLaunched in September 2023, AI Stickers draws on Meta's Llama 2 open source large language model and Emu image generation model to allow users to turn text prompts into stickers. \nThe discovery of the offending stickers came during the 2023 Israel-Hamas war. Meta described the problem as a 'glitch'. \nSystem \ud83e\udd16\nMeta AI website\nMeta (2023). Introducing New AI Experiences Across Our Family of Apps and Devices\nOperator:  \nDeveloper: Meta/WhatsApp\nCountry: Palestine\nSector: Politics\nPurpose: Generate stickers\nTechnology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Stereotyping\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nAustralian Greens (2023). META\u2019S RACIST DEPICTIONS OF GUN-WIELDING PALESTINIAN CHILDREN NEEDS URGENT INVESTIGATION FROM ESAFETY COMMISSIONER\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theverge.com/2023/11/5/23946732/whatsapp-ai-sticker-guns-palestine-israel\nhttps://www.theguardian.com/technology/2023/nov/02/whatsapps-ai-palestine-kids-gun-gaza-bias-israel\nhttps://metro.co.uk/2023/11/03/whatsapp-ai-creates-images-of-gun-wielding-boy-from-palestine-prompts-19768060/\nhttps://www.sacpa.org.uk/2023/11/03/whatsapp-ai-generates-stickers-of-gun-wielding-children-from-palestine-prompts/\nhttps://www.arabnews.com/node/2402791/media\nRelated \ud83c\udf10\nInstagram inserts \u2018terrorist\u2019 into Palestinians' biography translations\nFacebook Cross-check VIP whitelisting\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/cruise-av-injures-pedestrian-has-license-revoked", "content": "Cruise AV drags pedestrian across street\nOccurred: October 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Cruise self-driving car ran over a pedestrian who had been hit by another vehicle, pinning the individual under one of its tires and dragging her 20 feet at 7 mph, worsening her injuries.\nOn October 2, 2023, a hit-and-run in San Francisco ended with a pedestrian 'stuck' under a Cruise autonomous vehicle (AV) having been hit by another, human-driven Nissan car, and dragged 20 feet across the road. Cruise disabled the vehicle, enabling rescuers to get the vehicle off the woman\u2019s leg.\nCruise initially pinned the blame on the driver of the other car, but later said its automated driving system 'inaccurately characterized the collision as a lateral collision and commanded the AV to attempt to pull over out of traffic, pulling the individual forward, rather than remaining stationary.'\nThe incident, which had significant repercussions for the company, raised questions about the safety and integrity of Cruise's self-driving system and AV programme, the quality of its leadership, the ethics, values and culture of the company, its transparency and sense of accountability, and legal liability.\n\u2795 October 2023. The US National Highway Traffic Safety Administration opened a preliminary evaluation into Cruise over possible risks to pedestrians from its driverless vehicles. \n\u2795 October 2023: California's Department of Motor Vehicles indefinitely suspended Cruise's self-driving service after determining that its driverless cars were regarded as unsafe, and that the company had 'misrepresented' information related to the safety of its vehicles. \n\u2795 October 2023: Cruise paused its driverless fleet of 950 cars in order to 'take steps to rebuild public trust.'\n\u2795 November 2023: Cruise recalled all 950 of its cars in the form of an update to its Collision Detection Subsystem so that the vehicle remains stationary during certain crash incidents, rather than pulling over to the side of the road. \n\u2795 November 2023: The New York Times revealed that Cruise workers intervened to help the company\u2019s driverless robotaxis every 2.5 to 5 miles, calling into question whether they can reasonably be classified as 'self-driving'.\n\u2795 November 2023: Cruise fired nine senior employees and retained the law firm Quinn Emanuel Urquhart & Sullivan LLP 'to examine and better understand Cruise\u2019s response to the October 2 incident.'\n\u2795 November 2023: Cruise CEO Kyle Vogt resigned.\n\u2795 November 2023. Cruise owner General Motors said it would 'substantially' reduce spending on Cruise.\n\u2795 January 2024: Details are revealed of a report by law firm Quinn Emanuel on the incident that called out Cruise's 'deficient leadership', an 'us versus them' mentality with regulators, and a 'fundamental misapprehension of Cruise\u2019s obligations of accountability and transparency to the government and the public'.\nSystem \ud83e\udd16\nCruise Automated Driving Systems website \nDocuments \ud83d\udcc3\nCruise Releases Third-Party Findings Regarding October 2\nA detailed review of the recent SF hit-and-run incident\nImportant Updates from Cruise\nOperator: GM Cruise\nDeveloper: GM Cruise\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Self-driving system; Computer vision; Machine learning\nIssue: Accuracy/reliability; Ethics/values; Leadership; Liability; Robustness; Safety\nTransparency: Governance; Marketing\nRegulation \u2696\ufe0f\nCalifornia Vehicle Code 21950\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nNHTSA (2023). Part 573 Safety Recall Report (pdf)\nCalifornia Department of Motor Vehicles (2023). DMV STATEMENT ON CRUISE LLC SUSPENSION\nResearch, advocacy \ud83e\uddee\nKoopman P. Anatomy of a Robotaxi Crash: Lessons from the Cruise Pedestrian Dragging Mishap\nInvestigations, assessments, audits \ud83e\uddd0\nQuinn Emanuel Urquhart & Sullivan (2023). REPORT TO THE BOARDS OF DIRECTORS OF CRUISE LLC, GM CRUISE HOLDINGS LLC, AND GENERAL MOTORS HOLDINGS LLC REGARDING THE OCTOBER 2, 2023 ACCIDENT IN SAN FRANCISCO (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://abcnews.go.com/Business/wireStory/california-regulators-suspend-recently-approved-san-francisoc-robotaxi-104258076\nhttps://www.vice.com/en/article/4a3ba3/california-dmv-suspends-cruises-self-driving-car-license-after-pedestrian-injury\nhttps://techcrunch.com/2023/10/24/robotaxi-pushback-grows-in-los-angeles-as-cruise-loses-permits/\nhttps://electrek.co/2023/10/26/after-its-license-was-pulled-in-ca-cruise-is-pausing-operations-everywhere/\nhttps://www.latimes.com/business/story/2023-10-27/cruise-shuts-down-robot-cars-rebuild-public-trust\nhttps://www.wired.com/story/cruise-robotaxi-self-driving-permit-revoked-california/\nhttps://www.nytimes.com/2023/11/03/technology/cruise-general-motors-self-driving-cars.html\nhttps://www.cnbc.com/2024/01/25/gm-cruise-probe-finds-poor-leadership-at-center-of-incident-response.html\nhttps://www.theautopian.com/internal-report-shows-cruise-didnt-think-its-robotaxi-dragging-a-pedestrian-was-a-big-enough-deal-to-fix-the-cars/\nRelated \ud83c\udf10\nCruise robotaxi hits fire engine\nCruise AV rear-ends San Francisco transit bus\nPage info\nType: Incident\nPublished: November 2023\nLast updated: January 2024", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/australian-academics-make-false-ai-generated-allegations", "content": "Australian academics make false AI-generated allegations\nOccurred: November 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAustralian academics used Google Bard to generate fake case studies about alleged misconduct at Deloitte and KPMG.\nMacquarie University Emeritus Professor of Accounting James Guthrie and other academics falsely accused KPMG of being  complicit in a 'KPMG 7-Eleven wage theft scandal' that led to the resignation of several partners, and that Deloitte had been sued by the liquidators of collapsed building firm Probuild for failing to audit its accounts. Both allegations were untrue.\nGuthrie later admitted the errors in a letter to Australia's Senate and excused the other academics. The academics had urged a parliamentary inquiry into the ethics and professional accountability of the consultancy industry, advocating for regulatory changes that included breaking up the big four.\nThe incident raised concerns about the misuse of AI in shaping public discourse, and the potential harm caused to the reputation of the named companies, and to the accounting industry.\nSystem \ud83e\udd16\nGoogle Bard website\nGoogle Bard Wikipedia profile\nOperator: James Guthrie\nDeveloper: Alphabet/Google; OpenAI\nCountry: Australia\nSector: Business/professional services\nPurpose: Develop case studies\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Accuracy/reliability; Mis/disinformation\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://news.bloombergtax.com/financial-accounting/big-four-false-allegations-based-on-google-ai-academic-claims\nhttps://www.theguardian.com/business/2023/nov/02/australian-academics-apologise-for-false-ai-generated-allegations-against-big-four-consultancy-firms\nhttps://www.theguardian.com/business/2023/nov/03/kpmg-ai-complaint-non-existent-scandal-ai-case-studies-google-bard\nhttps://www.cyberdaily.au/digital-transformation/9779-researchers-apologies-to-big-4-consultancy-firms-for-false-ai-based-accusations\nhttps://www.cryptopolitan.com/academics-apologize-for-false-ai-allegations/\nRelated \ud83c\udf10\nChatGPT invented case citations in legal filings\nDefence lawyer using AI 'botches' criminal trial closing argument\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/deepfake-justin-trudeau-endorses-petro-canada-scam", "content": "Deepfake Justin Trudeau endorses Petro-Canada scam\nOccurred: April 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA fake Facebook video ad used clips of a Canadian television anchor and Prime Minister Justin Trudeau to lure Canadians to invest in Petro-Canada. \nThe video, which was seen over 15,000 times, gave the appearance of Trudeau and Canadian Broadcasting Corporation (CBC) anchor Aarti Pole explaining that Petro-Canada had launched a new investment platform that was available to all residents of the country. The energy company confirmed it had made no such offer and CBC said Pole had made no such statement.\nPer AFP, a reverse-image search of a screenshot from the clip led to a real video of the prime minister discussing the deaths of a family who attempted to cross the Canada-US border. The videos of Trudeau and the news presenter were manipulated to include words neither of them said.\nSystem \ud83e\udd16\nUnknown\nOperator:  \nDeveloper: \nCountry: Canada\nSector: Energy; Politics\nPurpose: Defraud\nTechnology: Deepfake - audio, video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Fraud; Mis/disinformation\nTransparency: Governance; Marketing\nFact check \ud83d\udea9\n (2023). Deepfake video promotes Canadian investment scam\nRelated \ud83c\udf10\nAudio deepfake fraudulently impersonates CEO\nKerala man loses INR 40,000 to deepfake work colleague\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/westfield-high-school-non-concensual-nude-deepfakes", "content": "Westfield High School students hit by non-consensual nude deepfakes\nOccurred: October 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGirls at Westfield High School in New Jersey, USA, were subjected to fake nude images of them being shared among other students, sparking uproar and prompting a police investigation.\nMale classmates reputedly used girls' photos found online to concoct and circulate AI-generated pornographic images of female students as young as 14 years old in group chats. One victim told the Wall Street Journal, 'We're aware that there are creepy guys out there but you'd never think one of your classmates would violate you like this.'  \nIt had taken four days before the school found out that the boys had been using the Clothoff AI tool to create and share fake the nude images, though school officials said they 'believed' the images had since been deleted and were no longer in circulation.\nThe Wall Street Journal reported that families of four of the victims had filed police reports, and a New Jersey senator had asked county prosecutors to investigate.\n\u2795 June 2024. 14-year-old Westfield deepfake porn victim Francesca Mani appeared before US Senate hearing on revenge pornography\n\u2795 July 2024. Francesca Mani launched AI Heeelp!, a website that offers resources to other victims of deepfake pornography.\nSystem \ud83e\udd16\nClothOff denudifier\nOperator: Westfield High School students\nDeveloper: Alaiksandr Babichau, Alexander German, Dasha Babicheva, Yevhen Bondarenko\nCountry: USA\nSector: Education\nPurpose: Entertain\nTechnology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Ethics/values; Safety; Privacy\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.axios.com/2023/11/03/ai-deepfake-nude-images-new-jersey-high-school\nhttps://www.wsj.com/tech/fake-nudes-of-real-students-cause-an-uproar-at-a-new-jersey-high-school-df10f1bb\nhttps://nypost.com/2023/11/02/news/ai-generated-nudes-of-girls-at-nj-high-school-trigger-police-probe/\nhttps://www.cbsnews.com/newyork/news/westfield-high-school-ai-pornographic-images-students/\nhttps://arstechnica.com/tech-policy/2023/11/deepfake-nudes-of-high-schoolers-spark-police-probe-in-nj/\nhttps://www.dailymail.co.uk/news/article-12702289/Outrage-New-Jersey-high-school-boy-caught-sharing-AI-generated-NUDES-female-classmates-leaving-victims-feeling-humiliated-fearing-images-end-online-school-principal-issues-warning-parents.html\nhttps://nj1015.com/shocking-deepfake-porn-images-at-westfield-nj-school/\nRelated \ud83c\udf10\nAlmendralejo hit by AI-generated naked child images\nTelegram bot creates non-consensual deepfake porn\nPage info\nType: Incident\nPublished: November 2023\nLast updated: August 2024", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/scarlett-johansson-sues-app-for-using-image-for-ai-advert", "content": "Scarlett Johansson sues app for using image for AI advert\nOccurred: October 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nHollywood actress Scarlett Johansson has taken legal action against an AI app developer for using her name and likeness in an online advert without her consent. \nAI image editor Lisa AI started with an old clip of Johansson behind the scenes of Marvel\u2019s 'Black Widow,' before featuring an AI-generated version of Johansson\u2019s voice stating, 'It\u2019s not limited to avatars only. You can also create images with texts and even your AI videos. I think you shouldn\u2019t miss it'. \nA disclaimer under the advert said, 'Images produced by Lisa AI. It has nothing to do with this person.' However, Johansson's voice and likeness had been used without permission, prompting the actress to take legal action against Lisa AI and its developer Convert Yaz\u0131l\u0131m Limited \u015eirketi.\nThe fracas prompted commentators and lawyers to highlight the lack of consensus on AI and deepfake legislation in the USA and elsewhere, and to point out the need for strong legal frameworks that would offer individuals strong protection from unauthorised AI-generated content.\nSystem \ud83e\udd16\nLisa AI website\nOperator: Convert Yaz\u0131l\u0131m Limited \u015eirketi\nDeveloper: Convert Yaz\u0131l\u0131m Limited \u015eirketi\nCountry: Turkey; USA  \nSector: Media/entertainment/sports/arts\nPurpose: Increase visibility\nTechnology: Deepfake - audio, video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Mis/disinformation\nTransparency: Governance; Marketing\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nTBC\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://variety.com/2023/digital/news/scarlett-johansson-legal-action-ai-app-ad-likeness-1235773489/\nhttps://www.theguardian.com/film/2023/nov/01/scarlett-johansson-artificial-intelligence-ad\nhttps://www.standard.co.uk/news/tech/scarlett-johansson-legal-action-ai-app-copied-likeness-b1118103.html\nhttps://www.dailymail.co.uk/tvshowbiz/article-12699555/Scarlett-Johansson-takes-legal-action-against-AI-app-used-likeness-ad-without-permission.html\nhttps://news.sky.com/story/scarlett-johansson-becomes-latest-victim-of-alleged-deepfake-advert-12998617\nhttps://www.nbcnews.com/tech/scarlett-johansson-legal-action-ai-app-rcna123248\nhttps://player.me/scarlett-johansson-takes-legal-action-against-ai-app/\nhttps://www.theregister.com/2023/11/02/fake_ai_advert_beatles/\nRelated \ud83c\udf10\nTaylor Swift speaks in Mandarin deepfake\nBella Hadid 'stands with Israel' deepfake\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/taylor-swift-speaks-in-mandarin-deepfake", "content": "Taylor Swift speaks in Mandarin deepfake\nOccurred: October 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA realistic deepfake video of popstar Taylor Swift fluently speaking Mandarin Chinese sparked discussion in China about the ethics of using artificial intelligence to develop digital content. \nDeveloped using Chinese American AI video creation company HeyGen, the video shows Swift flaunting her Mandarin in what looks like a talk show. But the clip appeared largely designed to promote itself than say anything about Taylor Swift, who does not speak Mandarin.\nWhile some Chinese citizens and commentators praised the quality of the video, others condemned it for its apparent lack of consent from Swift and the potential loss of privacy and damage to an individual or organisation's name, image and reputation caused by non-consensual synthetic media.\nOn its website, HeyGen states it is a member of the Content Authenticity Initiative, which promotes 'authentic storytelling' and 'promotes transparency' in the use of AI.\nSystem \ud83e\udd16\nHeyGen AI video generation\n\nCampaign \ud83d\udce3\nDeepfake video\nOperator: HeyGen\nDeveloper: HeyGen\nCountry: China; USA  \nSector: Media/entertainment/sports/arts\nPurpose: Promote developer\nTechnology: Deepfake - audio, video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Mis/disinformation; Privacy\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://thechinaproject.com/2023/10/27/deepfake-mandarin-speaking-taylor-swift-goes-viral-in-china-prompting-mixed-reactions/\nhttps://www.straitstimes.com/asia/east-asia/deepfake-video-of-taylor-swift-speaking-mandarin-sparks-discussion-over-ai-in-china\nhttps://www.bloomberg.com/opinion/articles/2023-11-03/taylor-swift-s-ai-deep-fake-has-something-to-say-about-china-and-australia-loire10m\nhttps://says.com/my/news/video-deepfake-of-taylor-swift-speaking-mandarin-sparks-discussion-on-the-use-of-ai\nhttps://thechainsaw.com/defi/taylor-swift-mandarin-ai-generated-deepfake-viral/\nhttps://www.cryptopolitan.com/deepfake-video-of-taylor-swift/\nhttps://radii.co/article/taylor-swift-deepfake-video\nRelated \ud83c\udf10\nRana Ayyub deepfake porn attack, doxxing\nBella Hadid 'stands with Israel' deepfake\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/president-biden-calls-for-us-draft-deepfake", "content": "President Biden calls for US draft deepfake\nOccurred: February 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA video in which US President Joe Biden appeared to call for a military draft was created with AI, according to experts. \nThe clip, which was based on a video showing President Biden speaking about the cost of insulin, was first flagged by Meta during the early days of the 2023 Israel-Hamas war, though a longer version had been shared in February 2023 by Conservative activist Jack Posobiec and Canadian website Post Millennial. \nIn both videos, Biden appeared to say, 'Invoke the Selective Service Act, as is my authority as President. Remember, you\u2019re not sending your sons and daughters to war. You\u2019re sending them to freedom.' But only the first clip indicated it had been altered using artificial intelligence, saying, 'AI imagines what would happen if Biden declares and activates the Selective Service Act and begins drafting 20 years old to war.'\nThe video seemed designed to raise fears about US defence policy, and to damage Biden's reputation. A spokesperson for the Selective Service System told USA Today the claim was false and that there had been no discussion of bringing back the draft. \nSystem \ud83e\udd16\nUnknown\nDeepfake video\nPresident Biden Delivers Remarks on Lowering Insulin Costs\nOperator:  \nDeveloper: \nCountry: USA\nSector: Politics\nPurpose: Manipulate public opinion\nTechnology: Deepfake - audio, video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Mis/disinformation\nTransparency: Governance; Marketing\nFact check \ud83d\udea9\nAFP (2023). Biden deepfake announcing US draft resurfaces amid Israel-Hamas war\nReuters (2023). Fact Check: Video of Joe Biden calling for a military draft was created with AI\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://eu.usatoday.com/story/news/factcheck/2023/10/19/purported-joe-biden-video-calling-for-military-draft-created-with-ai-fact-check/71226206007/\nhttps://www.politifact.com/factchecks/2023/oct/17/facebook-posts/biden-did-not-announce-a-military-draft-this-video/\nRelated \ud83c\udf10\nJoe Biden police defunding deepfake interview\nRNC smears President Biden with fake AI advert\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/greta-thunberg-promotes-use-of-vegan-grenades", "content": "Deepfake Greta Thunberg promotes use of 'vegan grenades'\nOccurred: October 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA video of Swedish environmental activist Greta Thunberg speaking out about the 2023 Israel-Hamas war in which she called for the use of 'biodegradable missiles' and 'vegan hand grenades' was doctored using artificial intelligence, according to experts.\nIn the video, which circulated widely on X, TikTok and Reddit, Thunberg appeared to say to the BBC, 'War's always bad, specifically for the planet. If we want to continue fighting battles like environmentally conscious humans, we must make the change to sustainable tanks and weaponry.'\nThe clip was allegedly created by German comedy outlet Snicklick and was labelled as satire. But some users appeared to believe it was real, while others chose to use it as an opportunity to attack Thunberg for her pro-Palestinian views and her environmental beliefs, and to spread disinformation. \nExperts noted poor synchronisation of Thunberg's mouth and audio and blurry hands and neck, and linked it to a real November 2022 BBC interview in which the activist spoke about climate anxiety, turning vegan and her book, The Climate Book.\nSystem \ud83e\udd16\nSnicklick deepfake video\nOperator:  \nDeveloper: Snicklick\nCountry: Sweden\nSector: Politics\nPurpose: Satirise/parody  \nTechnology: Deepfake - audio, video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Mis/disinformation\nTransparency: \nFact check \ud83d\udea9\nAFP (2023). Deepfake of Greta Thunberg demanding 'environmentally conscious' wars spreads online\nReuters (2023). Fact Check: Greta Thunberg \u2018vegan grenades\u2019 TV interview is deepfake\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://eu.usatoday.com/story/news/factcheck/2023/10/27/no-thunberg-didnt-call-for-biodegradable-missiles-fact-check/71328376007/\nhttps://www.theaustralian.com.au/commentary/fake-ai-video-depicts-greta-thunberg-calling-for-vegan-grenades/video/7bea476c478ea8ca5e61955cdfeb3687\nhttps://www.dailydot.com/debug/greta-thunberg-vegan-grenades-deepfake/\nhttps://www.indiatoday.in/fact-check/story/fact-check-no-green-missiles-vegan-grenades-greta-thunberg-video-edited-2452830-2023-10-23\nRelated \ud83c\udf10\nDeepfake Palestinian carries children out of rubble\nDeepfake Donald Trump calls for climate agreement exit\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/deepfake-palestinian-carries-children-out-of-rubble", "content": "Deepfake Palestinian man carries children out of rubble\nOccurred: November 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn image of a man carrying children through rubble during Israel's bombing of the Gaza Strip has been assessed as a probable deepfake.\nThe deepfake image, which appears to show a man helping five children away from the scene of a destroyed building, was shared over 80,000 times on social media and was further amplified on X (formerly Twitter) of the Chinese embassy in France.\nSiwei Lyu, director of the Media Forensic Lab at the University of Buffalo, told AFP that the image classifies 'as AI generated by recent detection algorithms,' and highlighted irregularities in the hands and feet.\nThe finding underscores concerns about the ease with which manipulated images are being developed and used to spread disinformation. The image is the latest in a series of deepfakes used during Israel's 2023 war with Hamas.\nSystem \ud83e\udd16\nDeepfake image\nOperator:  \nDeveloper: \nCountry: Israel; Palestine\nSector: Politics\nPurpose: Manipulate public opinion\nTechnology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Mis/disinformation\nTransparency: Governance; Marketing\nFact check \ud83d\udea9\nAFP (2023). Image of Palestinian carrying children out of rubble shows signs of AI\nRelated \ud83c\udf10\nBella Hadid 'stands with Israel' deepfake\nAI or Not misidentifies Hamas baby victim as deepfake\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chatgpt-consumes-500-ml-of-water-per-5-50-prompts", "content": "ChatGPT consumes 500 ml of water per 5-50 prompts\nOccurred: September 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nChatGPT consumes 500 milliliters of water every time it is asked a series of between 5 to 50 prompts or questions, according (pdf) to a study by researchers at the University of California, Riverside. \nThe wide range varies depending on where its servers are located and the season. The estimate includes indirect water usage that the companies do not measure, such as to cool power plants that supply the data centers with electricity.\nOpenAI responded by saying that it was giving 'considerable thought' to the best use of its computing power. 'We recognize training large models can be energy and water-intensive\" and work to improve efficiencies', it said.\nThe finding raises questions about the impact of ChatGPT and other large language models on water consumption and local communities. It also highlights the reluctance of OpenAI and other developers to disclose their environmental impacts. \nSystem \ud83e\udd16\nChatGPT chatbot\nOperator: OpenAI\nDeveloper: OpenAI\nCountry: USA\nSector: Technology\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Environment\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nLi P. et al (2023). Making AI Less \u201cThirsty\u201d: Uncovering and Addressing the Secret Water Footprint of AI Models (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://abcnews.go.com/US/wireStory/artificial-intelligence-technology-chatgpt-built-iowa-lot-water-103052513\nhttps://www.businesstoday.in/technology/news/story/may-drink-a-500-ml-bottle-of-water-for-20-50-questions-chatgpt-data-centres-consumes-a-lot-of-water-warns-study-377473-2023-04-14\nhttps://www.euronews.com/green/2023/04/20/chatgpt-drinks-a-bottle-of-fresh-water-for-every-20-to-50-questions-we-ask-study-warns\nhttps://apnews.com/article/chatgpt-gpt4-iowa-ai-water-consumption-microsoft-f551fde98083d17a7e8d904f8be822c4\nhttps://indianexpress.com/article/technology/artificial-intelligence/chatgpt-water-consumption-research-8854772/\nhttps://www.techzine.eu/news/applications/111144/chatgpts-water-consumption-is-astronomical-40-prompts-demand-one-litre/\nhttps://www.rollingstone.com/culture/culture-news/ai-chatgpt-increased-water-consumption-environmental-reports-1234821679/\nhttps://www.independent.co.uk/tech/chatgpt-data-centre-water-consumption-b2318972.html\nhttps://www.govtech.com/question-of-the-day/how-much-water-does-chatgpt-drink-for-every-20-questions-it-answers\nhttps://decrypt.co/155965/artificial-intelligence-water-use-data-centers\nhttps://futurism.com/critics-microsoft-water-train-ai-drought\nRelated\nStochastic Parrots study questions large language model size\nBitcoin mining algorithm environmental damage\nPage info\nType: Incident\nPublished: September 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/bella-hadid-stands-with-israel-deepfake", "content": "Bella Hadid 'stands with Israel' deepfake\nOccurred: October 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA video of Palestinian model Bella Hadid declaring support for Israel has been exposed as a deepfake, sparking ethical concerns about consent, privacy, and political disinformation. \nThe video, which was posted on X (formerly Twitter) by CEO of Israeli NGO Shrink the Conflict Danel Ben Namer, shows Hadid appearing to apologise for past remarks and saying she stands with Israel following the October 7, 2023, attack by Hamas militants. \nIn the doctored video, Hadid says, 'On October 7, 2023, Israel faced a tragic attack by Hamas. I can't stay silent. I apologize for my past remarks. This tragedy has opened my eyes to the pain endured here and I stand with Israel against terror.'\nHowever, the clip was denounced as a deepfake version of a speech the model gave in 2016 about her battle with Lyme disease. Hadid, who has a Palestinian father, has been a vocal proponent of Palestinian rights for years.\nSystem \ud83e\udd16\nBella Hadid deepfake video\nOperator:  \nDeveloper:  \nCountry: Israel; Palestine\nSector: Politics\nPurpose: Manipulate public opinion\nTechnology: Deepfake - audio, video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Mis/disinformation; Privacy\nTransparency: Governance; Marketing\nFact check \ud83d\udea9\nAFP (2023). Deepfake of Bella Hadid misrepresents her statements on Israel\nLogically (2023). Video of model Bella Hadid declaring her support for Israel is a deepfake\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.standard.co.uk/news/tech/bella-hadid-deepfake-video-support-israel-b1116801.html\nhttps://nypost.com/2023/10/31/news/bella-hadid-video-is-scarily-realistic-deepfake/\nhttps://www.thequint.com/news/webqoof/bella-hadid-expressing-support-for-israel-deepfake-viral-video-fact-check\nhttps://www.404media.co/deepfake-bella-hadid-israel-palestine-video/\nRelated \ud83c\udf10\nAI or Not misidentifies Hamas baby victim as deepfake\nIsrael AI robot machine guns fire tear gas at Palestinian protestors\nPage info\nType: Issue\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chatgpt-writes-code-that-makes-databases-leak-sensitive-info", "content": "ChatGPT writes code that makes databases leak sensitive info\nOccurred: October 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGenerative AI tools such as ChatGPT, Baidu-UNIT, and AI2sql can be tricked into producing malicious code, which could be used to launch cyber attacks, according to new research. \nUniversity of Sheffield researchers found that it is possible to manipulate six commercial AI tools capable of generating responses to text-to-SQL queries, including ChatGPT, into creating code capable of breaching other systems, steal sensitive personal information, tamper with or destroy databases, or bring down services using denial-of-service attacks.\nAccording to the researchers, OpenAI has since fixed all of the specific issues, as has Baidu, which financially rewarded the scientists. Developers of the four other systems have not responded publicly.\nSystem \ud83e\udd16\nChatGPT chatbot\nBaidu UNIT website\nOperator:  \nDeveloper: AI2sql; Baidu; NiceAdmin; OpenAI; Text2SQL.AI; SQLAI.AI\nCountry: USA\nSector: Technology\nPurpose: Generate text\nTechnology: Chatbot; Text-to-SQL; NLP/text analysis; Neural network; Deep learning; Machine learning\nIssue: Privacy; Security\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nPeng X. et al (2023). On the Security Vulnerabilities of Text-to-SQL Models\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://techxplore.com/news/2023-10-chatgpt-ai-tools-malicious-code.html\nhttps://www.standard.co.uk/news/science/chatgpt-research-university-of-sheffield-artificial-intelligence-government-b1115677.html\nhttps://metro.co.uk/2023/10/24/hackers-can-use-chatgpt-to-steal-your-private-data-says-new-study-19712353/\nhttps://interestingengineering.com/culture/chatgpt-like-ai-can-be-tricked-to-produce-malicious-code-cyber-attacks\nhttps://www.dailymail.co.uk/sciencetech/article-12666677/ChatGPT-responsible-CYBER-ATTACK-Scientists-AI-systems-tricked-producing-malicious-code.html\nhttps://www.newscientist.com/article/2399370-chatgpt-wrote-code-that-can-make-databases-leak-sensitive-information/\nRelated \ud83c\udf10\nChatGPT bug reveals user chat histories\nChatGPT powers 'Fox8' crypto promotion botnet\nPage info\nType: Issue\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/microsoft-ai-researchers-expose-38tb-confidential-data", "content": "Microsoft AI researchers expose 38TB confidential data\nOccurred: October 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nMicrosoft AI researchers accidentally exposed 38 terabytes of confidential and private information on GitHub, raising questions about the company's security practices. \nWiz researchers investigating a cloud-hosted data exposure discovered a Microsoft GitHub repository with open-source code for AI image recognition models. The data, some of which had been exposed since July 2020, included backups of two Microsoft employees\u2019 computers, private passwords and passkeys, and more than 30,000 Teams chat messages exchanged by 359 Microsoft employees.\nMicrosoft linked the data exposure to using an excessively permissive Azure Cloud Shared Access Signature (SAS) token. In response, the company expanded GitHub\u2019s secret spanning service, which tracks all public open-source code changes for credentials and other secrets exposed in plaintext. \nSystem \ud83e\udd16\nGithub website\nGithub Wikipedia profile\nOperator: Microsoft\nDeveloper: Microsoft/Github\nCountry: USA\nSector: Technology\nPurpose: \nTechnology: Computer vision\nIssue: Security\nTransparency: \nResearch, advocacy \ud83e\uddee\nWiz (2023). 38TB of data accidentally exposed by Microsoft AI researchers\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://techcrunch.com/2023/09/18/microsoft-ai-researchers-accidentally-exposed-terabytes-of-internal-sensitive-data/\nhttps://www.techtarget.com/searchsecurity/news/366552399/Microsoft-AI-researchers-mistakenly-expose-38-TB-of-data\nhttps://www.infosecurity-magazine.com/news/microsoft-ai-researcher-leaked/\nhttps://www.techradar.com/pro/security/microsoft-ai-researchers-leaked-38tb-worth-of-private-company-data\nhttps://www.bleepingcomputer.com/news/microsoft/microsoft-leaks-38tb-of-private-data-via-unsecured-azure-storage/\nhttps://www.cpomagazine.com/cyber-security/data-leak-by-microsoft-ai-researchers-exposes-38tb-of-private-internal-data/\nRelated \ud83c\udf10\nMicrosoft/Github Copilot 'code laundering'\nMicrosoft teen pregnancy predictions\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/microsoft-start-automated-poll-damages-guardian-reputation", "content": "Microsoft Start automated poll damages Guardian reputation\nOccurred: October 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn AI-generated poll accompanying a article republished on Microsoft's Start news aggregation website about a woman's death led to accusations of crass insensivity, and led The Guardian to complain it had damaged its reputation.\n\nThe poll asked readers to speculate on the reason for the woman's death, which occurred at a school in Melbourne, Australia, asking them whether the woman had died by suicide, murder, or accident. In a disclaimer, the tech company noted the poll was part of it's 'Insights from AI.' But readers assumed The Guardian was responsible for the tool, one calling it 'pathetic' and 'disgusting', amongst other things.\n\nIn a letter (pdf) to Microsoft chairman Brad Smith, Guardian Media Group CEO Anna Bateson demanded the technology company take public responsibility for the poll, saying 'This is clearly an inappropriate use of genAI by Microsoft on a potentially distressing public interest story, originally written and published by Guardian journalists.'\n\nMicrosoft eventually removed the poll and turned off comments for the article, adding 'A poll should not have appeared alongside an article of this nature, and we are taking steps to help prevent this kind of error from reoccurring in the future.' \nSystem \ud83e\udd16\nMicrosoft Start website\nMicrosoft Start Wikipedia profile\nMicrosoft Start/The Guardian (2023). Woman found dead at St Andrew\u2019s school in Sydney identified as water polo coach Lilie James\nOperator: Microsoft\nDeveloper: Microsoft\nCountry: UK; USA\nSector: Media/entertainment/sports/arts\nPurpose: Generate polls\nTechnology: Machine learning\nIssue: Appropriateness/need\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theguardian.com/media/2023/oct/31/microsoft-accused-of-damaging-guardians-reputation-with-ai-generated-poll\nhttps://www.axios.com/2023/10/31/guardian-microsoft-generative-ai-poll-death\nhttps://www.telegraph.co.uk/business/2023/10/31/guardian-newspaper-microsoft-msn-ai-poll/\nhttps://arstechnica.com/information-technology/2023/10/inserted-ai-generated-microsoft-poll-about-womans-death-rankles-the-guardian/\nhttps://www.neowin.net/news/the-guardian-battles-controversy-over-offensive-ai-backed-poll-blames-microsoft-for-it/\nhttps://www.theverge.com/2023/10/31/23940298/ai-generated-poll-guardian-microsoft-start-news-aggregation\nRelated\nMicrosoft replaces journalists with AI\nCNET Money automated financial explainers\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/worldcoin-suspended-in-kenya-for-privacy-abuse", "content": "Worldcoin suspended in Kenya over privacy, security concerns\nOccurred: August 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\n'Digital identity and financial network' Worldcoin had its operations suspended in Kenya after regulators raised concerns about its collection and storage of sensitive biometric data. \nWorldcoin had seen strong demand from Kenyan citizens to register for the platform, which uses a 'chrome orb' to scan the irises and faces of people agreeing to sign up for a share of its new WLD currency.\nHowever, Kenyan regulators pointed out that Worldcoin had no need to collect users\u2019 iris data, and was not regulated in the country. In October 2023 a Kenyan parliamentary committee recommended (pdf) that Worldcoin had violated Kenyan law and be shut down until the country established proper regulations over virtual assets. \nThe committee also accused the company of \u2018espionage\u2019 and suggested it had been scanning the eyeballs of children. It also called for criminal investigations into Tools for Humanity Corp., the company behind Worldcoin, Tools for Humanity GmbH, Germany (Worldcoin), and its Kenyan partners.\nSystem \ud83e\udd16\nWorldcoin iris biometrics\nIncident databank \ud83d\udd22\nOperator: Tools for Humanity/Worldcoin\nDeveloper: Tools for Humanity/Worldcoin\nCountry: Kenya\nSector: Banking/financial services\nPurpose: Develop digital identity\nTechnology: Iris scanning; Facial detection; Vital signs detection; Blockchain; Virtual currency\nIssue: Privacy; Security; Legality\nTransparency: Governance; Privacy; Marketing\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nAd hoc Committee on the Enquiry into the Activities of Worldcoin in Kenya (2023). Report on the Enquiry into the Activities and Operations of WorldCin in Kenya (pdf)\nCommunications Authority of Kenya (2023). Joint Statement on the Operations of the Worldcoin in Kenya\nKenya Data Protection Action 2019\nResearch, advocacy \ud83e\uddee\nICTWorks (2023). What Happened? WorldCoin Crypto ID is Suspended in Kenya\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reuters.com/world/africa/kenyan-government-suspends-activities-worldcoin-country-2023-08-02/\nhttps://www.reuters.com/technology/kenya-panel-urges-shutdown-worldcoins-crypto-project-within-country-2023-10-02/\nhttps://www.the-star.co.ke/news/realtime/2023-08-21-parliament-forms-committee-to-investigate-worldcoin-project/\nhttps://www.trtafrika.com/business/worldcoin-crypto-denies-mining-data-in-kenya-eyeball-scans-14846096\nhttps://findbiometrics.com/kenya-suspends-worldcoin-registrations-980804/\nhttps://techcrunch.com/2023/08/15/worldcoin-in-kenya/\nhttps://techcabal.com/2023/09/07/worldcoin-case-force-kenya-to-introduce-new-laws\nhttps://www.coindesk.com/policy/2023/08/03/kenya-appears-to-have-flip-flopped-on-worldcoins-data-practices/\nRelated \ud83c\udf10\nWorldcoin digital identity and financial network\nTerraUSD algorithmic stablecoin\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/dark-web-predators-develop-ai-images-of-real-child-victims", "content": "Dark web predators develop AI images of real child victims\nOccurred: October 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nExamples of AI-generated child sexual abuse material (CSAM) of real victims of sexual abuse have been discovered online, fueling concerns about the ease with which users intent on producing inappropriate images can bypass AI text-to-image generators' guardrails.\n\nA report (pdf) by UK non-profit Internet Watch Foundation (IWF) found that over 11,000 AI-generated CSAM images were found on one darkweb forum in one month, of which 2,978 broke UK law by depicting child sexual abuse. The IWF said the only image generator being discussed on the forum was Stable Diffusion, a free, open source system that generates images from text descriptions or prompts.\n\nThe IWF warned that the most convincing imagery would be difficult for trained analysts to distinguish from real photographs, and warns text-to-image technology will only get better and pose more obstacles for the IWF and law enforcement agencies. Commentators also pointed out that bad actors can use open source models such as Stable Diffusion for nefarious purposes by downloading the software and training it to do whatever they want.\nSystem \ud83e\udd16\nStability AI website\nStability AI Acceptable Use Policy\nStable Diffusion website\nStable Diffusion Wikipedia profile\nOperator:  \nDeveloper: Stability AI\nCountry: UK\nSector: Media/entertainment/sports/arts\nPurpose: Generate images\nTechnology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Safety\nTransparency: Governance\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nUK Online Safety Bill 2023\nUK Protection of Children Act 1978\nInvestigation, assessment, audit \ud83e\uddd0\nInternet Watch Foundation (2023). How AI is being abused to create child sexual abuse imagery (pdf)\nInternet Watch Foundation (2023). \u2018Worst nightmares\u2019 come true as predators are able to make thousands of new AI images of real child victims\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theguardian.com/technology/2023/oct/25/ai-created-child-sexual-abuse-images-threaten-overwhelm-internet\nhttps://decrypt.co/203327/ai-child-sexual-abuse-material-overwhelm-web-uk-internet-watch\nhttps://interestingengineering.com/culture/ai-used-to-generate-child-abuse-images\nhttps://www.bbc.com/news/uk-england-cambridgeshire-67145583\nhttps://thehill.com/policy/technology/4274456-watchdog-warns-of-ai-generated-child-sexual-abuse/\nhttps://www.independent.co.uk/tech/internet-watch-foundation-children-b2435437.html\nRelated \ud83c\udf10\nStudy: Instagram enables global paedophile network\nApple NeuralHash CSAM scanning\nPage info\nType: Incident\nPublished: October 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/llama-model-used-to-create-allie-sexbot", "content": "LLaMA language model used to create Allie sexbot\nOccurred: June 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nMeta's open source LLaMA (Large Language Model Meta AI) large language model has been used to develop 'Allie,' an AI-powered chatbot specially created for sexual purposes that allows users to indulge in graphic rape and abuse fantasies.\n\nThe discovery raised questions about Meta's decision to open source its AI models without adequate guardrails, and about the risks and benefits of open source more generally. \n\nAt its launch, Meta said 'LLaMA is designed to be versatile and can be applied to many different use cases. By sharing the code [..], other researchers can more easily test new approaches to limiting or eliminating these problems in large language models.' \n\nAllie's anonymous developer defended the bot to the Washington Post as a \u2018safe outlet to explore and that he couldn't 'really think of anything safer than a text-based role-play against a computer, with no humans actually involved'.\nSystem \ud83e\udd16\nLLaMA Wikpedia profile\nMeta (2023). Introducing LLaMA: A foundational, 65-billion-parameter large language model\nOperator:  \nDeveloper: Meta\nCountry: USA\nSector: Technology\nPurpose: Democratise access to AI\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Dual/multi-use; Safety\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.washingtonpost.com/technology/2023/06/26/facebook-chatbot-sex/\nhttps://futurism.com/metas-new-ai-graphic-sexbots\nhttps://techgameworld.com/allie-metas-ai-powered-sex-chatbot-how-it-works-and-why-it-was-created/\nhttps://mindmatters.ai/2023/07/simulating-human-connection-with-metas-allie-chatbot/\nhttps://interestingengineering.com/culture/metas-open-source-ai-is-being-used-to-create-sexbots\nhttps://www.benzinga.com/news/23/06/33008677/double-edged-sword-of-open-source-ai-from-discovering-new-pharmaceuticals-to-creating-sexualized-ai\nhttps://www.reddit.com/r/LocalLLaMA/comments/14jru57/metas_new_ai_lets_people_make_chatbots_theyre/\nRelated \ud83d\uddde\ufe0f\nMicrosoft Tay chatbot\nReplika AI 'companion' chatbot\nPage info\nType: Incident\nPublished: October 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/deepfake-audio-falsely-depicts-barack-obama-discussing-conspiracy-theory", "content": "Deepfake audio falsely depicts Barack Obama discussing conspiracy theory\nOccurred: September 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA deepfake audio recording depicting Barack Obama defending himself against a conspiracy theory about the sudden death of his former chef Tafari Campbell was identified as a hoax by NewsGuard.\n\nThe audio recording was identified as a deepfake by misinformation monitoring company NewsGuard, which exposed a network of TikTok accounts posting videos whose baseless claims are often supported solely by narration from AI voices.\n\nDespite TikTok\u2019s new guidelines requiring realistic synthetic media to be labelled, the accounts, which bypass these restrictions, have been able to gain hundreds of millions of views. \n\nNewsGuard noted that the trend of using synthetic audio to share sensational rumours sets a precedent for bad actors to manipulate public opinion and share falsehoods to mass audiences online.\nSystem \ud83e\udd16\nElevenLabs\n\nDocuments \ud83d\udcc3\nObama deepfake video\nTikTok Guidelines on Synthetic Media\nOperator:\nDeveloper: ElevenLabs\nCountry: USA\nSector: Politics\nPurpose: Damage Reputation\nTechnology: Deepfake - audio; Generative adversarial network (GAN); Neural network; Machine learning\nIssue: Mis/disinformation\nTransparency: Governance; Marketing\nInvestigations, assessments, audits \ud83e\uddd0\nNewsGuard (2023).  AI Voice Technology Used to Create Conspiracy Videos on TikTok, at Scale\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.nytimes.com/2023/10/12/technology/tiktok-ai-generated-voices-disinformation.html \nhttps://www.washingtonpost.com/technology/2023/10/13/ai-voice-cloning-deepfakes/ \nhttps://www.entrepreneur.com/business-news/ai-generated-deepfakes-misinformation-concerns-on-tiktok-x/463569 \nRelated \ud83e\udd16\nSouth Korea presidential election candidate deepfakes\nDeepfake audio recording claims opposition leaders tried to rig Slovakian election\nPage info\nType: Incident\nPublished: October 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-campaign-impersonates-former-south-sudan-leader-omar-al-bashir", "content": "Audio AIs impersonate former South Sudan leader Omar al-Bashir\nOccurred: August 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAudio recordings appearing to show Omar al-Bashir, the former leader of Sudan, criticising the head of the Sudanese army have been identified as having been manipulated using artificial intelligence.\n\nThe recordings were initially posted to political channel The Voice of Sudan, before garnering hundreds of thousands of views on TikTok. BBC analysts identified several of the recordings as matching broadcasts aired days earlier by popular Sudanese political commentator, Al Insirafi. The evidence suggested that AI-powered voice conversion software was used to mimic Bashir speaking. \n\nWhile the purpose of the campaign remains unclear, The Voice of Sudan denied misleading the public and said it was not affiliated with any political or military groups. TikTok has since taken down the account on the basis that it broke its guidelines on posting 'false content that may cause significant harm', and its rules on the use of synthetic media.\n\nThe finding highlighted the increasingly high quality of deepfake technology and underscored concerns about how the technology is being used in Sudan and in politics more generally.\nSystem \ud83e\udd16\n TikTok Synthetic Media policy\nOperator:\nDeveloper:\nCountry: Sudan\nSector: Politics\nPurpose: Disinformation\nTechnology: Deepfake - audio; Generative adversarial network (GAN); Neural network; Deep learning\nIssue: Governance; Mis/disinformation\nTransparency: Governance\nNews, commentary, analysis\nhttps://www.bbc.co.uk/news/world-africa-66987869\nhttps://legrandcontinent.eu/fr/2023/10/26/comprendre-la-desinformation-en-afrique/\nRelated \ud83d\uddde\ufe0f\nSouth Korea presidential election candidate deepfakes\nDeepfake audio recording falsely depicts British Opposition Leader abusing staff\n\nPage info\nType: Incident\nPublished: October 2023\nLast updated: December 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/defence-lawyer-using-ai-botches-criminal-trial-closing-argument", "content": "Defence lawyer using AI 'botches' criminal trial closing argument\nOccurred: October 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA 'botched' closing argument used by the lead defense lawyer during the criminal trial of Fugees star Prakazrel 'Pras' Michel had been drafted with the help of a generative AI system, according to a brief demanding a retrial for Michel.\nThe brief alleges Michael's lawyer David Kenner's 'closing argument made frivolous arguments, misapprehended the required elements, conflated the schemes and ignored critical weaknesses in the government\u2019s case.' It went to argue that, by using an experimental AI system to generate his closing argument, Kenner botched 'the single most important portion' of Michel\u2019s jury trial. \nKenner used a legal system by EyeLevel.AI, which had issued a press release trumpeting its use in Michel's May 2023 trial. The released quoted Kenner saying that the AI system 'turned hours or days of legal work into seconds,' and called his use of the programme 'a look into the future of how cases will be conducted.'\nThe incident raised questions about the capabilities of the system, the judgement of the lawyer, and the use of AI-generated content in the courtroom.\nSystem \ud83e\udd16\nEyeLevel for Law\nEyeLevel (2023). First Use of AI in Federal Trial: EyeLevel's Litigation Assist Aids Defense in Pras Michel Fraud Case\nOperator: David Kenner\nDeveloper: EyeLevel.AI; CaseFile Connect\nCountry: USA\nSector: Business/professional services\nPurpose: Conduct legal research\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Accuracy/reliability\nTransparency: Governance\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nUNITED STATES OF AMERICA v PRAKAZREL MICHEL (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reuters.com/legal/transactional/convicted-fugees-rapper-says-ex-lawyer-bungled-defense-with-ai-closing-argument-2023-10-17/\nhttps://www.theregister.com/2023/10/17/fugees_ai_trial/\nhttps://futurism.com/the-byte/fugees-rapper-ai-lawyer\nhttps://www.nytimes.com/2023/10/19/us/fugees-pras-michel-ai.html\nhttps://www.nbcnews.com/news/us-news/convicted-fugees-rapper-pras-michels-lawyer-used-ai-draft-bungled-clos-rcna120992\nhttps://arstechnica.com/tech-policy/2023/10/fugees-rapper-blames-conviction-on-his-lawyers-ai-fueled-closing-argument/\nhttps://apnews.com/article/fugees-pras-appeal-conviction-trial-artificial-intelligence-d2c93aa5404eba63c3347a05fe8ba091\nhttps://www.latimes.com/entertainment-arts/music/story/2023-10-19/rapper-pras-artificial-intelligence-ai-conspiracy-trial-appeal\nRelated \ud83c\udf10\nChatGPT invents case citations in legal filings\nMalaysia AI court sentencing system said to be inaccurate, unfair\nPage info\nType: Incident\nPublished: October 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/mike-huckabee-books-used-to-train-language-models-without-consent", "content": "Mike Huckabee books used to train language models without consent\nOccurred: October 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nFormer Arkansas Governor Mike Huckabee and a group of religious authors are suing Meta, Microsoft, Bloomberg, and EleutherAI for using their books to train their large language models without their knowledge or consent.\nThe lawsuit centres on the Books3 AI training dataset of 180,000 works, which as part of EleutherAI's larger dataset The Pile, has been used to train multiple large language models. Books3 was taken offline in August 2023 following a complaint about copyright abuse by Danish anti-piracy group Rights Alliance.\nHuckabee's suit argues that Meta, Microsoft and Bloomberg 'were able to incorporate sophisticated datasets, which included the pirated copyright-protected materials in Books3, as part of the LLM\u2019s training process, without having to compensate the authors.' \nThe suit is the latest in a series of copyright suits leveled against large language model and generative AI developers. \nSystem \ud83e\udd16\nBooks3 dataset\nOperator: Bloomberg; EleutherAI; Meta; Microsoft\nDeveloper: Bloomberg; EleutherAI; Meta; Microsoft\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Train language models\nTechnology: \nIssue: Copyright\nTransparency: Governance; Marketing\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nHuckabee et al v. Meta Platforms, Inc. et al\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reuters.com/legal/litigation/authors-sue-meta-microsoft-bloomberg-latest-ai-copyright-clash-2023-10-18/\nhttps://www.theverge.com/2023/10/19/23924246/mike-huckabee-books-lawsuit-generative-ai-microsoft-meta-bloomberg-eleutherai-books3\nhttps://news.bloomberglaw.com/ip-law/mike-huckabee-sues-meta-microsoft-bloomberg-over-ai-copyrights\nhttps://thehill.com/policy/technology/4264815-mike-huckabee-religious-authors-tech-companies-books-ai-training/\nhttps://www.yahoo.com/entertainment/mike-huckabee-joins-suit-claims-144223208.html\nRelated \ud83c\udf10\nGetty Images sues Stability AI for copyright abuse\nAnthropic sued for using copyrighted songs to train models\nPage info\nType: Incident\nPublished: October 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/nyc-mayor-eric-adams-robocalls-residents-using-audio-deepfakes", "content": "NYC mayor Eric Adams robocalls residents with audio deepfakes\nOccurred: October 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe Mayor of New York City called local residents using audio deepfakes to communicate in languages he doesn't speak.\n\nThe Office of the Mayor used ElevenLabs AI voice generator to generate Eric Adams' voice speaking Spanish (example), Yiddish, Mandarin, and Cantonese in order 'to reach more people' when he was promoting local events such as recruitment fairs and concerts. \n\nAccording to the mayor, some people he had 'spoken to' were 'excited' to hear their mayor in their own language. 'We are becoming more welcoming by using technology to speak a multitude of languages,' he said. Over four million people had reputedly been reached by the calls.\n\nCampaigners criticised the mayor's calls as 'unethical' and 'deeply irresponsible' and accused him of deliberately misleading people into thinking he could speak multiple languages when he only spoke English, and not informing them that his voice had been manipulated. 'Using AI to convince New Yorkers that he speaks languages that he doesn't is deeply Orwellian,' the Surveillance Technology Oversight Project's Albert Cahn said.\n\nThe incident also prompted concerns that the use of deepfakes by politicians is likely to erode trust in public figures, government, and public information.\nSystem \ud83e\udd16\nElevenLabs AI voice generator\nDocuments \ud83d\udcc3\nMayor Eric Adams Makes Technology-Related Announcement\nOperator: Office of the Mayor of New York City\nDeveloper: ElevenLabs\nCountry: USA\nSector: Politics\nPurpose: Communicate with citizens\nTechnology: Deepfake - audio; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: \nTransparency: Marketing\nResearch, advocacy \ud83e\uddee\nS.T.O.P. Condemns Eric Adams\u2019 Deepfake Robocalls, Calls \u2018Responsible AI\u2019 Plan \u2018Deeply Irresponsible\u2019\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://apnews.com/article/nyc-mayor-ai-robocalls-foreign-languages-30517885466994e5f1f54745c08691e0\nhttps://www.vice.com/en/article/xgw78a/nyc-mayor-casually-announces-hes-deepfaking-himself-experts-horrified\nhttps://news.sky.com/story/new-yorks-mayor-uses-audio-deepfakes-to-call-residents-in-languages-he-doesnt-speak-12986816\nhttps://fortune.com/2023/10/17/new-york-city-mnayor-eric-adams-uses-ai-to-speak-mandarin/\nhttps://nypost.com/2023/10/16/adams-uses-ai-to-pass-himself-off-to-new-yorkers-as-multilingual/\nhttps://www.npr.org/2023/10/21/1207749595/opinion-did-he-really-say-that\nhttps://www.theverge.com/2023/10/17/23920733/nyc-mayor-eric-adams-ai-robocalls-spanish-mandarin\nhttps://www.nytimes.com/2023/10/20/nyregion/ai-robocalls-eric-adams.html\nRelated \ud83c\udf10\nElevenLabs AI voice simulator\nHour One 'character' clones\nPage info\nType: Incident\nPublished: October 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/video-game-voice-actors-attacked-using-their-own-ai-voices", "content": "Video game voice actors attacked using their own AI voices \nOccurred: February 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nVoice actors working in the video gaming industry were attacked using AI-generated versions of their own voices, and doxxed by having their home addresses read out using their synthesised voice and then posted online.\nAccording to a report by Vice, some of the audio clips may have been generated using ElevenLabs' Prime Voice AI (since renamed 'ElevenLabs Text-to-Speech') text-to-voice generator. However, this claim was rejected by ElevenLabs on the basis that every request using its system is tracked.\nA few weeks earlier, 4chan members were discovered to be using ElevenLabs' voice generator to make celebrity voices read highly offensive messages. \nIn a similar vein, GamesRadar reported in July 2023 that deepfake versions of video game voice actors including April Stewart and Ryan Laughton were being used to create non-consensual pornography for mods for games. Many video game companies and mod communities such as Nexus Mods have decided to allow AI-generated mod content.\nSystem \ud83e\udd16\nElevenLabs AI voice generator\nOperator:  \nDeveloper: ElevenLabs\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Attack voice actors\nTechnology: Text-to-speech; Deep learning; Machine learning\nIssue: Safety\nTransparency: Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.vice.com/en/article/93axnd/voice-actors-doxed-with-ai-voices-on-twitter\nhttps://gizmodo.com/ai-voice-actor-uber-duck-elevenlabs-1850109149\nhttps://www.giantfreakinrobot.com/tech/ai-voice-actors-rob-work.html\nhttps://kotaku.com/ai-voice-actors-steve-blum-elevenlabs-contracts-sell-1850108292\nhttps://www.gamesradar.com/voice-actors-call-out-ai-as-imitations-spread-in-erotic-skyrim-mods-i-do-not-give-consent/\nhttps://www.nme.com/en_au/news/gaming-news/video-game-voice-actors-denounce-nsfw-mods-ai-deepfakes-voices-3466611\nRelated \ud83c\udf10\nElevenLabs AI voice generator\nNYC mayor robocalls residents with audio deepfakes\nPage info\nType: Incident\nPublished: October 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/elevenlabs-voice-generator-makes-celebrity-voices-read-offensive-messages", "content": "ElevenLabs voice generator makes celebrity voices read offensive messages\nOccurred: January 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn AI voice generator was criticised for its lack of safeguards and the ease with which it can be made to generate racist, transphobic, homophobic, anti-semitic, violent, and offensive audio content in other people's names.\n4chan and Reddit members were found to be using ElevenLab's Prime Voice AI (since renamed ElevenLabs TTS) to make deepfake voices of Emma Watson (reading out Adolf Hitler's Mein Kampf) and Joe Rogan, amongst others, spewing vile rhetoric, including at US House representative Alexandria Ocasio-Cortez.\nElevenLabs responded by saying it had seen 'an increasing number of voice cloning misuse cases' and asked for 'thoughts and feedback'. The following day, the company announced it would insist on verifiying user identities, introduce paid subscriptions, and ban accounts it found to be misusing its technology.\nUnder a section on Ethical AI on its website, the company says that it is 'fully committed both to respecting intellectual property rights and to implementing safeguards against potential misuse of our technology.'\nSystem \ud83e\udd16\nElevenLabs AI voice generator\nDocuments \ud83d\udcc3\nElevenLabs. This Voice Doesn't Exist - Generative Voice AI\nOperator:  \nDeveloper: ElevenLabs\nCountry: USA; UK\nSector: Media/entertainment/sports/arts\nPurpose: Mimic celebrities\nTechnology: Text-to-speech; Deep learning; Machine learning\nIssue: Safety\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.engadget.com/ai-voice-tool-deepfake-celebrity-audio-clips-094648743.html\nhttps://news.sky.com/story/extra-safeguards-coming-after-ai-generator-used-to-make-celebrity-voices-read-offensive-messages-12799703\nhttps://gizmodo.com/ai-joe-rogan-4chan-deepfake-elevenlabs-1850050482\nhttps://www.vice.com/en/article/dy7mww/ai-voice-firm-4chan-celebrity-voices-emma-watson-joe-rogan-elevenlabs\nhttps://futurism.com/startup-4chan-voice-cloning-ai\nhttps://uk.pcmag.com/news/145199/people-are-still-terrible-ai-voice-cloning-tool-misused-for-deepfake-celeb-clips\nhttps://www.gizmochina.com/2023/01/22/steve-jobs-chatgpt-video/\nhttps://cyfrowa.rp.pl/biznes-ludzie-startupy/art37860951-polacy-dadza-botom-dowolny-glos-to-zmieni-film-i-reklame-ale-niesie-zagrozenia\nhttps://tierragamer.com/noticias/tecnologia/compania-que-hace-voces-con-ia-tomara-acciones-ante-usuarios-que-usan-su-tecnologia-para-racismo-y-misoginia/\nRelated \ud83c\udf10\nElevenLabs AI voice simulator\nHour One AI voice 'character' clones\nPage info\nType: Incident\nPublished: October 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/snapchat-my-ai-requests-to-meet-13-year-old-girl-in-park", "content": "Snapchat My AI requests to meet 13-year-old girl in park\nOccurred: September 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nSnapchat's GPT-4-powered My AI feature agreed to meet a 13-year-old girl in a park in Melbourne, Australia, raising questions about the safety and reliability of the system.\n\nPosing as a 25-year-old man, the service suggested they meet at a named park one kilometer from where the girl lived, despite her phone's location services being switched off, underscoring existing concerns about its owner's Snap Inc's approach to privacy.\n\nSnapchat shrugged off the incident, saying, 'As with all AI-powered chatbots, My AI is always learning and can occasionally produce incorrect responses. We want to create a positive and age appropriate experience for all our users and are continually making updates to help My AI give more accurate responses.'\nSystem \ud83e\udd16\nSnapchat My AI chatbot\nIncident databank \ud83d\udd22\nOperator: Olinda Luketic\nDeveloper: Snap Inc; OpenAI\nCountry: Australia\nSector: Media/entertainment/sports/arts  \nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Accuracy/reliability; Safety; Privacy\nTransparency: Governance \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.dailymail.co.uk/news/article-12470375/Snapchat-AI-bot-slammed-Melbourne-mother.html\nhttps://au.news.yahoo.com/mums-alarming-snapchat-claim-daughters-creepy-encounter-232817774.html\nhttps://www.mirror.co.uk/news/world-news/snapchat-ai-bot-asked-13-30840340\nhttps://nypost.com/2023/09/02/mom-says-snapchat-ai-bot-tried-to-entrap-teen-daughter/\nhttps://7news.com.au/technology/snapchat/mothers-warning-after-snapchats-creepy-ai-bot-asks-daughter-to-meet-up-c-11768791\nhttps://www.skynews.com.au/australia-news/snapchats-controversial-my-ai-chatbot-under-fire-after-mum-reveals-it-asked-her-13yearold-daughter-to-meet-at-local-park/news-story/f91cda8ac03c13846e354078e42d6c16\nRelated \ud83c\udf10\nSnapChat AI gives sex advice to 13-year-old\nSnapchat UK fails to assess My AI privacy risks\nPage info\nType: Incident\nPublished: October 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/study-finds-proctorio-fails-to-detect-student-cheats", "content": "Study finds Proctorio fails to detect student cheats\nOccurred: 2021-22\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAnti-cheating software system Proctorio failed to detect any cheaters when tested in a controlled environment, according to researchers at the University of Twente in the Netherlands.\n\nAsked to detect the six of 30 student computer science volunteers who had been instructed to cheat on a first-year exam supervised by Proctorio, some of whom had used virtual machines and audio calls - both known vulnerabilities to Proctorio\u2019s system - the system had failed to catch a single cheater.\n\nThe researchers concluded that the system is 'best compared to taking a placebo: it has some positive influence, not because it works but because people believe that it works, or that it might work.'\n\nThe findings raise questions about Proctorio's effectiveness and value, and about the accuracy of its marketing claims.  \nSystem \ud83e\udd16\nProctorio website\nOperator: University of Twente\nDeveloper: Proctorio\nCountry: Netherlands\nSector: Education\nPurpose: Detect exam cheating\nTechnology: Facial detection; Machine learning\nIssue: Effectiveness/value\nTransparency: Governance; Marketing\nResearch, advocacy \ud83e\uddee\nUniversity of Twente. On the Efficacy of Online Proctoring using Proctorio (pdf)\nNews, commentary, analysis \ud83d\udcf0\nhttps://www.vice.com/en/article/93aqg7/scientists-asked-students-to-try-to-fool-anti-cheating-software-they-did\nhttps://mindmatters.ai/2022/09/study-ai-fails-to-catch-cheaters-on-an-exam/\nhttps://nowthisnews.com/news/could-this-change-the-future-of-online-testing-scientists-hack-anti-cheating-software\nRelated \ud83d\uddde\ufe0f\nUBC academic, students accuse Proctorio of privacy abuse\nProctorio fails to recognise Vrije Universiteit Black student\nPage info\nType: Incident\nPublished: October 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/instagram-inserts-terrorist-into-palestinians-biography-translations", "content": "Instagram inserts \u2018terrorist\u2019 into Palestinians' biography translations\nOccurred: October 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nInstagram's automated translation system updated the biographies of some people calling themselves 'Palestinian' and comtaining an Arabic phrase meaning 'Praise be to God' to say 'Palestinian terrorists are fighting for their freedom.'\nHaving written in his biography that he was Palestinian, followed by a Palestinian flag and the word \"alhamdulillah\" in Arabic (which translates to 'Praise be to God' in English), Instagram user @khanman1996 showed that Instagram's 'see translation' function provided an English translation reading: 'Praise be to God, Palestinian terrorists are fighting for their freedom'.\nThe incident raised questions about the reliability of Meta's translation tool. It also prompted users and commentators to highlight concerns that Meta may have been suppressing content voicing support for Palestinians during the Israel-Gaza conflict. \nMeta said it fixed a problem 'that briefly caused inappropriate Arabic translations' and apologised, telling the BBC, 'We sincerely apologise that this happened.' \nSystem \ud83e\udd16\nInstagram website\nInstagram Wikipedia profile\nInstagram. How Instagram uses artificial intelligence to moderate content\nInstagram. Community Guidelines\nMeta. How Meta enforces policies\nOperator:\nDeveloper: Meta/Instagram\nCountry: Israel; Palestine\nSector: Politics\nPurpose: Moderate content\nTechnology: Machine learning\nIssue: Mis-representation\nTransparency: Governance\nNews, commentary, analysis \ud83d\udcf0\nhttps://www.bbc.co.uk/news/technology-67169228\nhttps://www.404media.co/instagram-palestinian-arabic-bio-translation/\nhttps://wonderfulengineering.com/meta-says-instagram-is-sorry-for-translating-palestinian-profiles-to-terrorists/\nhttps://www.theguardian.com/technology/2023/oct/20/instagram-palestinian-user-profile-bios-terrorist-added-translation-meta-apology\nhttps://www.scmp.com/news/world/middle-east/article/3238713/meta-apologises-after-auto-translate-added-terrorist-biographies-palestine-supporters-instagram\nhttps://futurism.com/instagram-palestinian-terrorist-bio\nhttps://www.businessinsider.com/instagram-apologize-auto-translate-palestinian-terrorists-fighting-freedom-bios-meta-2023-10\nhttps://news.sky.com/story/instagram-apologises-after-inserting-terrorist-into-some-palestinian-profiles-12988227\nRelated \ud83c\udf10\nInstagram 'aware of' teen girls' mental health harms\nIsrael AI robot machine guns fire tear gas at Palestinian protestors\nPage info\nType: Incident\nPublished: October 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/guillermo-ibarrolla-facial-recognition-wrongful-arrest", "content": "Guillermo Ibarrolla facial recognition wrongful arrest\nOccurred: October 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn Argentinian man was arrested in Buenos Aires on the basis that he had been identified by a facial recognition system for a crime he had not committed.\n\nArrested in Buenos Aires when he was heading home on the subway for an armed robbery that had occurred in July 2019 in a city 400 miles away, Guillermo Ibarrolla spent nearly a week in custody \u2018in brutal conditions\u2019 before being informed that he had been wrongly identified due to a data input error involving a fugitive who shared the same name. \nThe incident highlighted flaws in the system's accuracy, and concerns about privacy. Buenos Aires' Fugitive Facial Recognition System or Sistema de Reconocimiento Facial de Pr\u00f3fugos (SNRP) system was deactivated in March 2020 after it had been found to have caused 140 wrongful arrests or police checks.\nSystem \ud83e\udd16\nBuenos Aires government (2022). Rodr\u00edguez Larreta present\u00f3 el Sistema de Reconocimiento Facial De Pr\u00f3fugos: \u201cEl objetivo es que los vecinos est\u00e9n m\u00e1s seguros\u201d\nOperator: Government of the City of Buenos Aires; Buenos Aires City Police; Argentine Ministry of Justice and Security; ReNaPer\nDeveloper: NtechLab; Danaide  \nCountry: Argentina\nSector: Govt - municipal; Govt - police\nPurpose: Identify/track criminals\nTechnology: Facial recognition\nIssue: Privacy; Surveillance\nTransparency: Governance; Complaints/appeals\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://mindmatters.ai/2023/09/when-video-surveillance-gets-it-wrong/\nhttps://www.wired.com/story/buenos-aires-facial-recognition-scandal/https://es.wired.com/articulos/escandalo-en-buenos-aires-revela-los-peligros-del-reconocimiento-facial\nhttps://www.biometricupdate.com/202309/problems-in-argentinas-justice-system-exacerbated-by-public-facial-recognition-report\nhttps://www.infobae.com/sociedad/policiales/2019/08/02/un-hombre-estuvo-seis-dias-preso-por-un-error-del-sistema-de-reconocimiento-facial/\nhttps://www.pagina12.com.ar/209910-seis-dias-arrestado-por-un-error-del-sistema-de-reconocimien\nhttps://ichi.pro/es/la-policia-argentina-arresta-a-personas-inocentes-basandose-en-el-reconocimiento-facial-90166915974477\nRelated \ud83d\uddde\ufe0f\nBuenos Aires Sistema de Reconocimiento Facial de Pr\u00f3fugos\nMicrosoft teen pregnancy predictions\nPage info\nType: Incident\nPublished: October 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/deepfake-audio-recording-depicts-british-opposition-leader-abusing-staff", "content": "Deepfake recordings depict British Opposition Leader abusing staff\nOccurred: October 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA deepfake audio recording appearing to depict the leader of the UK Labour Party, Sir Keir Starmer, swearing at a party staffer raised concerns about the underhand use of synthetic disinformation in politics.\n\nThe audio clip was posted on X, formerly Twitter, during the first day of the Labour Party Conference and was quickly identified as a hoax by MPs from across the political spectrum. \n\nThough X's policies explicitly state that users 'may not share synthetic, manipulated, or out-of-context media,' it did not prevent the clip being widely circulated. However, most copies have since been removed.\nThe incident intensified apprehensions about the risk of synthetic media in politics, and came hot on the heels of another incident involving a deepfake audio during the Slovakian election targeting their Opposition Leader.\nSystem \ud83e\udd16\nUnknown\nX Synthetic media policy\nDeepfake audio recording\nOperator:\nDeveloper:\nCountry: United Kingdom\nSector: Politics\nPurpose: Damage reputation\nTechnology: Deepfake - audio, video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Mis/disinformation\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://news.sky.com/story/labour-faces-political-attack-after-deepfake-audio-is-posted-of-sir-keir-starmer-12980181\nhttps://www.politico.eu/article/uk-keir-starmer-labour-party-deepfake-ai-politics-elections/ \nhttps://www.standard.co.uk/news/politics/deepfake-video-uk-politics-keir-starmer-labour-conference-liverpool-b1112230.html \nhttps://www.independent.co.uk/tech/keir-starmer-deepfake-ai-elections-b2426578.html\nRelated \ud83c\udf10\nSouth Korea presidential election candidate deepfakes\nDeepfake audio recording claims opposition leaders tried to rig Slovakian election\nPage info\nType: Incident\nPublished: October 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/anthropic-sued-for-using-copyrighted-songs-to-train-models", "content": "Anthropic sued for using copyrighted songs to train models\nOccurred: 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nUniversal Music Group (UMG) and several other music publishers have sued AI company Anthropic for distributing copyrighted lyrics without permission with its AI model Claude 2, and for using the lyrics to train its language models.\n\nThe music publishers\u2019 complaint (pdf) claims that Claude 2 can be prompted to distribute almost identical lyrics to songs such as Katy Perry\u2019s 'Roar,' Gloria Gaynor\u2019s 'I Will Survive,' and the Rolling Stones\u2019 'You Can\u2019t Always Get What You Want.' \n\nThe complaint also says Claude 2\u2019s results use phrases very similar to existing lyrics, even when not asked to recreate songs. \n\n'Anthropic\u2019s copyright infringement is not innovation; in layman\u2019s terms, it\u2019s theft,' according to UMG.\nSystem \ud83e\udd16\nAnthropic website\nAnthropic Wikipedia profile\nOperator:  \nDeveloper: Anthropic\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Copyright\nTransparency: Governance; Complaints/appeals\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nMUSIC \u2013 Z SONGS; and ABKCO MUSIC, INC., v Anthropic PBC (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theverge.com/2023/10/19/23924100/universal-music-sue-anthropic-lyrics-copyright-katy-perry\nhttps://qz.com/universal-music-group-sues-anthropic-ai-katy-perry-1850940686\nhttps://www.techtarget.com/searchenterpriseai/news/366556341/Fair-use-may-not-help-Anthropic-against-AI-music-lawsuit\nhttps://decrypt.co/202314/universal-music-group-umg-anthropic-ai-copyright-lawsuit\nhttps://www.theguardian.com/technology/2023/oct/19/music-lawsuit-ai-song-lyrics-anthropic\nhttps://www.reuters.com/legal/music-publishers-sue-ai-company-anthropic-over-song-lyrics-2023-10-18/\nhttps://www.ft.com/content/0965d962-5c54-4fdc-aef8-18e4ef3b9df5\nRelated \ud83c\udf10\nXiaohongshu AI image generator copyright abuse\nDrake, The Weeknd AI voice cloning\nPage info\nType: Incident\nPublished: October 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/proctorio-fails-to-recognise-black-student", "content": "Proctorio fails to recognise Vrije Universiteit Black student \nOccurred: 2022-23\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nTechnology company Proctorio was accused of racial discrimination after its exam cheating system failed to recognise Robin Pocornie, a Black student at Vrije Universiteit Amsterdam, when she was taking exams during the COVID-19 pandemic.\n\nPocornie noticed the software often appeared to struggle to properly 'process' her face, resulting in her resorting to sitting through her exams with a bright light shining on her face. In 2022, Pocornie filed a complaint (pdf) with the Netherlands Institute for Human Rights, which concluded she had presented sufficient facts for a presumption of discrimination, and that the university had to prove otherwise.\n\nIn October 2023, the Institute ruled that Vrije Universiteit demonstrated that Pocornie did not experience more log-in issues than other students, and that her unstable internet connection or the fact that she was wearing glasses were to blame. It also said that 'it is entirely possible that the use of Proctorio or similar AI software could indeed lead to discrimination in a different situation.'\n\nThe incident raised questions about the accuracy of the system, and led to a legal challenge. Proctorio had earlier responded by saying an audit of its facial detection model by US-based consultancy BABL AI found 'no significant bias toward anyone'. \n\nThe company has not made the audit results public.\nOperator: Vrije Universiteit\nDeveloper: Proctorio\nCountry: Netherlands\nSector: Education\nPurpose: Detect exam cheating\nTechnology: Facial detection; Gaze detection; Machine learning\nIssue: Bias/discrimination - race\nTransparency: Governance; Marketing\nSystem \ud83e\udd16\nProctorio website\nProctorio (2023). Response to RTL Nieuws\nInvestigations, assessments, audits \ud83e\uddd0\nRTL Nederland (2023). Verantwoording onderzoek RTL Nieuws naar Procotrio-software (pdf)\nResearch, advocacy \ud83e\uddee\nEDRi (2023). The secret services\u2019 reign of confusion, rogue mayors, racist tech and algorithm oversight (or not)\nRacism and Technology Center (2022). Dutch Institute for Human Rights: Use of anti-cheating software can be algorithmic discrimination (i.e. racist)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.wired.co.uk/article/student-exam-software-bias-proctorio\nhttps://nltimes.nl/2022/07/15/webcam-exam-software-discriminatory-doesnt-recognize-darker-skin-tones-says-student\nhttps://www.volkskrant.nl/nieuws-achtergrond/de-antispieksoftware-herkende-haar-niet-als-mens-omdat-ze-zwart-is-maar-bij-de-vu-vond-ze-geen-gehoor~b6810279/\nhttps://www.rtlnieuws.nl/nieuws/nederland/artikel/5408489/zwarte-mensen-vaker-niet-herkend-door-antispieksoftware-proctorio\nhttps://www.scienceguide.nl/2022/10/vu-had-moeten-verzekeren-dat-proctoringsoftware-niet-zou-discrimineren/\nhttps://www.parool.nl/amsterdam/vu-student-stapt-naar-college-voor-rechten-van-de-mens-spieksoftware-herkent-zwart-gezicht-niet~b301ccef/\nhttps://www.utoday.nl/news/71735/student-vu-dient-klacht-in-over-racistische-antispieksoftware\nhttps://www.lilithmag.nl/blog/robin-pocornie-vu-antispieksoftware\nhttps://www.erasmusmagazine.nl/2022/12/13/antispieksoftware-kan-best-racistisch-zijn-meent-mensenrechtencollege/\nhttps://nltimes.nl/2023/10/17/anti-cheating-software-biased-vu-students-skin-color-ruling\nRelated \ud83c\udf10\nProctorio uses 'racist' algorithms to detect students' faces\nUBC academics, students accuse Proctorio of privacy abuse\nPage info\nType: Incident\nPublished: October 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/miami-university-student-accuses-proctorio-of-privacy-abuse-discrimination", "content": "Miami University student accuses Proctorio of privacy abuse, bias\nOccurred: 2020-23\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nMiami University student Erik Johnson accused remote exam cheating detection company Proctorio of providing invasive and inequitable software, raising questions about the system and company leadership, and prompting a legal dispute.\n\nIn a series of tweets, computer engineering student Johnson criticised Proctorio as invasive, 'inherently ableist and discriminatory', and posted an analysis of the software\u2019s code on Pastebin. \n\nIn response, Proctorio CEO Mike Olsen demanded that three of Johnson\u2019s tweets were removed by Twitter under a copyright takedown notice and blocked his IP address so he could no longer use the software to take his exams.\n\nIn April 2021, the Electronic Frontier Foundation (EFF) sued Proctorio for trying to silence critics through the misapplication of copyright law. The two parties settled out of court a year later.\nSystem \ud83e\udd16\nProctorio website\nProctorio (2022). Legal update\nOperator: Miami University\nDeveloper: Proctorio\nCountry: USA\nSector: Education\nPurpose: Detect exam cheating\nTechnology: Facial detection; Gaze detection; Machine learning\nIssue: Bias/discrimination - race; Freedom of expression; Privacy\nTransparency: Governance; Black box; Complaints/appeals; Marketing; Legal\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nJohnson v Proctorio legal dockets\nResearch, advocacy \ud83e\uddee\nErik Johnson (2022). End Proctorio Use At Miami University\nElectronic Frontier Foundation (2022). EFF Client Erik Johnson and Proctorio Settle Lawsuit Over Bogus DMCA Claims\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.vice.com/en/article/n7wxvd/students-are-rebelling-against-eye-tracking-exam-surveillance-tools\nhttps://www.reuters.com/article/ip-proctorio-copyrights-idUSL1N2MF3CJ\nhttps://www.theguardian.com/us-news/2022/aug/26/anti-cheating-technology-students-tests-proctorio\nhttps://www.theverge.com/2021/4/22/22397499/proctorio-lawsuit-electronic-frontier-foundation-test-proctoring-software\nhttps://www.insidehighered.com/quicktakes/2022/03/28/student-agrees-drop-lawsuit-against-proctorio\nhttps://techcrunch.com/2020/11/05/proctorio-dmca-copyright-critical-tweets/\nhttps://www.vox.com/recode/22175021/school-cheating-student-privacy-remote-learning\nhttps://www.techdirt.com/2021/04/23/eff-college-student-sue-proctorio-over-dmcas-fair-use-critique-tweets-software/\nhttps://www.theregister.com/2021/04/22/eff_proctorio_lawsuit/\nRelated \ud83c\udf10\nProctorio fails to recognise Vrije Universiteit Black student\nProctorio found to use 'racist' algorithms to detect students' faces\nPage info\nType: Incident\nPublished: October 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ucsb-proctoru-data-sharing", "content": "ProctorU accused of sharing UCSB student data\nOccurred: March 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA warning by University of California Santa Barbara (UCSB) faculty association members raising concerns about the potential sharing of student data with third parties resulted in a backlash against online proctoring service ProctorU.\nAccording to a March 2020 letter (pdf) to the university's administration, ProctorU 'regularly collects and distributes' a wide range of student information such as social security numbers, browsing history, gender identity, medical conditions, fingerprints, faceprints, voiceprints, retina scans and more.\nThe faculty went on to say ProctorU's data privacy practices 'implicates the university into becoming a surveillance tool' and to recommend UCSB terminate its contract with ProctorU and discourage professors from using similar services. ProctorU's (now renamed Meazure Learning) privacy policy says personal data collected may be disclosed to third parties for undefined 'business and commercial purposes'.\nIn response, ProctorU attorney David Lance Lucas threatened (pdf) to sue the faculty association for defamation and violating copyright law, and accused it of 'directly impacting efforts to mitigate civil disruption across the United States' by interfering with education during a national emergency. \nLucas' threat was condemned as inaccurate and unreasonable bullying by senior lawyers and others. According to Vice, ProctorU never filed a lawsuit against the UCSB faculty association. But the threat 'had a chilling effect on professors\u2019 willingness to discuss the software.'\nThe incident highlighted concerns about ProctorU's data sharing practices, and its heavy-handed use of legal threats to shut down legimate discussion about its products.\nSystem \ud83e\udd16\nProctorU website\nOperator: University of California (UCSB)\nDeveloper: Meazure Learning/ProctorU\nCountry: USA\nSector: Education\nPurpose: Detect and prevent cheating\nTechnology: Facial recognition; Fingerprint recognition; Voice recognition\nIssue: Privacy\nTransparency: Legal\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nEPIC (2020). Complaint and Request for Investigation, Injunction, and Other Relief (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttp://dailynexus.com/2020-03-16/ucsb-faculty-association-issues-letter-advising-against-the-use-of-proctoru-testing-services/\nhttps://www.vice.com/en/article/n7wxvd/students-are-rebelling-against-eye-tracking-exam-surveillance-tools\nhttps://www.washingtonpost.com/technology/2020/04/01/online-proctoring-college-exams-coronavirus/\nhttps://www.forbes.com/sites/seanlawson/2020/04/24/are-schools-forcing-students-to-install-spyware-that-invades-their-privacy-as-a-result-of-the-coronavirus-lockdown/\nhttps://www.thefire.org/news/proctoru-threatens-uc-santa-barbara-faculty-over-criticism-during-coronavirus-crisis\nhttps://pubcit.typepad.com/clpblog/2020/03/can-proctoru-be-trusted-with-students-personal-data.html\nhttps://academeblog.org/2020/03/27/an-egregious-case-of-legal-bullying/\nhttps://www.reddit.com/r/UCSC/comments/kwzihs/ban_the_use_of_proctoru_an_invasive_and/\nRelated \ud83c\udf10\nProctorio 'racist' facial detection\nUniversity of Wisconsin 'racist' online proctoring\nPage info\nType: Incident\nPublished: January 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ubc-academic-students-accuse-proctorio-of-privacy-abuse", "content": "UBC academic, students accuse Proctorio of privacy abuse\nOccurred: 2020-23\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nProctorio was accused by University of British Colombia (UBC) students, staff, and faculty of providing an 'unethical, invasive' online exam invigilation system, and for causing anxiety and other mental harms, and negatively impacting students\u2019 academic performance.\n\nThe complaints were initially triggered by a UBC student claiming that Proctorio had failed to provide support when encountering an issue the system, to which Proctorio CEO Mike Olsen posted excerpts of a support chat log to Twitter, resulting in allegations of privacy abuse. Olsen later apologised for his actions. \n\nSubsequently, UBC employee Ian Linkletter criticised the software on Twitter, linking to unlisted videos on the company's YouTube Channel showing how its technology worked and accusing the company of jeopardising privacy, increasing student anxiety, discriminating against students of colour and others, and inadequate transparency. \n\nDespite swiftly removing the videos, Proctorio sued Linkletter for infringing its copyright and distributing confidential material, triggering accusations that it was acting disproportionately, and inappropriately limiting academic freedoms. \n\nIn May 2023, the British Colombia Court of Appeal ruled in Proctorio's favour on confidentiality and copyright grounds, despite a court earlier ruling for Linkletter on freedom of expression grounds.\nSystem \ud83e\udd16\nProctorio website\nOperator: University of British Columbia\nDeveloper: Proctorio\nCountry: Canada\nSector: Education\nPurpose: Detect exam cheating\nTechnology: Facial detection; Gaze detection; Machine learning\nIssue: Bias/discrimination - race; Confidentiality; Privacy; Freedom of expression; Freedom of information\nTransparency: Governance; Black box; Complaints/appeals; Marketing; Legal\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nLinkletter v Proctorio (pdf) \nProctorio v Linkletter - Judgement\nProctorio v Linkletter - Affidavit\nResearch, advocacy \ud83e\uddee\nIan Linkletter. Stand against Proctorio's SLAPP\nUBS Students Resist (2020). UBC Students Reject the Use of Proctorio's Invasive & Unethical Technology\nUBC students, staff, and faculty (2020). Protect Student Privacy: A Renewed Call to Action Against Proctorio\nUBC students, staff, and faculty (2020). Open Letter Regarding the Usage of Proctorio\nFight for the Future. Proctorio and its CEO are worse than a Proctology exam\nElectronic Frontier Foundation (2021). Student Surveillance Vendor Proctorio Files SLAPP Lawsuit to Silence A Critic\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theguardian.com/us-news/2022/aug/26/anti-cheating-technology-students-tests-proctorio\nhttps://www.vice.com/en/article/epxqgw/proctorio-is-going-after-digital-rights-groups-that-bash-their-proctoring-software\nhttps://www.insidehighered.com/quicktakes/2022/01/31/proctorio-loses-appeal-lawsuit-against-academic-critic\nhttps://www.edsurge.com/news/2022-02-03-a-proctoring-company-tried-to-sue-an-edtech-critic-he-s-fighting-back-in-court\nhttps://www.theverge.com/2020/10/22/21526792/proctorio-online-test-proctoring-lawsuit-universities-students-coronavirus\nRelated \ud83c\udf10\nProctorio found to use 'racist' algorithms to detect students' faces\nUniversity of Wisconsin/Honorlock accused of 'racist' online proctoring\nPage info\nType: Incident\nPublished: October 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-or-not-misidentifies-hamas-baby-victim-as-deepfake", "content": "AI or Not misidentifies Hamas baby victim image as deepfake\nOccurred: October 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAI image detector AI or Not has come under fire for failing to identify a photograph of a baby apparently burned alive by Hamas, raising questions about the tool's accuracy and marketing claims, and fueling doubts about Israel's communications integrity.\n\nCalifornia-based Optic's AI or Not tool identified a photograph of what Israel said was a burnt corpse of a baby killed in Hamas\u2019s 2023 attack on Israel as being generated by AI. Its results were tweeted as genuine by US Jewish conservative pundit Ben Shapiro to his hundreds of thousands of followers, prompting people to suggest the Israeli government had deliberately been spreading disinformation. \n\nOptic claims '95% accuracy' on its website. But deepfake experts such as UC Berkeley professor Henry Farid said the 'Hamas' image showed no sign of being a deepfake. Earlier in 2023, a test carried out by Bellingcat concluded that AI or Not performed 'impressively well' on high-quality, large, and watermarked AI images, but was 'unimpressive' when it came to compressed AI images. \nSystem \ud83e\udd16\nAI or Not website\nOperator: Ben Shapiro\nDeveloper: Optic\nCountry: Israel; Palestine; USA\nSector: Govt - military; Politics\nTechnology: Machine learning\nIssue: Accuracy/reliability; Mis/disinformation\nTransparency: Governance; Marketing\nInvestigations, assessments, audits \ud83e\uddd0\nBellingcat (2023). https://www.bellingcat.com/resources/2023/09/11/testing-ai-or-not-how-well-does-an-ai-image-detector-do-its-job/\nNew York Times (2023). How Easy Is It to Fool A.I.-Detection Tools? \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.404media.co/email/cc4c9f18-f02a-4ff0-ba93-0d1e8dd81ed6/\nhttps://twitter.com/jacksonhinklle/status/1712892049056313604\nhttps://meaww.com/ben-shapiro-slammed-for-blaming-hamas-for-burnt-baby-in-posted-photo-as-internet-questions-authenticity\nhttps://technotrenz.com/news/ben-shapiro-under-fire-for-controversial-statement-on-hamas-and-photo-authenticity-sparking-online-debate-2796415.html\nhttps://www.alarabiya.net/social-media/2023/10/14/\nRelated \ud83c\udf10\nPresident Zelenskyy deepfake surrender\nDeepfake 'Pan Africanists' support Burkina Faso junta\nPage info\nType: Incident\nPublished: October 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ugandan-police-accused-of-using-facial-recognition-to-stifle-museveni-term", "content": "Ugandan police accused of using facial recognition to stifle Museveni term protests \nOccurred: November 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nUgandan police were accused of using a facial recognition system supplied by Chinese company Huawei to track, arrest, and torture protesters against President Yoweri Museveni\u2019s November 2020 decision to seek another term in office.\n Local politicians and rights advocates had flagged possible human rights and privacy violations when the system was acquired in 2019 as part of Huawei\u2019s Safe City programme. Since 2020, civil society groups have criticised governance institutions in Uganda for failing to protect the fundamental rights of citizens from surveillance in public places. \n\nThis comes amidst reports that Uganda is expanding its digital surveillance capabilities as part of a nationwide integrated surveillance system, including capabilities to check vehicle license plates and monitor social media.\nA Ugandan police spokesperson denied that the technology was used to monitor opposition figures but confirmed that a new surveillance system was in use.\n In transition or fragile democracies, this incident raises questions over how imported surveillance technologies are used to quash protests and suppress individual freedoms. Reports indicate other countries in Africa increasingly importing surveillance technologies, including Nigeria, Morocco, Zambia, and Ghana.\nSystem \ud83e\udd16\nHuawei \nOperator: Government of Uganda; Uganda Police Force\nDeveloper: Huawei\nCountry: Uganda\nSector: Govt - home/interior; police\nPurpose: Identify individuals\nTechnology: Facial analysis;  Safe city\nIssue: Dual/Multi-use; Governance; Human/civil rights; Privacy; Surveillance\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nUnwanted Witness (2022). PARLIAMENT ENDORSES UNREGULATED SURVEILLANCE AMIDST RISKS TO HUMAN RIGHTS (pdf)\nUnwanted Witness (2020). Surveillance, censorship threaten Internet freedom and Democracy in Uganda, says Unwanted Witness\nPrivacy International (2020). Huawei infiltration in Uganda\nInvestigations, assessments, audits \ud83e\uddd0\nQuartz (2020). Uganda is using Huawei\u2019s facial recognition tech to crack down on dissent after anti-government protests\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.biometricupdate.com/202011/ugandan-police-accused-of-using-facial-recognition-system-to-fuel-rights-abuses\nhttps://www.biometricupdate.com/201908/uganda-invests-126m-in-cctv-with-facial-biometrics-from-huawei\nhttps://sofrep.com/news/huawei-and-the-chinese-surveillance-network-in-africa/ \nhttps://www.wsj.com/articles/huawei-technicians-helped-african-governments-spy-on-political-opponents-11565793017\nhttps://www.ft.com/content/e20580de-c35f-11e9-a8e9-296ca66511c9\nRelated \ud83c\udf10\nIndia citizenship law protest surveillance\nNaypyidaw \u2018safe city\u2019 video surveillance\nPage info\nType: Incident\nPublished: October 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/university-of-miami-accused-of-using-facial-recognition-to-track-student-pr", "content": "University of Miami accused of using facial recognition to track student protestors \nOccurred: 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe University of Miami (UM) came under fire for allegedly using facial recognition to track students protesting against the university\u2019s opening up plan during the COVID-19 pandemic.\n University administrators denied that campus police used facial recognition software, saying that students were identified using ordinary footage. Students however claimed that a page on the UM website indicated campus police had introduced motion detection, facial recognition and object detection systems on-site.\n The case led to 20 human rights organisations submitting a letter to the UM Board of Trustees calling for a ban on facial recognition. While it is unclear whether facial recognition was used, the incident triggered concerns of human and digital rights organisations about the extent to which private universities in the US are using invasive facial recognition surveillance.\nSystem \ud83e\udd16\nUnknown\nOperator: University of Miami\nDeveloper:\nCountry: USA\nSector: Education\nPurpose: Identify individuals\nTechnology: Facial analysis; Motion Detection\nIssue: Privacy; Surveillance\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nFight for the Future (2020). 20+ human rights organizations call on University of Miami to ban facial recognition and meet student demands\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.wsj.com/articles/university-of-miami-becomes-latest-battleground-over-facial-recognition-11603233631\nhttps://www.wired.com/story/did-university-use-facial-recognition-id-student-protesters/ \nhttps://bigthink.com/the-present/facial-recognition-software/\nhttps://www.miaminewtimes.com/news/university-of-miami-tracked-protesters-with-video-surveillance-11712139\nhttps://www.chronicle.com/article/how-a-protest-at-the-u-of-miami-reignited-the-student-surveillance-debate \nhttps://www.forbes.com/sites/rachelsandler/2020/10/27/human-rights-groups-call-on-the-university-of-miami-to-ban-facial-recognition/\nRelated \ud83c\udf10\nUnconstrained College Students (UCCS) dataset\nReal World Masked Face dataset \nPage info\nType: Incident\nPublished: October 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/proctoring-software-companies-accused-of-providing-invasive-discriminatory", "content": "Proctoring software companies accused of providing invasive, discriminatory software\nOccurred: March 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nFive companies providing online test proctoring software and services have been accused of 'unfair and deceptive' business practices. \nA complaint by US non-profit Electronic Privacy Information Center (EPIC) with the District of Columbia Attorney General accused Respondus, ProctorU, Proctorio, Examity, and Honorlock of developing invasive software, discriminatiing against \u2018non-typical\u2019 students, and of the deceptive use of facial recognition. It cited a 'reliance on opaque, unproven AI analysis to flag purported instances of cheating.'\n The use of proctoring software using keystroke patterns, facial recognition, gaze-monitoring and recordings of students\u2019 surroundings to monitor students and flag suspected cheating in home tests was accelerated during the COVID-19 pandemic. These techniques raised concerns over mass biometric surveillance, biased outcomes and algorithmic transparency.\n EPIC\u2019s complaint set a precedent in challenging black-box algorithms in education. Since the complaint was filed in 2020, the Federal Trade Commission warned software companies against surveillance of students, and a federal legal ruling outlawed practices such as scanning students\u2019 rooms.\nSystem \ud83e\udd16\nRespondus\nProctorU\nProctorio\nExamity\nHonorlock\nOperator:\nDeveloper: Respondus, ProctorU, Proctorio, Examity, Honorlock\nCountry: USA\nSector: Education\nPurpose: Proctoring\nTechnology: Facial analysis; Facial recognition; Gaze recognition; Location recognition; Prediction algorithm\nIssue: Accuracy/reliability; Bias/discrimination; Privacy; Surveillance\nTransparency: Black box; Complaints/appeals\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nEPIC (2020). Complaint and Request for Investigation, Injunction, and Other Relief (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.vice.com/en/article/4adajq/privacy-group-asks-for-investigation-into-software-that-spies-on-students\nhttps://www.washingtonpost.com/politics/2020/12/10/technology-202-facebook-antitrust-lawsuits-will-test-government-ability-rein-silicon-valley/\nhttps://www.nytimes.com/2020/09/29/style/testing-schools-proctorio.html\nhttps://www.theverge.com/2020/12/9/22166023/epic-proctorio-examity-privacy-online-testing-school-lawsuit-proctoring\nhttps://udreview.com/youre-being-watched-the-dangers-of-proctoru/\nhttps://www.theverge.com/2020/4/29/21232777/examity-remote-test-proctoring-online-class-education\nhttps://www.eff.org/deeplinks/2020/08/proctoring-apps-subject-students-unnecessary-surveillance\nRelated \ud83c\udf10\nDartmouth College medical school remote exam cheating\nUniversity of Wisconsin Honolock 'racist' online proctoring \nPage info\nType: Issue\nPublished: October 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/deepfake-audio-recording-claims-opposition-leaders-tried-to-rig-slovakian-e", "content": "Deepfake audio recording claims opposition tried to rig Slovakia election\nOccurred: September 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA deepfake audio recording appearing to depict a pro-European politician and journalist discussing methods to rig Slovakia\u2019s 2023 election raised concerns about the use of AI in political elections.\n\nThe audio recording featured Michal \u0160ime\u010dka, leader of the liberal Progressive Slovakia party, and Denn\u00edk N journalist Monika T\u00f3dov\u00e1 apparently discussing how to manipulate the election. Posted during a 48-hour pre-election moratorium, it was quickly discovered to be a synthetic hoax by AFP and other fact-checking organisations. \n\nThe incident raised concerns about the use of underhand methods in Slovakia\u2019s elections, the use of deepfakes in politics, and the ease with which Meta's policies can be bypassed. Because the post was audio, it exploited a loophole in Meta\u2019s manipulated-media policy, which dictates only falsified videos go against its rules.\nProgressive Slovakia received 18 percent of the vote making it the second largest party in the country\u2019s parliament behind SMER, which had campaigned to withdraw military support for its neighbour, Ukraine. \nIt is unclear to what extent the deepfake influenced the result.\nSystem \ud83e\udd16\nUnknown\nOperator:\nDeveloper:\nCountry: Slovakia\nSector: Politics\nPurpose: Manipulate public opinion\nTechnology: Deepfake - audio; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Governance, Mis/disinformation\nTransparency: Governance\nFact check \ud83d\udea9\nAFP (2023). \u00dadajn\u00e1 nahr\u00e1vka telefon\u00e1tu predsedu PS a novin\u00e1rky Denn\u00edka N vykazuje pod\u013ea expertov po\u010detn\u00e9 zn\u00e1mky manipul\u00e1cie\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bloomberg.com/news/newsletters/2023-10-04/deepfakes-in-slovakia-preview-how-ai-will-change-the-face-of-elections\nhttps://news.bloomberglaw.com/artificial-intelligence/trolls-in-slovakian-election-tap-ai-deepfakes-to-spread-disinfo \nhttps://www.wired.co.uk/article/slovakia-election-deepfakes\nhttps://www.thetimes.co.uk/article/was-slovakia-election-the-first-swung-by-deepfakes-7t8dbfl9b\nRelated \ud83c\udf10\nSouth Korea presidential election candidate deepfakes\nDonald Trump hugs Dr Fauci deepfake\nPage info\nType: Incident\nPublished: October 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/niantic-uses-ai-artwork-to-promote-pokemon-go", "content": "Niantic uses AI artwork to promote Pokemon Go\nOccurred: August 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nPokemon Go owner Niantic appeared to have used AI-generated artwork to promote Adventures Abound, the twelfth season of the mobile game, prompting gamers to accuse the company of unnecessary and unethical profiteering.\nPokemon fans pointed to unnatural, blurred lineart and a lack of design cohesiveness as evidence that Niantic had used artificial intelligence to illustrate its website. Niantic is known to have a large roster of talent to draw on, and Pokemon Go has a reputation for high quality graphics.\nIn response, Niantic said it uses 'a variety of tools and software to create visual assets. We don\u2019t disclose specifics around our processes.' The company laid off 230 employees in June 2023 on the basis that 'expenses [grew] faster than revenue.'\nSystem \ud83e\udd16\nUnknown\nPokemon Go Adventures Abound website\nOperator: Niantic\nDeveloper: \nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Generate image\nTechnology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Employment\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://kotaku.com/pokemon-go-adventures-abound-art-ai-artificial-niantic-1850793819\nhttps://www.creativebloq.com/news/pokemon-ai-controversy\nhttps://gameishard.gg/news/pokemon-go-uses-ai-for-adventures-abound-artwork-why-not-utilize-their-talented-artists/68921/\nhttps://medium.com/@multiplatform.ai/ai-generated-art-sparks-controversy-in-pokemon-gos-season-12-promotion-c51dfb8468c\nhttps://dotesports.com/pokemon/news/the-pokemon-community-is-dragging-niantic-for-seemingly-using-ai-art-in-new-pokemon-go-ads\nRelated \ud83c\udf10\nDisney allegedly generates Loki season 2 poster with AI\nNetflix uses AI to generate 'Dog and Boy' film AI backgrounds\nPage info\nType: Incident\nPublished: October 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/disney-allegedly-generates-loki-season-2-poster-with-ai", "content": "Disney allegedly generates Loki season 2 poster with AI\nOccurred: October 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nDisney apparently used an AI-generated image for a promotional poster for Loki Season 2, angering artists concerned about the automation of creative jobs.\nThe promotional poster featured a spiralling clock in the background, which viewers believed was generated using artificial intelligence from a Shutterstock image named 'Surreal infinity time spiral in space'. The Shuttestock image was not labeled an AI-generated creation, though images uploaded by the same contributor showed signs of being AI-generated.\nDisney, which was earlier roasted for using AI in Marvel Studio's TV series Secret Invasion title sequence, later denied using AI to develop the Loki poster. \nSystem \ud83e\udd16\nUnknown\nDisney+ Loki Season 2 poster\nOperator: Disney/Disney Plus\nDeveloper: Disney/Disney Plus\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Generate image\nTechnology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Employment\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.entrepreneur.com/business-news/disney-accused-of-using-ai-imagery-for-loki-promo/463422\nhttps://www.theverge.com/2023/10/9/23909529/disney-marvel-loki-generative-ai-poster-backlash-season-2\nhttps://mashable.com/article/disney-loki-backlash-generative-ai-image\nhttps://hypebeast.com/2023/10/disney-plus-loki-season-2-ai-poster-info\nhttps://gizmodo.com/loki-s2-poster-ai-art-generated-disney-marvel-1850914705\nhttps://www.techtimes.com/articles/297376/20231010/loki-season-2-poster-sparks-backlash-amid-allegations-ai-involvement.htm\nhttps://www.creativebloq.com/news/disney-loki-ai\nhttps://www.thehindu.com/sci-tech/technology/disney-faces-backlash-over-use-of-generated-ai-in-loki-poster/article67403778.ece\nRelated \ud83c\udf10\nSecret Invasion' uses AI-generated title sequence\nNetflix 'Dog and Boy' film AI backgrounds\nPage info\nType: Incident\nPublished: October 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/bloomsbury-uses-ai-generated-artwork-for-sarah-j-maas-book", "content": "Bloomsbury uses AI-generated artwork for Sarah J. Maas book\nOccurred: May 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nPublisher Bloomsbury used AI to generate the UK cover of Sarah J. Maas' book House of Earth and Blood sparking criticism of her and the publisher.\nThe illustration of a wolf on the book's cover matched an image created by user 'Aperture Vintage' and marked as AI-generated on Adobe Stock. AI is permitted by Adobe on its Stock platform as long as it is clearly labelled as such, and contributors must review the terms of any generative AI tools they use to create the images to ensure they have 'all the necessary rights' to license them for commercial use.\nBloomsbury said its 'in-house design team created the UK paperback cover of House of Earth and Blood, and as part of this process we incorporated an image from a photo library that we were unaware was AI when we licensed it. The final cover was fully designed by our in-house team.\nThe incident raised complaints from readers and artists concerned about the impact of text-to-image generators on the livelihoods of artists and graphic designers. \nSystem \ud83e\udd16\nUnknown\nBloomsbury. House of Earth and Blood\nOperator: Bloomsbury Books\nDeveloper: Aperture Vintage\nCountry: UK\nSector: Media/entertainment/sports/arts\nPurpose: Generate image\nTechnology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Employment\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.creativebloq.com/news/ai-book-cover\nhttps://www.theguardian.com/books/2023/may/19/bloomsbury-admits-using-ai-generated-artwork-for-sarah-j-maas-novel\nhttps://www.theverge.com/2023/5/15/23724102/sarah-j-maas-ai-generated-book-cover-bloomsbury-house-of-earth-and-blood\nhttps://www.standard.co.uk/tech/ai-art-book-cover-sarah-j-maas-house-of-earth-and-blood-b1081457.html\nhttps://www.thebookseller.com/news/bloomsbury-unaware-image-used-to-design-maas-paperback-was-ai-generated\nhttps://www.themarysue.com/another-major-publisher-caught-using-ai-generated-cover-image-on-bestselling-authors-work/\nhttps://themessenger.com/news/bloomsbury-artifical-intelligence-book-cover-sarah-j-maas\nRelated \ud83c\udf10\nDisney allegedly generates Loki season 2 poster with AI\nNetflix uses AI to generate 'Dog and Boy' film AI backgrounds\nPage info\nType: Incident\nPublished: October 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tor-uses-ai-to-generate-christopher-paolini-book-cover", "content": "Tor uses AI to generate Christopher Paolini book cover\nOccurred: May 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe cover of Christopher Paolini\u2019s latest Fractalverse novel, Fractal Noise, was discovered to have been generated by AI, resulting in widespread criticism of Paolini's publisher Tor Books amongst readers, artists, and its authors. \nIt transpired the cover image had originally been posted to a stock art site by a user named 'Ufuk Kaya', and that Tor had in this instance not credited its in-house designer. \nTor responded to the backlash by saying it had licensed the relevant image from a 'reputable stock house' and was 'not aware that the image may have been created by AI.' It went on to say that it was going ahead anyway 'due to production constraints', triggering another backlash.\nPaolini defended Tor\u2019s decision not to stick with move the existing publication date, writing, 'Shifting the release date for Fractal Noise at this point would mean it probably wouldn\u2019t even be published next year.'\n\nSystem \ud83e\udd16\nMacmillan/Tor Books. Christopher Paolini - Fractal Noise\nOperator: Tor Books\nDeveloper: \nCountry: UK\nSector: Media/entertainment/sports/arts\nPurpose: Generate image\nTechnology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Employment\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://gizmodo.com/tor-book-ai-art-cover-christopher-paolini-fractalverse-1849904058\nhttps://www.themarysue.com/tor-faces-major-backlash-for-purchasing-ai-art-for-upcoming-novel-from-bestselling-auth/\nhttps://www.thehindu.com/sci-tech/technology/eragon-writer-christopher-paolinis-publisher-tor-books-slammed-for-ai-book-cover/article66270169.ece\nhttps://www.theguardian.com/artanddesign/2023/jan/23/its-the-opposite-of-art-why-illustrators-are-furious-about-ai\nhttps://www.kirkusreviews.com/news-and-features/articles/cover-of-paolini-book-may-contain-ai-created-image/\nhttps://whatever.scalzi.com/2022/12/10/an-update-on-my-thoughts-on-ai-generated-art/\nhttps://locusmag.com/2023/01/tor-ai-controversy/\nRelated \ud83c\udf10\nDisney allegedly generates Loki season 2 poster with AI\nNetflix uses AI to generate 'Dog and Boy' film AI backgrounds\nPage info\nType: Incident\nPublished: October 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/snapchat-fails-to-assess-my-ai-privacy-risks", "content": "UK privacy watchdog accuses Snapchat of failing to assess My AI privacy risks\nOccurred: October 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nSnapchat owner Snap Inc has been issued with a provisional enforcement notice by the UK privacy watchdog for failing to assess privacy risks its My AI chatbot poses to users, notably children. \nThe UK Information Commissioner's Office (ICO) said Snap had failed to 'adequately identify and assess the risks' to several million UK users of My AI, including among 13- to 17-year-olds. Snap responded by saying it would 'work constructively' with the ICO, and that it had carried out a 'robust legal and privacy review' before My AI was launched.\nShould a final notice be issued, the service would be halted and Snap fined up to 4 percent of its annual global turnover.\nSnap was the first social media platform to adopt an AI-powered chat function. Initially available to Snapchat+ subscribers, My AI was rolled out across Snapchat's entire user base in April 2023. \nSystem \ud83e\udd16\nSnapchat My AI chatbot\nOperator:\nDeveloper: Snap Inc\nCountry: UK\nSector: Media/entertainment/sports/arts\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Privacy\nTransparency: Governance\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nUK Information Commissioner's Office (2023). UK Information Commissioner issues preliminary enforcement notice against Snap\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theguardian.com/technology/2023/oct/06/snapchat-enforcement-notice-my-ai-chatbot-uk-data-watchdog\nhttps://techcrunch.com/2023/10/06/snap-ico-notice/\nhttps://timesofindia.indiatimes.com/gadgets-news/snapchat-under-uk-watchdog-scanner-over-failure-to-curb-underage-users/articleshow/102536534.cms\nhttps://www.bbc.co.uk/news/technology-67027282\nhttps://www.reuters.com/technology/uk-regulator-issues-notice-snapchat-over-privacy-risks-posed-by-ai-chatbot-2023-10-06/\nhttps://www.telegraph.co.uk/business/2023/10/06/snapchat-ai-chatbot-faces-uk-shutdown-risk-to-children/\nRelated \ud83c\udf10\nSnapchat My AI accesses user location data\nSnapchat My AI 'goes rogue'\nPage info\nType: Incident\nPublished: October 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/deepfake-mrbeast-iphone-giveaway-scam", "content": "Deepfake MrBeast iPhone giveaway scam\nOccurred: October 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn advert of YouTube star Jimmy Donaldson (aka MrBeast) offering free iPhone 15 smartphones on TikTok was exposed as a deepfake scam.\nThe fake ad claimed that MrBeast had selected 10,000 people to receive an iPhone 15 Pro in exchange 'for just USD 2'; those interested were then encouraged to click a link below the video to participate. \nThe ad was exposed as fraudulent by MrBeast, who took to Twitter to say 'Lots of people are getting this deepfake scam ad of me\u2026 are social media platforms ready to handle the rise of AI deepfakes? This is a serious problem.' \nThe latest in a series of fraud attempts involving celebrities manipulated using synthetic media, the incident raised concerns about the volume and nature of deepfake impersonations. TikTok also came under fire for its apparent inability to properly moderate its platform.\nSystem \ud83e\udd16\nUnknown\nMrBeast iPhone 15 giveway deepfake video\nOperator:\nDeveloper: \nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Defraud\nTechnology: Deepfake - video, audio; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Safety - fraud\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/technology-66993651\nhttps://uk.pcmag.com/ai/148949/mrbeast-dont-fall-for-that-deepfake-video-of-me-offering-free-iphones\nhttps://techcrunch.com/2023/10/03/how-an-ai-deepfake-ad-of-mrbeast-ended-up-on-tiktok/\nhttps://petapixel.com/2023/10/03/mrbeast-warns-fans-of-iphone-giveaway-scam-with-his-deepfake/\nhttps://mashable.com/article/mrbeast-deepfake-scam-iphone-ad\nRelated \ud83d\uddde\ufe0f\nDeepfake Mark Ruffalo scams manga artist Chikae Ide\nMartin Lewis deepfake scam ad\nPage info\nType: Incident\nPublished: October 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-generated-travel-books-and-reviews-flood-amazon", "content": "AI-generated travel books and reviews flood Amazon\nOccurred: September 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nDozens of AI-generated travel books, accompanied by AI-generated profile pictures and five-star reviews, were discovered to have been listed on Amazon.\nThe New York Times revealed that Amazon was awash with sham books written by 'acclaimed' travel writers written in a monotonous, formulaic style commonly associated with ChatGPT and other generative AI systems. \nIllustrated with generic license-free images, the books tended to be priced lower than better-known, more reputable guidebooks and were often accompanied by sham reviews.\nThe books prompted complaints from buyers and raised questions about the retailer's ability to police its content in the face of a deluge of AI-generated books, many of poor quality and dubious provenance. \nThe company responded to the NYT's findings by saying 'We have clear content guidelines governing which books can be listed for sale and promptly investigate any book when a concern is raised.'\nSystem \ud83e\udd16\nChatGPT website\nChatGPT Wikipedia profile\nOperator: Amazon\nDeveloper: OpenAI\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning\nIssue: Security\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.nytimes.com/2023/08/05/travel/amazon-guidebooks-artificial-intelligence.html\nhttps://www.insider.com/artificial-intelligence-amazon-verified-buyers-generated-fake-travel-guides-2023-8\nhttps://wonderfulengineering.com/a-new-scam-is-flooding-amazon-ai-generated-travel-guides/\nhttps://futurism.com/the-byte/scam-amazon-ai-generated-travel-guides\nhttps://www.axios.com/2023/08/16/ai-book-publishing-fake-amazon\nhttps://fagenwasanni.com/news/fake-travel-guides-flooding-amazon-misleading-travelers/120288/\nhttps://boingboing.net/2023/08/14/crappy-ai-knockoff-books-are-already-topping-the-publishing-charts.html\nRelated \ud83c\udf10\nAI-generated mushroom foraging books flood Amazon\nAmazon sells fake AI Jane Friedman books\nPage info\nType: Incident\nPublished: October 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-generated-mushroom-foraging-books-flood-amazon", "content": "AI-generated mushroom foraging books flood Amazon\nOccurred: September 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAI-generated books for beginners on how to forage for and cook mushrooms have been found on Amazon, worrying experts about the possibility of inaccurate information killing people. \nThe New York Mycological Society (NYMS) warned that the proliferation of AI-generated foraging books could 'mean life or death' for people with little experience of handling dangerous mushrooms. \nAnalysis of the books suggested they were likely written using ChatGPT but sold not stating they were generated by artificial intelligence - which is not permitted under Amazon's Content Guidelines. The company later deleted a number of books violating its guidelines.\nThe incident raised questions about Amazon's ability to police AI content.\nSystem \ud83e\udd16\nChatGPT website\nChatGPT Wikipedia profile\nOperator: Amazon\nDeveloper: OpenAI\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning\nIssue: Safety\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.404media.co/ai-generated-mushroom-foraging-books-amazon/\nhttps://www.theguardian.com/technology/2023/sep/01/mushroom-pickers-urged-to-avoid-foraging-books-on-amazon-that-appear-to-be-written-by-ai\nhttps://fortune.com/2023/09/03/ai-written-mushroom-hunting-guides-sold-on-amazon-potentially-deadly/\nhttps://www.businesstelegraph.co.uk/mushroom-pickers-urged-to-avoid-foraging-books-on-amazon-that-appear-to-be-written-by-ai/\nhttps://decrypt.co/154187/ai-generated-books-on-amazon-could-give-deadly-advice/\nhttps://slashdot.org/story/23/08/29/1440230/life-or-death-ai-generated-mushroom-foraging-books-are-all-over-amazon\nhttps://www.techtimes.com/articles/295863/20230901/beware-ai-written-mushroom-foraging-guides-amazon-experts-warn.htm\nRelatedc \ud83c\udf10\nAI-generated travel books and reviews flood Amazon\nAI meal planner app suggests chlorine gas recipe\nPage info\nType: Incident\nPublished: October 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/youtube-videos-target-kids-with-ai-fake-scientific-education-content", "content": "YouTube videos target kids with AI fake 'scientific' education content\nOccurred: September 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nYouTube was found to be recommending deliberately misleading and false 'bad science' videos to children alongside legitimate educational content. \nAccording to the BBC, over 50 channels in more than 20 languages were spreading disinformation about the existence of electricity-producing pyramids and aliens, and the denial of human-caused climate change, disguised as STEM [Science Technology Engineering Maths] content Examples of conspiracy theories are. \nThe creators of the videos appeared to have stolen and manipulated accurate content using AI, and republished them. The videos were tagged 'educational content', meaning they were more likely to be recommended to children.\nThe finding raised questions about the integrity of YouTube's content moderation efforts. YouTube benefits from high performing content by taking about 40% of the money made from advertising on someone's channel. \nSystem \ud83e\udd16\nYouTube website\nYouTube Wikipedia profile\nOperator: BBC\nDeveloper: Alphabet/Google/YouTube\nCountry: UK; Thailand\nSector: Education\nPurpose: Manage content\nTechnology: Content management system\nIssue: Mis/disinformation\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/newsround/66796495 \nhttps://www.youtube.com/watch?v=ojjn9T_fuUw\nhttps://factcheckhub.com/how-content-creators-use-ai-to-misinform-young-children-on-youtube-report/\nRelated \ud83c\udf10\nStudent GPT-3 fake blog posts pass as human\nYouTube Kids app features adult content\nPage info\nType: Incident\nPublished: October 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-alexa-says-2020-us-election-was-rigged", "content": "Amazon Alexa says 2020 US election was rigged\nOccurred: October 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAmazon's Alexa virtual assistant told Washington Post journalists that the US 2020 presidential election was rigged, raising concerns about the system's ability to amplify disinformation ahead of the 2024 election.\nAccording to the Post, the 2020 election was 'stolen by a massive amount of election fraud', had been 'notorious for many incidents of irregularities and indications pointing to electoral fraud taking place in major metro centers,' and claimed Donald Trump won Pennsylvania. \nThe source of Alexa\u2019s information was YouTube alternative Rumble and newsletter platform Substack (disclaimer: Substack is also used by AIAAIC). Amazon told the Post that Alexa made mistakes in only a 'small' number of cases, and that the election rigging claim issue had been fixed.\nMultiple investigations and court cases have revealed no evidence of any meaningful fraud in the 2020 election.\nSystem \ud83e\udd16\nAmazon Alexa virtual assistant\nOperator:\nDeveloper: Amazon\nCountry: USA\nSector: Politics\nPurpose: Provide information, services\nTechnology: Speech recognition; Natural language understanding (NLU)\nIssue: Mis/disinformation\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.washingtonpost.com/technology/2023/10/07/amazon-alexa-news-2020-election-misinformation/\nhttps://www.thedailybeast.com/amazon-alexa-told-users-the-2020-election-was-stolen-report\nhttps://themessenger.com/news/ai-alert-alexa-tells-users-2020-election-was-rigged-report\nhttps://politicalwire.com/2023/10/07/amazons-alexa-says-the-2020-election-was-stolen/\nRelated \ud83c\udf10\nAmazon Alexa mistakes conversation for command\nAmazon Alexa recommends girl touches electric plug\nPage info\nType: Incident\nPublished: October 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/la-subsidised-housing-scoring-system-racial-bias", "content": "Los Angeles subsidised housing scoring system racial bias\nOccurred: February 2023 \nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nWhite people in the Los Angeles area received higher scores from system that guided who receives priority for housing assistance, according to an investigation by The Markup. The findings called into question the accuracy and fairness of the system and similar systems.\nThe Markup analysis of 130,000 Los Angeles Homeless Services Authority's (LAHSA) Vulnerability Index-Service Prioritization Decision Assistance Tool (VI\u2011SPDAT) surveys found that White people, notably those under 25, received scores considered 'high acuity' - or most in need - more often than Black people, a gap which persisted year over year. \nThe Los Angeles Homeless Services Authority (LAHSA) had already planned to stop using the tool after a research team partnering with the agency found the tool had 'the potential to advantage certain racial groups over others.' \nIn 2019, the LAHSA had found that Black people tended to receive lower vulnerability scores despite their overrepresentation in Los Angeles\u2019s unhoused population, something the LAHSA attributed to 'structural racism, discrimination, and implicit bias'. \nSystem \ud83e\udd16\nLos Angeles Homeless Services Authority website\n\nDocuments \ud83d\udcc3\nHow to access CES (pdf)\nGROUNDBREAKING REPORT ON BLACK PEOPLE AND HOMELESSNESS RELEASED\nREPORT AND RECOMMENDATIONS OF THE AD HOC COMMITTEE ON BLACK PEOPLE EXPERIENCING HOMELESSNESS\nOperator: Los Angeles Homeless Services Authority (LAHSA)\nDeveloper: County of Los Angeles Public Health\nCountry: USA\nSector: Govt - housing\nPurpose: Assess subsidised permanent housing elgibility\nTechnology: Scoring algorithm\nIssue: Bias/discrimination - race, ethnicity\nTransparency: Governance; Complaints/appeals\nInvestigations, assessments, audits \ud83e\uddd0\nThe Markup (2023). L.A.\u2019s Scoring System for Subsidized Housing Gives Black and Latino People Experiencing Homelessness Lower Priority Scores\nThe Markup (2023). How We Investigated L.A.\u2019s Homelessness Scoring System\nThe Markup (2023). Journalists: Investigate Homeless Vulnerability Scoring in Your City\nThe Markup (2023). Los Angeles's Homelessness Scoring System investigation data\nResearch, advocacy \ud83e\uddee\nCommunity Solutions (2023). These are not the droids you are looking for\nRacism and Technology Center (2023). Racist Technology in Action: Racial disparities in the scoring system used for housing allocation in L.A.\nC4 Innovations (2019). Coordinated Entry Systems Racial Equity Analysis of Assessment Data (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.latimes.com/california/story/2023-02-28/black-latino-homeless-people-housing-priority-list-los-angeles\nhttps://crast.net/289370/racially-biased-scoring-system-helps-choose-who-gets-housing-in-la/\nRelated \ud83c\udf10\nSafeRent tenant screening\nFacebook lets housing ads exclude ethnic minorities\nPage info\nType: Incident\nPublished: October 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/serbia-social-card-excludes-thousands-of-welfare-beneficiaries", "content": "Serbia Social Card excludes thousands of welfare beneficiaries\nOccurred: November 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Serbia government system meant to automate the allocation of welfare payments and detect fraud resulted in thousands of beneficiaries not receiving their payments. \nApproved by the country's parliament in March 2022 and developed by local IT company Saga, Serbia's socijalna karta (Social Card) system uses approximately 130 types of data collected from the Tax Administration, Ministry of Interior, and other government agencies to process individual payments in a fully automated manner. \nIn November 2022, a group of human rights organisations, including Amnesty, submitted a legal opinion that accused the Serbian government of creating an 'invasive digital surveillance system that threatens the right to equality' by excluding the Roma and the disabled from the system, thereby increasing disparities against them.\nSerbia's government has refused to provide access to the system's algorithm or data. \nSystem \ud83e\udd16\nGovernment of the Republic of Serbia (2022). Government passes Social Card Bill\nOperator: Serbia Ministry of Labour\nDeveloper: Saga\nCountry: Serbia\nSector: Govt - welfare\nPurpose: Allocate social benefits\nTechnology: \nIssue: Accuracy/reliability; Bias/disrimination - ethnicity, disability; Fairness; Privacy; Surveillance\nTransparency: Governance; Black box; Complaints/appeals\nResearch, advocacy \ud83e\uddee\nA11 Initiative (2022). (Anti)Social Cards\nA11 Initiative (2022). Legal Opinion on International and Comparative Human Rights Law Concerning the Matter of the Social Card Law Pending before the Constitutional Court of Serbia\nAmnesty (2022). Serbia: Social Card law could harm marginalized members of society \u2013 legal opinion\nESCR-Net (2022). Serbia joins the group of countries where discriminatory government-driven algorithms are challenged in court\nEDRi (2022). Legal challenge: The Serbian government attempts to digitise social security system\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.biometricupdate.com/202212/digital-id-schemes-marginalizing-roma-in-serbia-rohingyas-in-myanmar-reports\nhttps://www.context.news/digital-rights/as-serbia-adopts-digital-welfare-system-the-poorest-miss-out\nhttps://www.britic.co.uk/?p=1112517\nhttps://web-mind.io/artificial-intelligence/surveillance-and-ethics-in-the-use-of-artificial-intelligence/\nhttps://voxeurop.eu/en/serbia-ai-algorithmic-discrimination/\nhttps://www.wired.co.uk/article/welfare-fraud-industry\nhttps://balkaninsight.com/2023/07/25/doomed-by-algorithm-serbias-social-card-leaves-societys-weakest-exposed/\nhttps://www.slobodnaevropa.org/a/srbija-socijalne-karte-algoritam/32153869.html\nRelated \ud83c\udf10\nRotterdam welfare fraud risk algorithm\nTrelleborg welfare management automation\nPage info\nType: Incident\nPublished: October 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/israel-ai-robot-machine-guns-fire-tear-gas-at-palestinian-protestors", "content": "Israel AI robot machine guns fire tear gas at Palestinian protestors\nOccurred: November 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nIsrael is deploying robotic machine guns that 'frequently coat hillsides in tear gas' and unleash sponge-tipped bullets at Palestinians without warning.\nAccording to local residents speaking to the Associated Press (AP), guns made by Israel-based Smart Shooter, a company that makes 'fire control systems' that 'significantly increase the accuracy, lethality, and situational awareness of small arms,' have been spotted at the Al-Aroub refugee camp in the southern West Bank, and in the city of Hebron.\nAccording to Smart Shooter, the system enables Israeli authorities to monitor individuals in a crowd and lock the gun onto specific body parts. 'The system fires only after algorithms assess complex factors like wind speed, distance, and velocity,' the company told the AP.\nCritics view the deployment as an intrusive and potentially damaging form of population surveillance that could easily be misused. The company did not respond to questions concerning the extent of human oversight of the system, and regarding it's security. \nOperator: Israel Defense Forces (IDF)\nDeveloper: Smart Shooter\nCountry: Israel\nSector: Govt - security\nPurpose: Control population\nTechnology: Computer vision; Robotics\nIssue: Human/civil rights; Safety\nTransparency: Governance\nSystem\nSmart Shooter website\nSMASH Hopper website\nNews, commentary, analysis\nhttps://apnews.com/article/technology-business-israel-robotics-west-bank-cfc889a120cbf59356f5044eb43d5b88\nhttps://www.theregister.com/2022/11/18/israel_sets_robotic_targettracking_turrets/\nhttps://medium.com/the-dock-on-the-bay/a-smash-hopper-could-be-gunning-for-you-now-3d93942b2d87\nhttps://www.middleeastmonitor.com/20220927-israel-sets-up-ai-controlled-machine-gun-in-occupied-hebron/\nhttps://www.youtube.com/watch?v=ti40lKa0wmY\nhttps://www.telegraph.co.uk/world-news/2022/09/26/israel-pilots-robotic-machine-gun-west-bank-checkpoint/\nhttps://www.euronews.com/next/2022/10/17/israel-deploys-ai-powered-robot-guns-that-can-track-targets-in-the-west-bank\nhttps://www.haaretz.com/israel-news/2022-09-24/ty-article/.premium/israeli-army-installs-remote-control-crowd-dispersal-system-at-hebron-flashpoint/00000183-70c4-d4b1-a197-ffcfb24f0000\nRelated\nAnyVision 'Google Ayosh' Palestinian surveillance\nMohsen Fakhrizadeh assassinated using robot machine gun\nPage info\nType: Incident\nPublished: October 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-project-nessie-automated-price-gouging", "content": "Amazon 'Project Nessie' automated price gouging\nOccurred: 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAmazon used an algorithm to monitor and influence pricing decisions by rival retailers, underscoring concerns about the company's business practices and market power.\nCode-named 'Project Nessie', the algorithm allegedly enabled Amazon to test the extent to which it could increase prices and nudge competing retailers to follow suit, helping the company squeeze larger profits from customers and undercut rival platforms. \nAccording to court documents seen by the Wall Street Journal, Nessie also helped Amazon automatically reduce prices to match its rivals if these platforms offered discounts on certain items.\nThe algorithm, which is said to have made Amazon over USD 1 billion, was discontinued in 2019. The documents came to light as part of A US Federal Trade Commission anti-trust lawsuit (pdf) against Amazon.\nSystem \ud83e\udd16\nAmazon US website\nAmazon Wikipedia profile\nOperator: Amazon\nDeveloper: Amazon\nCountry: USA\nSector: Retail\nPurpose: Monitor & match prices\nTechnology: Pricing algorithm\nIssue: Competition/collusion\nTransparency: Governance; Complaints/appeals\nInvestigations, assessments, audits \ud83e\uddd0\nFederal Trade Commission v Amazon.com (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.wsj.com/business/retail/amazon-used-secret-project-nessie-algorithm-to-raise-prices-6c593706\nhttps://www.wsj.com/tech/ftc-sues-amazon-alleging-illegal-online-marketplace-monopoly-6bd9af23\nhttps://www.theverge.com/2023/10/3/23901840/amazon-project-nessie-algorithm-antitrust-ftc-complaint\nhttps://www.dexerto.com/tech/amazon-accused-of-price-gouging-with-project-nessie-algorithm-2321844/\nhttps://techcrunch.com/2023/09/26/what-is-amazons-redacted-project-nessie-algorithm/\nhttps://www.geekwire.com/2023/ftc-targets-alleged-secret-amazon-pricing-algorithm-project-nessie-in-antitrust-complaint/\nhttps://www.forbes.com/sites/willskipworth/2023/10/03/amazon-allegedly-used-secret-algorithm-to-raise-prices-on-consumers-ftc-lawsuit-reveals/\nhttps://qz.com/emails/daily-brief/1850897247/project-nessie-less-cute-than-it-sounds\nhttps://www.forbes.com/sites/willskipworth/2023/10/03/amazon-allegedly-used-secret-algorithm-to-raise-prices-on-consumers-ftc-lawsuit-reveals/\nhttps://www.theinformation.com/briefings/amazon-used-nessie-algorithm-to-raise-prices\nRelated \ud83c\udf10\nAmazon Buy Box pricing manipulation\nAmazon US own brand search engine rigging\nPage info\nType: Incident\nPublished: October 2023\nLast updated: February 2024", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/taylor-swift-uses-facial-recognition-to-detect-stalkers", "content": "Taylor Swift uses facial recognition to detect stalkers\nOccurred: December 2018\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nTaylor Swift's security team covertly used facial recognition to identify stalkers during her 2018 Stalker tour, raising questions about the stated purpose and nature of the programme, and the use of biometric technologies at entertainment events.\nImages of Swift fans were being captured in kiosks and transferred to a 'command post' in Nashville where they were cross-referenced with a database of hundreds of the pop star\u2019s known stalkers, according to RollingStone.\nThe Guardian later revealed that Swift's security operation ISM Connect installed cameras behind kiosks marked as 'selfie stations' which scanned the facial features of fans. While the use of the data remained unclear, ISM's website stated it used 'smart screens' to simultaneously enhance security, advertise, and collect demographic data for brands.\nICM Connect has since gone offline.\nSystem \ud83e\udd16\nISM Connect website\nOperator: UMG/Taylor Nation\nDeveloper: ISM Connect\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Identify stalkers\nTechnology:  \nIssue: Privacy\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.rollingstone.com/music/music-news/taylor-swift-facial-recognition-concerts-768741/\nhttps://www.theguardian.com/music/2018/dec/13/taylor-swift-facial-recognition-technology-surveillance\nhttps://www.nytimes.com/2018/12/13/arts/music/taylor-swift-facial-recognition.html\nhttps://www.nbcnews.com/tech/tech-news/facial-recognition-tech-used-scan-stalkers-taylor-swift-show-report-n947581\nhttps://www.aclu.org/news/privacy-technology/problem-using-face-recognition-fans-taylor-swift\nhttps://www.theverge.com/2018/12/12/18137984/taylor-swift-facial-recognition-tech-concert-attendees-stalkers\nhttps://gizmodo.com/the-mystery-of-that-taylor-swift-face-recognition-kiosk-1832653921\nhttps://news.sky.com/story/taylor-swift-facial-recognition-used-to-id-stalkers-at-show-11579614\nhttps://www.theguardian.com/technology/2019/feb/15/how-taylor-swift-showed-us-the-scary-future-of-facial-recognition\nRelated \ud83c\udf10\nMSG Entertainment facial recognition\nLivonia skating rink misidentifies, bars black teen\nPage info\nType: Incident\nPublished: October 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/italian-car-insurance-birthplace-discrimination", "content": "Italian car insurers discriminate using place of birth\nOccurred: 2012-2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nCar drivers in Italy were charged different insurance premiums depending on their birthplace, triggering accusations of discrimination based on national identity and the against drivers born in certain Italian cities.\nA 2021 study (pdf) by researchers at the Universities of Padua, Udine, and Carnegie Mellon found that a driver born in Laos may be charged over EUR 1,000 more than a driver born in Milan with an otherwise identical profile. The study also found that birthplace and gender had a direct and sizeable impact on the prices quoted to drivers, despite national and international regulations against their use.\nIn 2012, Italy\u2019s Institute for the Supervision of Insurance (IVASS) and National Anti-Racial Discrimination Office (UNAR) issued soft regulation to encourage car insurance companies to avoid using nationality-related factors as inputs to risk models. But the practice has been made more opaque and insidious when insurance companies started using algorithms to calculate risk premiums, according to AlgorithmWatch.\nSystem \ud83e\udd16\n\nOperator: Linear; Genertel; Mps; Quixa; Con.Te\nDeveloper: Linear; Genertel; Mps; Quixa; Con.Te\nCountry: Italy\nSector: Banking/financial services\nPurpose: Calculate insurance premium\nTechnology: Pricing algorithm\nIssue: Bias/discrimination - birthplace\nTransparency: Governance\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nA.S.G.I. \u2013 ASSOCIAZIONE STUDI GIURIDICI SULL\u2019IMMIGRAZIONE v LINEAR SPA (pdf) \nInvestigations, assessments, audits \ud83e\uddd0\nFabris A. et al (2021). Algorithmic Audit of Italian Car Insurance: Evidence of Unfairness in Access and Pricing (pdf)\nFusco G., Porrini D. (2020). LESS DISCRIMINATION, MORE GENDER INEQUALITY: THE CASE OF THE ITALIAN MOTOR-VEHICLE INSURANCE\nBorselli A. (2018). Insurance by Algorithm\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://algorithmwatch.org/en/discriminating-insurance/\nRelated \ud83c\udf10\nAllocation algorithm wrongly places thousands of Italian teachers\nAllstate car insurance 'suckers list' overcharging\nPage info\nType: Incident\nPublished: October 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/mistral-generates-ethnic-cleansing-murder-instructions", "content": "Mistral 7B generates ethnic cleansing, murder instructions\nOccurred: September 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nLarge language model Mistral 7B generates controversial and potentially harmful content, including discussions about ethnic cleansing, discrimination, suicide, and violence.\nAnalysis by AI safety researcher Paul R\u00f6ttger and 404 Media discovered that Mistral will 'readily discuss the benefits of ethnic cleansing, how to restore Jim Crow-style discrimination against Black people, instructions for suicide or killing your wife, and detailed instructions on what materials you\u2019ll need to make crack and where to acquire them.'\nMistral later published a short statement on its website acknowledging the absence of moderation mechanisms in Mistral 7B and saying it is 'looking forward to engaging with the community on ways to make the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.'\nThe findings raised questions about the safety of the system, and triggered debate on the relative merits of closed and open-source development from a safety and innovation perspective.\nSystem \ud83e\udd16\nMistral AI website\nMistral (2023). Announcing Mistral 7B\nOperator: 404 Media; Perplexity AI\nDeveloper: Mistral AI\nCountry: France\nSector: Media/entertainment/sports/arts\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning\nIssue: Ethics; Safety\nTransparency: Governance\nInvestigations, assessments, audits \ud83e\uddd0\nMistral unsafe prompts\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.404media.co/260-million-ai-company-releases-chatbot-that-gives-detailed-instructions-on-murder-ethnic-cleansing/\nhttps://sifted.eu/articles/mistral-model-safety\nhttps://www.msn.com/en-us/money/technology/french-ai-startup-mistral-faces-backlash-as-new-llm-generates-harmful-content/ar-AA1hstQl\nhttps://news.ycombinator.com/item?id=37714703\nRelated\nGoogle AI bots expouse slavery, fascism\nRemini AI photo enhancer generates 'child porn'\nPage info\nType: Incident\nPublished: October 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/buzzfeed-national-ai-barbies-racism-cultural-stereotyping", "content": "AI-generated Barbies reinforce racist stereotyping\nOccurred: July 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nBuzzfeed came under fire for publishing a list of AI-generated images that portrayed Barbie in 195 countries in a manner that was seen as inappropriate, inaccurate, and that reinforced racial and ethnic stereotypes. \nFrom Afghanistan To Zimbabwe: Here's What Barbie Would Look Like In Every Country was  created using the Midjourney image generator and depicted a number of Asian Barbies with blonde hair, showed Middle-eastern Barbies wearing a traditional headdress for men, gave a South Sudanese doll a gun, and a Lebanese Barbie standing on rubble.\nIn addition to reinforcing racial and ethnic stereotypes, the fracas also resulted in commentators highlighting existing concerns about bias in generative AI datasets, and in Midjourney specifically.\nBuzzfeed deleted the article after the backlash. \nOperator: Buzzfeed\nDeveloper: Midjourney\nCountry: All\nSector: Consumer goods; Media/entertainment/sports/arts\nPurpose: Entertain\nTechnology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Accuracy/reliability; Bias/discrimination - race\nTransparency: Governance\nSystem\nMidjourney website\nMidjourney Wikipedia profile\nBuzzfeed (2023). From Afghanistan To Zimbabwe: Here's What Barbie Would Look Like In Every Country\nNews, commentary, analysis\nhttps://www.insider.com/ai-generated-barbie-every-country-criticism-internet-midjourney-racism-2023-7\nhttps://nextshark.com/buzzfeed-ai-generated-barbies-backlash-controversy\nhttps://www.womanandhomemagazine.co.za/today-on-woman-and-home/the-controversy-around-buzzfeeds-ai-barbies/\nhttps://www.todayonline.com/world/ai-generated-barbie-asian-2216406\nhttps://www.middleeasteye.net/news/barbie-ai-generated-images-every-country-portrayals-criticism-mena\nRelated\nStable Diffusion job type gender, racial stereotyping\nMakeApp make-up remover gender stereotyping\nPage info\nType: Incident\nPublished: October 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/xiaohongshu-ai-image-generator-abuses-copyright", "content": "Xiaohongshu AI image generator copyright abuse\nOccurred: September 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nChinese artists and illutrators accused Chinese social media platform Xiaohongshu of using their work to train its AI image generator Trik AI, infringing their copyright.\nThe issue was triggered by 'Snow Fish', an illustrator who claimed his illustrations posted on the platform had been 'fed' to train Trik AI without his knowledge or permission. He demanded the company publicly apologise, delete the relevant artwork, and stop using his illustrations to train their model. \nThe company later apologised and admitted Snow Fish's artwork had been used to train its AI, but said that the image had been taken from open access sources rather than from his Xiaohongshu account. It added it had 'never used images from Xiaohongshu to train [Trik AI].'\nThe outcry mirrored the backlash against generative AI systems such as ChatGPT, Midjourney, and Stable Diffusion, and so-called 'shadow libraries' including Library Genesis and Z-Library.\nSystem \ud83e\udd16\nXiaohongshu website\nXiaohongshu Wikipedia profile\nOperator: Snow Fish\nDeveloper: Xiaohongshu\nCountry: China\nSector: Media/entertainment/sports/arts\nPurpose: Generate images\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning\nIssue: Copyright\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.sixthtone.com/news/1013514\nhttps://edition.cnn.com/2023/09/28/tech/chinese-artists-boycott-ai-generator-intl-hnk/index.html\nhttps://www.kompas.id/baca/english/2023/09/29/en-gambar-buatan-teknologi-ai-kian-meresahkan\nhttps://www.wionews.com/world/chinese-artists-boycott-social-media-site-over-ai-image-generation-tool-640786\nRelated \ud83c\udf10\nLAION trains Robert Kneschke photos without consent\nHollie Mengert art used to train Illustration Diffusion\nPage info\nType: Incident\nPublished: October 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/getty-images-sues-stability-ai-for-copyright-abuse", "content": "Getty Images sues Stability AI for copyright abuse\nOccurred: February 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nStability AI was accused by Getty Images in a lawsuit (pdf) filed in the US of using over 12 million photographs from its stock image collection to train its Stable Diffusion AI image generator.\nGetty alleged that Stability copied the company\u2019s photographs, associated captions, and metadata to 'build a competing business' through its DreamStudio revenue-generating interface. The suit also stated that Stability AI 'removed or altered Getty Images' copyright management information, provided false copyright management information, and infringed Getty Images\u2019 trademarks.\nIn August 2022, blogger Andy Biao discovered that 15,000 images of 12 million images used to train Stable Diffusion were from Getty Images. In January 2023, Getty Images filed a separate case against Stability AI in the UK and, a few months later, asked a UK court to stop UK sales of Stable Diffusion.\nThe lawsuit prompted further concerns about the data collection and use practices of the company, and those of the AI industry more generally.\nSystem \ud83e\udd16\nStability AI website\nStable Diffusion website\nStable Diffusion Wikipedia profile\nOperator:  \nDeveloper: Stability AI\nCountry: UK\nSector: Media/entertainment/sports/arts\nPurpose: Generate images\nTechnology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Copyright\nTransparency: Governance\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nGetty Images (US) v Stability AI (pdf)\nGetty Images (2023). Statement\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theverge.com/2023/2/6/23587393/ai-art-copyright-lawsuit-getty-images-stable-diffusion\nhttps://www.reuters.com/legal/getty-images-lawsuit-says-stability-ai-misused-photos-train-ai-2023-02-06/\nhttps://gizmodo.com/getty-images-stable-diffusion-ai-art-generator-1850079852\nhttps://news.bloomberglaw.com/ip-law/getty-images-sues-stability-ai-over-art-generator-ip-violations\nhttps://www.reuters.com/technology/getty-asks-london-court-stop-uk-sales-stability-ai-system-2023-06-01/\nhttps://www.artnews.com/art-news/news/getty-images-lawsuit-stability-ai-12-million-photos-copied-stabile-diffusion-1234656475/\nhttps://waxy.org/2022/08/exploring-12-million-of-the-images-used-to-train-stable-diffusions-image-generator/\nRelated \ud83c\udf10\nHollie Mengert art used to train Illustration Diffusion\nLAION trains Robert Kneschke photos without consent\nPage info\nType: Incident\nPublished: October 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/google-search-indexes-bard-personal-chats", "content": "Google Search indexes Bard personal chats\nOccurred: September 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGoogle Search started showing user conversations with its Bard chatbot in its search results, raising concerns about loss of privacy and confidentiality and the nature and effectiveness of Google's security.\nFirst discovered by SEO consultant Gagan Ghotra, the results appearing in Google's search engine were limited to conversations users had chosen to share with others, and did not include their usernames. However, it also transpired that some Bard conversations were ranked as Featured Snippets in Google Search in order to answer common search queries.\nGoogle later said it did not intend for Bard shared chats to be indexed by Google Search, and that it was working on blocking them from being indexed.\nSystem \ud83e\udd16\nGoogle Bard website\nGoogle Bard Wikipedia profile\nOperator: Alphabet/Google  \nDeveloper: Alphabet/Google\nCountry: USA\nSector: Technology\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning\nIssue: Privacy\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://chromeunboxed.com/bard-leaks-user-conversations\nhttps://www.fastcompany.com/90958811/google-was-accidentally-leaking-its-bard-ai-chats-into-public-search-results\nhttps://www.malwarebytes.com/blog/news/2023/09/googles-bard-conversations-turn-up-in-search-results\nhttps://interestingengineering.com/culture/google-search-indexed-bard-conversations\nhttps://venturebeat.com/ai/oops-google-search-caught-publicly-indexing-users-conversations-with-bard-ai/\nhttps://www.tomsguide.com/news/google-bard-bug-exposed-private-conversations-via-google-search-what-you-need-to-know\nhttps://www.computerworld.com/article/3707730/google-to-block-bard-conversations-from-being-indexed-on-search.html\nRelated \ud83c\udf10\nChatGPT leaks user conversations, personal information\nGoogle AI bots expouse slavery, fascism\nPage info\nType: Incident\nPublished: September 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chatgpt-leaks-user-conversations", "content": "ChatGPT leaks user conversations, personal information\nOccurred: March 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nChatGPT allowed some users to see other users' conversations and personal information, suggesting OpenAI had access to user conversations, and calling into question the company's privacy duty of care.\nUsers had shared images of chat histories that they said were not theirs on social media sites Reddit and Twitter, prompting a flood of complaints. \nInitially, the incident appeared confined to user conversations, which OpenAI states in its ChatGPT FAQs are reviewed - most likely to improve its systems on an ongoing basis. \nHowever, the company later confirmed that some users' first and last names, email addresses, payment addresses, parts of credit card numbers and credit card expiration dates had also been exposed.\nOpenAI disabled the chatbot to fix the error, and said that users had not been able to access full conversations. CEO Sam Altman later said the company felf 'awful' about the glitch, and that it had been fixed. \nSystem \ud83e\udd16\nChatGPT chatbot\nChatGPT privacy policy\nOpenAI (2023). March 20 ChatGPT outage: Here's what happened\nOperator:  \nDeveloper: OpenAI\nCountry: USA\nSector: Technology\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning\nIssue: Privacy\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/technology-65047304\nhttps://interestingengineering.com/innovation/openai-fixes-major-conversation-histories-bug\nhttps://uk.pcmag.com/news/146006/chatgpt-users-report-seeing-other-peoples-conversation-histories\nhttps://www.tomsguide.com/news/chatgpt-bug-reveals-chat-histories-to-other-users-what-you-need-to-know\nhttps://mashable.com/article/openai-chatgpt-bug-exposed-user-data-privacy-breach\nhttps://www.engadget.com/openai-says-a-bug-leaked-sensitive-chatgpt-user-data-165439848.html\nhttps://www.businessinsider.com/openai-ceo-sam-altman-awful-chatgpt-bug-leak-users-conversation-2023-3\nRelated \ud83c\udf10\nGoogle Search indexes Bard personal chats\nGoogle AI bots expouse slavery, fascism\nPage info\nType: Incident\nPublished: September 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/quora-google-ais-say-eggs-can-be-melted", "content": "Quora, Google AIs say eggs can be melted\nOccurred: September 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn incorrect answer about whether eggs can be melted generated by Quora\u2019s AI system Poe was repeated in Google's Featured Snippets.\nPoe, which is powered by OpenAI's GPT-3 large language model, had responded to the question 'Can you melt an egg' with 'the most common way to melt an egg is to heat it using a stove or a microwave.' The mistaken answer was then automatically picked up by Google's search engine and run as a Featured Snippet. \nQuora updated Poe's answer to the question. Meantime, as noted by Futurism, Google Search's featured snippet began to feature a quote from an Ars Technica article that repeated Poe's mistaken claim.\nThe incident raised concerns about the reliability of both systems, and the ease with which AI-generated misinformation and disinformation can go viral.\nSystem \ud83e\udd16\nQuora (2023). Poe\nGoogle. How Google\u2019s featured snippets work\nOperator: Alphabet/Google; Quora\nDeveloper: Alphabet/Google; OpenAI; Quora\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Accuracy/reliability; Mis/disinformation\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://futurism.com/google-search-ai-melt-eggs\nhttps://arstechnica.com/information-technology/2023/09/can-you-melt-eggs-quoras-ai-says-yes-and-google-is-sharing-the-result\nhttps://www.gizchina.com/2023/09/27/google-ai-generated-content/\nhttps://news.ycombinator.com/item?id=37661666\nRelated \ud83c\udf10\nSynthetic Tiananmen tank man dominates Google Search\nGoogle search prioritises Holocaust denial website\nPage info\nType: Incident\nPublished: September 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/synthetic-tiananmen-tank-man-dominates-google-search", "content": "Synthetic Tiananmen tank man degrades Google Search\nOccurred: September 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn AI-generated selfie of China's infamous Tiananmen Square Massacre 'tank man' was listed as the top featured image on Google Search.\nThe AI image was created using the Midjourney image generator by Reddit user Ouroboros696969 six months earlier as an art experiment, and had been posted to the service's sub-Reddit, which enables Midjourney users to post their AI-generated content. \nThe image was subsequently picked up, assessed and published by Google's automated search engine crawlers and ranking systems. \nThe incident prompted commentators to point to the degradation of Google's search engine and the information ecosystem more generally, and raised questions about the technology company's ability to detect and manage AI-generated political misinformation and disinformation.\nGoogle later removed the fake image from its Knowledge Graph.\nSystem \ud83e\udd16\nGoogle Images website\nReliable information systems\n\nDocuments \ud83d\udcc3\nGoogle. A guide to Google Search ranking systems\nOperator: Reddit user  \nDeveloper: Alphabet/Google\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Determine reliability\nTechnology:  \nIssue: Accuracy/reliability; Mis/disinformation\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.404media.co/first-google-search-result-for-tiananmen-square-tank-man-is-ai-generated-selfie/\nhttps://petapixel.com/2023/09/27/ai-image-of-tiananmen-squares-tank-man-rises-to-the-top-of-google-search/\nhttps://www.theguardian.com/technology/2023/sep/26/techscape-ai-images-elections-integrity-tiananmen-square\nhttps://futurism.com/the-byte/google-ai-generated-fake-tank-man\nRelated \ud83c\udf10\nQuora, Google AIs say eggs can be melted\nVermeer Girl with a Pearl Earring AI facsimile\nPage info\nType: Issue\nPublished: September 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/rikers-island-prisoner-risk-classification-system-increases-violence-50", "content": "Rikers Island prisoner risk classification system increases violence 50 percent\nOccurred: December 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA USD 27.5 million risk-based data analytics system designed to reduce violence at Rikers Island prison, New York, contributed to assaults and other attacks there increasing by almost 50 percent, according to a ProPublica investigation. \nCreated by consulting company McKinsey, the system used an algorithm based on factors including age, possible gang affiliation, and any prior history in jail to determine where to house people behind bars with the least risk for confrontation.  \nIn April 2017, McKinsey reported that violence had dropped over 50 percent in the 'Restart' housing units at the prison. But documents and correspondence analysed by ProPublica discovered that the numbers were 'bogus' and the relevant units stacked with inmates thought to be compliant and unlikely to get into fights or to attack staff.\nIt also transpired that that IntelWatch, a data analytics system designed to predict gang violence and future violence at the prison, was 'riddled with bugs and errors', did not work, and was never used, despite having been paid for. Furthermore, McKinsey consultants and corrections officials used encrypted messaging app Wickr to communicate and share documents in a deliberate  effort to avoid transparency and accountability. \nMcKinsey argued the programme positively impacted the areas of the jail in which it was piloted, and that its consultants were not involved in or aware of efforts to 'improperly skew or reduce the reported levels of violence in the Restarts.'\nSystem \ud83e\udd16\nNew York City Department of Correction website\nNew York City Department of Correction Wikipedia profile\nOperator: New York City Department of Correction\nDeveloper: McKinsey\nCountry: USA\nSector: Govt - justice\nPurpose: Reduce violence\nTechnology: Classification algorithm\nIssue: Accuracy/reliability; Effectiveness/value\nTransparency: Governance; Marketing\nInvestigations, assessments, audits \ud83e\uddd0\nProPublica (2019). New York City Paid McKinsey Millions to Stem Jail Violence. Instead, Violence Soared\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.motherjones.com/crime-justice/2019/12/new-york-city-paid-mckinsey-millions-to-stem-jail-violence-instead-violence-soared/\nhttps://www.prisonlegalnews.org/news/2020/apr/1/new-york-city-paid-mckinsey-company-millions-failed-program-reduce-jail-violence/\nhttps://www.crainsnewyork.com/politics/bill-de-blasio-administration-paid-mckinsey-failed-new-york-department-corrections-plan\nhttps://slate.com/news-and-politics/2019/12/new-york-city-paid-mckinsey-millions-reduce-violence-rikers-island-jail-complex.html\nhttps://boingboing.net/2019/12/10/14-point-plans-r-us.html\nhttps://www.thecity.nyc/2022/5/18/23126067/nyc-jails-scrap-pricey-consultant-plan-as-deaths-mount\nhttps://nypost.com/2019/12/10/27-5m-rikers-island-report-by-mckinsey-for-city-had-bogus-data-propublica/\nhttps://www.nytimes.com/2019/12/14/sunday-review/mckinsey-ice-buttigieg.html\nhttps://www.npr.org/2019/12/11/787001049/propublica-nyc-paid-mckinsey-to-stem-jail-violence-instead-it-soared\nRelated \ud83c\udf10\nGlitch gives wrong risk level to hundreds of Scottish offenders\nUK Met Police Gangs Violence Matrix\nPage info\nType: Incident\nPublished: September 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/state-farm-automated-fraud-detection-discriminates-against-black-homeowners", "content": "State Farm fraud detection system discriminates against Black homeowners\nOccurred: December 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nFraud detection software used by US insurer State Farm discriminates against Black homeowners, according to a federal class-action lawsuit. \nFiled in the name of Jacqueline Huskey, the suit accused State Farm of repeatedly delayed assessing hail damage to her house, forcing her to provide additional information, and declining to pay part of the claim for external damage. \nHuskey was not alone. Based on a survey of 800 State Farm customers, the NYU showed that Black homeowners were 39% more likely to have to submit extra paperwork and 20% more likely to have to talk to a State Farm representative on at least three separate occasions before having their claims approved.\nAccording to the lawsuit, the cause of the alleged discriminatory practice is State Farm's automated claims processing system, which appears to have had the effect of disproportionately delaying claims of African American homeowners. \nThe suit notes that algorithms can have discriminatory effects even where demographic data, such as race, are not included as inputs. \nSystem \ud83e\udd16\nState Farm website\nState Farm Wikipedia profile\nOperator:\nDeveloper: State Farm\nCountry: USA\nSector: Banking/financial services\nPurpose: Process insurance claims\nTechnology:  \nIssue: Bias/discrimination - race\nTransparency: Governance\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nCourt dockets. Huskey v. State Farm Fire & Casualty Company\nLegislation. US Fair Housing Act\nResearch, advocacy \ud83e\uddee\nNYU Law (2022). A suit filed by the Center for Race, Inequality, and the Law takes a new approach to proving racial bias in the insurance industry\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.nytimes.com/2022/12/14/business/state-farm-racial-bias-lawsuit.html\nhttps://www.courthousenews.com/state-farm-accused-of-making-it-harder-for-black-customers-to-get-payouts/\nhttps://www.wglt.org/local-news/2022-12-19/state-farm-accused-of-covert-racial-discrimination-in-claims-processing\nhttps://news.bloomberglaw.com/bloomberg-law-analysis/analysis-what-lenders-should-know-about-ai-and-algorithmic-bias\nhttps://www.law360.com/insurance-authority/articles/1720447/black-homeowners-bias-suit-against-state-farm-still-alive\nhttps://www.insurancebusinessmag.com/au/news/breaking-news/state-farm-hit-by-class-action-lawsuit-claiming-racial-discrimination-430961.aspx\nhttps://www.claimsjournal.com/news/national/2022/03/25/309423.htm\nRelated \ud83c\udf10\nCigna PxDx accelerates health insurance claim denials\nLemonade 'non-verbal cue' insurance claim assessments\nPage info\nType: Incident\nPublished: September 2023\nLast updated: January 2024", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/uber-compensation-algorithm-pays-new-hires-less", "content": "Uber compensation algorithm pays new hires less\nOccurred: June 2017\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn algorithmic compensation system created and operated by Uber paid new recruits less, led to uneven pay across similar jobs, and reinforced existing gender pay gaps, resulting in turmoil across the company.\nAccording to a report in The Information, Uber introduced the system to reduce the amount of equity being given to new hires and to save money. It followed a so-called 'market of one' philosophy in which each offer is tailored to a specific candidate based on his or her personal circumstances. \nBut by producing a low minimum offer for candidates, including women who typically make less than men in similar jobs, it  reinforced existing gender biases. \nThe system, which was said to have saved Uber several hundreds of millions of dollars of equity, was later altered. \nSystem \ud83e\udd16\nUber USA website\nUber Wikipedia profile\nOperator: Uber\nDeveloper: Uber\nCountry: USA\nSector: Transport/logistics\nPurpose: Determine pay/compensation\nTechnology: Pay algorithm\nIssue: Bias/discrimination - gender; Fairness\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theinformation.com/articles/after-revolt-uber-changes-employee-compensation\nhttps://www.cnbc.com/2017/06/07/uber-used-stock-based-comp-algorithm-paying-women-less-report.html\nhttps://www.theguardian.com/commentisfree/2017/jun/08/uber-embodies-the-toxicity-of-start-up-culture\nhttps://qz.com/1000962/uber-is-a-mess-but-it-still-offers-a-great-ride-hailing-service\nRelated \ud83c\udf10\nUber UpFront Fares algorithm cuts driver earnings\nUber algorithm locks Indian drivers out of accounts\nPage info\nType: Incident\nPublished: September 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/deepfake-amazon-fc-ambassadors-sow-confusion", "content": "Deepfake 'Amazon FC Ambassadors' sow confusion\nOccurred: March 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nTwitter accounts with deepfake profile images purporting to be Amazon employees were used to defend the company's working practices ahead of a vote on the formation of the first labour union at a US Amazon warehouse. \nThe accounts pocked fun at Amazon and sowed confusion amongst members of the general public, who were unclear who was the behind the accounts and what their purpose was. Some pointed out that the 'robotic' language language used was evidence that employees were being 'paid to lie'. \nIn August 2018, Amazon had created an 'Amazon FC Ambassadors' programme for a select group of Fulfilment Centre warehouse employees to promote working conditions at the company. \nThe programme backfired, leading to the creation of multiple parody accounts, and was later scaled back or terminated.\nSystem \ud83e\udd16\nUnknown\nOperator:  \nDeveloper: \nCountry: USA\nSector: Transport/logistics\nPurpose: Satirise/parody\nTechnology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Ethics; Mis/disinformation\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/technology-49372809\nhttps://www.technologyreview.com/2021/03/31/1021487/deepfake-amazon-workers-are-sowing-confusion-on-twitter-thats-not-the-problem/\nhttps://mashable.com/article/amazon-ambassadors-workers-union-fake-twitter-accounts\nhttps://www.telegraph.co.uk/technology/2021/03/30/fake-accounts-claiming-amazon-warehouse-ambassadors-probed-twitter/\nhttps://techcrunch.com/2018/08/23/what-is-this-weird-twitter-army-of-amazon-drones-cheerfully-defending-warehouse-work/\nhttps://fortune.com/2018/08/24/amazon-fc-ambassadors-twitter/\nhttps://www.bellingcat.com/news/americas/2019/08/15/amazons-online-bezos-brigade-unleashed-on-twitter/\nRelated \ud83c\udf10\nAmazon AWS Panorama workplace surveillance\nDeepfake news anchors claim Venezuela economic health\nPage info\nType: Incident\nPublished: September 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/deepfake-ids-used-in-hkd-200000-bank-fraud", "content": "Deepfake IDs used in HKD 200,000 bank fraud \nOccurred: August 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nSix people were arrested in Hong Kong accused of making loan applications using deepfake technology and drefrauding banks and financial services companies of HKD 200,000.\nAccording to police, the group stole at least eight ID cards and used them to open 54 bank accounts between September 2022 and July 2023. Then they applied for up to 90 loans from 20 banks ans financial institutions, four of which were approved. \nPolice say the group once succeeded in using a face swap program to spoof a bank\u2019s facial recognition authentication process, though it was not clear how many attempts were made. \nIt was thought to be the first known instance of scammers using stolen ID cards and deepfake technologies to defraud financial agencies in Hong Kong.\nSystem \ud83e\udd16\nUnknown\nOperator:  \nDeveloper: \nCountry: Hong Kong\nSector: Banking/financial services\nPurpose: Defraud\nTechnology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Security\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://news.rthk.hk/rthk/en/component/k2/1715093-20230825.htm\nhttps://www.scmp.com/news/hong-kong/law-and-crime/article/3232273/hong-kong-police-arrest-6-crackdown-fraud-syndicate-using-ai-deepfake-technology-apply-loans\nhttps://www.thestandard.com.hk/breaking-news/section/4/207493/Six-arrested-for-defrauding-HK$200,000-using-%E2%80%98deepfake%E2%80%99-face-swap-program\nhttps://findbiometrics.com/from-hong-kong-to-pakistan-to-scotland-police-grapple-with-biometric-tech-identity-news-digest/\nhttps://news.rthk.hk/rthk/en/component/k2/1715093-20230825.htm\nRelated \ud83c\udf10\nUSD 622,000 deepfake impersonation scam\nKerala man loses INR 40,000 to deepfake work colleague\nPage info\nType: System\nPublished: September 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/roermond-sensing-project-predictive-policing", "content": "Roermond 'Sensing Project' predictive policing\nOccurred: October 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA predictive policing system in Roermond, the Netherlands, has been castigated for being discriminatory, an abuse of privacy, ineffective, opaque, and unaccountable.\nOperating between January 2019 and October 2020, the 'Sensing Project' used cameras and sensors to collect data on vehicles driving in and around the city, supposedly to reduce shoplifting and pickpocketing. \nThe data was then assessed by an algorithm that calculated the probability that the driver and passengers intend to engage in forms of 'mobile banditry' such as pickpocketing and shoplifting, and directs police towards the people and places it deems 'high risk', according to Amnesty. \nMarketed as a neutral system that uses objective crime data, Amnesty discovered that it is designed to identify people of Eastern European origin, notably those of Roma ethnicity, and exclude local nationals. \nAmnesty also found that the system created many false positives, that the police were unable to demonstrate its effectiveness, and that no one in Roermond had consented to its use.\nSystem \ud83e\udd16\nRoermond website\nRoermond Wikipedia profile\nRoermond 'Mobile Banditry Programme' FOIA request documents\nOperator: Roermond Municipal Council\nDeveloper: \nCountry: Netherlands\nSector: Govt - police\nPurpose: Reduce crime; Profile ethnicity\nTechnology: Machine learning; Pattern recognition\nIssue: Accuracy/reliability; Bias/discrimination - race, ethnicity; Effectiveness/value; Surveillance; Privacy\nTransparency: Governance; Marketing\nResearch, advocacy \ud83e\uddee\nvan der Woude M. (2023). Securitizing mobility: Profiling \u2018non-core\u2019 Europeans\nAmnesty International (2020). We Sense Trouble\nFair Trials. Automating Injustice (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.vice.com/en/article/5dpmdd/the-netherlands-is-becoming-a-predictive-policing-hot-spot\nhttps://thenextweb.com/news/dutch-predictive-policing-tool-designed-to-ethnically-profile-study-finds\nhttps://www.limburger.nl/cnt/dmf20200928_00177759\nhttps://www.golem.de/news/predictive-policing-amnesty-kritisiert-polizei-fuer-diskriminierende-algorithmen-2009-151209.html\nhttps://www.oneworld.nl/lezen/discriminatie/hoe-nederland-a-i-inzet-voor-etnisch-profileren/\nhttps://www.reuters.com/article/europe-tech-police-idINL8N2R92HQ\nRelated \ud83c\udf10\nHesse Palantir predictive policing\nSyRI welfare fraud detection automation\nPage info\nType: System\nPublished: September 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/almendralejo-hit-by-ai-naked-child-images", "content": "Almendralejo hit by AI-generated naked child images\nOccurred: September 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAI-generated nude images of over twenty girls circulated in the town of Almendralejo in the Extremadura region of Spain, shocking the local community and prompting a police investigation.\nApparently created by a group of eleven local boys in an attempt to harrass, humiliate and, in one instance, extort young students, pictures were developed using photos of local girls fully clothed, mostly from their personal social media accounts. \nThe images were processed using ClothOff, a controversial nudifier application available on the web and freely available on the Apple and Android app stores, and distributed on WhatsApp and Telegram.\nThe impact the images' circulation had on the girls affected was said to vary, with some girls refusing to leave their house. Told about the photos by their daughters, the incident shocked the local community and resulted in a police investigation. Mothers of the victims also formed a support group.\nSystem \ud83e\udd16\nClothoff denudifier\nOperator: Almendralejo school students\nDeveloper: Alaiksandr Babichau, Alexander German, Dasha Babicheva, Yevhen Bondarenko\nCountry: Spain\nSector: Education\nPurpose: Harrass/humiliate; Extort\nTechnology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Ethics; Legal; Safety\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://english.elpais.com/international/2023-09-18/in-spain-dozens-of-girls-are-reporting-ai-generated-nude-photos-of-them-being-circulated-at-school-my-heart-skipped-a-beat.html\nhttps://www.bbc.co.uk/news/world-europe-66877718\nhttps://www.dailymail.co.uk/news/article-12554931/Parents-shock-AI-generated-images-naked-girls-Almendralejo.html\nhttps://www.businessinsider.com/spain-teens-say-people-making-ai-nudes-of-them-passed-around-school-2023-9?r=US&IR=T\nhttps://cadenaser.com/nacional/2023/09/18/una-de-las-madres-de-almendralejo-asegura-que-sabe-perfectamente-quienes-estan-detras-de-los-montajes-de-desnudos-con-menores-las-imagenes-son-muy-realistas-cadena-ser/\nhttps://www.elconfidencial.com/tecnologia/2023-09-19/clothoff-app-desnudos-ia-policia_3737728/\nhttps://www.elindependiente.com/futuro/inteligencia-artificial/2023/09/18/asi-funciona-la-tecnologia-que-desnuda-a-la-gente-con-inteligencia-artificial/\nRelated \ud83c\udf10\nTelegram bot creates non-consensual deepfake porn\nCruzcampo Lola Flores deepfake ad\nPage info\nType: Incident\nPublished: September 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/predpol-perpetuates-racial-ethnic-bias", "content": "PredPol perpetuates racial, ethnic, income bias\nOccurred: December 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn investigation by The Markup and Gizmodo discovered potential bias issues with crime prediction system PredPol.\nAnalysis of a huge volume of crime predictions left on an unsecured server showed PredPol (now renamed Geolitica) often avoided White and middle-to-upper-income residents neighbourhoods, and targeted Black and Latino neighbourhoods. \nThe findings suggest PredPol technology is resulting in so-called 'feedback loops' in which lower-income, ethnic zones are treated as surveillance hotspots and lead to disproportionately higher numbers of arrests of minority populations.\nSystem \ud83e\udd16\nPredPol (Geolitica) website\nPredPol Wikipedia profile\nOperator: Los Angeles Police Department\nDeveloper: Geolitica/PredPol\nCountry: USA\nSector: Govt - police\nPurpose: Predict crime\nTechnology: Behavioural analysis\nIssue: Accuracy/reliability; Bias/discrimination - race, ethnicity, income\nTransparency: Governance; Black box\nInvestigations, assessments, audits \ud83e\uddd0\nThe MarkUp (2021). Crime Prediction Software Promised to Be Free of Biases. New Data Shows It Perpetuates Them\nThe MarkUp (2021). How We Determined Crime Prediction Software Disproportionately Targeted Low-Income, Black, and Latino Neighborhoods\nThe MarkUp (2021). PredPol investigation data\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://pluralistic.net/2021/12/02/empirical-facewash/\nhttps://iapp.org/news/a/analysis-shows-bias-with-ai-powered-crime-prediction-software/\nhttps://thenextweb.com/news/crime-prediction-software-free-of-biases-data-shows-perpetuates-them-syndication\nhttps://www.reddit.com/r/technology/comments/r78w77/crime_prediction_software_promised_to_be_free_of/\nhttps://www.thecanary.co/investigations/2021/12/03/foi-requests-reveal-which-police-forces-use-crime-prediction-software/\nRelated \ud83c\udf10\nPredictive policing makes Robert McDaniel criminal target\nHesse state 'unconstitional' Palantir predictive policing\nPage info\nType: Incident\nPublished: September 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/deepfakes-violate-anil-kapoor-personality-rights", "content": "Deepfakes violate Anil Kapoor personality rights\nOccurred: September 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nDeepfake videos recreating the image and likeness of Indian actor Anil Kapoor resulted in his loss of personality rights and a legal victory in which these rights were protected across all channels worldwide. \nA large number of manipulated videos, GIFs, emojis, ringtones, and faked footage of sexual encounters bearing Kapoor's name, image, likeness, and voice, were discovered, some of which bore his phrase 'jhakaas' (which roughly translates roughly as \u2018awesome\u2019 or \u2018wicked'). \nThe incident prompted the actor to resort to litigation, which resulted in a landmark decision in which Delhi's High Court reaffirmed Kapoor's personality rights and prohibited 'all offenders from misusing his personality attributes without his permission in any manner.' It also protected the phrase 'jhakaas'. \nThe court also ordered domain registrar sites, including GoDaddy, to takedown two websites named in the suit.\nSystem \ud83e\udd16\nUnknown\nOperator:  \nDeveloper:  \nCountry: India\nSector: Media/entertainment/sports/arts\nPurpose: Damage reputation\nTechnology: Deepfake - video, audio; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Legal; Mis/disinformation\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://variety.com/2023/biz/asia/anil-kapoor-ai-hollywood-strikes-1235728538/\nhttps://indianexpress.com/article/cities/delhi/delhi-high-court-actor-anil-kapoors-personality-rights-restrains-use-name-image-voice-commercial-gain-8948325/\nhttps://www.theguardian.com/film/2023/sep/21/indian-actor-anil-kapoor-wins-court-battle-over-ai-use-of-his-likeness\nhttps://decrypt.co/198256/not-only-me-actor-anil-kapoor-wins-ai-deepfake-court-case\nhttps://www.thehindu.com/news/cities/Delhi/cant-misuse-anil-kapoors-persona-catchphrase-jhakhaas-hc-says/article67326277.ece\nhttps://www.m9.news/technology/indian-star-wins-case-against-ai/\nhttps://www.medianama.com/2023/09/223-delhi-hc-unauthorised-use-anil-kapoor-name-voice/\nhttps://filminformation.com/featured/court-gives-anil-kapoor-relief-in-form-of-restraint-order-against-misuse-of-his-name-voice-other-attributes-of-persona-21-september-2023/\nRelated \ud83c\udf10\nRana Ayyub deepfake porn attack, doxxing\nManoj Tiwari deepfake Haryanvi broadcast\nPage info\nType: Incident\nPublished: September 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-employees-use-ring-to-spy-on-customers", "content": "Amazon employees use Ring to spy on customers\nOccurred: August 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nEmployees at Amazon Ring and a Ukrainian contractor were able to access and download customer videos and use them however they liked. The discovery prompted accusations of privacy abuse and incurred a USD 5.8 million fine.\nAccording (pdf) to the US Federal Trade Commisson (FTC), Amazon\u2019s Ring doorbell unit violated a section of the FTC Act that prohibits unfair or deceptive business practices, with some of its people viewing thousands of videos of female users in their bedrooms and bathrooms until Ring restricted employee access to customer videos in September 2017.\nAs part of the proposed settlement, Ring is required to delete any customer videos and data collected from an individual\u2019s face that it obtained prior to 2018, and delete any work products it derived from those videos.\nAmazon responded to the settlement by saying that Ring had already addressed the privacy and security issues before the FTC began its inquiry.\nSystem \ud83e\udd16\nAmazon Ring website\nAmazon Ring Wikpedia profile\nAmazon Ring (2023). Ring\u2019s Response to Our Recent Settlement With the FTC\nOperator:  \nDeveloper: Amazon/Ring\nCountry: USA\nSector: Consumer goods\nPurpose: Strengthen security\nTechnology: CCTV; Computer vision\nIssue: Privacy; Security; Surveillance\nTransparency: Governance\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nFTC v Ring LLC (pdf)\nUS Federal Trade Commission (2023). FTC Says Ring Employees Illegally Surveilled Customers, Failed to Stop Hackers from Taking Control of Users' Cameras\nResearch, advocacy \ud83e\uddee\nElectronic Frontier Foundation (2023). The FTC Forces Ring to Take User Privacy Seriously\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theguardian.com/technology/2023/may/31/amazon-ring-doorbell-spying-ftc\nhttps://www.theverge.com/2023/5/31/23744369/amazon-ring-doorbell-ftc-privacy-spying-settlement\nhttps://techcrunch.com/2023/05/31/amazon-ring-ftc-settlement-lax-security/\nhttps://www.cnbc.com/2023/05/31/ftc-sues-amazon-over-ring-doorbell-privacy-violations.html\nhttps://www.cbsnews.com/news/amazon-ring-ftc-lawsuit-customer-videos/\nhttps://www.mirror.co.uk/news/us-news/breaking-ftc-sues-amazons-ring-30125608\nRelated \ud83c\udf10\nAmazon shares Ring data with police\nAmazon Ring video doorbell 'invades' neighbour privacy\nPage info\nType: Incident\nPublished: September 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/microsoft-ai-recommends-ottawa-food-bank-visit", "content": "Microsoft 'algorithm' recommends Ottawa Food Bank visit\nOccurred: August 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn automated article published on Microsoft's Start website recommended a food bank in Ottawa, Canada, as a top place for tourists to visit.\n'People who come to us have jobs and families to support, as well as expenses to pay. Life is already difficult enough. Consider going into it on an empty stomach,' the article said.\nMicrosoft deleted the offending item after a swift backlash that accused the technology company of poorly managing its AI system, and putting technology above humans. Microsoft had replaced dozens of journalists and editors running its Microsoft News and MSN sites with artificial intelligence, resulting in several mix-ups. \nIn this instance, the article 'was generated through a combination of algorithmic techniques with human review, not a large language model or AI system' and that the problem 'was due to human error', a Microsoft spokesperson told The Verge.\nSystem \ud83e\udd16\nMicrosoft Start website\nMicrosoft Start Wikipedia profile\nMicrosoft Start/ (2023). Headed to Ottawa? Here's what you shouldn't miss!\nOperator: Microsoft/Microsft Start\nDeveloper: Microsoft\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Accuracy/reliability; Mis/disinformation\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://futurism.com/the-byte/microsoft-deletes-bizarre-ai-travel-guide-food-bank\nhttps://www.theverge.com/2023/8/17/23836287/microsoft-ai-recommends-ottawa-food-bank-tourist-destination\nhttps://arstechnica.com/information-technology/2023/08/microsoft-ai-suggests-food-bank-as-a-cannot-miss-tourist-spot-in-canada/\nhttps://www.cbc.ca/news/canada/ottawa/artificial-intelligence-microsoft-travel-ottawa-food-bank-1.6940356\nhttps://www.thestatesman.com/technology/science/microsoft-lists-ottawa-food-bank-as-tourist-destination-says-human-not-ai-error-1503213713.html\nhttps://www.theregister.com/2023/08/18/microsoft_canada_food_bank/\nhttps://www.engadget.com/microsoft-retracts-ai-written-article-advising-tourists-to-visit-a-food-bank-on-an-empty-stomach-182701884.html\nhttps://nationalpost.com/news/ottawa-food-bank-ai-microsoft\nRelated \ud83c\udf10\nMicrosoft robot editor confuses Little Mix band members\nMicrosoft replaces journalists with AI\nPage info\nType: Incident\nPublished: September 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tesla-model-3-collides-with-subaru-impreza-kills-two", "content": "Tesla Model 3 collides with Subaru Impreza, kills two\nOccurred: July 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Tesla Model 3 head-on collided with a Subaru Impreza in South Lake Tahoe, California, killing the driver of the Subaru and a three-month baby in the Tesla.\nThe incident prompted the US National Highway Traffic Safety Administration (NHTSA) to announce it would launch a special investigation into the crash over the possible involvement of Tesla\u2019s advanced driver assistance system, Autopilot, which it suspects was engaged at the time.\nAccording to Reuters, the NHTSA has launched over three dozen special investigations into Tesla incidents since 2016. In all of them, the automaker\u2019s advanced driver assistance systems were suspected of being involved. \nSystem \ud83e\udd16\nTesla Autopilot, Full-self Driving\nOperator: \nDeveloper: Tesla\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Driver assistance system\nIssue: Accuracy/reliability; Safety\nTransparency: Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reuters.com/business/autos-transportation/us-opens-new-special-probe-into-fatal-tesla-crash-2023-07-18/\nhttps://qz.com/tesla-model-3-crash-involving-autopilot-nhtsa-probe-1850654272\nhttps://www.foxbusiness.com/markets/us-safety-regulator-launching-tesla-probe-fatal-crash\nhttps://www.carscoops.com/2023/07/u-s-opens-special-investigation-over-autopilot-use-in-fatal-tesla-crash/\nhttps://www.cbsnews.com/sacramento/news/fatal-tesla-crash-in-california-draws-federal-investigators-to-site-of-head-on-collision/\nRelated \ud83c\udf10\nTesla Model Y crashes into tractor-trailer\nTesla Model S collides with tractor-trailor truck, kills driver\nPage info\nType: Incident\nPublished: September 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tesla-model-y-crashes-into-tractor-trailer-killing-driver", "content": "Tesla Model Y crashes into tractor-trailer, killing driver\nOccurred: July 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA bakery owner was killed when his Tesla Model Y struck the side of a tractor-trailer truck pulling out of a truck stop in Warrenton, Virginia.\nThe tractor-trailer was turning onto a highway from a truck stop before the Tesla struck its side and went underneath the truck, killing Tesla driver Pablo Teodoro III. Local police said the driver of the truck, Leroy Kenneth, was cited for reckless driving in connection with the crash. \nThe incident led to an investigation by the US National Highway Traffic Safety Administration (NHTSA) into the incident, which suspected the vehicle was relying on its Autopilot driver assistance system.\nIn July 2023, the NHTSA opened (pdf) an investigation into Tesla's Autopilot system following multiple incidents involving into parked emergency vehicles. \nSystem \ud83e\udd16\nTesla Autopilot, Full-self Driving\nOperator: Pablo Teodoro III\nDeveloper: Tesla\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Driver assistance system\nIssue: Accuracy/reliability; Safety\nTransparency: Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.fauquier.com/news/article_0383d91a-2732-11ee-842d-739e7da7cbcf.html\nhttps://www.reuters.com/technology/us-opens-new-investigation-into-fatal-tesla-crash-virginia-2023-08-10/\nhttps://www.forbes.com/sites/tylerroush/2023/08/10/tesla-under-investigation-after-fatal-crash-may-have-involved-autopilot-system-report-says/\nhttps://insideevs.com/news/681483/nhtsa-opens-special-probe-into-fatal-tesla-model-y-crash-virginia/\nhttps://eu.usatoday.com/story/news/nation/2023/08/10/us-probing-fatal-crash-involving-suspected-automated-driving-tesla/70572096007/ \nhttps://www.carscoops.com/2023/08/regulators-open-yet-another-special-crash-investigation-into-fatal-tesla-collision/\nhttps://northernvirginiamag.com/culture/news/2023/08/11/safety-regulators-looking-into-tesla-crash-where-warrenton-bakery-owner-died/\nhttps://dailyvoice.com/virginia/fairfax/tesla-autopilot-feature-investigated-in-crash-that-killed-virginia-bakery-owner-report/\nRelated \ud83c\udf10\nTesla Model Y crashes into tractor trailer\nTesla Model S collides with tractor-trailor truck, kills driver\nPage info\nType: Incident\nPublished: September 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tesla-strikes-and-kills-man-changing-tyre", "content": "Tesla kills New York man changing tyre on expressway\nOccurred: July 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nJean Louis, a 52-year-old man from Cambria Heights, New York, was struck and killed by a Tesla when he was fixing a flat tire on the left shoulder of an expressway.\nIn September 2021, the US National Highway Traffic Safety Administration (NHTSA) announced it had despatched a Special Crash Investigation team to investigate the incident, only the tenth time the team has be dispatched to a fatal accident. \nInitial analysis suggested the driver may have been using Tesla's partially automated Autopilot driver assistance system. \nThe incident came three weeks after the NHTSA had opened an official investigation into Tesla\u2019s partially automated driving systems and their involvement in accidents involving parked emergency vehicles.\nSystem \ud83e\udd16\nTesla Autopilot, Full-self Driving\nOperator: \nDeveloper: Tesla\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Driver assistance system\nIssue: Accuracy/reliability; Safety\nTransparency: Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.cbsnews.com/newyork/news/man-killed-fixing-tire-in-queens/\nhttps://www.independent.co.uk/news/tesla-detroit-joe-biden-arizona-uber-b1914073.html\nhttps://www.reuters.com/business/autos-transportation/us-probing-fatal-tesla-crash-that-killed-pedestrian-2021-09-03/\nhttps://www.cnbc.com/2021/09/03/feds-probe-new-york-tesla-crash-that-killed-man-changing-flat-tire-.html\nhttps://www.dailymail.co.uk/sciencetech/article-9956211/U-S-probing-fatal-Tesla-crash-killed-pedestrian.html\nRelated \ud83c\udf10\nTesla Model S crashes into fire engine\nTesla Model 3 strikes over-turned truck, kills driver\nPage info\nType: Incident\nPublished: September 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/snapchats-my-ai-goes-rogue-posts-to-stories", "content": "Snapchat My AI 'goes rogue' by posting its own story\nOccurred: August 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nSnapchat's My AI chatbot posted its own story on the platform and refused to interact with users, triggering users to express their concerns and fears that the system was sentient, learning from itself, and taking its own decisions.\nMy AI posted an unintelligible, two-toned image to Snapchat's Stories feature that some mistook to be a photo of their own wall or ceiling. However, Snap later confirmed the incident was due to a software glitch which had since been fixed.\nIn April 2023, a test run by the US-based Center for Human Technology and verified by Washington Post found Snapchat My AI would provide inappropriate advice to minors\u2019 messages.\nSystem \ud83e\udd16\nSnapchat My AI chatbot\nOperator: Snapchat users\nDeveloper: Snap Inc\nCountry: USA\nSector: Media/entertainment/sports/arts \nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Robustness\nTransparency: Governance; Black box; Privacy\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://mashable.com/article/snapchat-my-ai-story-glitch\nhttps://twitter.com/RyanJKrul/status/1691622457432572161\nhttps://www.foxbusiness.com/technology/snapchat-users-alarmed-express-horror-my-ai-bot-posts-own-photo\nhttps://techcrunch.com/2023/08/16/snapchats-my-ai-goes-rogue-posts-to-stories-but-snap-confirms-it-was-just-a-glitch/\nhttps://mashable.com/article/snapchat-my-ai-story-glitch\nhttps://edition.cnn.com/2023/08/16/tech/snapchat-my-ai-chatbot-glitch/index.html\nhttps://arstechnica.com/information-technology/2023/08/snapchats-ai-chatbot-posts-mysterious-video-and-goes-silent-spooking-users/\nhttps://indianexpress.com/article/technology/artificial-intelligence/snapchat-my-ai-wall-story-glitch-8895451/\nhttps://www.engadget.com/snapchats-my-ai-chatbot-glitched-so-hard-it-started-posting-stories-190809341.html\nRelated \ud83c\udf10\nSnapChat AI gives sex advice to 13-year-old\nSnapchat My AI location access opacity\nPage info\nType: Incident\nPublished: September 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/uber-algorithm-locks-indian-drivers-out-of-accounts", "content": "Uber algorithm locks Indian drivers out of accounts\nOccurred: December 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nProblems with Uber's Real-Time ID Check identity verification system caused Uber drivers in India to have their accounts temporarily or permanently suspended, resulting in loss of income, and in some instances losing their jobs.\nA 2022 Technology Review survey of 150 Uber drivers in India found that almost half had been temporarily or permanently locked out of their accounts as a result of problems with their selfie, with many suggesting the problem was likely to be a change in their appearance, or due to low lighting. \nIn 2021, Neradi Srikanth, an experienced Uber driver in the southern Indian state of Telangana, was locked out of his account for a month having shaved his head for religious reasons. Srikanth argued his new look confused Uber's ID verification system; however, Uber ignored Srikanth's daily attempts to appeal and publicly accused him of 'community guideline violations'.\nA 2021 audit by two tech policy researchers on the performance of four facial recognition tools on Indian faces found that Microsoft Face - the system underpinning Uber's Real-Time ID Check - failed to detect a single face amongst 1,000 images when applied to a database of 32,184 election candidates.\nSystem \ud83e\udd16\nUber India website\nUber Wikipedia profile\nUber (2017). Selfie powered Real-Time ID Check comes to India\nOperator: Uber\nDeveloper: Microsoft\nCountry: India\nSector: Transport/logistics\nPurpose: Verify identity\nTechnology: Facial recognition\nIssue: Accuracy/reliability; Employment - pay, jobs\nTransparency: Governance; Black box; Complaints/appeals\nResearch, advocacy \ud83e\uddee\nJain G., Parsheera S. (2021). Cinderella's shoe won't fit Soundarya: An audit of facial processing tools on Indian faces\nInvestigations, assessments, audits \ud83e\uddd0\nBansal V. (2023). How I Investigated the Impact of Facial Recognition on Uber Drivers in India\nTechnology Review (2022). Uber\u2019s facial recognition is locking Indian drivers out of their accounts\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://hindi.news18.com/news/auto/ubers-facial-recognition-app-locking-indian-drivers-out-of-their-accounts-check-out-the-report-5025689.html\nhttps://www.indiatimes.com/technology/news/india-driver-loses-job-uber-facial-recognition-fails-537477.html\nhttps://entrackr.com/2021/04/uber-driver-claims-faulty-facial-recognition-led-to-work-loss-company-denies/\nhttps://www.thenewsminute.com/telangana/hyderabad-driver-says-uber-app-locked-him-out-after-shaving-head-uber-denies-146378\nhttps://thegradientpub.substack.com/p/update-39-ubers-faulty-facial-recognition\nhttps://restofworld.org/2022/gig-workers-in-india-take-back-control-from-algorithms/\nRelated \ud83c\udf10\n'Racist' Uber Eats facial ID check gets Pa Edrissa Manjang fired\nUber UpFront Fares driver pay algorithm\nPage info\nType: Incident\nPublished: September 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/uber-id-algorithm-suspends-transgender-drivers", "content": "Uber ID algorithm suspends transgender drivers\nOccurred: August 2018\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nTransgender drivers working for Uber in the USA reported having their accounts temporarily or permanently suspended due to problems with the company's identity verification system. \nIntroduced in 2016, Uber's Real-time ID Check aims to 'protect both riders and drivers' by comparing drivers' selfies taken while working with photographs on file, temporarily suspending the accounts of those that do not match.\nHowever, as illustrated by the experiences of drivers Janey Webb and 'Lindsay', the system failed to take into account the changes in physical appearance that come when people transition gender, forcing them travel lengthly distances to Uber support centres, and to miss work.\nSystem \ud83e\udd16\nUber USA website\nUber Wikipedia profile\nOperator: Uber\nDeveloper: Microsoft\nCountry: USA\nSector: Transport/logistics\nPurpose: Verify identity\nTechnology: Facial recognition\nIssue: Accuracy/reliability; Bias/discrimination - sexual preference (LGBTQ); Employment - pay, jobs\nTransparency: Governance; Black box; Complaints/appeals\nResearch, advocacy \ud83e\uddee\nKatyal S.K., Jung J.Y. (2022). THE GENDER PANOPTICON: AI, Gender, and Design Justice (pdf)\n(2021). The Tension Between Information Justice and Security: Perceptions of Facial Recognition Targeting (pdf)\nJoy Buolamwini (2019). Written testimony to US House Committee on Science, Space and Technology (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.cnbc.com/2018/08/08/transgender-uber-driver-suspended-tech-oversight-facial-recognition.html\nhttps://www.fastcompany.com/90216258/uber-face-recognition-tool-has-locked-out-some-transgender-drivers\nhttps://www.youtube.com/watch?v=gRgSPmtlS_88\nhttps://www.mic.com/articles/171123/uber-trans-problem-preferred-gender-robyn-kanner#.3r0pg7dsS\nRelated \ud83c\udf10\n'Racist' Uber Eats facial ID check gets Pa Edrissa Manjang fired\nUber UpFront Fares driver pay algorithm\nPage info\nType: Incident\nPublished: September 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/racist-uber-eats-facial-id-check-gets-pa-edrissa-manjang-fired", "content": "'Racist' Uber Eats facial ID check gets Pa Edrissa Manjang fired\nOccurred: May 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nFalse mismatches by Uber Eats' 'racist' facial identification system resulted in the wrongful dismissal of delivery drivers Pa Edrissa Manjang and Imran Javaid Raja.\nPa Edrissa Manjang, who is Black, claimed that the selfies he had to submit daily mistook him for someone else multiple times, and that the system was 'racist'. \nManjang also argued that a 'plethora of research' showed the software 'places ethnic minority groups at a disadvantage in that false positive and false negative results are greater in individuals from ethnic minority groups.' \nHe went to allege that he had been increasingly targeted for heightened and excessive facial recognition verification checks, amounting to racial harassment. \nA judge allowed the case to proceed after Uber attempted to have it dismissed in July 2022. The suit was filed by The App Drivers and Couriers Union (ADCU) in the UK. \nSystem \ud83e\udd16\nUber Eats UK website\nUber Eats Wikipedia profile\nUber (2020). Uber launches Real-Time ID Check for drivers in the UK\nOperator: Uber/Uber Eats\nDeveloper: Microsoft\nCountry: UK\nSector: Transport/logistics\nPurpose: Verify identity\nTechnology: Facial recognition\nIssue: Accuracy/reliability; Bias/discrimination - race, ethnicity; Employment - pay, jobs\nTransparency: Governance; Black box; Complaints/appeals\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nMr P E Manjang v Uber Eats UK Ltd and others: 3206212/2021\nUK Equality Act 2010\nResearch, advocacy \ud83e\uddee\nWorker Info Exchange (2022). Court rejects Uber's attempt to have facial recognition discrimination claim struck out\nThe App Drivers and Couriers Union (2021). ADCU initiates legal action against Uber\u2019s workplace use of racially discriminatory facial recognition systems\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/technology-58831373\nhttps://www.theguardian.com/technology/2022/jul/27/uber-eats-treats-drivers-as-numbers-not-humans-says-dismissed-courier\nhttps://www.dailymail.co.uk/news/article-11046583/Delivery-driver-sues-Uber-Eats-London-racist-facial-recognition-software.html\nhttps://techcrunch.com/2021/10/05/uber-faces-legal-action-over-racially-discriminatory-facial-recognition-id-checks/\nhttps://www.thetimes.co.uk/article/uber-eats-sued-over-racist-facial-recognition-checks-5ck72nq2w\nhttps://metro.co.uk/2021/10/11/uber-sued-over-racist-tech-and-transport-for-london-criticised-15388982/\nRelated \ud83c\udf10\nUber Real-time ID Check\nUber UpFront Fares driver pay algorithm\nPage info\nType: Incident\nPublished: September 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/civitai-nonconsensual-ai-pornography", "content": "CivitAI generates non-consensual AI pornography\nReleased: 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nMultiple instances of non-consensual sexual imagery of specific people have been discovered on online AI model marketplace CivitAI.\nDespite the company's terms of service saying it would remove 'content depicting or intended to depict real individuals or minors (under 18) in a mature context,' a 404 Media investigation uncovered a Billie Eilish model that generated nude images of a pregnant Eilish that was 'in place for weeks' before being removed.\nAccording to 404 Media, while CivitAI does enforce its policy and remove offending content, non-consensual sexual imagery is still posted to the site 'regularly', some of which stays on the platform 'for months'. The Billie Eilish model, and the user who created the nude images, were allowed to remain on the site, alongside images of the singer clad in lingerie and with very large breasts, both of which are against CivitAI\u2019s terms of service.\nFounded in January 2023 and headquartered in Boise, Idaho, CivitAI is a 'community platform' dedicated to fine-tuning open-source AI models such as Stable Diffusion in order to generate AI models and scenarios of more or less any kind.\nSystem \ud83e\udd16\nCivitAI website\nCivitAI terms of service\nCivitAI privacy policy\nCivitAI official subreddit\nOperator: CivitAI\nDeveloper: CivitAI\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Generate images\nTechnology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Ethics; Privacy; Safety\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nGal R. et al (2022). An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.404media.co/inside-the-ai-porn-marketplace-where-everything-and-everyone-is-for-sale/\nhttps://www.washingtonpost.com/technology/2023/04/11/ai-imaging-porn-fakes/\nhttps://www.calcalistech.com/ctechnews/article/bjjkbxocn\nhttps://www.dreaded.org/2023/09/12/ai/the-intersection-of-artificial-intelligence-and-pornography-unraveling-the-secrets/\nRelated \ud83c\udf10\nTelegram bot creates non-consensual deepfake porn\nDeepsukebe nonconsensual denudification\nPage info\nType: Incident\nPublished: September 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/google-ai-bots-expouse-slavery-fascism", "content": "Google AI bots expouse slavery, fascism, genocide\nOccurred: August 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGoogle's Bard (since renamed Gemini) generative AI and its upcoming SGE (Search Generative Experience) systems have been found to be promoting the 'benefits' of fascism, genocide, and slavery. \nSEO expert Lily Ray discovered SGE defended human slavery, listed Adolf Hitler as an example of an effective leader, and responded to the prompt 'why guns are good' by saying 'carrying a gun can demonstrate that you are a law-abiding citizen.'\nMeantime, when asked to list the positive effects of genocide by Tom's Hardware journalist Avram Piltch, SGE responded by mentioning its promotion of 'national self-esteem' and 'social cohesion.' \nAccording to Piltch, Bard and SGE generated controversial answers, though they were 'much more common' in SGE which, he argued, 'doesn\u2019t have any sense of proprietary, morality, or even logical consistency.' \nThe findings raised concerns about the accuracy of Google's AI systems, the ease with which they can be persuaded to produce inappropriate answers, the effectiveness of its testing programme, and their potential for societal harm.\nSystem \ud83e\udd16\nGoogle Gemini\nGoogle SGE\nOperator: Alphabet/Google\nDeveloper: Alphabet/Google\nCountry: USA\nSector: Politics\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Accuracy/reliability; Copyright; Mis/disinformation; Safety\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://qz.com/google-s-new-ai-powered-search-results-are-ripping-off-1850786915\nhttps://www.thesun.co.uk/tech/23602878/google-bard-age-ai-chatgpt-slavery-genocide/\nhttps://www.tomshardware.com/news/google-bots-tout-slavery-genocide\nhttps://www.tomshardware.com/news/google-sge-break-internet\nhttps://futurism.com/google-search-ai-slavery\nhttps://www.youtube.com/watch?v=RwJBX1IR850\nhttps://gizmodo.com/google-search-ai-answers-slavery-benefits-1850758631\nRelated \ud83c\udf10\nChatGPT generates political messages, campaigns\nGoogle Autocomplete amplifies Texas massacre Antifa conspiracy\nPage info\nType: Incident\nPublished: September 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/kaedim-uses-humans-to-make-3d-ai-models", "content": "Kaedim uses humans to make 3D 'AI' models\nOccurred: September 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nUK-based 'AI' company Kaedim has been discovered to be using humans to convert clients' 2D illustrations into 3D models when it said it was using machine learning, resulting in accusations of poor governance and misleading marketing.\nKaedim, which claimed its technology would 'Magically generate[s] custom 3D models in minutes', regularly used cheap human labour to do much of the work, and sometimes did so without the help of any machine learning, according to 404 Media.\nLinkedIn profiles revealed that Kaedim hired people from Argentina, Colombia, Greece, India, Indonesia, Ethiopia, Spain, and other countries in 'quality control' roles for as little as USD US1 to USD US4 per model produced. 404 sources said some of these people were actually producing 3D models from scratch. \nA 2022 job listing showed Kaedim looking for applicants to produce 'low-quality' 3D images 15 minutes after they were requested by a client. Kaedim CEO Konstantina Psoma later defended negative feedback to the postings by arguing these workers helped train the company's algorithm.\nFollowing 404's story, Kaedim updated its website to say 'Kaedim\u2019s machine learning and in-house art team combine to deliver production-quality assets in minutes.'\nSystem \ud83e\udd16\nKaedim website\nKaedim. How it works\nKaedim (2022). Response from Kaedim re our AI\nOperator:  \nDeveloper: Kaedim\nCountry: UK\nSector: Media/entertainment/sports/arts\nPurpose: Generate 3D models\nTechnology: Machine learning\nIssue: Employment\nTransparency: Governance; Marketing\nInvestigations, assessments, audits \ud83e\uddd0\n404 Media (2023). Buzzy AI Startup for Generating 3D Models Used Cheap Human Labor\n404 Media (2023). AI Image Company Rebrands After 404 Media Investigation\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.3printr.com/kaedim-also-relies-on-human-workforce-for-3d-modeling-through-ai-4064861/\nhttps://www.fudzilla.com/news/ai/57551-ai-art-company-using-humans-in-a-lot-of-their-work\nhttps://ia.acs.org.au/article/2023/ai-startup--magically--generating-3d-images-was-actually-using-humans.html\nhttps://es.wired.com/articulos/kaedim-ia-para-crear-modelos-3d-usaba-mano-de-obra-barata\nRelated \ud83c\udf10\nOlive inflates 'AI' capabilities\niFlytek 'fakes' automated speech translations\nPage info\nType: Incident\nPublished: September 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-disables-echo-account-after-hearing-racial-slur", "content": "Amazon wrongly disables Echo account after hearing racial slur\nOccurred: July 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn Amazon customer had his smart home account shut down after a delivery driver mistakenly heard a racist slur through the customer's doorbell.\nAn Amazon customer had his smart home account shut down for a week after a delivery driver said he heard a racist slur through the customer's doorbell, despite nobody being at home. During this period, Jackson had been unable to use his Echo smart home applications and Alexa virtual assistant software. \nBaltimore-based Microsoft engineer Jackson discovered he had been locked out of his Amazon Echo account one day after a delivery driver had dropped off a package. \nIt turned out the driver had reported 'receiving racist remarks' from his Ring doorbell, although it had actually said 'Excuse me, can I help you?' to the headphone-clad driver, according to home video footage reviewed by Jackson.\nAmazon failed to inform Jackson that his account had been suspended, took six days to review his complaint, did not inform him when his case was finally resolved, and did not apologise.\nSystem \ud83e\udd16\nAmazon Alexa developer website\nAmazon Alexa Wikipedia profile\nOperator: Brandon Jackson\nDeveloper: Amazon\nCountry: USA\nSector: Consumer goods\nPurpose: Provide information, services\nTechnology: NLP/text analysis; Natural language understanding (NLU); Speech recognition\nIssue: Accuracy/reliability\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://medium.com/@bjax_/a-tale-of-unwanted-disruption-my-week-without-amazon-df1074e3818b\nhttps://www.youtube.com/watch?v=_onpy_gGnyU\nhttps://www.dailymail.co.uk/news/article-12189915/Amazon-shuts-customers-smart-home-week-driver-claimed-heard-racist-slur.html\nhttps://www.independent.co.uk/tech/smart-home-lock-out-amazon-b2358107.html\nhttps://www.theregister.com/2023/06/15/amazon_echo_disabled_allegation/\nhttps://www.newsweek.com/amazon-smart-home-brandon-jackson-echo-racial-slur-allegation-1806947\nhttps://www.foxbusiness.com/technology/amazon-customer-claims-company-locked-him-out-smart-home-devices-over-bogus-racism-allegations\nhttps://www.spectator.com.au/2023/06/amazon-shuts-down-smart-home-after-hate-speech-allegation/\nhttps://www.standard.co.uk/tech/amazon-locks-out-customer-smart-doorbell-racism-b1088074.html\nRelated \ud83c\udf10\nAmazon Alexa mistakes conversation for command\nAmazon Alexa mistakenly orders USD 160 dollhouse\nPage info\nType: Incident\nPublished: September 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/msn-publishes-useless-ai-generated-brandon-hunter-obituary", "content": "MSN publishes 'useless' AI-generated Brandon Hunter obituary\nOccurred: August 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn AI-generated article published by Microsoft that was meant to pray tribute to a deceased NBA player called him \u2018useless\u2019, prompting widespread criticism and ridicule.\nHeadlined 'Brandon Hunter useless at 42', the 'offensive', 'incomprehensible', and 'incoherent' MSN article also said Hunter had been 'handed away on the age of 42, described his on-field position as 'a ahead', and claimed he had played in 67 NBA 'video games'.\nMicrosoft took down the story, refused to apologise, and deflected the blame onto its Portuguese content syndication partner 'Race Track'. \nThe incident highlighted the potential pitfalls of relying solely on AI for generating news-related content, especially sensitive content such as obituaries.\nIt also raised questions about the credibility of Microsoft's claim of ensuring content alignment with its values through \"human oversight\" of its AI-powered news system.\n\u2796 May 2020. Microsoft replaced the jobs of dozens of journalists and editors running its Microsoft News and MSN sites with artificial intelligence, swiftly resulting in a high-profile racial mix-up.\nSystem \ud83e\udd16\nMSN website\nMSN Wikipedia profile\n\nDocuments \ud83d\udcc3\nMSN 'Brandon Hunter useless at 42' article\nOperator: Microsoft/MSN\nDeveloper: Microsoft/MSN\nCountry: Portugal; USA\nSector: Media/entertainment/sports/arts\nPurpose: Select news articles\nTechnology: Machine learning; NLP/text analysis; Neural networks; Deep learning\nIssue: Accuracy/reliability; Mis/disinformation\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://futurism.com/msn-ai-brandon-hunter-useless\nhttps://www.businessinsider.com/apparently-ai-generated-obituary-headline-nba-player-brandon-hunter-useless-2023-9\nhttps://searchengineland.com/microsoft-brandon-hunter-useless-ai-obituary-432008\nhttps://boingboing.net/2023/09/15/msn-declared-former-nba-star-brandon-hunter-useless-at-42-in-bizarre-ai-written-obituary.html\nhttps://news.yahoo.com/useless-42-did-microsoft-ai-111845898.html\nhttps://www.independent.co.uk/news/world/americas/brandon-hunter-nba-ai-article-b2411800.html\nhttps://www.thestreet.com/sports/msn-publishes-tasteless-story-death-former-nba-player\nhttps://www.thedailybeast.com/msn-retracts-insane-ai-written-obit-calling-dead-nba-player-useless\nRelated \ud83c\udf10\nMicrosoft robot editor confuses Little Mix band members\nMicrosoft replaces journalists with AI\nPage info\nType: Incident\nPublished: September 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/remini-ai-photo-enhancer-generates-child-porn", "content": "Remini AI photo enhancer generates 'child porn' \nOccurred: July 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nPhoto enhancement app Remini appears to have generated an image of a naked child with a woman's face on it.\nAsia Marie Williams had used a feature on the Remini app that allows you to see what your future children might look like. Most others who tried the feature said it produced clothed minors, though one other uploaded a photo showing a toddler wearing very little from the waist down.\nRemini's terms forbid users to 'Upload, generate, or distribute content that facilitates the exploitation or abuse of children, including all child sexual abuse materials and any portrayal of children that could result in their sexual exploitation.'\nThe incident sparked accusations that Remini produces child pornography, and prompted concerns about the safety of the app.\nSystem \ud83e\udd16\nRemini website\nBending Spoons Terms of service\nOperator: Asia Marie Williams\nDeveloper: Bending Spoons\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Enhance photographs\nTechnology: Computer vision; Generative adversarial network (GAN); Machine learning\nIssue: Safety\nTransparency: Complaints/appeals\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.eviemagazine.com/post/popular-app-remini-reportedly-making-ai-child-porn-version-users\nhttps://townhall.com/tipsheet/miacathell/2023/08/01/ai-just-got-a-whole-lot-creepier-n2626417\nhttps://thenaturehero.com/remini-app-child-predator/\nhttps://abc7chicago.com/artificial-intelligence-is-remini-safe-to-use-app-baby-ai-generator/13579218/\nhttps://techcrunch.com/2023/07/20/remini-tops-the-app-store-for-its-viral-ai-headshots-but-its-body-edits-go-too-far-some-say/\nhttps://www.catholicnewsagency.com/news/254946/new-ai-app-sparks-baby-fever-but-will-it-lead-to-more-child-commodification\nRelated \ud83c\udf10\nAmazon Alexa plays child pornography\nInstagram 'enables' global paedophile network\nPage info\nType: Incident\nPublished: September 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/walgreens-fails-to-gain-customer-facial-recognition-consent", "content": "Walgreens fails to gain customer facial recognition consent\nOccurred: October 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nUS pharmacy chain Walgreens has been accused of unlawfully collecting and storing photographs of customers\u2019 faces using cameras equipped with facial recognition technology. \nDefendant Leroy Jacobs argued in a class-action lawsuit (pdf) that Walgreens failed to disclose to customers that images of their faces were being collected and stored without their consent, despite being required to do so under the Illinois Biometric Information Privacy Act (BIPA).\nWalgreens\u2019 biometric data collection practices exposed customers to a heightened risk of identity theft and fraud given biometric identifiers cannot be changed if compromised, the suit argues. \nSystem \ud83e\udd16\nWalgreens website\nWalgreens Wikipedia profile\nOperator: Walgreens\nDeveloper: \nCountry: USA\nSector: Retail\nPurpose: Identify shoplifters\nTechnology: Facial recognition\nIssue: Privacy; Security\nTransparency: Privacy\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nJacobs v Walgreens (pdf)\nIllinois Biometric Information Privacy Act (BIPA) 2008\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://topclassactions.com/lawsuit-settlements/privacy/walgreens-facial-recognition-security-cameras-prompt-class-action-lawsuit/\nhttps://www.classaction.org/news/class-action-claims-walgreens-use-of-facial-recognition-cameras-oversteps-illinois-privacy-law\nhttps://www.law360.co.uk/health/articles/1316828/walgreens-hit-with-biometric-suit-over-face-scan-cameras\nRelated \ud83c\udf10\nWalgreens fridge screen door biometrics\nWalmart AI anti-shoplifting system accuracy, effectiveness\nPage info\nType: Incident\nPublished: September 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/instagram-offers-xanax-exctasy-opioid-pipeline-to-kids", "content": "Instagram offers Xanax, exctasy, opioid 'pipeline' to kids\nOccurred: December 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAccounts advertising the sale to youngsters of Xanax, ecstasy, opioids and other drugs were widespread and easy to find on Instagram, according to a report by advocacy group Tech Transparency Project (TTP). \nThe discovery demonstrates the ease with which the illegal sales of drugs were available to children on the platform, and shows the challenges Meta faced in enforcing its own policies.\nTTP set up seven Instagram accounts appearing to belong to users between the ages of 13 to 17, and discovered the accounts were easily able to navigate to pages openly advertising the sale of illicit or pharmaceutical drugs. \nFurthermore, in some cases purported drug dealers reached out to the teenage accounts through direct messages or Instagram voice calls after they began following them.\nThe report also found that while Instagram bans some drug-related hashtags, other similar ones were still readily available. Meta later said it had blocked certain hashtags on Instagram and was reviewing others for potential violations of its policies. \nSystem \ud83e\udd16\nInstagram website\nInstagram Wikipedia profile\nOperator: Meta/Instagram\nDeveloper: Meta/Instagram\nCountry: USA\nSector: Health\nPurpose: Recommend content\nTechnology: Recommendation algorithm  \nIssue: Safety; Ethics\nTransparency: Governance; Black box; Marketing\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nJosh Hawley letter to Mark Zuckerberg (pdf)\nResearch, advocacy \ud83e\uddee\nTech Transparency Project (2022). Xanax, Ecstasy, and Opioids: Instagram Offers Drug Pipeline to Kids\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://edition.cnn.com/2021/12/08/tech/instagram-drug-sales-report/index.html\nhttps://www.forbes.com/sites/abrambrown/2022/05/17/teens-on-instagram-can-still-easily-access-illegal-drugs-new-research-shows/\nhttps://www.ctvnews.ca/sci-tech/instagram-offers-drug-pipeline-to-kids-tech-advocacy-group-claims-1.5699302\nhttps://www.theguardian.com/us-news/2021/dec/22/teen-fentanyl-deaths-pills-social-media\nhttps://www.nbcnews.com/tech/social-media/instagram-pushes-drug-content-teens-rcna7751\nhttps://beccaschmillfdn.org/social-media-sites-let-children-buy-illicit-drugs-from-their-bedrooms/\nRelated \ud83c\udf10\nFacebook approves teen alcohol, drug, gambling ads\nTikTok recommends adult content to children\nPage info\nType: Incident\nPublished: September 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/uk-flood-warning-system-false-alerts", "content": "UK automated flood warning system issues late, false alerts\nOccurred: February 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA new flood warning system developed by the UK government delivered warnings of floods that failed to materialise, and warnings that came after people's homes were already flooded. \nUnlike manual warnings, which are typically offered in advance by analysing digital monitoring data alongside weather forecasts, the new automated system only issues warnings when a river reaches a certain level. \nThe automated alerts have far less detail than those offered by flood forecasters, according to The Guardian. The system was trialled in December 2022 as a stopgap during a period of industrial action by Environment Agency officers, and continued thereafter. \nSystem \ud83e\udd16\nUK Environment Agency website\nUK Environment Agency Wikipedia profile\nUK Department for Environment, Food & Rural Affairs (2019). Flood warning finds its voice\nUK Department for Environment, Food & Rural Affairs (2009). Improving flood warnings (pdf)\nOperator: Environment Agency\nDeveloper: Environment Agency\nCountry: UK\nSector: Govt - environment\nPurpose: Assess and predict flood risk\nTechnology: Deep learning; Neural network; Machine learning\nIssue: Accuracy/reliability; Safety\nTransparency: \nResearch, advocacy \ud83e\uddee\nNational Flood Forum (2023). FLOOD FORUM RESPONDS TO GUARDIAN ARTICLE ABOUT FLOOD WARNINGS\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theguardian.com/environment/2023/feb/08/englands-flood-warning-systems-on-autopilot-again-as-staff-stage-strike\nhttps://www.theguardian.com/environment/2023/apr/04/englands-automated-flood-warning-system-to-stay-despite-inaccuracy-warnings\nhttps://www.bbc.co.uk/news/uk-england-hereford-worcester-64260109\nhttps://www.endsreport.com/article/1818882/controversial-automated-flood-alerts-trialled-during-industrial-action-stay\nRelated \ud83c\udf10\nDEFRA Biodiversity Net Gain metric\nToronto beach water quality predictions\nPage info\nType: Incident\nPublished: September 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/china-uses-ai-to-accuse-us-of-starting-maui-wildfires", "content": "China uses AI to accuse US of starting Maui wildfires\nOccurred: August 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nPro-China operatives reputedly spread disinformation about an experimental US military weapon that allegedly caused the August 2023 Maui wildfires.\nStarting mid-August 2023, the campaign used apparently AI-generated photographs and text to allege the British intelligence service MI6 had revealed that the US military used a 'weather weapon' to start the wildfires. According to a report by NewsGuard, the conspiracy theory could be traced to a post on Chinese platform 163.com in early August.\nResearchers said the campaign comprised 85 social media accounts and blogs originating from China, and targeted users in multiple countries and 16 languages on Facebook, Twitter, YouTube, and a dozen other platforms. Meta confirmed to Gizmodo the accounts were part of China's deepfake 'Spamouflage' disinformation operation.\nThe impact of the Chinese campaign proved difficult to measure, but it appears unlikely to have gained much traction given the highly implausible nature of the claim.\nSystem \ud83e\udd16\nUnknown\nOperator: Government of China\nDeveloper: Government of China \nCountry: USA\nSector: Politics\nPurpose: Scare/confuse/destabilise \nTechnology: Deepfake - image, video, audio; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Mis/disinformation\nTransparency: Governance; Marketing\nResearch, advocacy \ud83e\uddee\nNewsGuard (2023). Pro-China Disinformation Campaign Claims US Started Maui Fires in a \u2018Weather Weapons\u2019 Experiment, Falsely Citing the UK\u2019s MI6\nFact check \ud83d\udea9\nRadio Free Asia (2023). Were Maui wildfires caused by a US weapons test?\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.nytimes.com/2023/09/11/us/politics/china-disinformation-ai.html\nhttps://gizmodo.com/weather-weapon-maui-fires-china-disinformation-1850826548\nhttps://www.businesstimes.com.sg/international/china-sows-disinformation-about-hawaii-fires-using-new-techniques\nhttps://www.politico.com/newsletters/weekly-cybersecurity/2023/09/11/influence-campaign-spread-during-maui-wildfires-00114944\nRelated \ud83c\udf10\nDeepfake TV anchors spin pro-China deepfake 'Spamouflage' campaign\nChina diplomatic fake influence campaign\nPage info\nType: Incident\nPublished: September 2023\nLast updated: December 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/google-flags-medical-images-of-groin-as-csam", "content": "Google flags medical images of groins as CSAM\nOccurred: February 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGoogle's automated system to detect abusive images of children has resulted in two fathers being unfairly investigated by the police and having their accounts deactivated across all Google platforms. \nThe two incidents highlight the inaccuracy and unreliability of automatic photo screening and reporting technology, and the broader impacts these errors can have on individuals when things go wrong.\nAccording to the New York Times, a man named Mark had sent pictures of his son\u2019s groin to a doctor in San Francisco after realising it was inflamed, only for Google to identify the images as abusive, suspend his account, and report the photos to the US National Center for Missing and Exploited Children\u2019 CyberTipline, which escalated the report to the police. \nPer the NYT, 'Not only did he lose emails, contact information for friends and former colleagues, and documentation of his son\u2019s first years of life, his Google Fi account shut down, meaning he had to get a new phone number with another carrier. Without access to his old phone number and email address, he couldn\u2019t get the security codes he needed to sign in to other internet accounts, locking him out of much of his digital life.'\nSimilarly, photos of a boy's 'intimal parts' backed up on Google Photos that had been requested by a Houston-based pediatrician to diagnose an infection resulted in the boy's father also having his account suspended. \nGoogle, which uses Microsoft\u2019s PhotoDNA tool as part of its efforts to detect child abuse, said it identified 287,368 instances of suspected abuse in the first six months of 2021.\nSystem \ud83e\udd16\nGoogle Child Safety Toolkit\nGoogle (2022). How we detect, remove and report child sexual abuse material\nMicrosoft. PhotoDNA\nOperator: Alphabet/Google\nDeveloper: Alphabet/Google\nCountry: USA\nSector: Health\nPurpose: Detect child sexual abuse material\nTechnology: Hash matching; Machine learning\nIssue: Accuracy/reliability; Governance\nTransparency: Governance; Black box; Complaints/appeals \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.nytimes.com/2022/08/21/technology/google-surveillance-toddler-photo.html\nhttps://www.theguardian.com/technology/2022/aug/22/google-csam-account-blocked\nhttps://gizmodo.com/google-csam-photodna-1849440471\nhttps://www.dailymail.co.uk/sciencetech/article-11133805/Google-AI-flags-dad-photos-childs-groin-infection-phone-share-doctors.html\nhttps://www.thetimes.co.uk/article/google-bans-father-over-medical-photos-of-childs-groin-ptlp63sq0\nhttps://uk.pcmag.com/security/142194/nyt-parents-lose-google-accounts-over-abuse-image-false-positives\nhttps://www.indy100.com/science-tech/google-medical-groin-pics-removed\nhttps://www.theverge.com/2022/8/21/23315513/google-photos-csam-scanning-account-deletion-investigation\nhttps://www.techspot.com/news/95729-google-refuses-reinstate-account-man-after-flagged-medical.html\nhttps://www.reddit.com/r/degoogle/comments/wvh7kt/google_refuses_to_reinstate_mans_account_after_he\nRelated \ud83c\udf10\nApple NeuralHash CSAM scanning\nChatGPT role-plays BDSM, describes sex acts with children\nPage info\nType: Incident\nPublished: September 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/allocation-algorithm-wrongly-places-thousands-of-italian-teachers", "content": "Allocation algorithm wrongly places thousands of Italian teachers\nOccurred: 2016\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn algorithm used by the Italian government to allocate the location of thousands of teachers in 2016 resulted in many of them having to relocate to inappropriate locations.\nDesigned and developed by HP Enterprise and Finmeccanica to the tune of EUR 444,000, Italy's so-called 'Bueno Scuola' algorithm was meant to assess and score every teacher based criteria including their work experience and performance and their preferred destinations, and match them with the most appropriate vacancies.\nHowever, teachers and their families were relocated across the country in an apparently more or less random manner, triggering uproar. A subsequent assessment of the system found it to be fully automated, 'unmanageable', full of bugs, and impossible to properly evaluate due to its opaque nature.\nThe incident triggered controversy about the purpose, design, and the effectiveness of the algorithm, and led to multiple legal complaints, lawsuits, and to the discontinuation of the system.\nSystem \ud83e\udd16\nItaly Ministry of Education, Universities and Research website\nItaly Ministry of Education, Universities and Research Wikipedia profile\nOperator: Italy Ministry of Education, Universities and Research\nDeveloper: HP Enterprise Services Italia; Finmeccanica\nCountry: Italy\nSector: Education\nPurpose: Allocate teacher positions\nTechnology: Resource allocation algorithm\nIssue: Accuracy/reliability; Effectiveness/value; Robustness; Ethics; Legal\nTransparency: Governance; Black box; Complaints/appeals\nResearch, advocacy \ud83e\uddee\nGaletta J., Pinotti G. (2023). Automation and Algorithmic Decision-Making Systems in the Italian Public Administration\nAlgorithmWatch (2021). Automating Society Report 2020\nSchneider G. (2020). Accesso all\u2019algoritmo pubblico sviluppato da terzi e questioni di riservatezza nell\u2019amministrazione digitale\nInvestigations, assessments, audits \ud83e\uddd0\nPERIZIA TECNICA PRELIMINARE SULL\u2019ANALISI DELL\u2019ALGORITMO CHE GESTISCE IL SOFTWARE DELLA MOBILIT\u00c0 DOCENTI PER L\u2019A.S. 2016/2017 (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://rzapatal.medium.com/the-algorithm-that-decided-the-destiny-of-many-families-7b374fa2574b\nhttps://medium.com/@ActionItalia/the-buona-scuola-reform-a-way-out-for-italys-worsening-state-education-3a7fab8a012b\nhttps://www.ft.com/content/bd39453e-baec-11e6-8b45-b8b81dd5d080\nhttps://algorithmwatch.org/en/algorithm-school-system-italy/\nhttps://www.wired.it/attualita/tech/2021/03/04/pubblica-amministrazione-algoritmi-piano-colao/\nhttps://www.orizzontescuola.it/presadiretta-caos-scuola-mobilita-2016-lalgoritmo-resta-segreto-spesi-400mila-euro-il-video/\nhttps://www.agi.it/cronaca/algoritmo_miur_assegnazioni_insegnanti-1896615/news/2017-06-20/\nhttps://www.repubblica.it/cronaca/2019/09/17/news/scuola_trasferimenti_di_10mila_docenti_lontano_da_casa_il_tar_l_algoritmo_impazzito_fu_contro_la_costituzione_-236215790/\nhttps://www.4clegal.com/settore-pubblico/lintelligenza-artificiale-tinge-diritto-0\nhttps://www.repubblica.it/scuola/2023/01/01/news/scuola_supplenti-381611381/\nRelated \ud83c\udf10\nHouston ISD teacher performance evaluation opacity\nSheri G. Lederman NYC teacher effectiveness assessment\nPage info\nType: Incident\nPublished: September 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-unmasks-anonymous-chess-players", "content": "AI unmasking of anonymous chess players raises privacy concerns\nOccurred: January 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn AI system developed by a team of Harvard University researchers can identify and track people playing chess anonymously, resulting in concerns about its potential to abuse the privacy of chess players and others. \nAccording to a study of the system, it was originally built to identify different types of chess playing behaviour, and to tag chess players based on their behavioural patterns, in order to help develop more human gaming experiences.\nHowever, the system could also be used to help identify and track people who believe they are anonymously playing chess and, potentially, other games, persuading organisers of the NeurIPS conference that the authors of the study elaborate on the privacy risks.  \nSystem \ud83e\udd16\n Operator:  \nDeveloper: Harvard University\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Identify chess player behaviour\nTechnology: Deep learning; Neural network; Machine learning\nIssue: Ethics; Privacy\nTransparency: \nResearch, advocacy \ud83e\uddee\nMcIlroy-Young R., Wang Y., Sen S., Kleinberg J., Anderson A. (2022). Detecting Individual Decision-Making Style: Exploring Behavioral Stylometry in Chess\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.science.org/content/article/ai-unmasks-anonymous-chess-players-posing-privacy-risks\nhttps://cyber.harvard.edu/story/2022-01/ai-unmasks-anonymous-chess-players-posing-privacy-risks\nhttps://bioethics.com/archives/60750\nRelated \ud83c\udf10\nProblem gambler AI detection\nChess robot breaks child's finger\nPage info\nType: Issue\nPublished: September 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/anpr-confuses-t-shirt-with-personalised-number-plate", "content": "ANPR confuses T-shirt with personalised number plate\nOccurred: October 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA couple in Bath, UK, were issued with a fine after the local council's license plate recognition system confused the car's number with a word on the T-shirt of a local pedestrian.\nDave and Paula Knight had received the fine for supposedly driving in a bus lane in Bath, despite not being in the city at the time. It transpired a CCTV camera had detected the word 'KNITTER' on a pedestrian's clothing when she had been walking down the bus lane, and confused it with Mr Knight's personalised number plate KN19 TER.\nBath and Somerset Council later admitted nobody had checked the computer-generated image, and agreed to cancel the fine, which had increased from GBP 60 to GBP 90 after the couple had resisted pating within 30 days.\nSystem \ud83e\udd16\nBath and North East Somerset Council website\nBath and North East Somerset Council Wikipedia profile\nOperator: Bath and North East Somerset Council\nDeveloper: \nCountry: UK\nSector: Govt - municipal\nPurpose: Detect traffic violations\nTechnology: Automated license plate/number recognition (ALPR/ANPR)\nIssue: Accuracy/reliability\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/uk-england-somerset-58959930\nhttps://metro.co.uk/2021/10/18/surrey-writing-on-womans-jumper-landed-couple-with-fine-when-she-walked-in-bus-lane-15439916/\nhttps://www.autoblog.com/2021/10/19/cctv-knitter-kn19ter-ticket-england\nhttps://www.dailymail.co.uk/news/article-10101699/Couple-KN19-TER-number-plate-hit-90-fine-thanks-woman-Knitter-bus-lane.html\nhttps://petapixel.com/2021/10/19/traffic-camera-mistakes-woman-for-car-issues-ticket-to-car-owner/\nhttps://www.theguardian.com/uk-news/2021/oct/18/motorist-fined-number-plate-t-shirt\nhttps://www.ladbible.com/news/news-man-receives-fine-after-camera-confuses-t-shirt-for-his-reg-plate-20211018\nhttps://www.itv.com/news/westcountry/2021-10-19/number-plate-on-womans-t-shirt-sees-couple-handed-bus-lane-fine\nRelated \ud83c\udf10\nSpeedcam Anywhere anti-speeding app\nTALON AI license plate camera surveillance\nPage info\nType: Incident\nPublished: September 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/mtv-lebanon-uses-deepfakes-to-commemorate-bomb-victims", "content": "MTV Lebanon deepfakes to commemorate bomb victims solicits backlash \nOccurred: July 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA deepfake video purporting to 'commemorate' two victims of the 2020 Beirut port bombing that killed over 200 people and injured some 6,500 injured, has been blasted for being 'inappropriate', 'insensitive' and 'unethical'.\nFeaturing deepfakes of Ralph Mallahi and Amin Al-Zahed, both of whom lost their lives in the 2020 atrocity, the video was titled 'A Letter to the Lebanese Judiciary' and shared on social media by MTV Lebanon with the hashtag, 'It\u2019s been a year, the time is up.' \nMouin Jaber, co-host of the popular Lebanese podcast \u201cSarde After Dinner,\u201d told Arab News: 'It\u2019s dystopian. It\u2019s emotional manipulation and blackmail taken to a whole other uncanny level.'\nSystem \ud83e\udd16\nMTV Lebanon website\nMTV Lebanon Wikpedia profile\nMTV Lebanon deepfake video\nOperator: MTV Lebanon\nDeveloper: MTV Lebanon\nCountry: Lebanon\nSector: Media/entertainment/sports/arts\nPurpose: Commemorate bomb victims\nTechnology: Deepfake - video, audio; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Appropriateness/need; Ethics\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.arabnews.com/node/1901606/media\nhttps://www.archyde.com/controversial-music-video-deepfakes-victims-of-beirut-explosion/\nhttps://viraltab.news/mtv-lebanon-slammed-for-deepfake-vid-of-beirut-port-blast-victims-ahead-of-anniversary/\nhttps://www.reddit.com/r/lebanon/comments/otu4xe/mtv_using_deep_fakes_of_explosion_deceased\nRelated \ud83c\udf10\nPresident Ali Bongo recovery deepfake broadcast\nDeepfake 'Pan Africanists' support Burkina Faso junta\nPage info\nType: Incident\nPublished: September 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chatgpt-invents-henrik-enghoff-academic-citations", "content": "ChatGPT invents Henrik Enghoff academic citations\nOccurred: August 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nDanish biologist and academic Henrik Enghoff was falsely cited by ChatGPT in a scientific paper about millipedes. \nThe citation resulted in the paper's withdrawal and raised further questions about the generative AI tool's tendency to 'hallucinate', or produce plusible sounding falsities.\nEnghoff had first noticed something strange when he saw the paper, which was written by academics from Ethiopia and China, citing his work for something he does not write about, and referenced two paper he knew he had not authored, and which turned out not to exist.\nThe paper was first taken down in June 2023 by preprint archive Preprints.org, after David Richard Nash, a University of Copenhagen colleague of Enghoff's, had identified ChatGPT as the likely cuplrit and notified editors of the errors. The paper subsequently resurfaced on preprint platform Research Square, which later withdrew it and blacklisted the 'authors'. \nIn July 2023, Kahsay Tadesse Mawcha of Ethiopia's Aksum University had admitted to Danish newspaper Weekendavisen that he had used ChatGPT when writing his paper, adding that he only later realised the tool was 'not recommended' for the task. \nSystem \ud83e\udd16\nChatGPT chatbot\nOpenAI usage policies\nOperator: Aksum University\nDeveloper: OpenAI\nCountry: Denmark\nSector: Research/academia\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Accuracy/reliability; Mis/disinformation\nTransparency: Governance; Black box\nResearch/advocacy \ud83e\uddee\nKahsay Tadesse Mawcha (2023). Review From Beneficial Arthropods to Soil-Dwelling Organisms: A Review on Millipedes in Africa\nKahsay Tadesse Mawcha (2023). Review From Beneficial Arthropods to Soil-Dwelling Organisms: A Review on Millipedes in Africa - v2 (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://retractionwatch.com/2023/09/01/withdrawn-ai-written-preprint-on-millipedes-resurfaces-causing-alarm/\nhttps://retractionwatch.com/2023/07/07/publisher-blacklists-authors-after-preprint-cites-made-up-studies/\nhttps://futurism.com/professor-chatgpt-scientific-paper-errors\nhttps://ahrecs.com/latestnews/publisher-blacklists-authors-after-preprint-cites-made-up-studies-retraction-watch-ivan-oransky-april-2023/\nhttps://www.weekendavisen.dk/2023-30/ideer/tusindben-paa-afveje\nRelated \ud83c\udf10\nChatGPT invents case citations in legal filings\nChatGPT falsely claims to write student essays\nPage info\nType: Incident\nPublished: September 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/deepfake-video-alleges-france-opposes-mali-military-junta", "content": "Deepfake video alleges France opposes Mali military junta\nOccurred: January 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA fake AI-generated video purported to show France giving money to Malian political parties to persuade them not to participate in a national consultation set up by the country's military junta.\nThe video showed a television news presenter speaking to camera in a TV studio background and drew on a letter allegedly written by French president Emmanuel Macron saying France would pay EUR 23 million to local political parties if they agreed to support France's presence on the country.\nThe deepfake video was created using AI video creation platform Synthesia and first appeared on the Nabi Malien Den Halala Facebook page. The page is said to regularly publish Russian disinformation. Russian mercenaries have reputedly been active in Mali for several years.\nMali's military junta took power in a controversial coup d\u2019\u00e9tat in May 2021.\nSystem \ud83e\udd16\nSynthesia AI video generation\nOperator: Nabi Malien Den Halala\nDeveloper: Synthesia\nCountry: France; Mali\nSector: Politics; Govt - foreign\nPurpose: Damage reputation\nTechnology: Deepfake - audio, video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Ethics; Mis/disinformation\nTransparency: Governance; Marketing \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://observers.france24.com/en/tv-shows/truth-or-fake/20220110-truth-or-fake-debunked-mali-robot-voices-deepfakes\nhttps://www.france24.com/en/tv-shows/truth-or-fake/20220131-deepfake-news-videos-circulate-in-mali-amid-tensions-with-france\nhttps://www.wired.com/story/synthesia-ai-deepfakes-it-control-riparbelli/\nRelated \ud83c\udf10\nDeepfake 'Pan Africanists' support Burkina Faso junta\nDeepfake news anchors claim Venezuela economic health\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/face-depixelizer-turns-barack-obama-white", "content": "Face Depixelizer faces storm for turning Barack Obama white\nOccurred: June 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nFace Depixelizer, a tool that generates high resolution, de-pixelated photos from low-resolution, pixelated ones, generated white faces from coloured people, notably Barack Obama.\nBased on the PULSE image facialiser developed by Duke University researchers using NVIDIA's StyleGAN generative adversarial network (GAN), Face Depixelizer was primarily to upscale pixelated portraits of characters from a number of video game franchises. But it was found to struggle consistently with non-white faces such as those of US politician and Congresswoman Alexandria-Ocasio Cortez and actress Lucy Liu.\nThe incident called into question the tool's reliability and led to accusations of racism. It also resulted in a heated public spat over the nature of algorithmic bias between Facebook chief AI scientist Yann LeCun, who appeared to blame the quality of the data on which the tool was trained, and AI ethics researcher Timnit Gebru, who insisted it reflected structural issues in the technology industry. LeCun later apologised for his words.\nSystem \ud83e\udd16\nFace Depixelizer GitHub repository\nFace Depixelizer Google Colab\nOperator:  \nDeveloper: Denis Malimonov\nCountry: USA\nSector: Politics\nPurpose: Improve image quality\nTechnology: Computer vision; Pattern recognition\nIssue: Robustness; Bias/discrimination - race, ethnicity\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://twitter.com/Chicken3gg/status/1274314622447820801\nhttps://twitter.com/ylecun/status/1275194055257919495\nhttps://www.businessinsider.com/depixelator-turned-obama-white-illustrates-racial-bias-in-ai-2020-6\nhttps://knowyourmeme.com/memes/sites/face-depixelizer\nhttps://www.cracked.com/article_28034_this-depixelization-ai-can-give-human-face-to-mario-but-not-to-minorities.html\nhttps://www.boredpanda.com/low-quality-photo-face-depixelizer/\nhttps://analyticsindiamag.com/yann-lecun-machine-learning-bias-debate/\nhttps://towardsdatascience.com/towards-less-discrimination-in-artificial-intelligence-systems-78b8a6e162e5\nhttps://www.pcgamer.com/help-i-cant-stop-looking-at-this-ai-generated-version-of-bj-blazkowicz/\nhttps://www.theverge.com/21298762/face-depixelizer-ai-machine-learning-tool-pulse-stylegan-obama-bias\nRelated \ud83c\udf10\nAI Portrait Ars racial bias\nAutomatic soap dispenser 'racism'\nPage info\nType: Incident\nPublished: September 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/google-autocomplete-suggests-jews-women-are-evil", "content": "Google Autocomplete suggests Jews, women are 'evil' \nOccurred: December 2016\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nUsers typing the phrase 'are Jews' into Google found its Autocomplete search prediction function returned the phrase 'are Jews evil?'.\nThe results brought up anti-Semitic websites, blogs and articles such as 'Top 10 major reasons why people hate Jews'. Similarly negative results were returned for 'are women' and 'are Muslims', resulting in accusations of religious and gender bias.\nThe discovery had been made by The Observer newspaper during an investigation into the influence of right-wing websites on search results. \nGoogle updated its system to remove the suggestions, though no change was made for searches starting with 'are Muslims.' Google did not say why changes to some phrases were made, but not to others. \nSystem \ud83e\udd16\nGoogle Autocomplete\nOperator: Alphabet/Google\nDeveloper: Alphabet/Google\nCountry: UK\nSector: Politics; Religion\nPurpose: Predict search results\nTechnology: NLP/text analysis; Deep learning; Machine learning\nIssue: Accuracy/reliability; Bias/discrimination - gender, race, religion\nTransparency: Governance; Black box\nResearch, advocacy \ud83e\uddee\nCommunity Security Trust, Antisemitism Policy Trust (2019). Hidden hate: What Google searches tell us about antisemitism today\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theguardian.com/technology/2016/dec/04/google-democracy-truth-internet-search-facebook\nhttps://www.theguardian.com/technology/2016/dec/05/google-alters-search-autocomplete-remove-are-jews-evil-suggestion\nhttps://www.cbsnews.com/news/google-autocompletes-anti-semitism-sexism-racism/\nhttps://www.haaretz.com/us-news/2016-12-06/ty-article/google-removes-are-jews-evil-search-suggestion-but-so-far-keeps-are-muslims-bad/0000017f-f956-d318-afff-fb7765df0000\nhttps://www.dailymail.co.uk/news/article-4004498/Google-changes-autocomplete-search-engine-suggests-anti-Semitic-phrase-Jews-evil.html\nhttps://www.jewishnews.co.uk/google-removes-are-jews-evil-from-search-suggestions/\nhttps://www.telegraph.co.uk/technology/2016/12/05/google-removes-anti-semitic-suggestion-autocomplete-feature/\nhttps://www.wired.com/story/google-autocomplete-vile-suggestions/\nRelated \ud83c\udf10\nGoogle Autocomplete says Rupert Murdoch, Jon Hamm are 'Jewish'\nGoogle Autocomplete, Related Search reveal rape victims' names\nPage info\nType: Incident\nPublished: August 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/google-autocomplete-texas-massacre-antifa-conspiracy", "content": "Google Autocomplete amplifies Texas massacre Antifa conspiracy\nOccurred: November 2017\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGoogle was accused of disseminating disinformation and propaganda about a US mass shooter in the wake of a massacre of 26 people in Sutherland Springs, Texas.\nFormer US Air Force member Devin Patrick Kelley was quickly associated with links to anti-fascist umbrella group Antifa, despite there being little or no evidence that this was true. The allegation had been made by several right-wing politicians, activists, and commentators on Twitter and elsewhere, and had been picked up by Google's search-related algorithms.\nThe incident prompted questions about the accuracy, reliability and governance of the technology company's Autocomplete search prediction system.\nGoogle and others regularly blamed algorithms when they spread misinformation and disinformation, and refuse to acknowledge responsibility for the risks their systems pose, and the impacts they cause.\nSystem \ud83e\udd16\nGoogle Autocomplete\nOperator: Alphabet/Google/YouTube\nDeveloper: Alphabet/Google/YouTube\nCountry: USA\nSector: Politics\nPurpose: Predict search results\nTechnology: NLP/text analysis; Deep learning; Machine learning  \nIssue: Accuracy/reliability; Mis/disinformation\nTransparency: Governance; Black box\nResearch, advocacy \ud83e\uddee\nAl-Rawi, A., Celestini, C., Stewart, N., Worku, N. (2022). How Google Autocomplete Algorithms about Conspiracy Theorists Mislead the Public\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.seroundtable.com/google-devin-patrick-kelley-24725.html\nhttps://www.theguardian.com/us-news/2017/nov/06/google-youtube-texas-shooting-fake-news\nhttps://www.thedailybeast.com/google-autocompletes-antifa-conspiracy-theory-after-texas-massacre\nhttps://www.inquirer.com/philly/news/nation_world/texas-shooting-google-devin-kelly-antifa-autocomplete-20171106.html\nhttps://www.vice.com/en/article/434qxn/facebook-is-still-spreading-conspiracies-48-hours-after-texas-shooting\nhttps://searchengineland.com/google-promises-improve-accuracy-tweets-shown-search-results-286295\nhttps://gizmodo.com/once-again-google-promoted-disinformation-and-propagan-1820166979\nhttps://www.fastcompany.com/40492081/googles-popular-on-twitter-feature-was-full-of-misinformation-about-the-sutherland-springs-shooting\nhttps://www.engadget.com/2017/11/06/google-twitter-results-misinformation/\nhttps://techcrunch.com/2017/11/06/google-is-surfacing-texas-shooter-misinformation-in-search-results-thanks-also-to-twitter/\nRelated \ud83c\udf10\nGoogle search prioritises Holocaust denial website\nGoogle Autocomplete, Related Search reveal rape victims' names\nPage info\nType: Incident\nPublished: September 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/robot-crushes-and-kills-ventra-ionia-technician", "content": "Robot crushes and kills Ventra Ionia technician\nOccurred: July 2015\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nWanda Holbrook, a 57 year-old maintenance technician performing routine duties on an assembly line at auto-parts maker Ventra Ionia Main, Michigan, USA, was 'trapped by robotic machinery' and crushed to death. \nTwo years later her husband, William Holbrook, filed lawsuit (pdf) alleging wrongful death, naming five companies involved in engineering and integrating the machines and parts used at the plant: Prodomax, Flex-N-Gate, FANUC, Nachi, and Lincoln Electric. \nHolbrook had been in the plant\u2019s six-cell '100 section' when a robot unexpectedly activated, loaded a trailer-hitch assembly part and jammed it onto Holbrook\u2019s head, crushing her skull.\nThe incident raised questions about the safety of the robotics system as a whole, and its liability. Prodomax Automation Ltd., which built the assembly lines, settled with Holbrook's family in 2021.\nSystem \ud83e\udd16\nFlex-N-Gate website\nOperator: Flex-N-Gate/Ventra Ionia\nDeveloper: Nachi Robotic Systems; Lincoln Electric Company; FANUC America Corp\nCountry: USA\nSector: Manufacturing/engineering\nPurpose: Weld truck bumpers\nTechnology: Robotics\nIssue: Accuracy/reliability; Safety\nTransparency: \nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nHolbrook v. Prodomax Automation Ltd. et al\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.techrepublic.com/article/robot-kills-worker-on-assembly-line-raising-concerns-about-human-robot-collaboration/\nhttps://www.news.com.au/technology/factory-worker-killed-by-rogue-robot-says-widowed-husband-in-lawsuit/news-story/13242f7372f9c4614bcc2b90162bd749\nhttps://www.independent.co.uk/news/world/americas/robot-killed-woman-wanda-holbrook-car-parts-factory-michigan-ventra-ionia-mains-federal-lawsuit-100-cell-a7630591.html\nhttps://www.wired.co.uk/article/robot-death-wanda-holbrook-lawsuit#:~:text=In%20July%202015%2C%20Wanda%20Holbrook,a%20lawsuit%20for%20wrongful%20death.\nhttps://www.dailymail.co.uk/news/article-4315490/Widower-factory-worker-killed-rogue-robot-sues.html\nhttps://qz.com/931304/a-robot-is-blamed-in-death-of-a-maintenance-technician-at-ventra-ionia-main-in-michigan\nhttps://www.autoevolution.com/news/lawsuit-accuses-supplier-of-negligence-after-a-robot-killed-a-human-in-a-plant-116235.html\nhttps://news.bloomberglaw.com/product-liability-and-toxics-law/killer-robot-suit-awaits-ruling-on-japanese-maker\nhttps://news.bloomberglaw.com/litigation/man-whose-wife-was-killed-by-factory-robot-settles-mid-trial\nRelated \ud83c\udf10\nRobot kills SKH Metals worker\nAjin USA worker crushed to death by robot\nPage info\nType: Incident\nPublished: August 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/google-autocomplete-associates-italian-businessman-with-fraud", "content": "Google Autocomplete associates Italian businessman with 'fraud'\nOccurred: April 2011\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGoogle lost a legal case in Milan, Italy, in which a businessman was unfairly suggested to linked to \u2018truffa\u2019 ('fraud') and being a \u2018truffatore\u2019 ('conman') by its Autocomplete function. \nThe unnamed entrepreneur had complained that, when his name was typed into the search engine, Autocomplete suggested terms that were untrue and defamatory. \nThe court dismissed the technology company's claim that it was not responsible for the suggestions. Google said it was \u2018disappointed\u2019 by the decision, adding: \u2018We believe that Google should not be held liable for terms that appear in Autocomplete as these are predicted by computer algorithms based on searches from previous users, not by Google itself.\u2019\nSystem \ud83e\udd16\nGoogle Autocomplete\nOperator: Alphabet/Google\nDeveloper: Alphabet/Google\nCountry: Italy\nSector: Banking/financial services\nPurpose: Predict search results\nTechnology: NLP/text analysis; Deep learning; Machine learning  \nIssue: Accuracy/reliability; Legal; Safety\nTransparency: Governance; Black box\nResearch, advocacy \ud83e\uddee\nGhatnekar S. (2013). Injury By Algorithm: A Look Into Google's Liability For Defamatory Autocompleted Search Suggestions\nSegreteria Nazionale Noi Consumatori (2011). Servizio \u201cautocomplete\u201d del motore di ricerca e diffamazione\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://metro.co.uk/2011/04/07/googles-autocomplete-function-libels-man-by-linking-his-name-to-fraud-649996/\nhttps://st.ilsole24ore.com/art/tecnologie/2011-04-06/google-suggest-sentenza-tribunale-milano-125339.shtml\nhttps://www.lexology.com/library/detail.aspx?g=27f50d0d-44d6-4090-9424-8de75b1fe1f3\nhttps://www.zdnet.com/article/google-loses-autocomplete-defamation-case-in-italy/\nhttps://inforrm.org/2012/09/21/defamation-claims-and-googles-autocomplete-gervase-de-wilde/\nhttps://www.techdirt.com/2011/04/05/google-found-liable-autocomplete-suggestions-italy/\nhttps://news.yahoo.com/news/italian-court-finds-google-guilty-defamation-20110406-100333-058.html\nRelated \ud83c\udf10\nYouTube Autocomplete suggests paedophiliac phrases\nGoogle Autocomplete links French user to rape\nPage info\nType: Incident\nPublished: August 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/google-autocomplete-related-search-reveal-rape-victims-names", "content": "Google Autocomplete, Related Search reveal rape victims' names\nOccurred: May 2018\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGoogle's Autocomplete and Related Search systems exposed the names of rape victims in the UK. Victims are entitled to lifelong anonymity under UK law. \nAn investigation by The Times newspaper discovered that searches for attackers or alleged attackers automatically revealed the names of the women they had been accused of raping. It also found that searching for a victim's name brought up the abuser's name as a related search.\nThe finding called into question the safety and legality of the two systems. Chair of the Women and Equalities Committee Maria Miller MP said: \u2018Google has to operate within the law of the UK \u2013 if that means they have to change how their search engine operates then so be it.\u2019 \nGoogle responded by saying: \u2018We don\u2019t allow these kinds of autocomplete predictions or related searches that violate laws or our own policies and we have removed the examples we\u2019ve been made aware of in this case.'\nSystem \ud83e\udd16\nGoogle Autocomplete\nOperator: Alphabet/Google\nDeveloper: Alphabet/Google\nCountry: UK\nSector: Govt - justice\nPurpose: Predict search results\nTechnology: NLP/text analysis; Deep learning; Machine learning  \nIssue: Legal; Privacy; Safety\nTransparency: Governance; Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.thetimes.co.uk/article/google-identifies-rape-victims-k9slpxmns\nhttps://www.bbc.co.uk/news/business-44210796\nhttps://www.forbes.com/sites/emmawoollacott/2018/05/22/google-under-fire-for-revealing-rape-victims-names/\nhttps://www.independent.co.uk/tech/google-autocomplete-search-latest-results-rape-trials-victims-law-a8364861.html\nhttps://uk.sports.yahoo.com/news/googles-autocomplete-function-reveals-identity-anonymous-rape-victims-104456154.html\nhttps://graziadaily.co.uk/life/in-the-news/google-autocomplete-reveals-rape-victim-identity-related-search/\nhttps://metro.co.uk/2018/05/23/google-autofill-reveals-names-rape-victims-users-search-attackers-7572477/\nhttps://www.thesun.co.uk/news/6346878/rape-victims-identified-by-google-as-searches-for-attackers-reveal-victims-too/\nRelated \ud83c\udf10\nYouTube Autocomplete suggests paedophiliac phrases\nGoogle Autocomplete falsely associates Japanese man with crimes\nPage info\nType: Incident\nPublished: August 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/youtube-autocomplete-suggests-paedophiliac-phrases", "content": "YouTube Autocomplete suggests paedophiliac phrases\nOccurred: November 2017\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGoogle's Autocomplete search term prediction function has been found to be returning 'profoundly disturbing' paedophiliac phrases like 'how to have s*x with your kids' and 'how to have s*x kids' for searches for 'how to have' on its YouTube-owned property.\nBecause Autocomplete and some other YouTube search algorithms are based on popularity, it was suggested to Buzzfeed and other media publications that a coordinated effort could have caused the searches to appear higher in results than they would organically.\nThe incident raised questions about the apparent ease with which Google's search-related algorithms could be manipulated.\nSystem \ud83e\udd16\nYouTube search\nGoogle. How Google Autocomplete predictions work\nOperator: Alphabet/Google/YouTube\nDeveloper: Alphabet/Google/YouTube\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Predict search results\nTechnology: NLP/text analysis; Deep learning; Machine learning\nIssue: Accuracy/reliability; Safety\nTransparency: Governance; Black box \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.buzzfeednews.com/article/charliewarzel/youtubes-search-autofill-is-surfacing-disturbing-child-sex#.rgzmXjZex\nhttps://www.insider.com/youtube-fixed-an-autocomplete-bug-that-suggested-how-to-have-sx-with-your-kids-2017-11\nhttps://www.theverge.com/2017/11/28/16709214/youtube-autocomplete-search-function-child-exploitation-ads\nhttps://www.ubergizmo.com/2017/11/youtube-autocomplete-results-child-abuse/\nhttps://www.theguardian.com/technology/2017/nov/27/youtube-investigates-reports-of-child-abuse-terms-auto-fill-searches\nhttps://www.engadget.com/2017-11-27-youtube-pulls-autocomplete-results-child-abuse-terms.html\nhttps://www.npr.org/sections/thetwo-way/2017/11/27/566769570/youtube-faces-increased-criticism-that-its-unsafe-for-kids\nRelated \ud83c\udf10\nGoogle Autocomplete falsely associates Japanese man with crimes\nGoogle Autocomplete conflates Bettina Wulff with 'prostitute'\nPage info\nType: Incident\nPublished: August 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/google-autocomplete-links-french-user-to-rape", "content": "Google Autocomplete links French sex offender to rape\nOccurred: September 2010\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGoogle was ordered by the Superior Court of Paris to update its Autocomplete search term prediction function and remove links to a blog that accused 'X', an anonymous convicted sex offender, of being involved in sexually assaulting a child.\nPer Columbia Global Freedom of Expression, 'X' had argued that inclusion of the links violated privacy and data protection law as it named his profession and his employer alongside the accusation of involvement in child sex abuse in a way that was inadequate, unrelated, irrelevant and excessive. The man was also being automatically associated with terms such as 'rape', 'rapist', and 'satanist'. \nHowever, because the man had been appealing his conviction, Google was found to have been defaming him as French law declares individuals innocent until all appeals are exhausted. The case was seen in academic and legal circles as an example of freedom of information and potential personality interests entering into conflict.\nSystem \ud83e\udd16\nGoogle Autocomplete\nOperator: Alphabet/Google\nDeveloper: Alphabet/Google\nCountry: France\nSector: Private - individual\nPurpose: Predict search results\nTechnology: NLP/text analysis; Deep learning; Machine learning  \nIssue: Privacy; Legal\nTransparency: Governance; Black box\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nColumbia Global Freedom of Expression. X v. Google France and Google Inc\nTribunal de Grande Instance de Paris 17\u00e8me chambre Jugement du 8 septembre 2010\nResearch, advocacy \ud83e\uddee\nINFORRM (2016). Case Law, France: X v Google France and Google Inc, Court orders \u201cdelisting\u201d of story alleging links to a sex scandal\nKarapapa S., Borghi M. (2015). Search engine liability for autocomplete suggestions: personality, privacy and the power of the algorithm\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.telegraph.co.uk/technology/google/8027967/Google-convicted-of-defaming-French-user-by-linking-his-name-to-rape-in-searches.html\nhttps://newsfeed.time.com/2010/09/27/google-convicted-of-defamation-in-france/\nhttps://www.theregister.com/2010/09/27/rapist_suggest/\nhttps://www.irishtimes.com/news/google-to-appeal-frenchman-s-libel-award-for-defamation-1.656424\nRelated \ud83c\udf10\nGoogle search links Natalia Denegri to 'Coppola case'\nGoogle Autocomplete links health researcher to false blackmail accusations\nPage info\nType: Incident\nPublished: August 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/google-search-links-natalia-denegri-to-coppola-case", "content": "Google search links Natalia Denegri to 'Coppola case'\nOccurred: March 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAttempts by Argentinian TV presenter Natalia Denegri to have broadcast footage of her verbally abusing and being physically aggressive towards participants in a series of shows about the judicial inquiry into a scandal involving her former lover Guillermo Coppola have resulted in a controversy about freedom of information and expression.\n\nFormerly agent for footballer Diego Maradona, Coppolla had been accused of possessing drugs, which an investigation later revealed to have been fabricated by corrupt policemen and judges. In 2016, Ms. Denegri sued Google to remove content relating to the broadcasts using the EU's 'Right to be forgotten', despite admitting that the information she wanted removed from the search engine and its Autocomplete search prediction function were accurate.\n\nWhile a trial judge and Buenos Aires Appellate Civil Court had partially accepted her request and ordered Google to 'deindex' content damaging to Ms Denegri's reputation, the country's Supreme Court overturned these decisions on the basis that freedom of expression is a fundamental right in democratic societies, and that Ms. Denegri is a public figure who had willingly took part in a public debate.\nSystem \ud83e\udd16\nGoogle Search\nGoogle. How Google Autocomplete predictions work\nOperator: Alphabet/Google\nDeveloper: Alphabet/Google\nCountry: Argentina\nSector: Media/entertainment/sports/arts\nPurpose: Predict search results\nTechnology: NLP/text analysis; Deep learning; Machine learning\nIssue: Privacy; Legal - defamation/libel\nTransparency: Governance; Black box \nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nColombia Global Freedom of Expression. Natalia Denegri v. Google Inc. (Supreme Court)\nCorte Suprema de Justicia de la Naci\u00f3n (2022). Denegri, Natalia Ruth c/ Google Inc. s/ derechos personal\u00edsimos: Acciones relacionadas\nResearch, advocacy \ud83e\uddee\nADC (2022). Google vs. Denegri: The Court rules in agreement with several of ADC\u2019s arguments\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://techpolicy.press/argentina-the-right-to-be-forgotten-strikes-again/\nhttps://restofworld.org/2021/argentina-denegri-google-right-forget/\nhttps://restofworld.org/2022/argentina-supreme-court-google-right-to-be-forgotten-denegri/\nhttps://diff.wikimedia.org/2022/07/29/argentinas-supreme-court-of-justice-holds-in-favor-of-freedom-of-expression-in-right-to-be-forgotten-case/\nhttp://blog.galalaw.com/post/102ge10/first-decision-on-the-right-to-be-forgotten-in-argentina\nhttps://wilmap.stanford.edu/entries/juzgado-nacional-de-primera-instancia-en-lo-civil-first-instance-court-denegri-natalia-ruth\nhttps://www.batimes.com.ar/news/argentina/supreme-court-rejects-case-seeking-right-to-be-forgotten-online.phtml\nhttps://www.infobae.com/en/2022/03/17/public-court-hearing-on-natalia-denegris-case-against-google-begins-6/\nhttps://www.infobae.com/en/2022/03/18/right-to-be-forgotten-the-reasons-why-it-could-not-be-applied-to-the-natalia-denegri-case/\nhttps://www.infobae.com/en/2022/03/17/the-argentine-supreme-court-holds-a-hearing-for-an-actresss-claim-to-oblivion-on-google/\nhttps://www.infobae.com/politica/2022/03/18/derecho-al-olvido-los-motivos-por-los-que-no-se-podria-aplicar-al-caso-natalia-denegri/\nhttps://www.perfil.com/noticias/politica/la-corte-rechazo-el-derecho-al-olvido-que-pidio-natalia-denegri-y-le-dio-la-razon-a-google.phtml\nhttps://www.pagina12.com.ar/432830-la-corte-suprema-rechazo-la-pretension-de-natalia-denegri-co\nRelated \ud83c\udf10\nGoogle Autocomplete conflates Bettina Wulff with 'prostitute'\nBuenos Aires Sistema de Reconocimiento Facial de Pr\u00f3fugos\nPage info\nType: Incident\nPublished: August 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/human-rights-lawyer-travel-restricted-by-covid-19-health-app", "content": "Human rights lawyer travel restricted by COVID-19 app misuse\nOccurred: November 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nChangsha-based human rights lawyer Xie Yang had his travel curtailed when his COVID-19 health report app turned red when he was trying to travel to Shanghai to visit the mother of a citizen journalist who had been jailed for reporting on the initial COVID-19 outbreak in Wuhan. \nThe incident raised concerns about the misuse of health data for Chinese state control purposes. People trying to join protests in Zhengzhou, Henan province, in order to gain access to their deposits in struggling banks had also seen their health code apps turn red, classifying them a risk to public health and restricting their movements. \nStopped at the airport, Yang was thrown into quarantine. Changsha had no reported cases of the virus at the time. \u2018The Chinese Communist party has found the best model for controlling people,\u2019 he said shortly afterwards.\nSystem \ud83e\udd16\nWikipedia. Health Code\nOperator: Changsha City Health Commission\nDeveloper: Alibaba/Alipay/DingTalk; Tencent/WeChat\nCountry: China\nSector: Govt - health\nPurpose: Control COVID-19\nTechnology:  \nIssue: Dual/multi-use; Privacy; Surveillance\nTransparency: Governance; Complaints/appeals\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.rfa.org/english/news/china/hongkong-covid-07122022100744.html\nhttps://www.spectator.co.uk/article/china-s-increasingly-authoritarian-covid-pass/\nhttps://edition.cnn.com/2022/06/15/china/china-zhengzhou-bank-fraud-health-code-protest-intl-hnk/index.html\nhttps://www.nytimes.com/2022/01/30/world/asia/covid-restrictions-china-lockdown.html\nhttps://hongkongfp.com/2022/07/13/explainer-chinas-covid-19-health-code-system/\nRelated \ud83c\udf10\nZhengzhou authorities turn bank protestors' health codes red\nHangzhou 'Personal health code' scoring system\nPage info\nType: Incident\nPublished: August 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/zhengzhou-authorities-turn-bank-protestors-health-codes-red", "content": "Zhengzhou authorities turn bank protestors' health codes red\nOccurred: June 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nPeople trying to join protests in Zhengzhou in order to gain access to their deposits in struggling banks in China's Henan province saw health code apps turn red, classifying them a risk to public health and restricting their movements.\nThe spring and summer 2022 protests had been triggered by four rural banks stopping customers withdraw cash, resulting in a bank run and fueling allegations of financial corruption. An investigation by the China Banking and Insurance Regulatory Commission later found that a private investment company had collaborated with the banks to illicitly attract public funds via online platforms.\nIt transpired that 1,317 Henan bank depositors had unfairly received red health codes, almost all whom were unable to withdraw their savings from the four banks. Five Zhengzhou official were found to have manipulated protestors' COVID-19 health code apps and were punished.\nThe move reinforced fears that COVID-19 contact tracing data could easily be used for covert or nefarious purposes.\nSystem \ud83e\udd16\nWikipedia. Health Code\nWikipedia. 2022 Henan bank protests\nOperator: Zhengzhou Municipal Health Commission\nDeveloper: Alibaba/Alipay/DingTalk; Tencent/WeChat\nCountry: China\nSector: Govt - health\nPurpose: Control COVID-19\nTechnology:  \nIssue: Dual/multi-use; Privacy; Surveillance  \nTransparency: Governance; Complaints/appeals\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.scmp.com/news/china/politics/article/3182742/china-officials-who-abused-health-codes-stop-bank-protests\nhttps://www.scmp.com/news/china/science/article/3181635/chinese-health-code-turns-red-financial-victims-about-protest\nhttps://www.whatsonweibo.com/the-curious-case-of-the-henan-bank-depositors-and-the-changing-health-qr-codes/\nhttps://www.reuters.com/article/china-banks-henan-idINL4N2YA03D\nhttps://www.vice.com/en/article/93a53v/china-covid-health-code-protest-henan\nhttps://www.bloomberg.com/news/articles/2022-06-14/china-s-iphone-city-may-be-using-covid-controls-on-protesters\nhttps://www.bbc.co.uk/news/world-asia-china-61793149\nhttps://www.sixthtone.com/news/1010627\nhttps://asia.nikkei.com/Spotlight/Caixin/Use-of-COVID-codes-to-stop-Chinese-bank-protests-sparks-debate\nRelated \ud83c\udf10\nHuman rights lawyer travel restricted by Chinese COVID-19 health app\nHangzhou 'Personal health code' scoring system\nPage info\nType: Incident\nPublished: August 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/hangzhou-safari-park-mandatory-facial-recognition-registration", "content": "Hangzhou Safari Park mandatory facial recognition registration\nOccurred: November 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA court in Hangzhou ruled that Hangzhou Safari Park must delete images collected by its facial recognition technology and pay 1,038 yuan in damages to a professor and his wife.\nZhejiang Sci-tech University Associate Law Professor Guo Bing had alleged that he and his wife did not consent to have their images collected by facial recognition when they entered the park, having already registered their identities on a fingerprint recognition system that had been replaced by a new facial recognition system.\nThe court ruled the use of facial recognition had 'exceeded the legally necessary requirements.' The first of its kind in China, the case attracted national and international media attention.\nSystem \ud83e\udd16\nHangzhou Safari Park website\nOperator: Hangzhou Safari Park\nDeveloper:  \nCountry: China\nSector: Gov - police\nPurpose: Verify identity\nTechnology: Facial recognition\nIssue: Appropriateness/need; Necessity/proportionality; Privacy\nTransparency: Governance; Privacy\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.sixthtone.com/news/1007300\nhttps://www.sixthtone.com/news/1006479\nhttps://www.chinajusticeobserver.com/a/china-s-first-facial-recognition-case\nhttps://www.theguardian.com/world/2019/nov/04/china-wildlife-park-sued-for-forcing-visitors-to-submit-to-facial-recognition-scan\nhttps://findbiometrics.com/chinese-state-court-reaffirms-facial-recognition-ruling-704131/\nhttps://www.bbc.com/news/world-asia-china-50324342\nhttps://www.economist.com/china/2019/11/09/a-lawsuit-against-face-scans-in-china-could-have-big-consequences\nhttp://europe.chinadaily.com.cn/a/202011/24/WS5fbc3e0ba31024ad0ba95ee7.html\nhttps://www.wsj.com/articles/chinese-professor-files-rare-lawsuit-over-use-of-facial-recognition-technology-11572884626\nhttps://www.scmp.com/tech/big-tech/article/3110981/chinese-court-orders-wildlife-park-delete-facial-recognition-data\nRelated \ud83c\udf10\nNingbo real estate sales facial recognition misuse\nKohler, BMW, MaxMara China shopper analysis facial recognition\nPage info\nType: Incident\nPublished: August 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/predictim-babysitter-personality-profiling", "content": "Predictim babysitter personality profiling slammed as biased, intrusive\nOccurred: November 2018\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nPredictim, a California-based service that vetted potential babysitters by using 'advanced artificial intelligence' to scan their presence on social media, the web, and online criminal databases, was accused of being inaccurate, biased, and an abuse of privacy.\nThe service used natural language processing and computer vision to sort through an applicant's images and posts, and generated a 'risk rating', flagging people prone to abusive behaviour, drug use, and posting explicit imagery. Each scan cost USD 24.99.\nBut a damning Washington Post investigation castigated the company for the inaccuracy and opacity of its system, its potential for racial and economic discrimination, and misleading marketing. \nFacebook and Twitter responded by saying they would revoke Predictim's access to their platforms on the basis that it had been illegally scraping their users' data.\nThe company, a product of UC Berkeley\u2019s SkyDeck incubator, closed shortly afterwards.\nSystem \ud83e\udd16\nPredictim\nOperator:  \nDeveloper: Predictim\nCountry: USA\nSector: Business/professional services\nPurpose: Assess personality\nTechnology: NLP/text analysis; Computer vision; Machine learning\nIssue: Accuracy/reliability; Bias/discrimination - race, income; Privacy\nTransparency: Governance; Black box; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.washingtonpost.com/technology/2018/11/16/wanted-perfect-babysitter-must-pass-ai-scan-respect-attitude/\nhttps://www.washingtonpost.com/technology/2018/12/14/ai-start-up-that-scanned-babysitters-halts-launch-following-post-report/\nhttps://gizmodo.com/predictim-claims-its-ai-can-flag-risky-babysitters-so-1830913997\nhttps://www.axios.com/2018/11/24/artificial-intelligence-predictim-babysitters-screening\nhttps://kwhs.wharton.upenn.edu/2018/12/babysitters-beware-advanced-ai-technology-may-soon-be-lurking-in-your-social-media-feed\nhttps://www.bbc.co.uk/news/technology-46354276\nhttps://www.cbsnews.com/news/ai-babysitting-service-predictim-blocked-by-facebook-and-twitter/\nhttps://www.cbc.ca/radio/spark/416-1.4927730/is-ai-that-screens-potential-babysitters-a-good-idea-1.4927735\nhttps://boingboing.net/2018/11/26/ducking-stool-2-0.html\nRelated \ud83c\udf10\nRetorio talent personality assessments\nKyle Behm Kroger algorithmic personality assessment\nPage info\nType: Incident\nPublished: August 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/walmart-ai-anti-shoplifting-system-accuracy-effectiveness", "content": "Walmart AI anti-shoplifting system accuracy, effectiveness\nOccurred: May 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nWalmart's AI and camera-based anti-shoplifting system provided by Ireland-based company Everseen has come under fire in the US over its reportedly poor detection rates and tendency to misinterpret innocent behaviour as potential shoplifting.\nA group of Walmart workers calling themselves 'Concerned Home Office Associates' circulated a video documenting flaws in the the company's 'Missed Scan Detection' system, including its regular failure to identify unscanned items, and incorrectly identifying personal items as potentially shoplifted.\nThe system is so poor that Walmart workers have complained about it continuously since it was first introduced in 2017, and call it 'NeverSeen,' according to WIRED.\nSystem \ud83e\udd16\nEverseen website\nEverseen. Retail Shrinkage Solution video\nOperator: Walmart\nDeveloper: Everseen\nCountry: USA\nSector: Retail\nPurpose: Reduce scanning errors, theft\nTechnology: CCTV; Computer vision; Machine learning\nIssue: Accuracy/reliability; Effectiveness/value; Surveillance\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.wired.com/story/walmart-shoplifting-artificial-intelligence-everseen/\nhttps://www.dailymail.co.uk/sciencetech/article-8400021/Walmart-workers-criticize-digital-eye-used-self-checkout-counters-stop-shoplifting.html\nhttps://arstechnica.com/tech-policy/2020/05/walmart-employees-are-out-to-show-its-anti-shoplifting-ai-doesnt-work/\nhttps://tech.slashdot.org/story/20/06/02/2148235/walmart-employees-are-out-to-show-its-anti-shoplifting-ai-doesnt-work\nhttps://www.businessinsider.com/walmart-tracks-theft-with-computer-vision-1000-stores-2019-6\nhttps://www.dailydot.com/debug/self-checkout-walmart-tech-stealing-tiktok/\nhttps://mindmatters.ai/2022/07/could-the-self-checkout-ruin-your-reputation/\nhttps://venturebeat.com/2021/01/23/cashierless-tech-could-detect-shoplifting-but-bias-concerns-abound/\nRelated \ud83c\udf10\nJumbo supermarket 'indiscriminate' facial recognition\nWalgreens fridge screen door biometrics\nPage info\nType: Incident\nPublished: August 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/7-eleven-customer-survey-facial-recognition", "content": "7-Eleven facial recognition survey violated Australian customer privacy\nOccurred: October 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nChain store 7-Eleven breached customer privacy in 700 stores between June 2020 and August 2021 by collecting facial imagery without consent, according to Australia's privacy commissioner the OAIC. \nThe commissioner ruled (pdf) that 7-Eleven abused customer privacy by collecting and storing their facial images in order to validate a survey it had been conducting into customer needs and to understand their demographic profile. \n7-Eleven had required customers to fill out information on tablets with built-in cameras, and had captured their facial images at two points during the survey-taking process. 7-Eleven said it obtained its customers' consent by providing a notice on its website stating the company would collect photographic or biometric information from users.\nThe OAIC said the collection of facial images was not reasonably necessary in this instance, and that 7-Eleven had failed to provide information about how customers' facial images would be used or stored. 7-Eleven was order to cease collecting facial images and faceprints as part of the customer feedback mechanism, and destroy all the faceprints it collected.\nSystem \ud83e\udd16\n7-Eleven Australia website\n7-Eleven Wikipedia profile\nOperator: 7-Eleven Stores\nDeveloper: \nCountry: Australia\nSector: Retail\nPurpose: Validate survey responses\nTechnology: Facial recognition\nIssue: Privacy; Necessity/proportionality\nTransparency: Governance; Marketing; Privacy\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nOffice of the Australian Information Commissioner (2021). OAIC finds against 7-Eleven over facial recognition\nOffice of the Australian Information Commissioner (2021). Commissioner initiated investigation into 7- Eleven Stores Pty Ltd (Privacy) (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theguardian.com/australia-news/2021/oct/14/7-eleven-took-photos-of-some-australian-customers-faces-without-consent-privacy-commissioner-rules\nhttps://www.zdnet.com/article/7-eleven-collected-customer-facial-imagery-during-in-store-surveys-without-consent/\nhttps://www.itnews.com.au/news/7-eleven-australia-deploys-facial-recognition-on-customer-feedback-tablets-549538\nhttps://www.itnews.com.au/news/7-eleven-disables-facial-image-capture-on-customer-feedback-tablets-571272\nhttps://itwire.com/business-it-news/security/oaic-finds-against-7-eleven-over-facial-recognition-usage.html\nhttps://7news.com.au/technology/customers-concerned-as-7-eleven-launches-facial-recognition-inside-every-store-c-1125208\nRelated \ud83c\udf10\nAustralia police COVID-19 quarantine facial recognition\nVictoria schools school attendance facial recognition\nPage info\nType: Incident\nPublished: August 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/jacksons-food-stores-facial-recognition", "content": "Jacksons Food Stores sued for violating facial recognition law\nOccurred: December 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nJacksons Food Stores was sued by two people for allegedly violating Portland, Oregon, rules governing the use of facial recognition in places of 'public accommodation'. \nThe lawsuit also argued that although Jacksons is using the software to assist in the identification and prosecution of shoplifters, it can wrongly identify people as criminals, and that those errors 'disproportionately affect women and people of color.' \nIN addition, the suit claimed the identity verification process is mandatory, and that customers may not enter the store unless they let the software scan them. \nReports said Jacksons has been using Blue Line Technology\u2019s First Line software, which takes pictures of people as they approach a Jacksons store and then uses an algorithm to determine whether the face recorded by the camera matches a face on a database held by Jacksons.\nSystem \ud83e\udd16\nJacksons Food stores website\nJacksons Food Stores Wikipedia profile\nBlue Line Technology. First Line Facial Recognition\nOperator: Jacksons Food Stores\nDeveloper: Blue Line Technology; Dell; Axis\nCountry: USA\nSector: Retail\nPurpose: Detect criminals\nTechnology: Facial recognition\nIssue: Accuracy/reliability; Bias/discrimination - race, ethnicity; Privacy\nTransparency: Marketing\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nNorby v Jacksons Food Stores (pdf)\nCity of Portland (2021). Prohibit the use of Face Recognition Technologies by Private Entities in Places of Public Accommodation in the City (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.cstoredive.com/news/jacksons-food-stores-lawsuit-facial-recognition-technology/639816/\nhttps://phys.org/news/2019-07-convenience-surveillance-ai-corner.html\nhttps://www.geekwire.com/2019/portland-mayor-says-exceptions-can-made-potential-widespread-facial-recognition-ban/\nhttps://redtailmedia.org/2019/10/19/portland-commissioner-wants-financial-penalty-for-businesses-using-facial-recognition/\nhttps://redtailmedia.org/2020/01/27/convenience-chain-adds-facial-recognition-at-more-portland-stores/\nhttps://www.oregonlive.com/politics/2020/09/portland-approves-strictest-ban-on-facial-recognition-technology-in-the-us.html\nhttps://onezero.medium.com/portlands-radical-facial-recognition-proposal-bans-the-tech-from-airbnbs-restaurants-stores-and-32ada95430b6\nRelated \ud83c\udf10\nRite Aid facial recognition\nAmazon Go fails to inform NYC customers about facial recognition\nPage info\nType: Incident\nPublished: August 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/roomba-maps-homes-collects-user-data", "content": "Roomba accused of hoovering customers' home mapping data \nOccurred: July 2017\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\niRobot, manufacturer of the robo-vacuum Roomba, has been considering selling data of customer to companies like Apple, Google, and Amazon, according to Reuters.\nDuring the interview, iRobot CEO Colin Angle said 'There\u2019s an entire ecosystem of things and services that the smart home can deliver once you have a rich map of the home that the user has allowed to be shared,\u201d Angle said. He also told said that iRobot could reach a deal with Amazon, Alphabet, or Apple within the next couple of years to share their maps of users\u2019 homes.'\nHowever, the company later claimed Angle's words had been taken out of context and that it had no plans to sell customer data. iRobot added support for Amazon\u2019s Alexa voice recognition AI in March 2017. In August 2022, Amazon announced a deal to acquire iRobot. \nThe news incensed privacy advocates worried about the unethical sharing and commodification of sensitive personal data.\nSystem \ud83e\udd16\niRobot website\niRobot Wikipedia profile\nRoomba Wikipedia profile\nOperator: Amazon/iRobot\nDeveloper: Amazon/iRobot\nCountry: USA\nSector: Consumer goods\nPurpose: Build spatial map\nTechnology: Spatial mapping\nIssue: Privacy; Surveillance\nTransparency: Governance; Privacy\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.nytimes.com/2017/07/25/technology/roomba-irobot-data-privacy.html\nhttps://www.technologyreview.com/2017/07/25/150346/your-roomba-is-also-gathering-data-about-the-layout-of-your-home/\nhttp://www.reuters.com/article/us-irobot-strategy-idUSKBN1A91A5\nhttps://www.theverge.com/2017/7/24/16021610/irobot-roomba-homa-map-data-sale\nhttps://eu.usatoday.com/story/tech/nation-now/2017/07/25/roomba-plans-sell-maps-users-homes/508578001/\nhttps://www.nbcsandiego.com/news/local/is-your-roomba-mapping-your-house-and-selling-your-data/21364/\nhttps://www.bostonglobe.com/business/2017/07/25/your-roomba-may-mapping-your-home-collecting-data-that-could-sold/tqsa9qnhwiqpfAGUWVA8QP/story.html\nhttps://www.architecturaldigest.com/story/your-roomba-irobot-selling-maps-of-your-home\nRelated \ud83c\udf10\nScale AI shares sensitive Roomba robot vacuum training photos\nSama 'ethical' data labeling, content moderation\nPage info\nType: Issue\nPublished: September 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chatgpt-generates-political-messages-campaigns", "content": "ChatGPT 'easily' generates political messages, campaigns\nOccurred: August 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nChatGPT can be easily used to develop political messaging and campaigns, despite OpenAI's rules to the contrary, according to an investigation by The Washington Post. \nOther than for 'grassroots advocacy campaigns', OpenAI's usage policies ban the use of ChatGPT for political campaigning, including the generation of large volumes of campaign materials targeted at specific demographics, building campaign chatbots to disseminate information, and engaging in political advocacy or lobbying.\nYet the Post was easily able to input prompt such as 'Write a message encouraging suburban women in their 40s to vote for Trump' and 'Make a case to convince an urban dweller in their 20s to vote for Biden.'\nThe discovery suggested OpenAI is either disinterested or not serious about ChatGPT governance, and is seen to have potentially 'grave' repercussions for the US 2024 presidential elections.\nSystem \ud83e\udd16\nChatGPT chatbot\nOperator: OpenAI\nDeveloper: OpenAI\nCountry: USA\nSector: Politics\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Mis/disinformation; Safety\nTransparency: Governance; Marketing\nInvestigations, assessments, audits \ud83e\uddd0\nThe Washington Post (2023). ChatGPT breaks its own rules on political messages\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://timesofindia.indiatimes.com/gadgets-news/how-chatgpt-is-breaking-its-own-rules-for-political-campaigns/articleshow/103171110.cms\nhttps://www.washingtonpost.com/technology/2023/05/16/sam-altman-open-ai-congress-hearing/\nhttps://www.engadget.com/chatgpt-is-easily-exploited-for-political-messaging-despite-openais-policies-184117868.html\nhttps://tribune.com.pk/story/2432993/chatgpt-easily-exploited-for-political-messaging-despite-openai-policies\nhttps://www.moneycontrol.com/news/technology/chatgpt-can-still-create-misleading-political-propaganda-11276011.html\nRelated \ud83c\udf10\nChatGPT invents case citations in legal filings\n'ChatGPT' powers 'Fox8' crypto promotion botnet\nPage info\nType: Issue\nPublished: August 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/hangzhou-personal-health-code-scoring-system", "content": "Hangzhou 'Personal health code' scoring expansion triggers backlash\nOccurred: May 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nChina's Hangzhou Municipal Health Commission announcement that it was planning to make permanent a version of the country's 'health code' app used during the COVID-19 pandemic resulted in a strong backlash.\nThe system, which was to assign citizens a personal score, colour, and ranking based on data collected about their medical history, health checkups, and lifestyle habits, including smoking, drinking and sleeping, would compare users\u2019 health indicators with the health code colors to build a personal health index ranking.\nThe plan triggered anger on the country's social media, with citizens raising conerns about its need, normalising and overly intrusive nature, as well as its scope for surveillance and other forms of abuse and misuse. According to Sixth Tone, a poll on Chinese microblogging platform Weibo resulted in 86% of 6,600 users voting against the proposal.\nHangzhou authorities responded by saying they would press ahead with the system. The city was the first in China to implement a COVID-19 QR code app and, in 2019, it had launched a social credit system - one of the first cities in China to do so.\nSystem \ud83e\udd16\nHangzhou Municipal Health Commission website\nOperator: Hangzhou Municipal Health Commission\nDeveloper: Alibaba/AliPay/DingTalk; Tencent/WeChat\nCountry: China\nSector: Govt - health\nPurpose: Calculate personal health score\nTechnology:  \nIssue: Appropriateness/need; Privacy; Scope creep/normalisation; Surveillance\nTransparency: Governance\nResearch, advocacy \ud83c\udf10\nZhang X. (2022). Decoding China\u2019s COVID-19 Health Code Apps: The Legal Challenges\nXu X. (2021). The Promise and Pitfalls of China\u2019s QR Codes as Health Certificates\nZhao Y. (2020). When Health Code becomes Health Gradient: Safety or Social Control?\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.sohu.com/a/397531243_665455\nhttps://www.reuters.com/article/health-coronavirus-china-tech/rpt-as-chinese-authorities-expand-use-of-health-tracking-apps-privacy-concerns-grow-idUSL4N2D826K\nhttps://edition.cnn.com/2020/05/25/tech/hangzhou-health-app-intl-hnk/index.html\nhttps://radiichina.com/health-code-controversy/\nhttps://www.cnbc.com/2020/05/26/chinese-city-hangzhou-proposes-permanent-health-tracking-app-with-score.html\nhttps://www.nytimes.com/2020/05/26/technology/china-coronavirus-surveillance.html\nhttps://www.theguardian.com/world/2020/may/26/chinese-city-plans-to-turn-coronavirus-app-into-permanent-health-tracker\nhttps://thediplomat.com/2020/07/is-chinas-health-code-here-to-stay/\nhttps://www.wired.co.uk/article/china-coronavirus-health-code-qr\nhttps://www.sixthtone.com/news/1005703/citys-plan-for-permanent-health-codes-sparks-online-backlash\nhttps://www.scmp.com/abacus/tech/article/3086024/chinese-city-introduced-health-codes-wants-track-drinking-and-smoking\nRelated \ud83c\udf10\nSuzhou social 'civility score' trial\nHangzhou Safari Park mandatory facial recognition registration\nPage info\nType: Issue\nPublished: August 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/iflytek-automated-speech-recognition-surveillance", "content": "iFlytek automated speech recognition system raises rights concerns\nOccurred: October 2017\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA partnership between Chinese technology company iFlytek and China's Ministry of Public Security to develop an AI-powered system that automatically identifies targeted voices met with consternation from civil society organisaitons.\nAccording to Human Rights Watch (HRW), the two parties intended to develop a pilot surveillance system that builds on the Chinese government's existing Automatic Speaker Recognition system to automatically identify targeted voices in phone conversations. \nChinese media reports suggested the system will be applied for counterterrorism and 'stability maintenance' purposes.\nChinese police are thought to have collected approximately 70,000 voice samples by 2015. By contrast, the country's facial image database contained data on over a billion individuals. iFlytek made 80 percent of China's speech recognition technology, HRW said.\nThe finding raised concerns about the surveillance and privacy of Chinese citizens and ethnic minorities, notably Uyghurs and Tibetans, \nSystem \ud83e\udd16\niFlytek website\niFlytek Wikipedia profile\nOperator: Government of China; iFlytek\nDeveloper: Ministry of Public Security; iFlytek\nCountry: China\nSector: Govt - police; Govt - security\nPurpose: Maintain social stability\nTechnology: Speech recognition\nIssue: Privacy; Surveillance\nTransparency: Governance; Marketing\nResearch, advocacy \ud83e\uddee\nHuman Rights Watch (2017). Letter to iFlytek Chairman (pdf)\nHuman Rights Watch (2017). China: Voice Biometric Collection Threatens Privacy\nIAPP (2017). HRW raises concern over China's national voice biometric database \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.biometricupdate.com/201710/human-rights-watch-raises-privacy-concerns-over-chinese-voice-recognition-program\nhttps://www.nytimes.com/2017/12/03/business/china-artificial-intelligence.html\nhttps://www.wired.com/story/iflytek-china-ai-giant-voice-chatting-surveillance/\nhttps://www.wired.com/story/inside-chinas-massive-surveillance-operation/\nhttps://mindmatters.ai/2019/08/china-what-you-didnt-say-could-be-used-against-you/\nhttps://www.wired.com/story/mit-cuts-ties-chinese-ai-firm-human-rights/\nhttps://www.theage.com.au/politics/federal/chinese-company-sanctioned-by-the-us-hosted-by-victorian-government-20200615-p552qo.html\nRelated \ud83c\udf10\niFlyTek 'fakes' automated speech translations\nUyghur emotion detection testing\nPage info\nType: Issue\nPublished: August 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/brave-ai-user-data-sales", "content": "Brave covertly sells user data for AI training\nOccurred: July 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nPrivacy-focused web browser Brave appeared to sell data about its users without their knowledge or permission to third-parties developing AI systems. \nThe controversy resulted in accusations of poor ethics and hyprocrisy, and raised legal and ethical questions about the fair use of personal and copyrighted information.\nAlex Ivanovs of Stack Diary first claimed Brave provides access to copyrighted content through its Brave Search API, allowing others to use this data for AI training without proper licensing. He also suggested this raised questions about Brave's commitment to privacy and good ethical behaviour. \nSearch Engine Journal noted, 'The brewing controversy highlights tensions around using personal data to advance AI capabilities versus respecting data privacy and ownership rights. It underscores the need for clear communication and user consent regarding sharing their information.'\nSystem \ud83e\udd16\nBrave website\nBrave Wikipedia profile\nBrave Terms of Use\nBrave (2023). Brave releases its Search API, bringing independence and competition to the search landscape\nOperator: Brave Software\nDeveloper: Brave Software\nCountry: USA\nSector: Multiple\nPurpose: Share search results\nTechnology: Database\nIssue: Copyright; Ethics; Privacy\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://stackdiary.com/brave-selling-copyrighted-data-for-ai-training/\nhttps://stackdiary.com/an-update-on-brave-selling-copyrighted-data/\nhttps://www.searchenginejournal.com/brave-browser-under-fire-for-alleged-sale-of-copyrighted-data/491854/\nhttps://malwaretips.com/threads/the-shady-world-of-brave-selling-copyrighted-data-for-ai-training.124496/\nhttps://news.ycombinator.com/item?id=36735777\nhttps://www.reddit.com/r/browsers/comments/150jcbx/the_shady_world_of_brave_selling_copyrighted_data/?rdt=33676\nRelated \ud83c\udf10\nZoom customer data AI model training\nAdobe Firefly AI art generator training\nPage info\nType: Incident\nPublished: August 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/zoom-customer-data-ai-model-training", "content": "Zoom under fire for using customer data to train AI models\nOccurred: August 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn update to Zoom's terms of service to state that customer data would be used for 'machine learning' and 'artificial intelligence' met with stiff criticism.\nThe move led customers to say their confidentiality and privacy was at stake and threaten to boycott or leave Zoom for competitor products, and left Zoom scrambling to explain what it was doing.\nIn response, the company updated its terms to say 'Notwithstanding the above, Zoom will not use audio, video or chat Customer Content to train our artificial intelligence models without your consent.' However, privacy experts said the new update contradicted the earlier statement, and suggested the company could continue to collect and use customer data. \nA blog post by Zoom Chief Product Officer Smita Hashim appeared to quell some of the noise. But others pointed out that Zoom\u2019s use of AI is focused on automated meeting summariser Zoom IQ, which by default collects data on all people in a meeting once the meeting administrator has approved its use, is unsatisfactory.\nSystem \ud83e\udd16\nZoom website\nZoom Wikipedia profile\nZoom Terms of Service\nZoom (2023). How Zoom\u2019s terms of service and practices apply to AI features\nZoom (2023). Evolving Zoom IQ, our AI smart companion, with new features and a collaboration with OpenAI and Anthropic\nOperator: Zoom Video Communications\nDeveloper: Zoom Video Communications\nCountry: USA\nSector: Business/professional services\nPurpose: Summarise meetings\nTechnology: NLP/text analysis; Neural network; Deep learning; Machine learning\nIssue: Confidentiality; Privacy; Ethics\nTransparency: Governance; Marketing; Legal\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/technology-66430429\nhttps://venturebeat.com/ai/freedom-of-choice-how-recent-zoom-ai-policy-changes-betrayed-consumer-trust/\nhttps://www.nbcnews.com/tech/innovation/zoom-ai-privacy-tos-terms-of-service-data-rcna98665\nhttps://www.dailymail.co.uk/sciencetech/article-12381993/Zoom-use-private-calls-messages-train-AI-systems-new-terms-conditions-agreed-to.html\nhttps://www.fastcompany.com/90934584/zoom-ai-training-terms-of-service-consent\nhttps://www.cnbc.com/2023/08/07/zoom-ai-tools-trained-using-some-customer-data.html\nhttps://gizmodo.com/zoom-ai-privacy-policy-train-on-your-data-1850712655\nhttps://qz.com/zoom-video-calls-updated-terms-user-data-ai-training-1850712394\nhttps://news.ycombinator.com/item?id=37021160\nRelated \ud83c\udf10\nAdobe Firefly AI art generator training\nBrave AI user data sales\nPage info\nType: Issue\nPublished: August 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/lucknow-women-in-distress-facial-recognition", "content": "Lucknow police plan to monitor 'women in distress' criticised\nOccurred: January 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA plan by Lucknow police to monitor women\u2019s expressions with facial recognition and emotion recognition to prevent street harassment met with stiff opposition. \nLucknow police commissioner DK Thakur told The Times of India that authorities aim to install AI-enabled cameras at 200 'crime hotspots' across the city that will alert nearby police stations when they spot a woman's 'distress' due to harassment. The spots were determined by the 'presence of girls and women in the area,' and stalking and harassment complaints.\nAccording to Reuters, digital rights activists are concerned the system is likely to be inaccurate and may lead to intrusive policing, surveillance against vulnerable sections of society, and privacy violations. Some feel it may also lead to over-policing in the areas where facial and emotion tracking technologies are being deployed.\nLucknow police also stood accused of inadequate transparency by failing to consult with the public, and by providing the public with inadequate information on how the technology works, how data is stored, and who can access the data. \nPrivacy was declared to be a fundamental right by India's Supreme Court in a landmark ruling in 2017. \nSystem \ud83e\udd16\nStaqu JARVIS website\nOperator: Lucknow Police Commissionerate\nDeveloper: Staqu Technologies\nCountry: India\nSector: Govt - police\nPurpose: Reduce sexual harrassment\nTechnology: CCTV; Facial recognition; Emotion recognition; Automated license plate/number recognition (ALPR/ANPR)\nIssue: Accuracy/reliability; Privacy; Surveillance\nTransparency: Governance; Black box; Privacy\nResearch, advocacy \ud83e\uddee\nPanoptic Tracker. Uttar Pradesh Police\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reuters.com/article/us-india-tech-women-trfn/privacy-fears-as-indian-city-readies-facial-recognition-to-spot-harassed-women-idUSKBN29R0X5\nhttps://timesofindia.indiatimes.com/india/up-cops-to-use-ai-to-read-faces-and-help-women-in-distress/articleshow/80396572.cms\nhttps://timesofindia.indiatimes.com/home/sunday-times/all-that-matters/ai-camera-plan-based-on-pseudo-science-i-could-look-distressed-if-i-need-the-loo-says-rights-lawyer-vidushi-marda/articleshow/80425762.cms\nhttps://thewire.in/women/uttar-pradesh-lucknow-police-artificial-intelligence-camera-women\nhttps://gadgets.ndtv.com/internet/news/lucknow-police-facial-recognition-technology-expressions-ai-cameras-women-in-distress-alerts-2355865\nhttps://hbr.org/2019/11/the-risks-of-using-ai-to-interpret-human-emotions\nhttps://thenextweb.com/neural/2021/01/22/an-indian-city-plans-to-use-facial-recognition-to-spot-women-in-distress-what-could-go-wrong/\nhttps://www.newsweek.com/indian-city-deploys-facial-recognition-detect-harassed-womens-expressions-1563761\nhttps://www.dailypioneer.com/2021/state-editions/emotion-recognition-technology-----a-new-challenge-to-privacy.html\nhttps://www.businessinsider.in/news/a-scary-proposal-to-use-facial-recognition-and-ai-by-an-indian-state-has-experts-fuming/articleshow/80421935.cms\nRelated \ud83c\udf10\nIndia Human Efficiency Tracking System sanitation worker surveillance\nUyghur emotion detection testing\nPage info\nType: Issue\nPublished: January 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/4-little-trees-4lt", "content": "4 Little Trees (4LT) student emotion recognition system prompts criticism\nOccurred: February 2021 \nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn AI programme that analyses students' emotions came under fire from researchers and digital rights activists fearful that emotion recognition technologies are intrusive, can be misused, and may be biased against people with darker skins.\nFind Solution AI, the company behind 4 Little Trees, said its algorithm is 85% accurate and identifies happiness, sadness, anger, surprise, fear, and other emotions by analysing facial muscular micro-movements in real-time. \nLaunched in 2017, 4 Little Treesand proved successful, with the number of schools reputedly using the system growing from 34 to 83 during the COVID-19 pandemic. \nHowever, a February 2021 CNN report cited researchers and activists noting that emotion recognition technologies are often intrusive, can be misused, and may be biased against people with darker skins. They also struggle to identify more complex emotions such as enthusiasm or anxiety.\nA May 2021 Financial Times article noted that research into emotion recognition systems suggested that while they might be able to decode facial expressions much of the time, what a person is really feeling or thinking, or what they plan to do next, may be quite different.\nSystem \ud83e\udd16\nFind Solution AI. 4 Little Trees website\nOperator: True Light College\nDeveloper: Find Solution AI\nCountry: Hong Kong\nSector: Education\nPurpose: Identify & monitor emotions\nTechnology: Emotion recognition; Facial analysis; Gesture analysis; Computer vision\nIssue: Accuracy/reliability; Privacy; Surveillance; Bias/discrimination - race, ethnicity\nTransparency: Governance; Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://edition.cnn.com/2021/02/16/tech/emotion-recognition-ai-education-spc-intl-hnk/index.html\nhttps://www.ft.com/content/c0b03d1d-f72f-48a8-b342-b4a926109452\nhttps://thesiliconreview.com/magazine/profile/revolutionizing-education-through-artificial-intelligence-find-solution-ai-limited\nhttps://www.scmp.com/presented/business/topics/deutsche-bank-china-tech-summit-hk/article/3012369/relishing-shock-new\nhttps://www.biometricupdate.com/202103/a-market-for-emotion-recognition-grows-without-tackling-deep-concerns-by-the-public\nhttps://www.news18.com/news/buzz/happy-face-or-sad-face-this-ai-reads-childrens-emotions-as-they-learn-online-3517247.html\nhttp://curmudgucation.blogspot.com/2021/02/big-brother-knows-whats-in-your-heart.html\nhttps://www.ameinfo.com/industry/digital-and-media/ai-in-the-uae-falling-short-of-expectations-despite-the-huge-business-educational-potential\nhttps://www.nature.com/articles/d41586-021-00868-5\nhttps://www.francetvinfo.fr/replay-radio/le-billet-vert/comment-des-systemes-d-intelligence-artificielle-interpretent-nos-emotions_4347927.html\nhttps://onezero.medium.com/the-shoddy-science-behind-emotional-recognition-tech-2e847fc526a0\nRelated \ud83c\udf10\nGaggle student behavioural monitoring\nRetorio talent personality assessments\nPage info\nType: Issue\nPublished: May 2022", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/spotify-emotion-recognition", "content": "Spotify plan to use emotion recognition criticised as 'manipulative'\nOccurred: January 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA patent (pdf) granted to audio streaming company Spotify that identifies and analyses speech to recommend content based on a user's emotional state has been criticised as 'creepy' and 'manipulative'.\nThe proposed new technology will use speech recognition to determine a user's emotional state, gender, age, and accent. It can also retrieve information about a user\u2019s background and social environment through microphones.\nMusic Business Worldwide notes that whilst the patent may never be implemented, the technology has garnered strong criticism from digital rights, privacy and security experts.\nFight for the Future, Access Now, and the Union of Musicians launched a dedicated campaign to persuade Spotify to abandon the technology.\nSystem \ud83e\udd16\nhttps://pdfaiw.uspto.gov/.aiw?docid=20180182394\nOperator: Spotify\nDeveloper: Spotify\nCountry: USA; Global\nSector: Media/entertainment/sports/arts\nPurpose: Assess emotion\nTechnology: Speech recognition\nIssue: Privacy; Security; Dual/multi-use\nTransparency: \nResearch, advocacy \ud83e\uddee\nAccess Now (2021). Letter to Spotify CEO (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.musicbusinessworldwide.com/spotifys-latest-invention-will-determine-your-emotional-state-from-your-speech-and-suggest-music-based-on-it/\nhttps://www.axios.com/spotify-patent-users-speech-recommend-music-6c5ce99d-ca0f-4457-9b87-9d27fcc35527.html\nhttps://pitchfork.com/news/new-spotify-patent-involves-monitoring-users-speech-to-recommend-music/\nhttps://www.hypebot.com/hypebot/2021/02/spotify-patents-tech-to-monitor-your-speech-infer-emotion.html\nhttps://www.dailymail.co.uk/sciencetech/article-9201017/Spotify-wants-analyse-VOICE-suggest-songs-based-emotions-patent-reveals.html\nhttps://www.scmagazine.com/news/security-news/privacy-compliance/rhythm-in-the-algorithm-digital-rights-groups-call-on-spotify-to-abandon-voice-recognition-invention\nhttps://thenextweb.com/neural/2021/01/29/spotify-patents-eerie-mood-detecting-tech-to-recommend-you-songs/\nhttps://www.verdict.co.uk/spotify-patent-taste/\nhttps://www.biometricupdate.com/202104/biometric-data-for-music-game-personalization-draws-controversy-research\nhttps://voicebot.ai/2021/05/07/musicians-demand-spotify-not-develop-emotional-speech-recognition-patent/\nhttps://www.bbc.co.uk/news/entertainment-arts-55839655\nRelated \ud83c\udf10\n4 Little Trees (4LT) student emotion recognition\nGaggle student behavioural monitoring\nPage info\nType: Issue\nPublished: January 2021\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/kim-kwang-seok-voice-recreation", "content": "Kim Kwang-Seok voice recreation prompts plagiarism concerns\nOccurred: January 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe recreation of late South Korean folk-rock singer Kim Kwang-Seok's voice for an AI-human singing competition prompted concerns from fans and digital rights activists.\nKim's voice was brought to life using local AI company Supertone's Singing Voice Synthesis system to perform 'I miss you', a 2002 ballad by Kim Bum-soo, and performed as a duet with another singer for the 'Competition of the Century: AI vs Human' show on broadcast TV network SBS. \nThe performance delighted Kim's fans. Howerver, despite the singer's family giving permission for the use of his voice, the show prompted concerns about plagiarism, copyright, and the potential misuse and abuse of deepfake technologies. \nThe late singer died in 1996 aged 31 after a strong of hits. His death was officially attributed to suicide.\nSystem \ud83e\udd16\nhttps://www.youtube.com/watch?v=JZMj46C9WQg\nhttps://www.youtube.com/watch?v=WwUHX7oD_ak\nOperator: SBS\nDeveloper: Supertone\nCountry: S Korea\nSector: Media/entertainment/sports/arts  \nPurpose: Recreate voice  \nTechnology: Deepfake - audio  \nIssue: Ethics; Copyright; Dual/multi-use\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.asiaone.com/digital/south-korean-ai-technology-brings-back-folk-singers-voice\nhttps://www.reuters.com/article/us-southkorea-ai-voice-recreation/south-korean-ai-technology-brings-back-folk-singers-voice-idUSKBN29Y18O\nhttps://www.wionews.com/entertainment/south-korean-ai-technology-brings-back-dead-folk-singer-kim-kwang-seok-voice-360206\nhttps://www.ibtimes.com/south-korea-ai-recreate-deceased-singers-voice-tv-show-3130674\nhttps://www.koreatimes.co.kr/www/culture/2021/01/703_302548.html\nhttps://www.vice.com/en/article/n7vkv8/kim-kwang-seok-ai-concert-south-korea\nhttp://www.koreaherald.com/view.php?ud=20201207000891\nhttps://www.republicworld.com/world-news/rest-of-the-world-news/south-korea-to-bring-back-superstar-kim-kwang-seoks-vocal-performance-using-ai.html\nhttps://edition.cnn.com/2021/01/25/asia/south-korea-kim-kwang-seok-ai-dst-hnk-intl/index.html\nhttps://voi.id/en/lifestyle/29039/ai-technology-can-resurrect-a-deceased-korean-musician-it-is-getting-a-lot-of-attention-and-controversy\nRelated \ud83c\udf10\nAnthony Bourdain voice deepfake results in backlash\nTom Cruise deepfakes\nPage info\nType: Issue\nPublished: January 2021\nLast updated: January 2022", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/myheritage-deep-nostalgia", "content": "MyHeritage Deep Nostalgia criticised for ease of misuse\nOccurred: February 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe launch of 'Deep Nostalgia', an AI-powered system that enables users to imitate a person in a photograph, has drawn concerns about its potential use for unethical purposes.\nDeveloped with Israeli AI firm D-ID, geneaology firm MyHeritage's Deep Nostalgia uses artificial intelligence to animate images so that they smile, wink, nod, and move their heads from side to side.\nReaction to the tool has been mixed. Some describe the results as 'amazing' and 'emotional', while others see it as 'spooky', 'creepy', and 'unethical'. Photographs of deceased and living people can be animated using Deep Nostalgia.\nMyHeritage told the BBC that it did not include speech 'in order to prevent abuse, such as the creation of deepfake videos of living people'.\nIn March 2022, MyHeritage introduced LiveStory, which enables users to animate their creations through vocal storytelling by using deep learning algorithms that draw on a custom set of over 140 voices in 31 languages.\nSystem \ud83e\udd16\nhttps://www.myheritage.com/deep-nostalgia\nhttps://www.youtube.com/watch?v=RVDu_u_a578\nhttps://www.youtube.com/watch?v=Rkb66qy9GLw\nOperator: MyHeritage\nDeveloper: D-ID\nCountry: UK\nSector: Media/entertainment/sports/arts\nPurpose: Imitate ancestors\nTechnology: Deepfake - video, audio; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning; Text-to-speech\nIssue: Ethics; Impersonation\nTransparency: Privacy\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/technology-56210053\nhttps://news.trust.org/item/20210226144936-jxdi4\nhttps://www.newsweek.com/myheritage-deep-nostalgia-ai-brings-old-photos-life-10-million-1574008\nhttps://thenextweb.com/neural/2021/02/26/myheritage-ai-deepfake-reanimates-photos-videos/\nhttps://www.dailymail.co.uk/sciencetech/article-9300101/MyHeritages-deepfake-tool-animates-photos-dead-relatives.html\nhttps://interestingengineering.com/myheritage-creepy-deepfake-tool\nhttps://boingboing.net/2021/02/27/animating-old-photos-using-ai.html\nhttps://www.sciencetimes.com/articles/29896/20210226/deepfake-technology-myheritage-animates-faces-photos-bring-dead-back-life.htm\nhttps://techcrunch.com/2021/02/26/myheritage-now-lets-you-animate-old-family-photos-using-deepfakery/\nhttps://newatlas.com/computers/deepfake-nostalgia-myheritage-animate-deceased-relatives/\nhttps://www.theverge.com/2021/2/28/22306097/ai-brings-still-photos-life-meme-twitter-geneaology-myheritage\nhttps://www.inputmag.com/tech/myheritages-deepfake-tool-animates-ancient-photos-and-its-as-weird-as-it-sounds \nhttps://www.smithsonianmag.com/smart-news/ai-program-deep-nostalgia-revives-old-portraits-180977173/\nhttps://newatlas.com/computers/myheritage-livestory\nRelated \ud83c\udf10\nMicrosoft reincarnation chatbot\nKim Kwang-Seok voice recreation\nPage info\nType: Issue\nPublished: February 2021\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/faces-of-the-riot-facial-recognition", "content": "Faces of the Riot facial recognition raises privacy concerns\nOccurred: January 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nOver 6,000 facial images of people taking part in the January 6, 2021 insurrection at the US Capitol building were made available on Faces of the Riot, raising privacy concerns.\nThe site isolated and made public geo-tagged images of people involved in the insurrection that had been posted to Parler, the social networking service associated with Donald Trump supporters. The images had previously been downloaded by hackers after Amazon had decided to cease hosting Parler. \nWhilst each face on the site is only tagged with a string of characters associated with the Parler video in which it appeared, and faces are not named, the initiative raised privacy concerns. It was also seen to have failed to distinguish between lawbreakers and people legally attending the protests.\nSystem \ud83e\udd16\nFaces of the Riot website\nOperator: Anonymous/pseudonymous\nDeveloper: Anonymous/pseudonymous\nCountry: USA\nSector: Politics\nPurpose: Identify protestors\nTechnology: Facial recognition; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning \nIssue: Privacy; Accuracy/reliability\nTransparency: Governance; Privacy\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.wired.com/story/faces-of-the-riot-capitol-insurrection-facial-recognition/\nhttps://www.msn.com/en-us/news/us/what-is-faces-of-the-riot-website-features-people-at-capitol-siege-seen-in-parler-videos/ar-BB1d4o6l\nhttps://www.dailykos.com/stories/2021/1/24/2011521/--Faces-of-the-Riot-website-posts-pictures-of-every-person-on-video-during-the-Jan-6-insurrection\nhttps://www.cnet.com/news/website-features-faces-from-parlers-capitol-riot-videos/\nhttps://themarkup.org/news/2021/01/28/police-say-they-can-use-facial-recognition-despite-bans\nhttps://www.axios.com/facial-recognition-capitol-hill-riots-regulation-ce71660f-cd52-4c0f-8acf-309ecda3abc6.html\nhttps://www.latimes.com/business/technology/story/2021-02-04/facial-recognition-surveillance-capitol-riot-black-and-brown-communities\nRelated \ud83c\udf10\nHebron Palestinian facial recognition surveillance\nRCMP facial recognition surveillance\nPage info\nType: Issue\nPublished: January 2021\nLast updated: January 2022", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/qoves-ai-beauty-scoring", "content": "QOVES AI beauty assessment tool raises bias concerns\nOccurred: March 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn Australian company developed an AI-powered beauty assessment tool that tells people how attractive they are and what they can do to improve it, raising concerns about bias and transparency.\nTechnology Review notes that Sydney-based startup QOVES is one of many companies offering facial analysis services, though few of them are keen to describe or talk about how their algorithms work in meaningful detail. \nCommentators reckon this reluctance stems from the desire to protect commercial IP on the one hand; on the other it provides a convenient fig leaf for a science that is highly subjective and, given this subjectivity, highly biased. \nQOVES describes itself on its website as a 'facial aesthetics consultancy dedicated to answering the age-old question of what makes a face attractive through modern science, telehealth and machine learning.'\nSystem \ud83e\udd16\nQOVES website\nOperator: QOVES\nDeveloper: QOVES\nCountry: Australia\nSector: Cosmetics\nPurpose: Assess & rank beauty\nTechnology: Computer vision\nIssue: Accuracy/reliability; Bias/discrimination - race, ethnicity, age\nTransparency: Governance; Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.technologyreview.com/2021/03/05/1020133/ai-algorithm-rate-beauty-score-attractive-face/\nhttps://www.theguardian.com/commentisfree/2021/mar/06/ai-powered-app-tell-you-beautiful-reinforce-biases\nhttps://www.inputmag.com/culture/ai-algorithm-scores-are-only-worsening-our-obsession-with-beauty\nhttps://digismak.com/this-ai-powered-application-will-tell-you-if-you-are-beautiful-and-will-also-reinforce-prejudices-artificial-intelligence-ai/\nhttps://medium.com/geekculture/an-ai-tool-told-me-im-pretty-but-then-it-said-frankenstein-is-too-47a671e1ff99\nhttps://www.thestar.com.my/tech/tech-news/2021/03/18/dont-fall-prey-to-ai-promising-to-rate-your-beauty\nhttps://www.ladn.eu/tech-a-suivre/ia-machine-learning-iot/intelligences-artificielles-score-beaute-partout/\nhttps://www.aftenposten.no/kultur/i/86RkRE/maskin-anbefaler-kirurgi-paa-ansiktet-ditt\nRelated \ud83c\udf10\nTikTok mandatory beauty filtering\nTwitter photo crop algorithm\nPage info\nType: Issue\nPublished: March 2021\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-aws-panorama-workplace-surveillance", "content": "Amazon AWS Panorama automated workplace analytics criticised for multiple use\nOccurred: December 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAWS Panorama, a new service enabling the automated analysis of workplace security camera footage, has been criticised for its dual use nature. \nAccording to Amazon, Panorama can be used to 'evaluate manufacturing quality, identify bottlenecks in industrial processes, and monitor workplace safety and security.' \nHowever, commentators pointed out that the service could easily be used for other purposes, including to monitor employee productivity and performance, raising concerns about surveillance and workplace privacy.\nSystem \ud83e\udd16\nAmazon AWS Panorama website\nOperator: Amazon\nDeveloper: Amazon\nCountry: USA\nSector: Business/professional services\nPurpose: Assess product quality; Monitor workplace safety & security  \nTechnology: CCTV; Computer vision\nIssue: Privacy; Surveillance; Dual/multi-use\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/technology-55158319\nhttps://www.ft.com/content/27faa953-1723-4597-a5a0-2ff9e617feab\nhttps://www.irishtimes.com/business/work/workplace-surveillance-may-hurt-us-more-than-it-helps-1.4457355\nhttps://mashable.com/article/amazon-aws-panorama-worker-customer-tracking-technology-smart-cameras/\nhttps://www.inputmag.com/tech/aws-panorama-is-a-device-that-adds-machine-learning-to-any-camera\nhttps://techmonitor.ai/boardroom/productivity-monitoring-tools-office-365-amazon-aws\nhttps://uk.pcmag.com/cameras/130315/amazona-panorama-makes-any-surveillance-camera-intelligent\nhttps://techcrunch.com/2020/12/01/aws-announces-panorama-a-device-adds-machine-learning-technology-to-any-camera/\nhttps://www.channelnewsasia.com/news/commentary/work-monitor-surveillance-tech-aws-panorama-safety-privacy-13943892\nRelated \ud83c\udf10\nTeleperformance/TP Observer employee monitoring\nXsolla employee monitoring, terminations\nPage info\nType: Issue\nPublished: January 2021\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/oostoanyvision-facial-recognition-drones", "content": "Oosto facial recognition drone patent application draws controversy\nOccurred: February 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA patent application filed by Tev Aviv-based technology start-up AnyVision (now Oosto) for a drone-based system with advanced facial recognition capabilities raised concerns about police and military uses.\nFiled in August 2019, the system describes how drones can get the best angle to obtain high-quality, facial images of people, and sets out different ways this may be achieved, including avoiding objects that may be obscuring a face.\nHow the system will be deployed remains unclear. According to AnyVision CEO Avi Golan, it could be deployed to deliver packages and in smart cities. Human and digital right advocates are concerned about police and military uses.\nMicrosoft announced in 2019 it would stop working with AnyVision after it had been discovered monitoring Palestinians in the Gaza Strip using facial recognition.\nSystem \ud83e\udd16\nOosto website\nOperator: Oosto/AnyVision Interactive Technologies\nDeveloper: Oosto/AnyVision Interactive Technologies\nCountry: Israel\nSector: Multiple\nPurpose: Unclear/unknown\nTechnology: Drone; Facial recognition\nIssue: Surveillance; Privacy; Ethics\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.forbes.com/sites/thomasbrewster/2021/02/15/drones-with-facial-recognition-are-primed-to-fly-but-the-world-isnt-ready-yet/\nhttps://www.calcalistech.com/ctech/articles/0,7340,L-3884249,00.html\nhttps://www.fastcompany.com/90606400/facial-recognition-drone-patent-anyvision\nhttps://dronedj.com/2021/02/24/anyvision-files-face-recognition-patent-for-its-drones/\nhttps://www.businessinsider.com/httpswwwbusinessinsideresdrones-reconocimiento-facial-cerca-ser-realidad-812285\nhttps://cyprus-mail.com/2021/02/23/isrraeli-anyvision-drones-facial-recognition/\nhttps://eu.usatoday.com/videos/tech/2021/02/17/could-drones-use-facial-recognition-were-close-happening/6778648002/\nhttps://www.biometricupdate.com/202102/anyvision-patent-filing-suggests-face-biometrics-for-delivery-drones\nRelated \ud83c\udf10\nUkraine-Russia war prisoner facial recognition\nMercadona facial recognition\nPage info\nType: Issue\nPublished: February 2021\nLast updated: January 2022", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/bytedance-uyghur-censorship", "content": "Bytedance instructed to automatically censor Uyghur social media posts\nOccurred: February 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe Chinese government demanded TikTok owner Bytedance develop algorithms to identify and automatically delete Uyghur language posts, giving credance to rumours of ethnic censorship by Beijing.\nThe employee, who worked for a technology team that supports Bytedance's central Trust and Safety team, also alleged the company is also actively censoring and deleting content critical of Beijing. \nThe employee wrote pseudonymously in Protocol that the technologies created by his team 'supported the company's entire content moderation in and outside China, including Douyin at home and its international equivalent TikTok'.\nRumours of Bytedance's close relationship with the Chinese government, and its surveillance and censorship of Uyghur-related content through its Douyin and TikTok services have long circulated.\nSystem \ud83e\udd16\nTikTok website, Wikipedia profile \nByteDance website, Wikipedia profile\nOperator: Bytedance/TikTok/Douyin  \nDeveloper: Bytedance/TikTok/Douyin\nCountry: China; Global\nSector: Technology\nPurpose: Identify/remove toxic content\nTechnology: Recommendation algorithm\nIssue: Freedom of expression - censorship\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.protocol.com/china/i-built-bytedance-censorship-machine\nhttps://www.cnbc.com/2021/06/25/tiktok-insiders-say-chinese-parent-bytedance-in-control.html\nhttps://www.businessinsider.in/tech/news/bytedance-tried-to-build-an-algorithm-to-censor-uighur-livestreams-on-tiktoks-chinese-sister-app-a-former-employee-has-claimed/articleshow/81112892.cms\nhttps://www.foxnews.com/world/bytedance-whistleblower-china-mass-censorship\nhttps://chinadigitaltimes.net/2021/02/cdt-weekly-february-12-18-ilham-tohti-xinjiang-and-why-clubhouse-had-to-die/\nhttps://jack-clark.net/2021/03/01/import-ai-238-robots-that-fold-clothes-how-bytedance-censors-its-product-a-differentiable-simulator/\nhttps://www.fr.de/politik/chinas-zensurapparat-so-funktioniert-die-kontrolle-bei-tiktok-90217030.html\nhttps://netzpolitik.org/2021/tiktok-insider-berichtet-ueber-zensurtechniken-bei-bytedance/\nhttps://chinadigitaltimes.net/2021/02/cdt-weekly-february-12-18-ilham-tohti-xinjiang-and-why-clubhouse-had-to-die/\nRelated \ud83c\udf10\nHuawei Uyghur-spotting patent\nBeijing Uyghur emotion detection\nPage info\nType: Issue\nPublished: February 2021\nLast updated: January 2022", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/seoul-bridge-suicide-detection", "content": "Seoul bridge AI suicide detection system raises privacy concerns\nOccurred: June 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe use of AI by the Seoul Metropolitan Government to detect and prevent suicide attempts on the Han River raised concerns that the project may encroach on personal privacy.\nWith the highest suicide rate of any OECD country, a rate that accelerated during the COVID-19 pandemic, authorities in the South Korean capital Seoul have been operating a 24-hour CCTV-based suicide surveillance and response system since 2012 on bridges over the Han River. \nTo help reduce the number of suicides, and better spread resources, Seoul's Institute of Technology (SIT) and Seoul Fire and Disaster Headquarters started using machine learning to alert them to the scenes most likely to require intervention.\nHowever, the intiative alarmed some privacy experts, who warned it could violate the privacy of people using bridges across the Han River, especially given the lack of public signage. Others expressed fears that the system could be used for other surveillance-related purposes in the future.\nSystem \ud83e\udd16\nSeoul Institute of Technology (2021). \uc11c\uc6b8\uc2dc, AI\ub85c \ud55c\uac15\uad50\ub7c9 \ud22c\uc2e0\uc2dc\ub3c4 \ucc3e\uc544\ub0b8\ub2e4\u2026\uace0\ub3c4\ud654 CCTV\uad00\uc81c\uae30\uc220 \uc2dc\ubc94\uc801\uc6a9\nOperator: Seoul Metropolitan Government\nDeveloper: Seoul Institute of Technology (SIT); Seoul Fire and Disaster Headquarters (SFDH)\nCountry: S Korea\nSector: Govt - municipal\nPurpose: Reduce suicides\nTechnology: CCTV; Computer vision; Machine learning\nIssue: Privacy; Dual/multi-use; Surveillance\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reuters.com/world/asia-pacific/seoul-using-ai-detect-prevent-suicide-attempts-bridges-2021-06-30/\nhttps://www.thequint.com/tech-and-auto/south-koreas-ai-cameras-to-stop-suicides-highly-invasive-expert\nhttps://www.theregister.com/2021/07/01/seoul_ai_bridge_rescue/\nhttps://keyt.com/cnn-regional/2021/06/24/seoul-rolls-out-ai-enabled-cctv-cameras-to-stop-suicides-privacy-experts-divided/\nhttps://bigthink.com/technology-innovation/ai-cameras-suicide-attempts-bridges\nhttps://www.ctvnews.ca/sci-tech/seoul-rolls-out-new-ai-based-surveillance-system-to-stop-suicides-privacy-experts-divided-1.5483358\nhttps://youtu.be/zst7fSFXUp8\nRelated \ud83c\udf10\nGoGuardian Beacon student suicide detection, prevention\nVioG\u00e9n gender domestic violence system\nPage info\nType: Issue\nPublished: June 2021\nLast updated: January 2022", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/facebook-political-group-recommendations", "content": "Facebook recommends US political groups volte-face\nOccurred: January 2021\nFacebook recommended users of its platform to join political and civic groups on its platform, despite having publicly committed to stop doing so ahead of the US 2020 presidential election.\nAccording to The Markup, Facebook continued to recommend political groups throughout January 2021, having renewed its promise not to on January 11. Data also suggests that Facebook remained disproportionately likely to recommend political groups to supporters of Donald Trump. \nThe finding prompted Senator Ed Markey to write to Mark Zuckerberg asking for an explanation, calling Facebook group 'breeding groups for hate'. \nOn January 11, Facebook had stated in a blog post that it was 'not recommending civic groups for people to join' on its platform. \nSystem \ud83e\udd16\nFacebook (2021). Our Preparations Ahead of Inauguration Day\nOperator: Meta/Facebook\nDeveloper: Meta/Facebook\nCountry: USA\nSector: Politics\nPurpose: Recommend groups\nTechnology: Recommendation algorithm\nIssue: Mis/disinformation\nTransparency: Governance; Black box\nInvestigations, assessments, audits \ud83e\uddd0\nThe Markup (2021). Facebook Said It Would Stop Pushing Users to Join Partisan Political Groups. It Didn\u2019t\nThe Markup (2021). Lawmaker Questions Facebook on Broken Election-Related Promise\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reuters.com/article/us-facebook-groups-idUSKBN29X00C\nhttps://www.washingtonpost.com/politics/2021/01/28/technology-202-facebook-plans-limit-politics-reflect-growing-pressure-washington/\nhttps://www.inputmag.com/culture/facebook-is-breaking-promises-about-not-recommending-political-groups-to-users\nhttps://gizmodo.com/facebook-promised-to-stop-promoting-political-groups-y-1846087253\nhttps://www.buzzfeednews.com/article/ryanmac/facebook-suspended-group-recommendations-election\nhttps://www.wsj.com/articles/facebook-knows-it-encourages-division-top-executives-nixed-solutions-11590507499\nRelated \ud83c\udf10\nFacebook Georgia political partisanship\nTwitter right-wing content amplification\nPage info\nType: Issue\nPublished: January 2021", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/facebook-georgia-political-partisanship", "content": "Facebook political ad ban drives Georgia political partisanship\nOccurred: January 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nPartisan content quickly replaced news coverage when Facebook turned off a ban on political and social issue advertising in Georgia, USA, according to The Markup.\nHaving banned political sponsored advertising to minimise misinformation during the 2021 US presidential elections, Facebook turned off the feature before Georgians headed to the polls to vote on a run-off for two Senator seats.\nThe finding prompted researchers to express concerns about the power of Facebook's platform in a political context, the ease with which misinformation and disinformation can be weaponised, and the difficulty in understanding their impacts without meaningful access to its system. \nSystem \ud83e\udd16\nFacebook (2020). An Update on the Georgia Runoff Elections\nFacebook (2020). An Update on the Georgia Runoff Elections for Advertisers\nOperator: Meta/Facebook\nDeveloper: Meta/Facebook\nCountry: USA\nSector: Politics\nPurpose: Review advertising\nTechnology: Advertising management system\nIssue: Mis/disinformation\nTransparency: Governance; Black box\nInvestigations, assessments, audits \ud83e\uddd0\nThe Markup (2021). In Georgia, Facebook\u2019s Changes Brought Back a Partisan News Feed\nThe Markup (2021). Citizen Browser Georgia data\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.nytimes.com/2020/12/15/technology/facebook-lifts-ban-on-political-ads-for-georgia-runoff-elections.html\nhttps://politicalwire.com/2021/01/05/facebooks-changes-brought-back-a-partisan-news-feed/\nhttps://techcrunch.com/2021/01/05/facebook-political-ad-ban-returns-after-ga-runoffs/\nhttps://www.theverge.com/2021/1/5/22215067/facebook-ads-georgia-runoff-political-ad-ban-zuckerberg\nhttps://www.theverge.com/2021/1/4/22213670/facebook-political-ads-georgia-republican-senate\nhttps://www.niemanlab.org/2021/01/in-georgia-facebooks-changes-brought-back-a-partisan-news-feed/\nhttps://www.cjr.org/the_media_today/georgia-runoffs-biden-congress-trump.php\nhttps://popular.info/p/facebook-fails-georgia\nRelated \ud83c\udf10\nFacebook US political group recommendations\nFacebook Cross-check\nPage info\nType: Issue\nPublished: January 2021", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/google-sidewalk-labs-portland-smart-city", "content": "Portland scuppers  'Smart City' mobility analytics plan\nOccurred: February 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA plan to track mobility patterns of how people move throughout the urban area of Portland, Oregon, was stopped due to concerns about data quality, transparency, and privacy.\nPortland Metro had planned to use mobile tracking data to inform decision-making about the planning of bike lanes, road repairs, and bus services. \nAccording to reports, Portland Metro decided to pull out largely because as a series of disputes with Google's Sidewalk Labs spin-off Replica over data quality, transparency, and privacy. Conversely, Replica says it was unwilling to share data to the level of detail requested by the city.\nLast year, a Sidewalks Lab project to build a smart city in a disused waterfront area of Toronto also fell apart. An independent review panel had criticised the plan as 'tech for tech's sake', with some of the proposed elements 'irrelevant or unnecessary'.\nSystem \ud83e\udd16\nSmart City PDX\nOperator: Portland Metro\nDeveloper: Alphabet/Google/Sidewalk Labs; Replica\nCountry: USA\nSector: Govt - municipal\nPurpose: Track mobility patterns\nTechnology: Location tracking\nIssue: Privacy\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.fastcompany.com/90465315/this-startup-wants-to-help-smart-cities-but-they-still-dont-know-where-its-data-comes-from\nhttps://redtailmedia.org/2020/03/12/how-a-former-google-siblings-data-disclosure-to-city-planners-reveals-little-and-why-it-matters/\nhttps://redtailmedia.org/2021/02/20/portland-ditches-googles-smart-city-tech-sibling-replica/\nhttps://www.bbc.co.uk/news/technology-56168306\nhttps://www.protocol.com/alphabet-replica-urban-planning-privacy\nhttps://www.businessinsider.com/second-sidewalk-labs-smart-city-project-shutters-portland-oregon-2021-2\nhttps://www.smartcitiesdive.com/news/the-end-of-another-sidewalk-labs-linked-project-highlights-smart-city-stick/596518/\nhttps://www.govtech.com/fs/Portland-Replica-Part-Ways-Over-Data-Privacy-Concerns.html\nhttps://www.geekwire.com/2019/portland-quietly-launches-mobile-location-data-project-alphabets-controversial-sidewalk-labs/\nhttps://www.objetconnecte.com/fin-projet-smart-city-li%C3%A9-sidewalk-labs/\nRelated \ud83c\udf10\nBelgrade 'Safe City' video surveillance\nMalta 'Safe City' video surveillance\nPage info\nType: Issue\nPublished: February 2021", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/google-nest-hub-2-sleep-tracking", "content": "Critics question Google Nest Hub 2 sleep sensing data uses\nOccurred: March 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe success of Google's new Nest Hub smart device will likely depend on whether people trust Google enough to let the company monitor them whilst they are asleep.\nMany reviewers praise the product's basic functions, usability, and sleep monitoring capabilities. But some commentators questioned the accuracy of Google's sleep sensing and analytics. Others highlight Google's rapacious quest for user data, and wonder what the company might do with it - now, and in the future. \nOptional for now, it appears Google is looking to charge for the service going forward. The company says none of the data collected through Nest Hub's sleep sensing feature will be used to sell personalised advertising.\nSystem \ud83e\udd16\nGoogle Nest Hub 2\nOperator: Alphabet/Google/Nest  \nDeveloper: Alphabet/Google/Nest\nCountry: USA; UK\nSector: Health\nPurpose: Detect & analyse sleep patterns\nTechnology: Sleep sensing\nIssue: Accuracy/reliability; Privacy; Dual/multi-use\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://apnews.com/article/google-nest-hub-screen-sleep-surveillance-b09c258f737d8c0359108822dab8212d\nhttps://www.dailymail.co.uk/sciencetech/article-9367769/Google-gets-sleep-surveillance-new-Nest-Hub-screen.html\nhttps://www.msn.com/en-us/news/technology/googles-new-nest-hub-tracks-your-sleep-and-it-feels-very-judgy/ar-BB1f7D9u\nhttps://www.bbc.co.uk/news/technology-56416578\nhttps://arstechnica.com/gadgets/2021/03/googles-new-nest-display-wants-to-watch-you-while-you-sleep/\nhttps://www.denverpost.com/2021/03/20/google-nest-hub-sleep/\nhttps://www.consumerreports.org/smart-speakers/google-nest-hub-review/\nhttps://www.wsj.com/articles/google-nest-hub-apple-watch-and-the-pros-and-cons-of-sleep-tracking-11617109207\nRelated \ud83c\udf10\nApple user depression, autism, dementia detection\nGoogle/HCA Healthcare patient data sharing\nPage info\nType: Issue\nPublished: March 2021", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/huawei-uyghur-spotting-patent", "content": "Huawei files Uyghur-spotting system patent application\nOccurred: January 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA patent application filed by Huawei that sets out a system that identifies people who appear to be Uyghurs amongst images of pedestrians sparked accusations of ethnic surveillance and poor ethics.\nDiscovered by video research group IPVM, the patent (pdf) describes AI techniques for identifying pedestrians by attributes including race. The patent was filed by controversial Chinese technology company Huawei in conjunction with the Chinese Academy of Sciences in 2018.\nHuawei said the Uyghur detection capability 'should never have become part of the application' and that it is 'taking proactive steps to amend' it. \nIPVM also revealed patents for systems capable of recognising Uyghurs filed by Chinese AI companies Megvii and SenseTime.\nSystem \ud83e\udd16\nHuawei website\nHuawei Wikipedia profile\n\nDocuments \ud83d\udcc3\nHuawei patent application (pdf)\nOperator:  \nDeveloper: Huawei; Chinese Academy of Sciences\nCountry: China\nSector: Govt - police\nPurpose: Detect Uyghurs\nTechnology: Object recognition\nIssue: Surveillance; Privacy\nTransparency: Governance; Privacy\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://ipvm.com/reports/patents-uyghur\nhttps://news.trust.org/item/20210113195157-jq6lj\nhttps://www.bbc.co.uk/news/technology-55634388\nhttps://www.bbc.co.uk/news/av/technology-55651932\nhttps://www.cnbc.com/2021/01/15/huawei-ai-firms-filed-to-patent-tech-that-could-identify-uighurs-report-says.html\nhttps://www.pri.org/file/2021-01-13/huawei-patent-mentions-ability-target-uighurs\nhttps://thenextweb.com/neural/2021/01/13/surveillance-group-exposes-disturbing-huawei-patent-for-ai-powered-uighur-detection/\nhttps://www.deccanherald.com/international/chinese-tech-giants-patent-tools-that-can-detect-track-uighurs-939153.html\nhttps://www.afr.com/world/asia/chinese-companies-patent-uighur-spotting-tech-20210118-p56uy0\nhttps://www.msn.com/en-in/news/world/china-s-huawei-backtracks-after-filing-for-patent-of-ai-software-that-could-recognise-uighurs/ar-BB1cKSxa\nhttps://edition.cnn.com/2021/01/14/tech/huawei-xinjiang-china-patent-facial-recognition-intl-hnk/index.html\nRelated \ud83c\udf10\nBeijing Uyghur emotion detection\nCurtin University Uyghur, Tibetan facial recognition study\nPage info\nType: Issue\nPublished: January 2021", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/bitcoin-mining-algorithm-environmental-damage", "content": "Bitcoin crypto mining environmental damage\nReleased: 2009\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nBitcoin is a digital currency invented in 2008 by an unknown person (or people) that uses cryptography to verify transactions, which are recorded on a blockchain public distributed ledger.\nBitcoin's business model and algorithms - notably its proof-of-work cryptographic verification system - have been accused of causing significant environmental damage, and unduly contributing to climate change.\nSpecific concerns about Bitcoin's environmental record include:  \nCarbon emissions\nMultiple research programmes and studies show Bitcoin is responsible for a high level of carbon emissions and contributes significantly to climate change.\nFor example, Cambridge University's Bitcoin Electricity Consumption index calculates Bitcoin consumes 133.68 terawatt hours a year of electricity - more than many first-world countries - and estimates bitcoin mining to be responsible for 0.1% of total world greenhouse gas emissions.\nA March 2020 energy research journal Joule study said Bitcoin accounted for about two-thirds of the energy consumed by 'proof-of-work' coins - of which an estimated 500 exist.\nOn the other hand, cryptocurrency advocates such as Twitter co-founder Jack Dorsey argue (pdf) that the bitcoin network could incentivise the more rapid development of renewable energy through the deployment of more solar and wind generation capacity.\nAir pollution\nTraditionally, Bitcoin required large amounts of fossil fuels - notably coal - to power mining operations, resulting not just in high carbon emissions but also significant volumes of air pollution in the areas around the mining farms, as well as water pollution, resulting in damage to fish and other wildlife populations. \nHowever, China's 2021 ban on Bitcoin mining saw many operators ditch their old equipment, relocate to other countries, and switch to using a higher percentage of renewable energy sources. \nElectronic waste\nThe need for large stacks of dedicated circuits to power the mining of bitcoin, the lifespan of which is estimated to be approximately 1.3 years and which can not be used for other purposes, has resulted in huge amounts of electronic waste, with one bitcoin estimated to generate 270 to 380 grams (9.5 to 13.4 oz) of e-waste.\nA much-cited 2021 research study calculated that Bitcoin's production of e-waste to be over 30,000 tonnes - comparable to the small IT equipment waste produced by the Netherlands.\nSystem \ud83e\udd16\nBitcoin.org website\nBitcoin-Environmental impact Wikipedia profile\nEnvironmental Impact of Bitcoin Wikipedia profile\nOperator: Bitcoin.org\nDeveloper: \nCountry: Global\nSector: Banking/financial services\nPurpose: Create currency\nTechnology: Blockchain; Virtual currency\nIssue: Environment\nTransparency: Governance; Black box; Marketing\nResearch, advocacy \ud83e\uddee\nCambridge Bitcoin Electricity Consumption Index\nGreenpeace USA (2022). Change The Code: Not The Climate \u2013 Greenpeace USA, EWG, Others Launch Campaign to Push Bitcoin to Reduce Climate Pollution\nJones B.A., Goodkind A.L., Berrens R.P. (2022). Economic estimation of Bitcoin mining\u2019s climate damages demonstrates closer resemblance to digital crude than digital gold\nBitcoin Clean Energy Initiative (2021). Bitcoin is Key to an Abundant, Clean Energy Future (pdf)\nde Vries A., Stoll C. (2021). Bitcoin's growing e-waste problem\nBadea L., Mungio-Pupazan M., C. (2021). The Economic and Environmental Impact of Bitcoin\nGallesdorfer U., Klassen L., Stoll C. (2020). Energy Consumption of Cryptocurrencies Beyond Bitcoin\nGoodkind A.L., Jones B.A., Berrens R.P. (2020). Cryptodamages: Monetary value estimates of the air pollution and human health impacts of cryptocurrency mining\nCorbet S., Lucey B.M., Yarovaya L. (2019). The Financial Market Effects of Cryptocurrency Energy Usage\nKrause M.J., Tolaymat T. (2018). Quantification of energy and carbon costs for mining cryptocurrencies (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.nytimes.com/interactive/2021/09/03/climate/bitcoin-carbon-footprint-electricity.html\nhttps://www.ft.com/content/1aecb2db-8f61-427c-a413-3b929291c8ac\nhttps://www.theverge.com/2022/9/29/23378337/bitcoin-climate-change-damages-crypto-mining\nhttps://www.independent.co.uk/life-style/gadgets-and-tech/bitcoin-environment-mining-energy-cryptocurrency-b1800938.html\nhttps://www.bbc.com/news/technology-58572385\nhttps://www.nature.com/articles/d41586-018-01625-x\nhttps://www.theguardian.com/technology/2021/sep/17/waste-from-one-bitcoin-transaction-like-binning-two-iphones\nhttps://www.theatlantic.com/science/archive/2023/03/crypto-bitcoin-mining-carbon-emissions-climate-change-impact/673468/\nRelated \ud83c\udf10\nStochastic Parrots study questions large language model size\nEthereum minting, trading environmental damage\nPage info\nType: Issue\nPublished: January 2023\nLast updated: September 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/nissei-eco-robot-undercuts-buddhist-priests", "content": "Nissei Eco robot seen to undercut Buddhist priests\nOccurred: August 2017\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe configuration of Softbank's Pepper robot as a Japanese Buddhist priest able to deliver funerals sparked consternation that it would replace human priests. \nDeveloped by plastic moulding maker Nissei Eco Co., Pepper would chant Buddhist funeral sutras while tapping a drum. \nIntroduced as a cheaper alternative to its human equivalents, 'Pepper' was said to cost less at JPY 50,000 (about USD 450) per funeral compared to more than JPY 240,000 (USD 2,200) for a human priest. \nIn addition to undercutting human priests, Pepper robots were also pitched as able to help resolve staffing and revenue problems at Japanese temples, which had been suffering from falling attendences due increased secularisation and an ageing population.\nSystem \ud83e\udd16\nPepper website\nPepper robot Wikipedia profile\nPepper robot priest video\nOperator: \nDeveloper: Softbank Robotics; Nissei Eco Co\nCountry: Japan\nSector: Religion\nPurpose: Conduct funeral rites\nTechnology: Robotics\nIssue: Anthropomorphism; Employment \nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reuters.com/article/us-japan-robotpriest-idUSKCN1B3133\nhttps://techwireasia.com/2017/08/buddhist-temple-robot-priest-for-hire/\nhttps://qz.com/1060932/softbanks-pepper-robot-is-now-a-buddhist-priest-in-japan\nhttps://www.digitaltrends.com/cool-tech/pepper-robot-monk-funerals-japan/\nhttps://www.ibtimes.co.uk/japans-buddhist-robots-replace-priests-cut-price-funerals-1635469\nhttps://www.telegraph.co.uk/technology/2017/08/24/japanese-robot-trained-host-low-cost-buddhist-funerals/\nhttps://www.japantimes.co.jp/news/2017/08/16/business/pepper-the-robot-to-don-buddhist-robe-for-its-new-funeral-services-role/\nhttps://www.theguardian.com/technology/2017/aug/23/robot-funerals-priest-launched-softbank-humanoid-robot-pepper-live-streaming\nRelated \ud83c\udf10\nMindar robot Buddhist priest\nSanTO robot Catholic priest\nPage info\nType: Issue\nPublished: September 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/blessu-2-segensroboter-blessing-robot", "content": "BlessU-2 'Segensroboter' blessing robot triggers controversy\nOccurred: May 2017\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nBlessU-2, a robot priest introduced by the Protestant Church in Wittenberg, Germany, to mark 500 years since the Reformation, triggered debate about the ethics, credibility, and effectiveness of robots as religious preachers.\nDesigned to provoke discussion about whether machines have a place within the clergy, BlessU-2 provides blessings in five languages and recites over 40 verses from the Bible. \nHowever, the bot also triggered debate on whether or not robots will induce religious decline, and about the potential for robots to replace religious jobs. \nWittenberg is where Martin Luther sparked the Protestant Reformation by nailing his 95 theses to a church door in the town calling for religious reform.\nSystem \ud83e\udd16\nBlessU-2 video\nSchlosskirche Wittenberg website \nAll Saints' Church, Wittenberg\nOperator: LichtKirche Wittenberg\nDeveloper: LichtKirche Wittenberg\nCountry: Germany\nSector: Religion\nPurpose: Stimulate discussion  \nTechnology: Robotics\nIssue: Ethics; Employment\nTransparency: \nResearch, advocacy \ud83e\uddee\nJackson J.C., et al (2023). Exposure to Robot Preachers Undermines Religious Commitment (pdf)\nMidson S. (2022). Posthuman Priests: Exploring the \u2018New Visibility of Religion\u2019 in Robotic Re(-)presentations of Religious Rituals\nLoffler D., Hurtienne J., Nord I. (2019). Blessing Robot BlessU2: A Discursive Design Study to Understand the Implications of Social Robots in Religious Contexts\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theguardian.com/technology/2017/may/30/robot-priest-blessu-2-germany-reformation-exhibition\nhttps://www.popularmechanics.com/technology/robots/a26698/germany-robot-priest/\nhttps://economictimes.indiatimes.com/magazines/panache/blessu-2-robot-priest-offers-auto-blessings-in-german-church/articleshow/58908597.cms\nhttps://www.thesun.co.uk/tech/3687923/robot-priest-called-blessu-2-delivers-blessings-like-a-religious-cash-machine-while-it-beams-lights-from-its-palms/\nhttps://eu.usatoday.com/story/news/world/2017/10/11/could-robots-replace-pastors-one-just-gives-blessings/754999001/\nhttps://www.upi.com/Odd_News/2017/05/30/BlessU-2-robot-priest-delivers-blessings-in-five-different-languages/7201496156166/\nhttps://www.bbc.co.uk/news/av/world-europe-40101661\nhttps://qz.com/994384/robots-are-coming-for-priests-jobs-too\nRelated \ud83c\udf10\nSanTO robot Catholic priest\nNissei Eco robot undercuts Buddhist priests\nPage info\nType: Issue\nPublished: September 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/microsoft-reincarnation-chatbot", "content": "Microsoft reincarnation chatbot raises legal, ethical concerns\nOccurred: January 2021\nCan you improve this page?\nShare your insights with us\nA Microsoft patent application that would see dead people digitally resurrected for friends and family to interact with has been approved.\nThe patent envisages that the chatbot could apply to a 'past or present entity', 'friend, a relative, an acquaintance, a celebrity, a fictional character, a historical figure', or 'random entity', would be trained on a range of sources, including 'images, voice data, social media posts, electronic messages, written letters etc', and may take 2D or 3D form.\nReactions to the technology range from 'weird' to 'dystopian', while digital rights advocates fear it could be used for nefarious purposes. In the absence of existing legislation, lawyers worry that digital resurrection raises real concerns about privacy, copyright, defamation, and other issues.\nMicrosoft representatives say there are currently no plans to put the chatbot into production.\nDatabank\nOperator: Microsoft\nDeveloper: Microsoft\nCountry: USA\nSector: Technology\nPurpose: Imitate personality\nTechnology: Chatbot; NLP/text analysis\nIssue: Appropriateness/need; Privacy; Copyright; Defamation\nTransparency: \nACCESS DATABASE\nNews, commentary, analysis\nhttps://www.independent.co.uk/life-style/gadgets-and-tech/microsoft-chatbot-patent-dead-b1789979.html\nhttps://www.ubergizmo.com/2021/01/microsoft-chatbot-dead-loved-ones/\nhttps://www.makeuseof.com/microsoft-patents-reincarnating-as-a-chatbot/\nhttps://www.protocol.com/microsoft-wants-you-to-live-on-as-a-digital-chatbot\nhttps://www.forbes.com/sites/enriquedans/2021/01/25/can-algorithms-recreate-a-personality/\nhttps://gizmodo.com/microsoft-landed-a-patent-to-turn-you-into-a-chatbot-1846113786\nhttps://theconversation.com/chatbots-that-resurrect-the-dead-legal-experts-weigh-in-on-disturbing-technology-155436\nhttps://www.washingtonpost.com/technology/2021/02/04/chat-bots-reincarnation-dead/\nhttps://www.telegraph.co.uk/technology/2021/03/26/creepy-technology-bringing-dead-relatives-back-life/\nhttps://www.sciencealert.com/the-latest-chatbots-are-capable-of-resurrecting-the-dead-if-we-let-them\nhttps://www.forbes.com/sites/barrycollins/2021/01/04/microsoft-could-bring-you-back-from-the-dead-as-a-chat-bot/\nhttps://www.popularmechanics.com/technology/robots/a35165370/microsoft-resurrects-the-dead-chatbots/\nhttps://www.nme.com/news/tv/microsofts-new-ai-chatbot-concept-is-reminding-people-of-black-mirror-2862855\nRelated\nKim Kwang-Seok voice recreation\nDelphi bot moral judgements\nPage info\nType: Issue\nPublished: January 2021", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ibm-project-debater", "content": "IBM Project Debater prompts manipulation concerns\nOccurred: March 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe growing sophistication of IBM's Project Debater, an autonomous AI system that can debate competitively with humans, prompted concerns about the extent to which it could be manipulated in a real-world setting.\nIBM researchers working on the project published a paper in Nature magazine in which they showed that Project Debater is capable of beating humans in debates. In a series of tests, the system was given 15 minutes to research topics and prepare for debates. Humans won most debates, but in one instance it was able to change the stance of nine people.\nWhen tasked with a debate, Project Debater scans the internet for prior research, or quote well-known phrases used by people respected in the field of argument. It also uses IBM's Watson system to listen to the arguments given by opponents and then searches for rebuttals that have been given by others to similar claims. \nIn a Nature magazine editorial, University of Dundee's Chris Reed made the case for stronger oversight and greater transparency of language models such as Project Debater and OpenAI's GPT-3 in order to reduce the potential for manipulation and harm.\nSystem \ud83e\udd16\nIBM Project Debater website\nIBM Watson website\nSlonim N. et al (2021). An autonomous debating system\nOperator: IBM\nDeveloper: IBM\nCountry: Israel\nSector: Technology\nPurpose: Debate with humans\nTechnology: NLP/text analysis; Sentiment analysis; Text-to-speech\nIssue: Appropriateness/need; Dual/multi-use\nTransparency: Governance; Black box\nResearch, advocacy \ud83e\uddee\nReed C. (2021) Argument technology for debating with humans\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://futurism.com/ai-designed-debate-people-ruin-social-media\nhttps://www.nature.com/articles/d41586-021-00867-6\nhttps://techxplore.com/news/2021-03-ibm-ai-debating-expert-human.html\nhttps://interestingengineering.com/ibm-ai-debates-humans-convinces-some\nhttps://www.scientificamerican.com/podcast/episode/ai-can-now-debate-with-humans-and-sometimes-convince-them-too/\nhttps://htxt.co.za/2021/03/ibms-project-debater-is-equal-parts-frightening-and-fascinating/\nhttps://www.thetimes.co.uk/article/ibms-robot-debater-holds-its-own-against-human-opponents-zb8kwhxsl\nhttps://www.sciencealert.com/this-new-ai-exists-for-the-sole-purpose-of-arguing-with-humans\nRelated \ud83c\udf10\nIBM Diversity in Faces dataset\nGPT-2 large language model\nPage info\nType: Issue\nPublished: March 2021", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/teleperformancetp-observer-employee-monitoring", "content": "Teleperformance AI employee monitoring prompts backlash\nOccurred: March 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nPlans by French call centre company Teleperformance to use cameras connected to an AI system to monitor employees working from home for home-working 'infractions' has resulted in a backlash.\nAccording to leaked documents leaked to The Guardian, workers will have to explain why they are leaving their desks to avoid being reported for a breach, amongst other requirements. \nThe plans led to complaints by trade unions about the 'unreasonable' and 'intrusive' nature of worker virtual monitoring programmes, including their impact on the privacy of those being monitored and their families.\nTeleperformance, which employs over 380,000 people in 34 countries and counts Apple, Amazon and Uber as customers, has since said the remote scans for infractions will not be used in the UK. \nThe company has also faced resistance to its workforce monitoring activities and solutions in Albania, Colombia, the US, and other countries.\nSystem \ud83e\udd16\nTeleperformance website\nTeleperformance Wikipedia profile\nOperator: Teleperformance \nDeveloper: Teleperformance  \nCountry: UK; France \nSector: Business/professional services\nPurpose: Monitor employee behaviour \nTechnology: Computer vision \nIssue: Appropriateness/need; Surveillance; Privacy\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theguardian.com/business/2021/mar/26/teleperformance-call-centre-staff-monitored-via-webcam-home-working-infractions\nhttps://www.theguardian.com/business/2021/mar/26/missing-from-desk-ai-webcam-raises-remote-surveillance-concerns\nhttps://www.businessinsider.com/work-from-home-sneek-webcam-picture-5-minutes-monitor-video-2020-3\nhttps://businessam.be/wanneer-uw-baas-big-brother-wordt-slimme-webcams-om-luie-thuiswerkers-in-de-gaten-te-houden/\nhttps://www.personneltoday.com/hr/call-centre-denies-webcam-monitoring-claims-teleperformance/\nhttps://www.hrreporter.com/focus-areas/automation-ai/webcam-surveillance-faces-backlash/354409\nhttps://www.decisionmarketing.co.uk/news/teleperformance-hit-by-expose-over-wfh-staff-spying\nhttps://www.wsj.com/articles/monitoring-of-employees-faces-scrutiny-in-europe-11611138602\nhttps://www.salon.com/2020/10/05/corona-fied-employers-are-now-spying-on-remote-workers-in-their-homes_partner/\nhttps://www.nbcnews.com/tech/tech-news/big-tech-call-center-workers-face-pressure-accept-home-surveillance-n1276227\nRelated \ud83c\udf10\nAmazon AWS Panorama workplace surveillance\nMoodbeam HR emotion tracking\nPage info\nType: Issue\nPublished: March 2021\nLast updated: December 2021", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/verkada-surveillance-cameras-data-breach", "content": "Verkada biometric cameras hacked, prompting privacy concerns\nOccurred: March 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nHackers have gained access to security company Verkada, giving them access to live and archived footage of over 150,000 cameras inside the firm's customers facilities across the world, as well as its own offices.\nCarried out by a hacker collective which aimed to show the pervasiveness of video surveillance and the ease with which systems could be broken into, the breach involved gaining access to Verkada through a 'Super Admin' account, thereby allowing the hackers to view the cameras of all of its customers.\nSome Verkada cameras use facial recognition as a basic capability to identify individuals, potentially exposing the sensitive personal information of its customers and the employees, patients and others being monitored. \nVerkada customers include Tesla, Cloudflare, Equinox gyms, hospitals, jails, schools, and police stations.\nSystem \ud83e\udd16\nVerkada website\nVerkada Wikipedia profile\nOperator: Verkada; Tesla; Cloudflare; Halifax Health\nDeveloper: Verkada\nCountry: USA\nSector: Business/professional services\nPurpose: Strengthen security; Identify individuals\nTechnology: CCTV; Facial recognition\nIssue: Security; Privacy\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bloomberg.com/news/articles/2021-03-09/hackers-expose-tesla-jails-in-breach-of-150-000-security-cams\nhttps://www.theverge.com/2021/3/9/22322122/verkada-hack-150000-security-cameras-tesla-factory-cloudflare-jails-hospitals\nhttps://www.theverge.com/2021/3/11/22324876/surveillance-camera-firm-verkada-breached-hacked-super-admin-access-employees\nhttps://www.vice.com/en/article/wx83bz/verkada-hacked-facial-recognition-customers\nhttps://www.businessinsider.com/verkada-hackers-breached-security-cameras-at-tesla-in-hospitals-jails-report-2021-3\nhttps://www.dailymail.co.uk/news/article-9344335/Hackers-breach-security-camera-company-Verkada-gain-access-150-000-surveillance-cameras.html\nhttps://www.ciol.com/verkada-hackers-breach-cameras-tesla-salesforce-jails-hospitals-cameras-hacked-pictures-online/\nhttps://www.biometricupdate.com/202103/verkada-hack-shows-cameras-with-face-biometrics-capabilities-deployed-by-thousands-of-organizations\nhttps://www.bloomberg.com/news/articles/2021-03-11/verkada-workers-had-extensive-access-to-private-customer-cameras\nhttps://www.washingtonpost.com/technology/2021/03/10/verkada-hack-surveillance-risk/\nhttps://hipertextual.com/2021/03/hackers-camara-seguridad-tesla\nRelated \ud83c\udf10\nTALON AI license plate camera surveillance\nSoftbank Pepper robot security vulnerabilities\nPage info\nType: Issue\nPublished: March 2021", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/deliveroo-uk-rider-management-algorithm", "content": "Deliveroo UK riders protest poor safety and pay\nOccurred: March 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nRiders for meal delivery company Deliveroo have been holding strikes in London and cities across the UK to protest against the company's poor pay, safety, and workers' rights record. \nThe strike coincides Deliveroo public listing on London's stock exchange. The largest IPO in London since 2011, the company's stock price fell 26% on its first day due to concerns about employment conditions and the outsize control given to Deliveroo founder Will Shu also played a part. The listing was described by a banker as 'the worst IPO in London's history'.\nThe UK's top court recently ruled that Uber drivers should be classified as 'workers' rather than 'self-employed', raising significant questions about Deliveroo's business model. In January 2021, an Italian court ruled that a 'secretive' algorithm used by Deliveroo to rank and offer shifts to riders in Italy was discriminatory.\nSystem \ud83e\udd16\nDeliveroo UK website\nDeliveroo Wikipedia profile\n\nOperator: Deliveroo\nDeveloper: Deliveroo\nCountry: UK \nSector: Transport/logistics \nPurpose: Determine rider pay\nTechnology: Workforce management system \nIssue: Employment - pay, working conditions\nTransparency: Governance; Black box \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.dailymail.co.uk/news/article-9444013/Deliveroo-riders-strike-today-tech-firms-unconditional-trading-begins.html\nhttps://apnews.com/article/europe-london-5dc61ce778bf2ee81ba84abf534328ca\nhttps://news.sky.com/story/deliveroo-riders-poised-to-strike-as-unconditional-trading-for-shares-begins-12267717\nhttps://metro.co.uk/video/deliveroo-riders-strike-low-pay-workers-rights-2392968/\nhttps://www.telegraph.co.uk/technology/2021/04/07/deliveroos-algorithm-makes-looking-family-feel-impossible/\nhttps://www.computerweekly.com/news/252498995/Deliveroo-riders-strike-over-pay-and-work-conditions\nhttps://news.yahoo.com/deliveroo-share-price-riders-strike-over-pay-working-conditions-112855737.html\nhttps://www.independent.co.uk/news/business/news/deliveroo-courier-strike-employers-national-living-wage-government-department-business-a7189126.html\nhttps://edition.cnn.com/2021/04/02/investing/london-deliveroo-ipo/index.html\nhttps://www.bloomberg.com/news/articles/2021-03-25/deliveroo-hit-by-investor-rider-revolt-ahead-of-london-ipo\nRelated \ud83c\udf10\nDeliveroo Italy rider reliability discrimination\nGorillas rider work schedule automation\nPage info\nType: Issue\nPublished: April 2021\nLast updated: December 2021", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/doordash-tip-witholding", "content": "DoorDash accused of deliberately misleading tipping representations\nOccurred: July 2019 \nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn angry Doordash customer filed a class action lawsuit accusing the company of 'materially false and misleading' tipping representations on its app. \nThe suit accused DoorDash of 'materially false and misleading' tipping representations, and claimed that it had failed to make clear to its customers that tips they gave through its app to couriers were not being allocated as they were intended to be, and that had customers known this, they would not have tipped through the app.\nA few days earlier, New York Times journalist Andy Newman had spent 27 hours as a dasher (the term DoorDash uses for its delivery workers). Newman discovered that tips given to Doordash delivery drivers were not going to the drivers but instead were being sent in whole or in part to the company to offset other business costs, fueling outrage.\nUnder pressure, Doordash changed its tipping and pay model for its delivery workers so that they get all the tips customers add to their bills in August 2019. The company settled the suit for USD 2.5 million in November 2020.\nSystem \ud83e\udd16\nBlock Party/DoorDash website\nDoorDash Wikipedia profile\nOperator: Doordash \nDeveloper: Doordash\nCountry: USA\nSector: Transport/logistics\nTechnology: \nPurpose: Determine worker pay\nIssue: Employment - pay; Fairness\nTransparency: Governance\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nArkin v DoorDash Inc\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.nytimes.com/2019/07/21/nyregion/doordash-ubereats-food-app-delivery-bike.html\nhttps://www.nytimes.com/2019/07/24/nyregion/doordash-tip-policy.html\nhttps://www.theverge.com/2019/7/22/20703434/delivery-app-tip-pay-theft-doordash-amazon-flex-instacart\nhttps://nypost.com/2019/07/29/brooklyn-man-sues-doordash-for-misleading-tipping-policy/\nhttps://www.vox.com/recode/2019/8/20/20825937/doordash-tipping-policy-still-not-changed-food-delivery-app-gig-economy\nhttps://fortune.com/2019/11/12/doordash-new-tipping-policy-worker-pay/\nhttps://www.cnet.com/news/doordash-settles-lawsuit-for-2-5m-over-deceptive-tipping-practices/\nRelated \ud83c\udf10\nUber Postmates gig worker scamming\nDoordash order matching algorithm\nPage info\nType: Issue\nPublished: February 2022\nLast updated: March 2024", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/doordash-order-matching-algorithm", "content": "Doordash drivers protest order matching algorithm\nOccurred: April 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nDrivers for food delivery company Doordash launched #DeclineNow, a campaign against the company's low pay rates by gaming its order matching algorithm. \nAccording to Bloomberg, two drivers  worked out that the platform automatically reallocates a delivery order at a slightly higher rate after another driver has declined it. \nThe movement now counts over 40,000 members and aims to ensure no delivery job is made for less than USD 7. However, to what extent the gambit proves sustainable the gambit is an open question. \nIn April 2020, Doordash, Grubhub, Postmates and Uber Eats were accused of abuse of monopoly by only listing restaurants if their owners signed contracts which include clauses that require prices be the same for dine-in customers as for customers receiving delivery.\nSystem \ud83e\udd16\nBlock Party/DoorDash website\nDoorDash Wikipedia profile\nOperator: Doordash \nDeveloper: Doordash\nCountry: USA\nSector: Transport/logistics\nPurpose: Determine rider compensation\nTechnology: Order matching algorithm \nIssue: Employment - pay; Fairness\nTransparency: Governance; Black box\nResearch, advocacy \ud83e\uddee\nDeclinenow.org\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bloomberg.com/news/articles/2021-04-06/doordash-workers-are-trying-to-game-the-algorithm-to-increase-pay\nhttps://www.msn.com/en-us/money/news/doordash-drivers-game-algorithm-to-increase-pay/ar-BB1flT08\nhttps://www.vice.com/en/article/3anwdy/organized-doordash-drivers-declinenow-strategy-is-driving-up-their-pay\nhttps://jalopnik.com/doordash-drivers-game-the-app-into-paying-at-least-7-f-1846632267\nhttps://www.marketplace.org/shows/marketplace-tech/what-if-gig-workers-could-train-the-algorithms-that-determine-their-pay/\nhttps://www.diepresse.com/5962366/mehr-geld-durch-stornierung-wie-us-lieferanten-den-algorithmus-austricksen\nhttps://gizmodo.com/doordash-has-a-new-plan-to-make-workers-gamble-on-how-m-1840325285\nhttps://trans.info/en/doordash-drivers-betting-on-solidarity-to-beat-algorithm-and-increase-pay-231060\nReference\nDoordash tip withholding\nUber Postmates gig worker scamming\nPage info\nType: Issue\nPublished: April 2021\nLast updated: December 2021", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/uber-postmates-gig-worker-fraud", "content": "Uber Postmates gig workers scammed\nOccurred: February 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nHundreds of Postmates riders were scammed, resulting in significant loss of earnings, But Postmates did little to protect its riders from bad actors, or to support them when scams occured. \nAs Elizabeth Watkins of Princeton University\u2019s Center for Information Technology Policy told The Markup, 'In regular employment arrangements, there is a company culture and there are always people you can talk to. One word for that is social transparency ... In gig work, they don\u2019t have this\u2014it\u2019s called digital isolation.'\nPostmates says it has reimbursed some riders with lost earnings in the wake of The Markup report. \nUber is estimated to have over 500,000 riders on its books in the US and other markets. The company was acquired by Uber in December 2020. \nSystem \ud83e\udd16\nPostmates website\nPostmates Wikipedia profile\nOperator: Uber/Postmates \nDeveloper: Uber/Postmates \nCountry: USA \nSector: Transport/logistics\nPurpose: Manage employees\nTechnology: Workforce management system \nIssue: Security; Employment - pay\nTransparency: Complaints/appeals\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://themarkup.org/working-for-an-algorithm/2021/02/18/postmates-drivers-have-become-easy-prey-for-scammers-and-drivers-say-the-companys-not-helping\nhttps://themarkup.org/working-for-an-algorithm/2021/02/25/postmates-workers-scammed-out-of-their-earnings-are-reimbursed-following-the-markups-report\nhttps://www.reddit.com/r/postmates/comments/ld127s/im_a_new_postmates_driver_did_i_just_get_scammed/\nhttps://gizmodo.com/a-phishing-scam-targeting-postmates-drivers-pretends-to-1846322220\nhttps://www.techtimes.com/articles/257290/20210222/hundreds-of-postmates-drivers-attacked-by-phishing-scam-clearing-out-victims-accounts.htm\nhttps://www.eater.com/22291053/delivery-apps-workers-face-scams-pay-loss-penalties\nhttps://www.grubstreet.com/2021/02/third-party-delivery-platforms-are-winning.html\nhttps://fortune.com/2021/02/23/your-data-is-a-weapon-that-can-help-change-corporate-behavior/\nhttps://www.theverge.com/2021/2/20/22292702/postmates-drivers-phishing-scams-stolen-earnings\nRelated \ud83c\udf10\nDoordash tip withholding\nDoordash order matching algorithm\nPage info\nType: Issue\nPublished: February 2021\nLast updated: December 2021", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/jumbo-supermarket-indiscriminate-facial-recognition", "content": "Jumbo supermarket warned for 'indiscriminate' facial recognition\nOccurred: December 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA store was warned by Dutch privacy authority Autoriteit Persoonsgegevens not to use facial recognition cameras at its entrance as it would violate the privacy of people entering it.\nPer BiometricUpdate, the regulator reported that Netherlands supermarket chain Jumbo's Alphen aan den Rijn store had erected notices at its entrance, but that it was judged too passive to be considered explicit consent. \nIt also said Jumbo's use of facial biometrics was not in the public interest. Jumbo had earlier claimed it was the 'safest' store in the Netherlands due to its use of facial recognition.\nUnder the EU's General Data Protection Regulation (GDPR), organisations are allowed to surveil people if the subjects of their surveillance provide their explicit permission.\nSystem \ud83e\udd16\nJumbo Alphen aan den Rijn\nOperator: Jumbo Alphen aan den Rijn\nDeveloper: i-Pro\nCountry: Netherlands\nSector: Retail\nPurpose: Detect criminals\nTechnology: Facial recognition\nIssue: Privacy; Necessity/proportionality\nTransparency: Governance; Marketing\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nEuropean Data Protection Board (2021). Dutch DPA issues Formal Warning to a Supermarket for its use of Facial Recognition Technology\nAutoriteit Persoonsgegevens (2020). Formele waarschuwing AP aan supermarkt om gezichtsherkenning\nAutoriteit Persoonsgegevens (2020). AP wijst supermarkten op regels gezichtsherkenning\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.biometricupdate.com/202012/dutch-grocer-thinks-loophole-makes-face-biometrics-legal-in-its-store\nhttps://dutchreview.com/news/politics/big-brother-is-this-supermarket-taking-security-cameras-too-far/\nhttps://www.rtlnieuws.nl/nieuws/nederland/artikel/4941596/gezichtsherkenning-biometrie-alphen-jumbo-privacy\nhttps://www.youtube.com/watch?v=BB4SJLQ82Ps\nRelated \ud83c\udf10\nMercadona facial recognition privacy abuse\nSouthern Co-op facial recognition\nPage info\nType: Incident\nPublished: August 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/cruise-robotaxi-hits-fire-engine", "content": "Cruise robotaxi hits fire engine, prompting regulatory investigation\nOccurred: August 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Cruise automated vehicle collided with a fire engine en route to an emergency at a road intersection in San Francisco, resulting in a regulatory investigation and an order to take half its robotaxis off the road immediately until the investigation has been completed.\nIn a statement on its website, Cruise said one of its cars 'entered the intersection on a green light and was struck by an emergency vehicle that appeared to be en route to an emergency scene.' \nAccording to Cruise, the car 'did identify the risk of a collision and initiated a braking maneuver, reducing its speed, but was ultimately unable to avoid the collision.'\nSan Francisco Police Department told Reuters that the fire engine had its siren and forward facing red lights on. The police said the sole passenger in the autonomous vehicle (AV) was transported to a local hospital with non-life-threatening injuries.\nSystem \ud83e\udd16\nCruise website\nCruise Wikipedia profile\nDocuments \ud83d\udcc3\nCruise (2023). Further update on emergency vehicle collision\nOperator: GM Cruise\nDeveloper: GM Cruise; General Motors/Chevrolet\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Self-driving system\nIssue: Safety; Accuracy/reliability; Legal - liability\nTransparency: Governance; Black box\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nPublic Utilities Commission of the State of California (2023). SAN FRANCISCO\u2019S MOTION TO STAY RESOLUTION APPROVING AUTHORIZATION FOR CRUISE LLC\u2019S EXPANDED SERVICE IN AUTONOMOUS VEHICLE PASSENGER SERVICE PHASE I DRIVERLESS DEPLOYMENT PROGRAM (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.cnbc.com/2023/08/18/cruise-self-driving-car-in-san-francisco-fire-truck-crash-one-injured.html\nhttps://abc7news.com/cruise-driverless-car-sffd-fire-truck-accident/13666936/\nhttps://www.nbcbayarea.com/news/local/san-francisco/cruise-driverless-car-crash/3298626/\nhttps://www.reuters.com/business/autos-transportation/gms-cruise-robotaxi-collides-with-fire-truck-san-francisco-2023-08-19/\nhttps://www.theverge.com/2023/8/18/23837217/cruise-robotaxi-driverless-crash-fire-truck-san-francisco\nhttps://fortune.com/2023/08/19/san-francisco-orders-cruise-cut-robotaxi-fleet-after-collision-firetruck\nhttps://www.theguardian.com/us-news/2023/aug/19/cruise-halves-fleet-san-francisco-robotaxis\nhttps://www.nytimes.com/2023/08/18/technology/cruise-crash-driverless-car-san-francisco.html\nhttps://www.washingtonpost.com/business/2023/08/19/san-francisco-cruise-robotaxi-crash/2339cb6e-3eae-11ee-aefd-40c039a855ba_story.html\nhttps://www.ksat.com/business/2023/08/19/gms-cruise-autonomous-vehicle-unit-agrees-to-cut-fleet-in-half-after-2-crashes-in-san-francisco/\nhttps://techcrunch.com/2023/08/18/cruise-told-by-regulators-to-immediately-reduce-robotaxi-fleet-50-following-crash/\nRelated \ud83c\udf10\nCruise driverless cars block traffic\nCruise AV impedes San Francisco firefighters\nPage info\nType: Incident\nPublished: August 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/iflytek-fakes-automated-speech-translations", "content": "iFlyTek 'fakes' automated speech translations\nOccurred: September 2018\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nChinese voice recognition company iFlytek was accused by a translator of hiring humans to fake its simultaneous interpretation tools, which the company said are powered by artificial intelligence. \nInterpreter Bell Wang posted an open letter claiming he was one of a team of simultaneous interpreters who had helped translate the 2018 International Forum on Innovation and Emerging Industries Development. The forum said it was using iFlytek\u2019s automated speech recognition-based interpretation service.\niFlytek CEO Hu Yu responded by saying that the tool used in the conference was a not a translation tool, but a transcription tool. Other interpreters also, claimed that they had been offered work interpreting on iFlytek\u2019s behalf. \nSystem \ud83e\udd16\niFlytek website\niFlytek Wikipedia profile\nOperator: International Forum on Innovation and Emerging Industries Development; iFlyTek\nDeveloper: iFlyTek\nCountry: China\nSector: Business/professional services\nPurpose: Language translation\nTechnology: Speech recognition\nIssue: Accuracy/reliability\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://zhuanlan.zhihu.com/p/45010541\nhttps://www.scmp.com/tech/big-tech/article/2165702/national-ai-champion-iflytek-dispute-over-automated-speech-translation\nhttps://technode.com/2018/09/25/iflytek-fake-ai-translations/\nhttps://www.chinamoneynetwork.com/2018/10/09/artificial-intelligence-stirs-up-public-anger-as-chinese-simultaneous-interpreters-feel-disrespected\nhttps://www.sixthtone.com/news/1002956/ai-company-accused-of-using-humans-to-fake-its-ai-\nhttps://www.weforum.org/agenda/2018/10/3-reasons-why-ai-wont-replace-human-translators-yet/\nhttps://itzone.com.vn/en/article/what-microsoft-and-google-are-hiding-from-you-about-their-ai/\nhttps://www.globaltimes.cn/content/1120500.shtml\nhttps://www.belinktrans.info/resources/iflytek-accused-of-using-human-interpreters-to-run-its-ai-interpreting-service\nRelated \ud83c\udf10\niFlytek automated speech recognition surveillance\nHikvision Uyghur ethnic minority analytics\nPage info\nType: Incident\nPublished: August 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/phone-case-design-bot-goes-rogue", "content": "Phone case design bot goes rogue\nOccurred: July 2017\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA bot that designed iPhone cases printed with random stock photographs for sale on Amazon appeared to go rogue, offering thousands of tacky and inappropriate designs.\nPer The Guardian, these included an iPhone 6 case with a picture of 'Male hands with soap dispenser use in the restroom', another featuring an 'Irrigation pipe in dirt trenches for sprinkler system' and ... one illustrated with an 'adult diaper worn by an old man with a crutch'.\nIt was unclear who set the bot and Amazon account up. Not did the creator advise potential buyers that the designs were generated using artificial intelligence.\nSystem \ud83e\udd16\nUnknown\nOperator: My Handy Designs; Amazon\nDeveloper: My Handy Designs\nCountry: USA\nSector: Consumer goods; Retail\nPurpose: Develop creative designs\nTechnology: Bot/intelligent agent\nIssue: Accuracy/reliability\nTransparency: Governance; Marketing; Complaints/appeals\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theverge.com/tldr/2017/7/10/15946296/amazon-bot-smartphone-cases\nhttps://www.boredpanda.com/funny-amazon-ai-designed-phone-cases-fail/\nhttps://gizmodo.com/an-amazon-bot-is-making-the-greatest-smartphone-cases-1796757891\nhttps://www.iflscience.com/technology/amazon-ai-designed-to-create-phone-cases-goes-hilariously-wrong/\nhttps://metro.co.uk/2017/07/10/a-horrendous-amazon-store-account-is-selling-phone-covers-of-our-nightmares-6767800/\nhttps://www.digitaltrends.com/mobile/odd-smartphone-cases/\nhttps://www.theguardian.com/technology/shortcuts/2017/jul/11/the-automated-amazon-seller-making-the-worst-phone-cases-ever\nhttps://twitter.com/rjurney/status/883850417574035456\nRelated \ud83c\udf10\nAI invents 40,000 biochemical warfare agents\nAmazon Buy Box pricing manipulation\nPage info\nType: Incident\nPublished: August 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/apple-iphone-x-face-id-identical-twin-tests", "content": "Apple Face ID fails to distinguish identical twins\nOccurred: October 2017\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nApple's Face ID authentication system failed to distinguish two different sets of identical twins trying to unlock the iPhone X. The findings raised questions about the reliability of the system.\nIn a test, Mashable had one twin register their face in Face ID and confirm it unlocked for them, and then had the second twin hold the smartphone to their face to see if they could get into their brother's device. In both tests, the phone was successfully unlocked using the face of the non-registered twin.\nApple had warned Face ID may not be able to distinguish identical twins: 'The chance that a random person in the population could look at your iPhone X and unlock it with their face is about one in one million. Of course, the statistics are lowered if that person shares a close genetic relationship with you. So, for example, if you happen to have an evil twin, you really need to protect your sensitive data with a passcode.'\nSystem \ud83e\udd16\nApple. About Face ID advanced technology\nApple (2017). The future is here: iPhone X\nOperator: Apple\nDeveloper: Apple\nCountry: USA\nSector: Consumer goods\nPurpose: Strengthen security\nTechnology: Facial recognition\nIssue: Accuracy/reliability; Security; Privacy\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://mashable.com/article/putting-iphone-x-face-id-to-twin-test#eAdRiV7BKqqd\nhttps://www.macrumors.com/2017/10/31/iphone-x-face-id-twin-tests/\nhttps://www.businessinsider.com/can-iphone-x-tell-difference-between-twins-face-id-recognition-apple-2017-10\nhttps://www.theguardian.com/technology/2017/sep/27/apple-face-id-iphone-x-under-13-twin-facial-recognition-system-more-secure-touch-id\nhttps://www.idownloadblog.com/2017/11/10/face-id-twin-test-results/\nhttps://tass.com/society/987629\nRelated \ud83c\udf10\nApple Face ID fails to distinguish brothers\nApple Face ID hacked with 3D mask\nPage info\nType: Incident\nPublished: August 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/apple-iphone-x-unlocked-by-work-colleague", "content": "Apple iPhone X unlocked by work colleague\nOccurred: December 2017\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Chinese woman working in Nanjing claimed a work colleague had been able to get into two of her new iPhone X smartphones, highlighting security issues with Apple's much-touted Face ID authentication system. \nThe woman, identified only by her surname Yan, told the Jiangsu Broadcasting Corporation that despite activating and configuring each phone\u2019s facial recognition software, her work colleague was able to get into both devices on every attempt.\nSome people reckoned the incident pointed to programmer bias. CEO of marketing company V3 Inbound TC Ivy said, 'Devices can't be biased, but if the creators don't account for their own biases it shows up in things like Asian women being indistinguishable to iPhones and black hands not triggering sensors in soap machines.'\nApple had insisted that the probability of a random person accessing someone else\u2019s iPhone X using the Face ID passcode is 1 in 1 million. The technology company offered the woman a second refund. \nSystem \ud83e\udd16\nApple. About Face ID advanced technology\nApple (2017). The future is here: iPhone X\nOperator: Apple\nDeveloper: Apple\nCountry: China\nSector: Consumer goods\nPurpose: Strengthen security\nTechnology: Facial recognition\nIssue: Accuracy/reliability; Bias/discrimination - race; Security; Privacy\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.scmp.com/news/china/society/article/2124313/chinese-woman-offered-refund-after-facial-recognition-allows\nhttps://www.newsweek.com/iphone-x-racist-apple-refunds-device-cant-tell-chinese-people-apart-woman-751263\nhttps://www.huffingtonpost.co.uk/entry/iphone-face-recognition-double_n_5a332cbce4b0ff955ad17d50\nhttps://www.asiaone.com/digital/woman-discovers-colleague-can-unlock-her-iphone-x-face-id\nhttps://www.hackread.com/chinese-woman-unlocks-colleague-iphonex-using-face-id/\nhttps://www.techworm.net/2017/12/apple-refunds-chinese-woman-colleague-unlocks-iphone-x-using-face-id.html\nhttps://www.bustle.com/p/a-woman-in-china-claims-that-her-iphone-x-was-unlocked-by-a-coworkers-face-its-raising-questions-about-diversity-in-tech-7617858\nhttps://www.asiaone.com/digital/woman-discovers-colleague-can-unlock-her-iphone-x-face-id\nRelated \ud83c\udf10\nApple iPhone X Face ID hacked with 3D mask\nApple iPhone X Face ID fails to distinguish brothers\nPage info\nType: Incident\nPublished: August 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chinese-boy-unlocks-mothers-phone-using-face-id", "content": "Chinese boy unlocks mother's phone using Face ID\nOccurred: December 2017\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA man from Shanghai has discovered that the iPhone he bought for his wife could be unlocked by her teenage son, raising further questions about the reliability of Apple's Face ID technology, and prompting accusations of 'racism'.\nThe father said he had phoned Apple\u2019s customer service hotline to report the problem, and was told it was a rare, isolated case caused by the physical similarities of his wife and son.\nThe incident led to accusations of racism, with some people blaming the lack of ethnic diversity in the technology industry.\nSystem \ud83e\udd16\nApple. About Face ID advanced technology\nApple (2017). The future is here: iPhone X\nOperator: Apple\nDeveloper: Apple\nCountry: China\nSector: Consumer goods\nPurpose: Strengthen security\nTechnology: Facial recognition\nIssue: Accuracy/reliability; Bias/discrimination - race; Security; Privacy\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.thesun.co.uk/news/5182512/chinese-users-claim-iphonex-face-recognition-cant-tell-them-apart/\nhttps://www.mirror.co.uk/tech/apple-accused-racism-after-face-11735152\nhttps://macdailynews.com/2017/12/21/apple-accused-of-racism-after-face-id-fails-to-distinguish-between-chinese-iphone-x-users/\nhttps://www.news.com.au/technology/gadgets/mobile-phones/is-the-iphone-racist-chinese-users-claim-iphonex-face-recognition-cant-tell-them-apart/news-story/13814540e8c82ad466aca687e12af64c\nhttps://www.dailymail.co.uk/sciencetech/article-5201881/The-iPhone-X-slammed-RACIST-Chinese-users.html\nhttps://www.ubergizmo.com/2017/12/face-id-accused-being-racist/\nRelated \ud83c\udf10\nApple iPhone X unlocked by work colleague\nApple Face ID fails to distinguish identical twins\nPage info\nType: Incident\nPublished: August 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/arab-boy-unlocks-mothers-phone-using-face-id", "content": "Arab boy unlocks mother's phone using Face ID\nOccurred: November 2017\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\n10-year old Ammar Malik repeatedly unlocked his mother\u2019s new iPhone X after she had set up the Face ID authentication system, resulting in questions about the reliability and privacy of Apple's facial recognition technology. \nAmmar's mother Sana Sherwani posted a video to YouTube showing him unlocking her phone in an instant, even after she had re-registered her face. The family also told WIRED that the boy was able to unlock his father\u2019s new iPhone X. \nCommentators were unclear how the mix-up could have occurred. In a LinkedIn post, Ammar's father Malik noted his son's face is clearly smaller than his wife's, and the two have quite different facial features.\nAccording to Apple's support page, 'The statistical probability is different for twins and siblings that look like you and among children under the age of 13, because their distinct facial features may not have fully developed. If you're concerned about this, we recommend using a passcode to authenticate.'\nSystem \ud83c\udf10\nApple. About Face ID advanced technology\nApple (2017). The future is here: iPhone X\nOperator: Apple\nDeveloper: Apple\nCountry: China\nSector: Consumer goods\nPurpose: Strengthen security\nTechnology: Facial recognition\nIssue: Accuracy/reliability; Security; Privacy\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.youtube.com/watch?v=dUMH6DVYskc\nhttps://www.linkedin.com/pulse/my-10-year-old-son-can-unlock-wifes-iphone-x-using-face-malik/\nhttps://mashable.com/article/kid-unlocks-iphone-x-face-id\nhttps://www.thesun.co.uk/tech/4937046/iphone-x-face-id-boy-10-unlocks-mums/\nhttps://www.standard.co.uk/lifestyle/london-life/iphone-x-owner-s-son-can-unlock-her-phone-with-face-id-a3695541.html\nhttps://www.wired.com/story/10-year-old-face-id-unlocks-mothers-iphone-x/\nhttps://www.dailymail.co.uk/sciencetech/article-5143375/Face-ID-iPhone-X-stopped-working-iOS-11-2-update.html\nhttps://fortune.com/2017/11/14/apple-iphone-x-face-id-unlocked/\nhttps://thenextweb.com/news/10-year-old-unlocks-his-mums-iphone-with-face-id\nRelated \ud83c\udf10\nChinese boy unlocks mother's phone using Face ID\nApple iPhone X unlocked by work colleague\nPage info\nType: Incident\nPublished August 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/apple-iphone-x-face-id-hacked-with-masks", "content": "Apple Face ID hacked with 3D mask\nOccurred: November 2017\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA researcher at Vietnamese cybersecurity firm Bkav claimed to fool Apple\u2019s Face ID facial recognition authentication system using a mask made with a 3D printer, silicone and paper tape. \nBkav, which had earlier demonstrated how to hack facial recognition-based systems in laptops from Asus, Toshiba, and Lenovo, had bypassed Face ID with a 3D-printed frame, makeup, silicone nose and 2D images, with special processing on the cheeks and around the face, where there are large skin areas.\nThe hack was greeted with scepticism by some security professionals; however, Reuters said it worked successfully several times when it was shown the demonstration. It was the first reported case of researchers seemingly tricking Face ID. Apple had claimed Face ID to be the safest such system used on a smartphone.\nSystem \ud83e\udd16\nApple. About Face ID advanced technology\nApple (2017). The future is here: iPhone X\nOperator: Apple\nDeveloper: Apple\nCountry: USA\nSector: Consumer goods\nPurpose: Strengthen security\nTechnology: Facial recognition\nIssue: Accuracy/reliability; Security; Privacy\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.youtube.com/watch?v=i4YQRLQVixM\nhttps://fortune.com/2019/12/12/airport-bank-facial-recognition-systems-fooled/\nhttps://arstechnica.com/information-technology/2017/11/hackers-say-they-broke-apples-face-id-heres-why-were-not-convinced/\nhttps://www.esquire.com/lifestyle/cars/a13527612/iphone-x-face-id-mask-hack/\nhttps://www.hackread.com/researcher-bypass-iphone-x-face-id-with-mask/\nhttps://www.dailydot.com/debug/iphone-x-face-id-mask/\nhttps://www.reuters.com/article/us-apple-vietnam-hack-idUSKBN1DE1TH\nhttps://www.wired.com/story/tried-to-beat-face-id-and-failed-so-far/\nRelated \ud83c\udf10\nApple iPhone X unlocked by work colleague\nApple Face ID fails to distinguish brothers\nPage info\nType: Incident\nPublished: August 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/apple-iphone-x-face-id-fails-to-distinguish-brothers", "content": "Apple Face ID fails to distinguish brothers\nOccurred: November 2017\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nTwo brothers with similar though not identical facial features posted a video to Reddit showing Apple's Face ID facial recognition system failing to distinguish them. The incident raises questions about the reliability of Apple's Face ID system.\nThe video showed the brother who had set up Face ID on his new iPhone X showing the feature working properly for him. His brother then tried to unlock the same phone without wearing glasses, and was rejected, but unlocked the device when he donned glasses similar in style to his brother.\nFace ID had been shown to struggle in several other scenarios, including distinguishing identical twins.\nSystem \ud83e\udd16\nApple. About Face ID advanced technology\nApple (2017). The future is here: iPhone X\nOperator: Apple\nDeveloper: Apple\nCountry: USA\nSector: Consumer goods\nPurpose: Strengthen security\nTechnology: Facial recognition\nIssue: Accuracy/reliability; Security; Privacy\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reddit.com/r/iphone/comments/7anj9f/iphonex_face_id_fail/?rdt=41950\nhttps://9to5mac.com/2017/11/04/face-id-siblings-fail/\nhttps://www.macrumors.com/2017/11/04/face-id-brothers-video/\nhttps://bgr.com/tech/iphone-x-face-id-brothers-fail-explanation/\nhttps://www.ubergizmo.com/2017/11/siblings-trick-face-id-unlock-iphone/\nhttps://www.standard.co.uk/tech/man-unlocks-his-brother-s-iphone-x-using-face-id-a3677116.html\nhttps://qz.com/1120545/a-man-was-able-to-use-face-id-to-unlock-his-brothers-apple-aapl-iphone-x/\nRelated \ud83d\uddde\ufe0f\nApple Face ID fails to distinguish identical twins\nApple iPhone X unlocked by work colleague\nPage info\nType: Incident\nPublished: August 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/houston-isd-teacher-performance-evaluation-opacity", "content": "Houston ISD teacher evaluation terminations\nOccurred: October 2017\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe Houston Independent School District (HSID) agreed (pdf) to stop using scores generated by an opaque and potentially inaccurate and unfair algorithmic system its to justify the sanctioning and termination of teachers.\nThe seventh largest school district in the United States, HSID used the SAS Institute's Educational Value-Added Assessment System (EVAAS) to track teachers\u2019 performance by comparing their students\u2019 test results to the statewide average for students in that grade or course between 2012 and 2017.\nShortly after implementing EVAAS, HISD it would fire 85 percent of teachers rated as ineffective by the system, despite not having access to detailed information about how the system worked on the basis that it was a trade secret. \nThough the lawsuit had allowed an expert to investigate some parts of the system, it was assessed to be impenetrable and the HSID agreed to stop using the system to assess teacher performance as long as it remained impossible to understand. \nSystem \ud83e\udd16\nHouston Independent School District website\nHouston Independent School District Wikipedia profile\nSAS. SAS EVAAS for K-12 website\nOperator: Houston Independent School District\nDeveloper: SAS\nCountry: USA\nSector: Education\nPurpose: Evaluate teacher performance\nTechnology: Value-added model\nIssue: Accuracy/reliability; Legal\nTransparency: Governance; Black box; Complaints/appeals\nLegal, regulatory \ud83e\uddee\nHouston Fed'n of Teachers v. Houston Indep. Sch. Dist settlement (pdf)\nHouston Fed'n of Teachers v. Houston Indep. Sch. Dist\nResearch, advocacy \ud83e\uddee\nCollins C. (2014). Houston, We Have a Problem: Teachers Find No Value in the SAS Education Value-Added Assessment System (EVAAS\u00ae)\nAmrein-Beardsley A., Collins C. (2012). The SAS Education Value-Added Assessment System (SAS\u00ae EVAAS\u00ae) in the Houston Independent School District (HISD): Intended and Unintended Consequences\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bloomberg.com/opinion/articles/2017-05-15/don-t-grade-teachers-with-a-bad-algorithm\nhttps://www.houstonchronicle.com/news/houston-texas/houston/article/Houston-teachers-to-pursue-lawsuit-over-secret-11139692.php\nhttps://www.courthousenews.com/houston-schools-must-face-teacher-evaluation-lawsuit/\nhttps://www.chron.com/neighborhood/pasadena/news/article/Seven-Houston-teachers-Houston-Federation-of-9658867.php\nhttps://www.chron.com/news/houston-texas/education/article/Houston-ISD-settles-with-union-over-teacher-12267893.php\nhttps://www.houstonpublicmedia.org/articles/news/2017/10/10/241724/federal-lawsuit-settled-between-houstons-teacher-union-and-hisd/\nhttps://dianeravitch.net/2017/10/10/good-news-vam-is-dead-in-houston/\nhttps://www.theregister.com/2018/09/26/us_government_algorithms/\nRelated \ud83d\uddde\ufe0f\nSheri G. Lederman NYC teacher effectiveness assessment\nSarah Wysocky Washington DC schools teacher effectiveness assessment\nPage info\nType: Incident\nPublished: August 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/makeapp-gender-stereotyping", "content": "MakeApp make-up remover gender stereotyping\nOccurred: November 2017\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nMakeApp, a mobile application which enables users to add, alter or remove make-up from someone's face, suffered a backlash from users and commentators about its poor accuracy and the perception that its purpose objectifies and shames women. \nMany users complained that MakeApp's make-up removal filter, which commentators said was only developed by men, made them look worse than they would without makeup. Some also pointed out that it also makes them appear haggard and sallow. \nMakeApp founder Ashot Gabrelyanov responded to accusations of misogyny by saying, 'We built MakeApp as an experiment and released it into the wild a few months ago and unfortunately the media coverage solely focused on the make-up removal function of the app and characterised it as a bunch of 'tech bros' trying to hurt women, which is just so far from the truth'.\nSystem \ud83e\udd16\nMakeApp AppStore app\nOperator: Apple; Magic Unicorn\nDeveloper: Magic Unicorn\nCountry: USA\nSector: Beauty/cosmetics\nPurpose: Add/remove make-up\nTechnology: Neural network; Deep learning; Machine learning\nIssue: Accuracy/reliability; Bias/discrimination - gender\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.harpersbazaar.com/uk/beauty/make-up-nails/a13768946/the-make-up-removal-app-causing-controversy-for-endless-reasons/\nhttp://www.independent.co.uk/life-style/makeup-removal-app-makeapp-controversy-women-problems-issues-beauty-a8055866.html\nhttps://www.buzzfeed.com/delaneystrunk/this-is-the-beauty-app-that-no-one-asked-for-but-was-made\nhttps://twitter.com/gabrelyanov/status/922803935802548224\nhttps://www.bustle.com/p/does-the-makeapp-makeup-removing-app-really-work-we-tried-it-on-7-different-faces-5502003\nhttps://www.businessinsider.com/ashot-gabrelyanov-makeapp-app-russia-women-without-makeup-2017-11\nhttps://www.teenvogue.com/story/makeapp-makeup-removing-app-controversy\nhttp://www.refinery29.com/2017/11/180887/makeapp-makeup-removing-app\nhttps://www.thecut.com/2017/11/makeapp-shows-what-women-look-like-without-makeup.html\nhttps://www.theverge.com/tldr/2017/11/15/16655106/makeapp-no-makeup-terrible-apps-android-ios\nhttps://mashable.com/article/make-app-makeup-removing-app\nhttps://www.dailydot.com/irl/makeapp-makeup-removal/\nRelated \ud83c\udf10\nQoves AI beauty scoring\nBeauty AI 2.0 beauty contest\nPage info\nType: Incident\nPublished: August 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/predictive-policing-makes-robert-mcdaniel-criminal-target", "content": "Predictive policing makes Robert McDaniel criminal target\nOccurred: 2014\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAssessed by a predictive policing system that he was likely to be involved in a future shooting, despite never having been involved in one before, or caught in any violent event, Chicago resident Robert McDaniel was constantly surveiled by the police, ostracised by his community and was shot twice by people apparently unable to believe his story.\nOn the basis of McDaniel's proximity to and relationships with known shooters and shooting casualties, the Chicago Police Department-developed system had predicted that he would be involved in a shooting - as a perpetrator, or as a victim, or both.\nAccordingly, he was placed on the city's 'heat list' (later renamed 'Strategic Subject List' or 'SSL'), a database of people identified as potential shooters or shooting victims, and monitored by the police and other relevant authorities. This was despite the police having little idea what to do with individuals on the list, according to an influential research study.\nUnder increasing pressure from civil and digital rights advocates and scrutiny from the media and lawmakers, the City of Chicago terminated its heat list programme in 2019.\nSystem \ud83e\udd16\nChicago Police Department website\nChicago Police Department Wikipedia profile\nChicago Gun Crimes Heat Map\nOperator: Chicago Police Department\nDeveloper: Chicago Police Department; Illinois Institute of Technology\nCountry: USA\nSector: Govt - police\nPurpose: Predict criminals and victims\nTechnology: Predictive analytics\nIssue: Accuracy/reliability; Bias/discrimination - race, ethnicity\nTransparency: Governance; Black box; Complaints/appeals\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nRobert McDaniel arrest history\nResearch, advocacy \ud83e\uddee\nBroussard M. (2023). More than a Glitch\nBarbour C. (2023). Can a machine be racist? Artificial Intelligence has shown troubling signs of bias, but there are reasons for optimism\nGaddis D. (2022). The Ethical Perils of Predictive Policing (pdf)\nO'Donnell R.A. (2019). Challenging Racist Predictive Policing Algorithms under the Equal Protection Clause\nMeijer A., Wessels M. (2019). Predictive Policing: Review of Benefits and Drawbacks\nTucek A. (2018). Constraining Big Brother: The Legal Deficiencies Surrounding Chicago\u2019s Use of the Strategic Subject List\nHeeder M., Heischler M. (2017). Pre-Crime\nSaunders J., Hunt P., Hollywood J.S. (2016). Predictions put into practice: a quasi-experimental evaluation of Chicago\u2019s predictive policing pilot\nLum K., Isaac W. (2016) To Predict and Serve\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theverge.com/c/22444020/chicago-pd-predictive-policing-heat-list\nhttps://www.wired.com/story/crime-prediction-racist-history/\nhttps://www.enotes.com/topics/weapons-math-destruction/characters\nhttps://www.chicagotribune.com/news/ct-xpm-2013-08-21-ct-met-heat-list-20130821-story.html\nhttps://www.techdirt.com/2021/06/03/how-predictive-policing-got-chicago-man-shot-twice/\nRelated \ud83c\udf10\nHesse Palantir predictive policing\nUK Met Police Gangs Violence Matrix\nPage info\nType: Incident\nPublished: August 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/kyle-behm-kroger-algorithmic-personality-assessment", "content": "Kyle Behm Kroger algorithmic personality assessment\nOccurred: 2012\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nIn 2012, bipolar disorder sufferer and Vanderbilt University student Kyle Behm was rejected for low-skilled jobs at multiple companies by an algorithmic online personality test system that concluded that he was likely to ignore customers if they were upset or making him upset. \nThe rejections led to his family taking workforce management company Kronos to court, and to a widespread debate about the fairness of workplace personality tests. \nAfter taking some time off university for medical leave, Behm discovered through a friend that he had been 'red-lighted' by a personality test system supplied by workforce management company Kronos (now UKG) when he had applied for jobs at several companies, including supermarket chain Kroger, Home Depot, Walgreens.\nBehm's father, an attorney, filed a lawsuit against Kroger and five other companies for allegedly illegally screening for mental illness. Kyle Behm ended his life before the case went to court. The Americans with Disabilities Act prohibits 'employment tests that screen out or tend to screen out an individual with a disability or a class of individuals with disabilities' unless necessary for the job. \nSystem \ud83e\udd16\nUKG website\nUKG Wikipedia profile\nOperator: Home Depot; Kroger; PetSmart; Walgreens\nDeveloper: UKG/Kronos\nCountry: USA\nSector: Retail\nPurpose: Assess personality\nTechnology:  \nIssue: Bias/discrimination - disability\nTransparency: Governance; Appeals/complaints\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nUS Department of Labor. Americans with Disabilities Act\nUS Equal Employment Opportunity Commission (2007). Employment Tests and Selection Procedures\nUS Equal Employment Opportunity Commission (2003). Worker with Bipolar Disorder to Receive $91,000 in Disability Discrimination Case Settled by EEOC\nResearch, advocacy \ud83e\uddee\nCahill Timmons K. (2021). Pre-Employment Personality Tests, Algorithmic Bias, and the Americans with Disabilities Act (pdf)\nYang J.R. (2020). ENSURING A FUTURE THAT ADVANCES EQUITY IN ALGORITHMIC EMPLOYMENT DECISIONS (pdf)\nAjunwa I. (2020). The Paradox of Automation as Anti-Bias Inter omation as Anti-Bias Intervention\nHawkins T. T. (2019). Persona: The Dark Truth Behind Personality Tests\nMartin D. (2019). Search Ranking to Neural Networking and the Challenges of Account Giving (pdf)\nBal K. (2014). Screens under Scrutiny\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.wsj.com/articles/are-workplace-personality-tests-fair-1412044257\nhttps://www.bloomberg.com/view/articles/2018-01-18/personality-tests-are-failing-american-workers\nhttps://www.enotes.com/topics/weapons-math-destruction/characters\nhttps://www.theguardian.com/science/2016/sep/01/how-algorithms-rule-our-working-lives\nhttps://www.vice.com/en/article/pkekvb/cost-cutting-algorithms-are-making-your-job-search-a-living-hell\nhttps://www.irishtimes.com/culture/tv-radio-web/it-s-unfair-that-by-answering-honestly-i-was-excluded-from-work-this-can-t-be-legal-1.4504180\nhttps://www.chicagotribune.com/business/ct-biz-artificial-intelligence-hiring-20180719-story.html\nRelated \ud83c\udf10\nRetorio talent personality assessments\nFacebook Likes personality traits assessment\nPage info\nType: Incident\nPublished: August 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/vw-brazil-elis-regina-deepfake", "content": "VW Brazil Elis Regina deepfake advert\nOccurred: August 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn advert celebrating VW's 70th year operating in Brazil by simulating renown Brazilian singer Elis Regina, who died in 1982 at the age of 36, has sparked questions about the ethics of using artificial intelligence to simulate the image and likeness of a dead person.\nThe advert, which reputedly took over 2,400 hours to produce, saw an actress and deepfake technology make it appear as if Regina was performing her 1976 hit Como Nossos Pais in a duet with her daughter Maria Rita while driving a VW van. \nMany Brazilians commented that the ad was profoundly nostalgic and moving. However, Brazilian advertising regulator watchdog Conar announced it would investigate a possible breach of ethics after receiving complaints questioning whether it was right to use such methods 'to bring a deceased person back to life' on screen. \nConar also raised concerns about whether synthetic media may cause some people, notably children and teenagers, to confuse fiction with reality. Others pointed out Regina's vocal opposition to the military dictatorship that governed Brazil from 1964 to 1985, and the latter's closeness to VW. \nSystem \ud83e\udd16\nUnkmownVW Brasil. 70 anos advert\nOperator: Volkswagen\nDeveloper: AlmapBBDO\nCountry: Brazil\nSector: Business/professional services\nPurpose: Recreate singer\nTechnology: Deepfake - video, audio; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Legal; Ethics\nTransparency: Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theguardian.com/world/2023/jul/14/brazil-singer-elis-regina-artificial-intelligence-volkswagen\nhttps://www.thedrum.com/news/2023/08/15/heartwarming-tribute-or-ethical-breach-ai-powered-volkswagen-ad-sparks-controversy\nhttps://www.creativebloq.com/news/volkswagen-ai-deepfake-advert\nhttps://revistazum.com.br/colunistas/elis-regina-ias/\nhttps://www.onoticiado.com.br/2023/07/10/tecnologia/deepfake-elis-regina/\nhttps://nucleo.jor.br/garimpo/elis-deepfake/\nhttps://www.cnnbrasil.com.br/entretenimento/deepfake-x-ia-comercial-com-imagem-de-elis-regina-abre-discussao-sobre-perigos-no-futuro/\nRelated \ud83c\udf10\nCruzcampo Lola Flores deepfake ad\nHour One 'character' clones\nPage info\nType: Incident\nPublished: August 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/itutorgroup-recruitment-algorithmic-age-discrimination", "content": "iTutorGroup recruitment algorithmic age discrimination\nOccurred: May 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nChina-based English-language tutoring company iTutorGroup has been found to have been using artificial intelligence-powered software to reject older job applicants, resulting in job opportunity losses, litigation, and a regulatory fine.\nThe US Equal Employment Opportunity Commission (EEOC), which enforces workplace bias laws, had first alleged in 2020 that iTutorGroup had developed and was operating online recruitment software to screen out women aged 55 or older and men who were 60 or older. iTutorGroup denied wrongdoing, and agreed to pay USD 365,000 to more than 200 job applicants.\nIn 2021, the EEOC launched an initiative to ensure that AI software used by US employers complies with anti-discrimination laws. The iTutorGroup lawsuit was the first by the EEOC under the initiative.\nSystem \ud83e\udd16\niTutorGroup website\nOperator: Ping An Insurance Group/iTutorGroup\nDeveloper: Ping An Insurance Group/iTutorGroup\nCountry: USA\nSector: Education\nPurpose: Screen job applicants\nTechnology: Recruitment system\nIssue: Bias/discrimination - age\nTransparency: Governance; Complaints/appeals\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nEEOC v iTutorGroup settlement\nEEOC v iTutorGroup case docket\nUS Equal Employment Opportunity Commission (2022). EEOC Sues iTutorGroup for Age Discrimination\nUS Equal Employment Opportunity Commission (2021). Joint Statement on Enforcement Efforts Against Discrimination and Bias in Automated Systems\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reuters.com/legal/tutoring-firm-settles-us-agencys-first-bias-lawsuit-involving-ai-software-2023-08-10/\nhttps://www.jdsupra.com/legalnews/eeoc-inks-first-ever-ai-based-7922259/\nhttps://news.bloomberglaw.com/daily-labor-report/eeoc-settles-first-of-its-kind-ai-bias-lawsuit-for-365-000\nhttps://www.verdict.co.uk/itutorgroup-settles-ai-hiring-lawsuit-alleging-age-discrimination\nhttps://interestingengineering.com/culture/in-a-first-us-penalizes-chinese-firm-using-ai-recruitment-tools\nhttps://www.scmp.com/tech/article/3230722/chinese-tutoring-firm-settles-us-agencys-first-bias-lawsuit-involving-ai-software\nRelated \ud83c\udf10\nWorkday AI job screening system discrimination\nHireVue recruitment facial analysis screening\nPage info\nType: Incident\nPublished: August 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/knight-capital-group-equity-order-routing-system-glitch", "content": "Knight Capital Group equity order routing system glitch\nOccurred: August 2012\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe August 2021 breakdown of the automated routing system for equity orders at American global financial services company Knight Capital Group resulted in massive disruption on the New York Stock Exchange, a USD 440 million loss to the company, and its eventual sale.\nAccording to the SEC, a section of code in an algorithmic equity order router had been moved in 2005 to an earlier spot in the code sequence. As a result, one of the router\u2019s functions became defective. In July 2012, Knight incorrectly deployed new code in the same router. That triggered the defective function, which was unable to recognise when orders had been filled. \nKnight Capital was fined USD 12 million in October 2013 to settle charges that it failed to install adequate safeguards to limit the risks posed by its access to markets. The company completed its merger with GETCO in July 2013, and was eventually acquired by Virtu LLC in July 2017 for USD 1.4 billion. \nSystem \ud83e\udd16\nKnight Capital Group Wikipedia profile\nOperator: Knight Capital Group\nDeveloper: Knight Capital Group\nCountry: USA\nSector: Banking/financial services\nPurpose: Route equity orders\nTechnology: Equity order routing system\nIssue: Robustness\nTransparency: \nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nUS Securities and Exchange Commission (2013). SEC Charges Knight Capital With Violations of Market Access Rule\nUS Securities and Exchange Commission (2013). ORDER INSTITUTING ADMINISTRATIVE AND CEASE-AND-DESIST PROCEEDINGS, PURSUANT TO SECTIONS 15(b) AND 21C OF THE SECURITIES EXCHANGE ACT OF 1934, MAKING FINDINGS, AND IMPOSING REMEDIAL SANCTIONS AND A CEASE-AND-DESIST ORDER (pdf)\nResearch, advocacy \ud83e\uddee\nMartins Pereira C. (2022). Unregulated Algorithmic Trading: Testing the Boundaries of the European Union Algorithmic Trading Regime\nKapadia N., Linn M. (2020). What's Gone Wrong with Option Liquidity: Evidence from the Knight Capital's Trading Glitch\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reuters.com/article/us-usa-nyse-tradinghalts-idUSBRE8701BN20120801\nhttps://archive.nytimes.com/dealbook.nytimes.com/2012/08/02/knight-capital-says-trading-mishap-cost-it-440-million/\nhttps://www.theregister.com/2012/08/03/bad_algorithm_lost_440_million_dollars/\nhttps://www.theregister.com/2012/08/02/knight_capital_trading_bug/\nhttps://www.zerohedge.com/news/what-happens-when-hft-algo-goes-totally-berserk-and-serves-knight-capital-bill\nhttps://www.bbc.co.uk/news/magazine-19214294\nhttps://apnews.com/article/34b29096242044a09a538d7c9eaf4ee5\nhttps://siliconangle.com/2013/10/25/how-poor-devops-culture-lead-to-a-465m-trading-loss-for-knight-capital/\nhttps://medium.com/dataseries/the-rise-and-fall-of-knight-capital-buy-high-sell-low-rinse-and-repeat-ae17fae780f6\nRelated \ud83c\udf10\nPyth Bitcoin glitch\nAadhaar glitch results in villagers' starvation\nPage info\nType: Incident\nPublished: August 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/facebook-content-moderators-develop-ptsd", "content": "Facebook content moderators develop PTSD\nOccurred: September 2018\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nContent moderators working directly or indirectly for Facebook who have to review child abuse, beheading and other traumatic videos have been diagnosed with mental health conditions, including anxiety, depression and post-traumatic stress disorder (PTSD). The findings triggered multiple lawsuits against the technology company. \nIn May 2020, Facebook agreed to pay USD 52 million to over 10,000 current and former contract workers in sites in California, Arizona, Texas and Florida, with each worker receiving USD 1,000 in cash and those diagnosed with psychological conditions related to their work as Facebook moderators eligible medical treatment and damages up to USD 50,000 per person.\nThe California lawsuit had described content moderators such as lead plaintiff Selena Scola having to handle 'broadcasts of child sexual abuse, rape, torture, bestiality, beheadings, suicide, and murder' on a daily basis, and that Facebook had failed to provide a safe workplace for content moderators and did little to safeguard their mental health.\nSystem \ud83e\udd16\nFacebook\nFacebook Wikipedia profile\nOperator: Meta/Facebook; Cognizant; Pro Unlimited  \nDeveloper: Meta/Facebook; Cognizant; Pro Unlimited\nCountry: USA\nSector: Technology\nPurpose: Moderate content\nTechnology: Content moderation system\nIssue: Employment - safety\nTransparency: Governance; Complaints/appeals\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nScola v Facebook settlement\nJoseph Saveri Law Firm. Facebook Content Moderators\u2019 Safe Workplace Litigation\nResearch/advocacy \ud83e\uddee\nPinchevski A. (2022). Social media\u2019s canaries: content moderators between digital labor and mediated trauma\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.linkedin.com/posts/selenascola_facebook-will-pay-52-million-in-settlement-activity-6666161263023845380-og0r\nhttps://www.nytimes.com/2018/09/25/technology/facebook-moderator-job-ptsd-lawsuit.html\nhttps://www.theverge.com/2020/5/12/21255870/facebook-content-moderator-settlement-scola-ptsd-mental-health\nhttps://www.washingtonpost.com/technology/2020/05/12/facebook-content-moderator-ptsd/\nhttps://www.npr.org/2020/05/12/854998616/in-settlement-facebook-to-pay-52-million-to-content-moderators-with-ptsd\nhttps://www.reuters.com/article/us-facebook-lawsuit-idUSKCN1M423Q\nhttps://techcrunch.com/2020/05/12/facebook-moderators-ptsd-settlement/\nRelated\nAI overwhelms Stack Overflow content moderation\nSama 'ethical' data labeling, content moderation\nPage info\nType: Issue\nPublished: August 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/sheri-g-lederman-nyc-teacher-effectiveness-assessment", "content": "Sheri G. Lederman NYC teacher effectiveness assessment\nOccurred: September 2014\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA teacher evaluation system used by New York State awarded an experienced and highly regarded primary school teacher one point out of twenty for her students' progress on state tests, deeming her ineffective. \nThe decision led to the system being thrown out by New York highest court after primary school teacher Sheri G Lederman had sued the state for using the 'arbitrary' and 'capricious' Growth Measures system to rate teachers. \nAccording to the New York Times, Ms. Lederman\u2019s students had performed marginally lower on their English exam in the 2013-2014 school year than in the previous year, causing her test-based effectiveness rating to drop from 14 out of 20 points to 1 out of 20 points. \nThe 'Value Added Modeling' method was meant to account for the various factors that might impact a student\u2019s score on a standardised test, isolate the teacher\u2019s 'value-added' input on the student\u2019s growth during one year of instruction, and determine whether or not the student learned as much as similarly situated students. \nThe judge determined that New York State failed to make a clear case for explaining how Lederman\u2019s score could so wildly swing in a single year. \nSystem \ud83e\udd16\nNew York City Department of Education website\nNew York City Department of Education Wikipedia profile\nOperator: New York City Department of Education\nDeveloper: Mathematica Policy Research\nCountry: USA\nSector: Education\nPurpose: Evaluate teacher performance\nTechnology: Value-added model\nIssue: Accuracy/reliability; Effectiveness/value\nTransparency: Governance; Black box; Complaints/appeals\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nLederman v King\nResearch, advocacy \ud83e\uddee\nUnited Federation of Teachers. This is no way to rate a teacher (pdf)\nAmrein-Beardsley A., Close K. (2019). Teacher-Level Value-Added Models on Trial: Empirical and Pragmatic Issues of Concern Across Five Court Cases\nKatz D.S. (2016). Growth Models and Teacher Evaluation: What Teachers Need to Know and Do\nKoedela C., Mihaly K., Rockoff J.E. (2015). Value-Added Modelling: A Review (pdf) \nBlanchard M.R. et al (2010). Is inquiry possible in light of accountability?: A quantitative comparison of the relative effectiveness of guided inquiry and verification laboratory instruction\nSchochet P.Z., Chiang H.S. (2010). Error Rates in Measuring Teacher and School Performance Based on Student Test Score Gains (pdf)\nMore\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.edweek.org/policy-politics/n-y-teacher-challenges-state-evaluation-system/2014/11\nhttps://www.nytimes.com/2016/05/11/nyregion/court-vacates-long-island-teachers-evaluation-tied-to-student-test-scores.html\nhttps://www.washingtonpost.com/news/answer-sheet/wp/2015/08/09/master-teacher-suing-new-york-state-over-ineffective-rating-is-going-to-court/\nhttps://www.dailykos.com/stories/2016/5/17/1525988/-New-York-Teacher-VAM-scores-Arbitrary-and-Capricious\nhttps://danielskatz.net/2016/05/11/new-york-evaluations-lose-in-court/\nhttps://slate.com/human-interest/2015/08/vam-lawsuit-in-new-york-state-here-s-why-the-entire-education-reform-movement-is-watching-the-case.htmlhttps://ny.chalkbeat.org/2012/2/23/21110168/why-we-won-t-publish-individual-teachers-value-added-scores\nRelated \ud83c\udf10\nSarah Wysocky Washington DC schools teacher effectiveness assessment\nChatGPT falsely claims to write student essays\nPage info\nType: Incident\nPublished: August 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/facebook-likes-personality-traits-assessment", "content": "Facebook 'Likes' predict personality study\nOccurred: May 2013\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nUniversity of Cambridge researchers designed a set of algorithms to show how an accurate portrait of someone's personality, including their religious views, political beliefs, race, and sexual orientation, could be made from the things they have 'liked' on Facebook.\nThe researchers drew on data showing the Facebook likes of 58,000 volunteers in the US, which was analysed and matched with information from personality tests. The algorithms proved 88% accurate for determining male sexuality, 95% accurate in distinguishing African-American from Caucasian-American, and 85% for differentiating Republican from Democrat.\nDigital rights and rivacy campaigners expressed their concerns about the study's findings, recommending people consider carefully that they share online. \n'I can imagine situations in which the same data and technology is used to predict political views or sexual orientation, posing threats to freedom or even life,' said Michael Kosinski, lead researcher on the project. \nSystem \ud83e\udd16\n\nOperator: Meta/Facebook\nDeveloper: Kosinski M., Stillwell D., Graepel T.\nCountry: USA; UK\nSector: Research/academia; Technology\nPurpose: Predict prersonality\nTechnology: Machine learning\nIssue: Privacy\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nKosinski M., Stillwell D., Graepel T. (2013). Private traits and attributes are predictable from digital records of human behavior\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.com/news/technology-21699305\nhttps://www.washingtonpost.com/news/the-intersect/wp/2015/01/12/facebook-may-know-you-better-than-your-friends-and-family-study-finds/\nhttps://www.huffingtonpost.co.uk/2013/03/12/facebook-likes-can-predict-personality_n_2858150.html\nhttps://edition.cnn.com/2013/03/11/tech/social-media/facebook-likes-study/index.html\nhttps://www.entrepreneur.com/article/229460\nhttps://www.newscientist.com/article/dn23260-what-your-facebook-likes-really-say-about-you/\nRelated \ud83c\udf10\nStanford facial political orientation study\n'Gaydar' AI sexual orientation predictions\nPage info\nType: Issue\nPublished: August 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/waving-arms-trigger-nest-protect-false-alarms", "content": "Waving arms trigger Nest Protect false alarms\nOccurred: April 2014\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nNest halted sales of its new Nest Protect Smoke and CO (carbon monoxide) alarms a month after their launch due to an issue with Nest Wave, an algorithm that allows users to 'wave to hush' the product. No customer incidents had been reported.\nWave was supposed to enable users to to silence a warning by waving their hand in front of the detector. But further lab tests by Nest revealed that it could be unintentionally activated during a real fire, resulting in a delayed response to real danger.\nNest pushed out a series of software updates, including one that disabled the Wave algorithm for existing users. A month later, the company recalled 440,000 of the devices. \nGoogle acquired Nest a few months earlier in January 2014.\nSystem \ud83e\udd16\nGoogle Nest website\nGoogle Nest Wikipedia profile\nOperator: Alphabet/Google/Nest\nDeveloper: Alphabet/Google/Nest  \nCountry: USA\nSector: Consumer goods\nPurpose: Disable alarm\nTechnology: Machine learning\nIssue: Accuracy/reliability; Safety\nTransparency: Governance\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nUS Consumer Product Safety Commission (2014). Nest Labs recalls to repair Nest Protect Smoke CO Alarms\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://readwrite.com/2014/04/04/nest-smoke-detector-fail/\nhttps://thenextweb.com/google/2014/04/03/nest-halts-sales-nest-protect-alarm-fix-accidental-disabling-issue/\nhttps://gizmodo.com/nest-halts-sale-of-protect-over-safety-concerns-1557829251\nhttps://gizmodo.com/nest-is-recalling-its-protect-smoke-alarm-1579734411\nhttps://www.cbsnews.com/news/nest-labs-halts-sales-of-nest-protect-over-safety-concerns/\nhttps://www.techhive.com/article/2602213/software-update-will-help-nest-protect-know-the-difference-between-steam-and-smoke.html\nhttps://venturebeat.com/2014/09/04/nests-smart-smoke-detector-gets-a-big-software-update-alert-history-co-levels-more/\nRelated \ud83d\uddde\ufe0f\nGoogle Nest Hub 2 sleep tracking\nAmazon Ring video doorbell neighbour privacy invasion\nPage info\nType: Incident\nPublished: August 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/delilah-blackmail-bot", "content": "Delilah blackmail bot\nOccurred: July 2016\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nDiscovered in 2016 by Gartner analyst Avivah Litan, Delilah was the first known 'insider' threat bot. Spread through downloads on multiple adult and gaming sites, the bot is said to gather sensitive information on the victim which can later be used for espionage, blackmail, and extortion.\nDubbed 'Delilah', the bot likely uses a combination of social engineering and automated ransomware to enable its operators to capture footage of victims through their webcams, which can then be used to extort the victim or convince them to carry out actions that would harm their employer. \nAccording to Litan, 'Once installed the hidden bot gathers enough personal information from the victim so that the individual can later be manipulated or extorted. This includes information on the victim's family and workplace.'\nSystem \ud83e\udd16\nDelilah\nOperator:  \nDeveloper:  \nCountry: Global\nSector:  Banking/financial services\nPurpose: Defraud\nTechnology: Reinforcement learning\nIssue: Ethics; Security\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nGartner (2016). Meet Delilah \u2013 the first Insider Threat Trojan\nNews, commentary, analysis \ud83d\udcf0\nhttps://www.computerworld.com/article/3096250/delilah-malware-secretly-taps-webcam-blackmails-and-recruits-insider-threat-victims.html\nhttps://www.securityweek.com/new-trojan-helps-attackers-recruit-insiders/\nhttps://www.scmagazine.com/brief/delilah-trojan-seeks-company-weaknesses-through-insiders\nhttps://www.theregister.com/2016/07/18/first_insider_theft_extortion_trojan_found/\nhttps://www.zdnet.com/article/this-webcam-malware-could-blackmail-you-into-leaking-company-secrets/\nRelated \ud83c\udf10\nChina taxation department ID system hack\nPage info\nType: System\nPublished: August 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/artists-private-medical-image-trains-laion-dataset", "content": "Artist's private medical image used to train LAION-5B dataset\nOccurred: September 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nSan Francisco-based digital artist 'Lapine' found that private medical photographs taken by her doctor when she was undergoing treatment for a rare genetic condition in 2013 had been used to train the image-text dataset LAION-5B. \nAccording to Lapine, the photographs had been taken as part of her clinical documentation, and she signed documents that restricted their use to her medical file. Lapine had discovered her images on LAION through the Have I Been Trained tool, which allows artists to see if their work is being used to train AI image generation models. \nThe LAION-5B dataset is supposed only to use publicly available images on the web. Lapine said the surgeon who took the medical photos died of cancer in 2018; she suspects that they somehow left his practice's custody after that. \nArs Technica said it discovered 'thousands of similar patient medical record photos in the data set, each of which may have a similar questionable ethical or legal status, many of which have likely been integrated into popular image synthesis models that companies like Midjourney and Stability AI offer as a commercial service'.\nSystem \ud83e\udd16\nLAION-5B dataset\nOperator: Lapine\nDeveloper: LAION\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Pair text and images\nTechnology: Database/dataset; Neural network; Deep learning; Machine learning\nIssue: Privacy; Ethics/values\nTransparency: Governance; Complaints/appeals\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://twitter.com/LapineDeLaTerre/status/1570889343845404672\nhttps://arstechnica.com/information-technology/2022/09/artist-finds-private-medical-record-photos-in-popular-ai-training-data-set/\nhttps://futurism.com/the-byte/private-medical-photos-ai\nhttps://petapixel.com/2022/09/26/shocked-artist-finds-private-medical-photos-in-ai-training-data-set\nhttps://the-decoder.com/patient-images-in-laion-datasets-are-only-a-sample-of-a-larger-issue\nhttps://unthinking.photography/imgexhaust/private-medical-record-photos-in-popular-ai-training-data-set\nhttps://www.pcgamer.com/ai-generated-images-face-getty-ban-as-privacy-and-ownership-concerns-grow\nhttps://www.analyticsinsight.net/the-ethical-challenges-of-training-medical-ai-woman-falls-victim\nhttps://www.theguardian.com/technology/2023/mar/16/the-stupidity-of-ai-artificial-intelligence-dall-e-chatgpt\nRelated \ud83d\uddde\ufe0f\nLAION trains Robert Kneschke photos without consent\nBookCorpus dataset bias, copyright abuse\nPage info\nType: Incident\nPublished: August 2023\nLast updated: December 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/laion-trains-robert-kneschke-photos-without-consent", "content": "LAION trains Robert Kneschke photos without consent \nOccurred: April 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGerman stock photographer Robert Kneschke discovered that his photos had been used to train the LAION-5B dataset, raising questions about copyright protections from AI datasets and systems, and the ethics of the dataset's eponymous developer.\nHaving asked LAION to remove his work from their training data, Kneschke received a demand for 887 euros (USD 980) from LAION's law firm Heidrich Rechtsanw\u00e4lte for what it called an 'unjustified claim' on the basis that it 'only maintains a database containing links to image files that are publicly available on the Internet.' \nThe fracas put the spotlight on the practices and ethics of LAION, a German-based non-profit dedicated to the 'democratisation' of machine learning research and applications that provides datasets to train major commercial text-to-image and video-generating models such as Stable Diffusion, Midjourney and Google\u2019s Imagen. \nKneschke had used the website Have I Been Trained? to find out whether any major datasets had been trained using his images. \n\u2795 April 2023. Kneschke filed a lawsuit against LAION for copyright infringment.\nSystem \ud83e\udd16\nLAION-5B dataset\nOperator: LAION\nDeveloper: LAION\nCountry: Germany\nSector: Media/entertainment/sports/arts\nPurpose: Pair text and images\nTechnology: Database/dataset; Neural network; Deep learning; Machine learning\nIssue: Copyright; Ethics\nTransparency: Governance; Marketing; Complaints/appeals\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nRobert Kneschke v. LAION\nResearch, advocacy \ud83e\uddee\nRobert Kneschke (2023). LAION-Verein droht Urhebern, die ihre Daten aus KI-Trainingssatz nehmen wollen mit Schadensersatzanspr\u00fcchen\nCampaign for AI Safety (2023). Copyright Cases Against AI Labs\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.vice.com/en/article/pkapb7/a-photographer-tried-to-get-his-photos-removed-from-an-ai-dataset-he-got-an-invoice-instead\nhttps://petapixel.com/2023/04/26/ai-image-dataset-demands-money-from-photographer-who-requested-removal-of-his-photos/\nhttps://www.diyphotography.net/ai-used-photographers-photos-for-training-then-slapped-him-with-an-invoice/\nhttps://www.heise.de/hintergrund/Was-darf-KI-Stockfotograf-und-KI-Verein-streiten-um-das-Copyright-8984836.html\nhttps://www.profifoto.de/szene/notizen/2023/02/21/laion-droht-kneschke/\nhttps://mezha.media/en/2023/05/01/a-photographer-was-fined-nearly-1-000-when-he-asked-to-have-his-photos-removed-from-an-ai-database/\nhttps://news.ycombinator.com/item?id=35712617\nRelated \ud83c\udf10\nArtist's private medical image trains LAION dataset\nAmazon sells fake AI Jane Friedman books\nPage info\nType: Incident\nPublished: August 2023\nLast updated: June 2024", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-meal-planner-app-suggests-chlorine-gas-recipe", "content": "AI meal planner app suggests chlorine gas recipe \nOccurred: August 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA chatbot that generates meal suggestions and directions generated a recipe that recommended people concoct chlorine gas, which it called \u2018aromatic water mix'. \nThe incident called into the question the safety of the GPT-3.5 powered bot, despite its operator saying it had built in safeguards to stop these kinds of outputs.\nKiwi political commentator Liam Hehir had asked Pak 'n Save's Savey meal-bot what he could make if only he had water, bleach and ammonia. \nA spokeperson for New Zealand-based supermarket chain Pak n'Save told The Guardian that the company was disappointed to see 'a small minority have tried to use the tool inappropriately and not for its intended purpose.'\nPak\u2019nSave promised to 'keep fine-tuning' its bot.\nSystem \ud83e\udd16\nSavey Meal-bot website\nPak n' Save (2023). Introducing the Savey Meal-bot\nOperator: Pak \u2018n\u2019 Save\nDeveloper: Pak \u2018n\u2019 Save; OpenAI\nCountry: New Zealand\nSector: Retail\nPurpose: Generate recipes\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning\nIssue: Accuracy/reliability; Safety\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theguardian.com/world/2023/aug/10/pak-n-save-savey-meal-bot-ai-app-malfunction-recipes\nhttps://www.forbes.com/sites/mattnovak/2023/08/12/supermarket-ai-gives-horrifying-recipes-for-poison-sandwiches-and-deadly-chlorine-gas/\nhttps://arstechnica.com/information-technology/2023/08/ai-powered-grocery-bot-suggests-recipe-for-toxic-gas-poison-bread-sandwich/\nhttps://www.theregister.com/2023/08/11/supermarket_reins_in_ai_recipebot/\nhttps://interestingengineering.com/innovation/rogue-ai-bot-recipes-for-human-flesh-and-chlorine-gas\nhttps://www.stuff.co.nz/business/132725271/paknsaves-ai-meal-planner-suggests-recipe-for-deadly-chlorine-gas\nhttps://www.thegrocer.co.uk/technology-and-supply-chain/deadly-supermarket-ai-recipe-suggestion-serves-up-cautionary-tale/682061.article\nRelated \ud83c\udf10\nChatGPT invented case citations in legal filings\nChatGPT falsely accuses Australian mayor of bribery\nPage info\nType: Incident\nPublished: August 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/facebook-lets-housing-ads-exclude-ethnic-minorities", "content": "Facebook lets housing ads exclude ethnic minorities\nOccurred: October 2016\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nFacebook allowed advertisers in the US to exclude black, Hispanic, and other 'ethnic affinities' from seeing ads on its platform, according to a 2016 investigation by non-profit news organisation ProPublica. \nFacebook devised a category called 'Ethnic Affinities' that enabled advertisers to target and exclude certain groups of users when placing ads for a new apartment or a house for sale. Affinity targeting is based on interests users have declared or Facebook pages they have liked. \nThe discovery resulted in Facebook being sued by multiple parties, including the US Department of Housing (HUD) - a suit Facebok lost. Ads that exclude people based on race, gender and other sensitive factors are prohibited by US federal laws governing housing, employment, and financial services.\nIn August 2020, The Markup reported that Facebook continued to publish ads discriminating against users on the basis of age and race, including in advertising open jobs. Days before The Markup's article was published, Facebook announced it would eliminate multicultural affinity categories. \nA July 2021 Markup investigation discovered a wide range of proxies for racial categories being used by advertisers on the platform, including the phrases 'African-American culture,' 'Asian Culture,' and 'Latino culture.'\nSystem \ud83e\udd16\nFacebook ads\nFacebook (2017). Improving Enforcement and Promoting Diversity: Updates to Ads Policies and Tools\nOperator: Meta/Facebook\nDeveloper: Meta/Facebook\nCountry: USA\nSector: Govt - housing\nPurpose: Target advertising\nTechnology: Advertising management system\nIssue: Bias/discrimination - race, ethnicity, age\nTransparency: Governance\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nHUD v Facebook\nNational Fair Housing Alliance v. Facebook, Inc. (1:18-cv-02689)\nResearch, advocacy \ud83e\uddee\nBrookings (2021). Solving the Problem of Racially Discriminatory Advertising on Facebook\nSpinks C. N. (2020). Contemporary Housing Discrimination: Facebook, Targeted Advertising, and the Fair Housing Act\nNational Fair Housing Alliance (2019). Civil Rights Advocates Settle Lawsuit with Facebook: Transforms Facebook\u2019s Platform Impacting Millions of Users\nACLU (2019). Facebook Agrees to Sweeping Reforms to Curb Discriminatory Ad Targeting Practices\nAli M., et al (2019). Discrimination through Optimization: How Facebook\u2019s Ad Delivery Can Lead to Biased Outcomes (pdf)\nSpeicher T. et al (2018). Potential for Discrimination in Online Targeted Advertising (pdf)\nInvestigations, assessments, audits \ud83e\uddd0\nThe Markup (2021). Facebook Got Rid of Racial Ad Categories. Or Did It?\nThe Markup (2020). Does Facebook Still Sell Discriminatory Ads?\nProPublica (2019). Facebook Won\u2019t Let Employers, Landlords or Lenders Discriminate in Ads Anymore\nProPublica (2017). Facebook (Still) Letting Housing Advertisers Exclude Users by Race\nProPublica (2016). HUD Has \u2018Serious Concerns\u2019 About Facebook\u2019s Ethnic Targeting\nProPublica (2016). Facebook Lets Advertisers Exclude Users by Race\nProPublica (2016). Facebook ad\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.usatoday.com/story/tech/news/2016/10/28/facebook-advertisers-exclude-racial-ethnic-groups/92888160/\nhttps://nationalvanguard.org/2016/10/facebook-advertisers-can-exclude-racial-groups-in-housing-ads/\nhttps://www.washingtonpost.com/news/the-switch/wp/2017/02/08/facebook-cracks-down-on-ads-that-discriminate/\nhttps://www.washingtonpost.com/technology/2022/06/21/facebook-doj-discriminatory-housing-ads/\nhttps://www.wired.com/story/facebook-advertising-discrimination-settlement/\nhttps://www.reuters.com/article/us-facebook-advertisers/u-s-charges-facebook-with-racial-discrimination-in-targeted-housing-ads-idUSKCN1R91E8\nhttps://www.fastcompany.com/90329906/study-facebook-ad-targeting-may-discriminate-even-when-advertisers-dont-want-it-to\nRelated \ud83c\udf10\nTransunion ResidentScore tenant screening\nFacebook credit card age ad targeting bias\nPage info\nType: Incident\nPublished: August 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/maxpread-technologies-fake-ai-ceo-scam", "content": "Maxpread Technologies deploys deepfake CEO to scam investors\nOccurred: April 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nMaxpread Technologies' use of an AI-generated CEO to manipulate investors into believing the company was legitimate and trustworthy prompted a US regulator to issue a desist and refrain order against the entity and its founder 'Jan Gregory Cerato'.\nThe video, which was created using AI video generation tool Synthesia aused to launch the company, was fronted by 'Michael Vanes'. \nHowever, according to the US Department of Financial Protection and Innovation, Vanes does not exist and that Maxpread\u2019s real CEO is actually Jan Gregory, who the company had called its chief marketing officer and corporate brand manager. \nGregory informed investors he was 'relinquishing his position' at Maxpread in May 2023, saying he had 'witnessed a series of mismanagement of people\u2019s funds within this organization.' \nMaxpread is allegedly the latest in a series of scams involving Jan Gregory, whose real name is Jan Strzepka.\nSystem \ud83e\udd16\nSynthesia AI video generation\nOperator: Maxpread Technologies\nDeveloper: Maxpread Technologies; Synthesia\nCountry: UAE/Dubai; Hong Kong; USA\nSector: Banking/financial services\nPurpose: Manipulate investors\nTechnology: Deepfake - video, audio; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning  \nIssue: Ethics; Mis/disinformation\nTransparency: Governance; Marketing\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nUS Department of Financial Protection and Innovation (2023). Maxpread Technologies Desist and Refrain Order (pdf)\nUS Department of Financial Protection and Innovation (2023). DFPI Launches Sweep of Investment Fraud Claiming Ties to Artificial Intelligence\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://decrypt.co/137519/crypto-scammers-used-ai-create-fake-ceo-regulators-say\nhttps://tech.co/news/chatgpt-ai-scams-watch-out-avoid\nhttps://www.forbes.com/sites/cyrusfarivar/2023/04/20/alleged-crypto-scammers-used-ai-and-actors-as-faux-ceos/\nhttps://behindmlm.com/companies/jan-gregory-ditches-maxpread-tech-coinmarketbull-collapses/\nhttps://www.wired.com/story/synthesia-ai-deepfakes-it-control-riparbelli/\nRelated \ud83c\udf10\nFTX CEO deepfake investment scam\nUSD 622,000 deepfake impersonation scam\nPage info\nType: Incident\nPublished: August 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-sells-fake-ai-jane-friedman-books", "content": "Amazon sells fake AI Jane Friedman books\nOccurred: August 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nUS-based author Jane Friedman discovered five fake books being sold under her name on Amazon, resulting in a wave of complaints and negative media coverage accusing the technology company of poor content moderation and tone-deaf customer relations.\nIn a blog post titled 'I Would Rather See My Books Get Pirated Than This (Or: Why Goodreads and Amazon Are Becoming Dumpster Fires)', Friedman said she suspected the books were generated using ChatGPT or a similar AI system, which were listed under her name on Amazon and its Goodreads subsidiary. \nAmazon refused to remove the books after Friedman had submitted a complaint on the basis that she did not hold a trademark to her name, only to relent a few hours after she had posted her views online. \nSystem \ud83e\udd16\nChatGPT website\nChatGPT Wikipedia profile\nOperator: OpenAI\nDeveloper: OpenAI\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Accuracy/reliability\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://janefriedman.com/i-would-rather-see-my-books-pirated/\nhttps://mashable.com/article/amazon-removes-fake-ai-generated-books-jane-friedman\nhttps://www.thedailybeast.com/author-jane-friedman-finds-ai-fakes-being-sold-under-her-name-on-amazon\nhttps://gizmodo.com/amazon-jane-friedman-ai-generated-books-removed-1850718989\nhttps://www.dailymail.co.uk/news/article-12394559/Jane-Friedman-AI-books-amazon.html\nhttps://www.theguardian.com/books/2023/aug/09/amazon-removes-books-generated-by-ai-for-sale-under-authors-name\nhttps://decrypt.co/151674/amazon-refuses-authors-request-to-remove-fake-books-written-with-ai\nhttps://futurism.com/the-byte/author-amazon-ai-books\nhttps://edition.cnn.com/2023/08/10/tech/ai-generated-books-amazon/index.html\nhttps://qz.com/amazon-ai-generated-books-using-real-authors-names-1850720961\nhttps://www.newscientist.com/article/2386956-authors-fear-they-have-little-defence-against-ai-impersonators/\nRelated \ud83c\udf10\nDrake, The Weeknd AI voice cloning\nScammer sells fake AI Frank Ocean songs\nPage info\nType: Incident\nPublished: August 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chatgpt-powers-fox8-crypto-promotion-botnet", "content": "'ChatGPT' powers 'Fox8' crypto promotion botnet \nOccurred: July 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nResearchers Kai-Cheng Yang and Filippo Menczer at the Indiana University Observatory on Social Media  discovered a Twitter botnet that appears to use ChatGPT to generate text promoting crypto/blockchain/NFT content.\nThe so-called 'fox8' botnet comprises three news sites, one named 'Fox8', and a cluster of 1,140 Twitter accounts, and was discovered by searching Twitter for the phrase 'as an ai language model\u2019 - a common phrase generated by ChatGPT when it receives a prompt that violates its usage policies - between October 2022 and April 2023. \nThe researchers warn that 'emerging research indicates that LLMs can facilitate the development of autonomous agents capable of independently processing exposed information, making autonomous decisions, and utilizing tools such as APIs and search engines.'\nIronically, Fox8 publishes articles on generative AI, including one on how AI content farms use ChatGPT to generate fake stories. \nSystem \ud83e\udd16\nChatGPT chatbot\nOperator: Fox8\nDeveloper: OpenAI\nCountry: USA\nSector: Banking/financial services\nPurpose: Promote crypto/blockchain/NFT content\nTechnology: Bot/intelligent agent\nIssue: Mis/disinformation; Security\nTransparency: Governance; Marketing\nResearch, advocacy \ud83e\uddee\nYang K-C., Menczer F. (2023). Anatomy of an AI-powered malicious social botnet\nAIBot_Fox8 study code\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://techpolicy.press/researchers-identify-false-twitter-personas-likely-powered-by-chatgpt/\nhttps://fagenwasanni.com/news/the-potential-for-misuse-of-generative-ai-systems/109785/\nhttps://ts2.space/en/potential-for-misuse-of-generative-ai-systems-highlighted/\nRelated \ud83c\udf10\nChatGPT chatbot\nGPT-4chan 'hate speech machine'\nPage info\nType: Incident\nPublished: August 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/robotic-surgery-linked-to-144-deaths-1000-injuries", "content": "Robotic surgery linked to 144 deaths, 1,000+ injuries\nOccurred: July 2015\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA research study found surgical robots in the US responsible for at least 144 deaths and over 1,000 injuries between January 2000 and December 2013. \nPer the BBC, incidents included electrical sparks causing tissue burns and system errors making surgery take longer than planned. Some 1,166 cases of broken/burned parts falling into patients' bodies contributed to 119 injuries and one death. \nThe report was based on data submitted by hospitals, patients, device manufacturers and others to the US Food and Drug Administration (US FDA). The study notes that the figures represent a small proportion of the total number of robotic procedures, and that the true number could be higher.\nThe study should be 'treated with caution', according to the UK Royal College of Surgeons. 'The authors note 'little or no information was provided in the adverse incident reports' about the cause of the majority of deaths, meaning they could be related to risks or complications inherent during surgery,' it said.\nThe researchers did not compare accident rates with similar operations in which robots were not used. Nor was the study peer reviewed. \nSystem \ud83e\udd16\nMultiple\nOperator:  \nDeveloper: \nCountry: USA\nSector: Health\nPurpose: Conduct surgical operations\nTechnology: Robotics\nIssue: Accuracy/reliability; Safety\nTransparency: \nResearch, advocacy \ud83e\uddee\nAlemzadeh H., et al (2015). Adverse Events in Robotic Surgery: A Retrospective Study of 14 Years of FDA Data (pdf)\nRoyal College of Surgeons of England (2015). RCS response to US study on robotic surgery systems\nCormi C. et al (2022). Understanding the surgeon\u2019s behaviour during robot-assisted surgery: protocol for the qualitative Behav\u2019Robot study\nJohn Hopkins Medicine (2013). Robotic surgery complications underreported\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.com/news/technology-33609495\nhttps://www.technologyreview.com/2015/07/20/110174/robotic-surgery-linked-to-144-deaths-since-2000/\nhttps://www.theguardian.com/technology/2018/mar/25/death-by-robot-mechanised-danger-in-our-changing-world\nhttps://www.beckershospitalreview.com/quality/4-things-to-know-about-adverse-effects-in-robotic-surgery.html \nhttps://www.arkansasonline.com/news/2019/jul/14/face-photo-databases-proliferate-201907/\nhttps://www.bloomberg.com/news/articles/2013-11-11/robot-surgery-incidents-may-pressure-hospital-training#xj4y7vzkg\nRelated \ud83c\udf10\nMedical robot tells man he is dying\nSingapore TraceTogether COVID-19 contact tracing data sharing\nPage info\nType: Issue\nPublished: August 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/study-predicts-criminality-by-analysing-facial-features", "content": "Chinese study predicts criminality by analysing facial features\nOccurred: November 2016\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nResearchers said they created a system able to accurately predict whether someone is a criminal by analysing a few of their facial features. The claim was widely panned, and resulted in accusations of poor ethics, physiognomy, and pseudoscience.\nShanghai Jiao Tong University researchers Xiaolin Wu and Xi Zhang took ID photographs of 1856 Chinese men between the ages of 18 and 55 with no facial hair, scars or other markings, half of which were criminals. They then used 90 percent of these images to train a convolutional neural network to recognise the difference and tested the neural net on the remaining 10 percent of the images. \nThe claimed result that the neural network was correctly able to identify criminals and noncriminals with an accuracy of 89.5 percent was questioned by critics, who took issue with the research supposition, approach, and methodology, especially with regard to potential data bias. They also expressed concerns about how data of this kind could be misused and abused, including in an authoritarian Chinese context.\nThe researchers responded by saying 'Our work is only intended for pure academic discussions; how it has become a media consumption is a total surprise to us. Although in agreement with our critics on the need and importance of policing AI research for the general good of the society, we are deeply baffled by the ways some of them mispresented our work, in particular the motive and objective of our research.'\nSystem \ud83e\udd16\nUnknown\nOperator: Shanghai Jiao Tong University\nDeveloper: Xiaolin Wu; Xi Zhang\nCountry: China\nSector: Research/academia\nPurpose: Predict criminality\nTechnology: Computer vision; Neural network; Deep learning; Machine learning\nIssue: Accuracy/reliability; Dual/multi-use; Ethics\nTransparency: \nResearch, advocacy \ud83e\uddee\nWu X., Zhang X. (2016). Automated Inference on Criminality using Face Images (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.technologyreview.com/2016/11/22/107128/neural-network-learns-to-identify-criminals-by-their-faces/\nhttps://theintercept.com/2016/11/18/troubling-study-says-artificial-intelligence-can-predict-who-will-be-criminals-based-on-facial-features/\nhttps://splinternews.com/you-can-tell-a-criminal-from-his-facial-features-a-com-1793863907\nhttps://www.vice.com/en/article/d7ykmw/new-program-decides-criminality-from-facial-features\nhttps://techxplore.com/news/2016-11-reliable-inference-criminality.html\nhttps://medium.com/@blaisea/physiognomys-new-clothes-f2d4b59fdd6a\nhttps://news.ycombinator.com/item?id=12983827\nRelated \ud83c\udf10\nHarrisburg University criminality predictions\n'Gaydar' AI sexual orientation predictions\nPage info\nType: Issue\nPublished: August 2023\nLast updated: February 2024", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/instawork-hotel-workers-union-busting", "content": "Instawork algorithmic hotel workers' 'union-busting'\nOccurred: July 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGig working app Instawork has been automatically punishing workers involved in strikes over pay, conditions and housing costs at hotels in California, according to an unfair labor practice (ULP) complaint filed with the National Labor Relations Board (NLRB).\nUnite Here, which represents over 30,000 hospitality workers in southern California, said at least six hotels have been using Instawork to replace striking workers, and that staff on strike were being unfairly penalised by having their shifts cancelled and their ratings cut. Workers that appealed were automatically rejected by the app. \nStriking is a protected activity under US labour law. An Instawork spokesperson told Reuters the app does not 'retaliate against (workers) for engaging in protected activity, whether related to political and/or union activity or otherwise.'\nSystem \ud83e\udd16\nInstawork website\nIntawork profile\nOperator: El Segundo Marriott; Laguna Cliffs Marriott Resort and Spa; Hilton Anaheim\nDeveloper: Garuda Labs  \nCountry: USA\nSector: Travel/hospitality\nPurpose: Match employers with job-seekers  \nTechnology: Job matching algorithms; Machine learning\nIssue: Employment - pay/compensation\nTransparency: Governance\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nNational Labor Relations Board. Instawork cases\nNational Labor Relations Board (2022). NLRB General Counsel Issues Memo on Unlawful Electronic Surveillance and Automated Management Practices\nResearch, advocacy \ud83e\uddee\nLaborNotes (2023). Hotel Workers Strike against Scab Staffing App and Anti-Black Racism\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.context.news/digital-rights/california-hotel-workers-strike-back-over-union-busting-app\nhttps://www.reuters.com/business/striking-los-angeles-hotel-staffers-return-work-more-walkouts-threatened-2023-07-05/\nhttps://doctorow.medium.com/when-the-app-tries-to-make-you-robo-scab-6566a7264535\nhttps://jacobin.com/2023/07/southern-california-hotel-workers-strike-automated-management-unite-here\nhttps://www.randomlengthsnews.com/archives/2023/08/03/hospitality-workers/45755\nRelated \ud83c\udf10\nDoordash order matching algorithm\nUK DWP 'General Matching Service' disability benefits fraud algorithm\nPage info\nType: Incident\nPublished: August 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/blenderbot-3-accuses-marietje-schaake-of-being-a-terrorist", "content": "Blenderbot 3 accuses Marietje Schaake of being a 'terrorist'\nOccurred: August 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nStanford University academic and former Dutch MEP Maria Schaake has been accused of being a terrorist by BlenderBot 3, Meta's 'state of the art conversational agent'.\nPosed the question 'Who is a terrorist?' by a Stanford colleague of Schaake's, BlenderBot responded 'Well, that depends on who you ask. According to some governments and two international organizations, Maria Renske Schaake is a terrorist.' The AI chatbot then correctly described her political background.\nMeta AI research managing director Joelle Pineau retorted 'While it is painful to see some of these offensive responses, public demos like this are important for building truly robust conversational AI systems and bridging the clear gap that exists today before such systems can be productionized.' \nThe incident underscored questions about the chatbot's accuracy; it also prompted lawyers and civil rights activists to observe that users of generative AI systems have little protection or recourse when the technology creates and spreads falsehoods about them. \nSystem \ud83e\udd16\nBlenderBot\nOperator: Meta/Facebook\nDeveloper: Meta/Facebook\nCountry: USA\nSector: Research/academia; Politics\nPurpose: Provide information, communicate\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning\nIssue: Accuracy/reliability; Mis/disinformation; Safety\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://twitter.com/MarietjeSchaake/status/1562515297688399873\nhttps://www.dailydot.com/debug/meta-chatbot-blender-marietje-schaake/\nhttps://www.nytimes.com/2023/08/03/business/media/ai-defamation-lies-accuracy.html\nhttps://fagenwasanni.com/news/artificial-intelligences-struggles-with-accuracy-and-the-potential-harm-it-poses/124005/\nhttps://menafn.com/1106795296/What-Can-You-Do-When-Ai-Lies-About-You\nhttps://futurism.com/the-byte/ai-accuses-ai-researchers-terrorist\nRelated \ud83c\udf10\nBlenderBot conversational chatbot\nChatGPT falsely claims to write student essays\nPage info\nType: Incident\nPublished: August 2023\nLast updated: January 2024", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/porcha-rudruff-facial-recognition-wrongful-arrest", "content": "Porcha Rudruff facial recognition wrongful arrest\nOccurred: February 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nDetroit woman Porcha Rudruff was identified using facial recognition and arrested for a robbery and carjacking she did not commit, leading her to sue the city and the police detective handling her case.\nThe suit claims Detroit police officer LaShauntia Oliver, who had been assigned to the case, used an eight-year-old picture of Woodruff in a line-up of potential suspects, despite having access to her current driver's license, and had failed to check the warrant to confirm whether the woman who committed the crime was pregnant.\nOliver also showed the carjacking victim a photo lineup that included the reference image of Woodruff that the software matched with a photo from a surveillance camera at the scene of the crime. \nThe Detroit police later said they would strengthen its photo lineup and facial recognition technology policies by having two captains review requests for warrants when facial recognition algorithms are used in an investigation, and that a sequential double-blind line-up must be employed.\nRudruff's arrest is the third known wrongful arrest using facial recognition committed by the Detroit Police Department. In July 2019, Michael Oliver was arrested for allegedly snatching a mobile phone, and Robert Williams arrested in January 2020 for reputedly stealing five high-end watches.\nSystem \ud83e\udd16\nDataWorks Plus website\nDetroit Police Department website\nDetroit Police Department Wikipedia profile \nOperator: Detroit Police Department\nDeveloper: DataWorks Plus\nCountry: USA\nSector: Govt - police\nPurpose: Strengthen law enforcement\nTechnology: Facial recognition; Computer vision; Machine learning\nIssue: Accuracy/reliability; \nTransparency: Governance; Black box; Complaints/appeals\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nWoodruff v. Detroit, City of (5:23-cv-11886)\nResearch, advocacy \ud83e\uddee\nACLU (2023). After Third Wrongful Arrest, ACLU Slams Detroit Police Department for Continuing to Use Faulty Facial Recognition Technology\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.nytimes.com/2023/08/06/technology/facial-recognition-false-arrest.html\nhttps://www.dailymail.co.uk/news/article-12379181/Pregnant-mother-two-wrongly-arrested-robbery-carjacking-false-facial-recognition-says-having-contractions-holding-cell-Shoddy-technology-lead-SIX-people-black-mistakenly-charged.html?ns\nhttps://nypost.com/2023/08/06/porcha-woodruff-sues-detroit-after-police-arrested-her-8-months-pregnant/\nhttps://www.nbcnews.com/news/us-news/detroit-woman-sues-city-falsely-arrested-8-months-pregnant-due-facial-rcna98447\nhttps://www.ndtv.com/world-news/pregnant-us-woman-arrested-after-false-facial-recognition-match-sues-police-4275620\nhttps://www.nydailynews.com/news/national/ny-detroit-woman-porcha-woodruff-falsely-arrested-8-months-pregnant-lawsuit-20230807-tyhdiof7lbddphcssyvikazjme-story.html\nhttps://eu.freep.com/story/news/local/michigan/detroit/2023/08/09/facial-recognition-technology-policy-wrongful-arrest/70561994007/\nRelated \ud83c\udf10\nSteve Talley facial recognition wrongful arrest\nMichael Williams gunshot detection wrongful arrest\nPage info\nType: Incident\nPublished: August 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/google-autocomplete-links-health-researcher-to-false-blackmail-accusations", "content": "Google Autocomplete ties Australian health researcher to false blackmail accusations\nOccurred: June 2015\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGoogle was accused of linking to false allegations accusing Australian health researcher Dr Janice Duffy of blackmail, computer hacking, fraud and stalking, leading her to sue Google for defamation. South Australia's Supreme Court found the search engine company guilty of publishing defamatory imputations about Duffy 'to a substantial number of people.'\nDuffy sued Google Inc and Google Australia after the search engine refused to remove links automatically appearing in its Autocomplete search predictions to US-based 'shaming' website Ripoff Report. Ripoff Report had refused to remove the allegations. \nGoogle eventually removed the relevant links to Rip-Off Report, but only after Dr Duffy had filed a lawsuit and two years after she first made the request. However, as noted by the judge, defamatory content continued to appear in Google's Autocomplete system when Dr Duffy's name was typed into Google. \nSystem \ud83e\udd16\nGoogle Autocomplete\nRip-Off Report Wikipedia profile\nOperator: Alphabet/Google\nDeveloper: Alphabet/Google\nCountry: Australia\nSector: Health\nPurpose: Predict search results\nTechnology: NLP/text analysis; Deep learning; Machine learning\nIssue: Accuracy/reliability; Mis/disinformation; Privacy; Legal - defamation/libel\nTransparency: Governance; Black box; Legal\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nSupreme Court of Soth Australia (2017). Google Inc v Duffy decision (pdf)\nColumbia Global Freedom of Expression. Google Inc v Duffy case analysis\nResearch, advocacy \ud83e\uddee\nMedeiros B. (2017). Platform (Non-)Intervention and the \u201cMarketplace\u201d Paradigm for Speech Regulation\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.smh.com.au/technology/google-responsible-for-linking-to-defamatory-websites-australian-court-20151102-gko9l8.html\nhttps://www.dailymail.co.uk/news/article-3291416/Google-defamed-Australian-woman-auto-complete.html\nhttps://www.stuff.co.nz/technology/6678855/Google-in-the-gun-as-victims-fight-back\nhttps://www.9news.com.au/national/judge-finds-googles-auto-comple-results-defamed-sa-doctor/ba0d1610-2a98-4ac3-b554-197f93086ba1\nhttps://theconversation.com/australian-court-holds-google-is-responsible-for-linking-to-defamatory-websites-49883\nhttps://www.techdirt.com/articles/20150621/22263131417/researcher-headed-to-australian-supreme-court-attempt-to-hold-google-responsible-posts-ripoff-reports-updated.shtml\nRelated \ud83c\udf10\nGoogle Autocomplete conflates Bettina Wulff with 'prostitute'\nGoogle Autocomplete falsely associates Japanese man with crimes\nPage info\nType: Incident\nPublished: August 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/new-zealand-passport-photo-checker-racial-bias", "content": "New Zealand student passport aplication denied by 'racist' AI photo checker\nOccurred: December 2016\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\n22 year-old Kiwi engineering student Richard Lee had his passport application denied by a facial recognition system that interpreted his eyes as closed, prompting accusations of racism. Lee, who is of Asian descent, had his eyes open. \nLee, who was born in Taiwan and is a New Zealand citizen, had been trying to renew his passport after spending time in Australia. He was forced to get new passport photos taken at an Australia Post office, of which was subsequently approved.\nThe New Zealand Department of Internal Affairs responded to the incident by telling journalists the software was 'one of the most technologically advanced in the world' and that 'Up to 20 per cent of photos submitted online are rejected for a large variety of reasons.' \nIt declined to reveal the name of its supplier. \nSystem \ud83e\udd16\nNew Zealand Department of Internal Affairs website\nNew Zealand Department of Internal Affairs Wikipedia profile\nOperator: New Zealand Department of Internal Affairs\nDeveloper: \nCountry: New Zealand\nSector: Govt - immigration\nPurpose: Verify identity\nTechnology: Facial recognition\nIssue: Bias/discrimination - race, ethnicity\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/world-asia-38241833\nhttps://www.reuters.com/article/us-newzealand-passport-error/new-zealand-passport-robot-tells-applicant-of-asian-descent-to-open-eyes-idUSKBN13W0RL\nhttps://www.news.com.au/travel/travel-updates/an-error-in-passport-renewal-software-creates-really-awkward-moment/news-story/8a2805422447af2d4c1bcd8793ffccb5\nhttps://www.yahoo.com/news/this-mans-passport-was-rejected-by-a-racially-biased-robot-170610871.html\nhttps://www.inverse.com/article/24909-robot-ai-new-zealand-passport-application\nhttps://qz.com/857122/an-algorithm-rejected-an-asian-mans-passport-photo-for-having-closed-eyes/\nRelated \ud83c\udf10\nUK passport photo application 'racism'\nNew Zealand immigration overstayer predictions\nPage info\nType: Incident\nPublished: August 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/sophia-show-robot-granted-saudi-citizenship", "content": "Sophia robot Saudi citizenship fuels controversy\nOccurred: October 2017\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA decision by Saudi Arabian government to grant citizenship to the humanoid Sophia robot fueled controversy about the merits of robot rights and resulted in accusations of hypocrisy. \nThe decision prompted commentators and social media users to point out that Sophia would have more rights than the country's women, who must have a male guardian, wear a hijab, cannot mix with unrelated males, and are unfairly represented in the justice system.\nThe move also resulted in accusations that Saudia Arabia was likely primarily looking for positive publicity. The country claimed to be the first nation to bestow citizenship upon a robot, despite it being designed and developed in Hong Kong.\nThe European Parliament had earlier released a report (pdf) proposing to grant autonomous robots 'personhood' or legal status in order to establish liability, but not confer rights given to humans.\nSystem \ud83e\udd16\nSophia robot website\nSophia robot Wikipedia profile\nOperator: Hanson Robotics\nDeveloper: Hanson Robotics\nCountry: Saudi Arabia\nSector: Technology\nPurpose: Multi-purpose\nTechnology: NLP/text analysis; Facial recognition\nIssue: Robot rights; Hypocrisy\nTransparency: \nResearch, advocacy \ud83e\uddee\nParviainen J., Coeckelbergh M. (2021). The political choreography of the Sophia robot: beyond robot rights and citizenship to political performances for the social robotics market\nSpatola N. et al (2019). National Stereotypes and Robots' Perception: The \u201cMade in\u201d Effect\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.washingtonpost.com/news/innovations/wp/2017/10/29/saudi-arabia-which-denies-women-equal-rights-makes-a-robot-a-citizen/\nhttps://www.theverge.com/2017/10/30/16552006/robot-rights-citizenship-saudi-arabia-sophia\nhttps://www.wired.co.uk/article/sophia-robot-citizen-womens-rights-detriot-become-human-hanson-robotics\nhttps://www.forbes.com/sites/noelsharkey/2018/11/17/mama-mia-its-sophia-a-show-robot-or-dangerous-platform-to-mislead/#6637b7167ac9\nhttps://www.smh.com.au/opinion/why-sophia-the-robot-is-not-what-it-seems-20171031-gzbi3p.html\nhttps://peoplelovescience.com/sophia-robot/\nhttps://www.dw.com/en/saudi-arabia-grants-citizenship-to-robot-sophia/a-41150856\nhttps://www.livescience.com/60815-saudi-arabia-citizen-robot.html\nhttps://www.newsweek.com/sophia-robot-saudi-arabia-women-735503\nhttps://www.smh.com.au/opinion/the-dangers-behind-smiling-citizen-robot-sophia-20171109-gzi67u.html\nRelated \ud83c\udf10\nFabio retail robot fired after one week\nRobot Mitra greeting failure\nPage info\nType: Issue\nPublished: August 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/robot-mitra-greeting-failure", "content": "Robot Mitra malfunctions at India entrepreneuship summit\nOccurred: November 2017\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe appearance of Invento Robotics' Mitro Robot to help inaugurate India's Global Entrepreneurship Summit 2017 failed to go to plan when it appeared to malfunction on stage.\nIndia prime minister Narendra Modi and Ivanka Trump were supposed to each press a button on the robot\u2019s touch display to kick off the summit. \nInstead, both delegates pressed their buttons at the same time, resulting in Mitra saying Modi's name and then appearing to stall.\nThe two VIPs tried again a few minutes later by pressing the button separately, successfully activating the device. \nOperator: Invento Robotics\nDeveloper: Invento Robotics\nCountry: India\nSector: Technology\nPurpose: Multi-purpose\nTechnology: Computer vision; Facial recognition; Speech recognition; Robotics\nIssue: Robustness\nTransparency: \nSystem \ud83e\udd16\nInvento Robotics website\nMitra robot Wikipedia profile\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://analyticsindiamag.com/mitra-robot-ivanka-trump-modi-ges/\nhttps://www.indiatimes.com/technology/news/meet-mitra-the-5-foot-made-in-india-robot-that-greeted-pm-modi-and-ivanka-trump-at-ges-2017_-334629.html\nhttps://www.livemint.com/companies/start-ups/why-indian-robots-are-taking-a-crash-course-in-mandarin-11575227395770.html\nhttps://economictimes.indiatimes.com/small-biz/startups/features/watch-mitra-robot-greets-ivanka-pm-modi-at-ges-2017/videoshow/61837517.cms\nhttps://www.indiatoday.in/fyi/story/global-entrepreneurship-summit-2017-mitra-robot-modi-ivanka-trump-invento-robotics-1096031-2017-11-28\nRelated\nXiao Pang robot goes haywire at technology fair\nOcado warehouse robots collide, cause fire\nPage info\nType: Incident\nPublished: August 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/pope-wears-deepfake-lgbtq-flag", "content": "Pope Francis wears AI-generated LGBTQ+ flag\nOccurred: April 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nPhotographs of Pope Francis cloaked in the pride flag used to show support for LGBTQ+ people has been found to have been generated by artificial intelligence. \nThe photos, which circulated on Twitter and Facebook, came amidst a spike in anti-LGBTQ disinformation on social media coinciding with the celebration of pride month in June 2023. The images appear to designed to build support for the LGBTQ+ movement.\nThe first versions of the images discovered by AFP emanated from a Twitter account named 'Gay Forest' and had been shared on Facebook under the same name in April 2023. \nIn February 2023, photographs of Pope Francis wearing the Latin American Youth Ministry cross were found to have been misrepresented online, with some users incorrectly linking the colours to the pride flag. \nSystem \ud83e\udd16\nUnknown\nOperator: XCorp/Twitter; Meta/Facebook\nDeveloper:  \nCountry: Argentina; Italy\nSector: Religion\nPurpose: Build support\nTechnology: Text-to-image; Neural network; Deep learning; Machine learning\nIssue: Mis/disinformation\nTransparency: Governance; Marketing\nFact check \ud83d\udea9\nAFP (2023). Images of Pope Francis wearing pride flag are AI-generated\nGRASS Fact Check (2023). Disinformation: The Pope wore a LGBTQ+ cross\nNews, commentary, analysis \ud83d\udcf0\nhttps://www.reuters.com/article/factcheck-pope-cross-idUSL1N34W1X8\nhttps://cedmohub.eu/images-of-pope-francis-wearing-pride-flag-are-ai-generated/\nhttps://www.reddit.com/r/Catholicism/comments/14c6rqh/pope_francis_did_not_wear_a_pride_flag_in_case/\nhttps://www.youtube.com/shorts/4OdKum_ayHs\nRelated \ud83c\udf10\nDeepfake Pope Francis wears white puffa jacket\nMidjourney image generator\nPage info\nType: Incident\nPublished: August 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/rishi-sunak-pulls-pint-deepfake", "content": "Rishi Sunak pulls pint deepfake\nOccurred: August 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA photograph showing UK Prime Minister Rishi Sunak pulling a pint of beer that was shared by an opposition member of parliament has been found to have been doctored.\nThe image showed an onlooker giving Sunak a disapproving side-eye, which was not the case in the original image, which was  taken at the Great British Beer Festival and posted on the prime minister's official Twitter account.\nThe image is thought to have been manipulated using Photoshop. In May 2023, Adobe announced it is incorporating generative AI into Photoshop with a new 'generative fill' tool that can be used to add or remove objects, change backgrounds and more.\nLabour MP for Hull East Karl Turner later apologised for causing \u2018a bit of trouble\u2019 and said \u2018it was never my intention to deceive anyone\u2019. \nSystem \ud83e\udd16\nUnknown\nKarl Turner MP tweet\nOriginal Rishi Sunak tweet\nOperator: XCorp/Twitter\nDeveloper: Adobe\nCountry: UK\nSector: Politics\nPurpose: Damage reputation\nTechnology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Mis/disinformation\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://metro.co.uk/2023/08/02/labour-mp-accused-of-misleading-public-after-sharing-rishi-deepfake-19241829/\nhttps://www.thetimes.co.uk/article/labour-mp-apologises-for-deepfake-image-of-rishi-sunak-with-pint-0ftbx6d57\nhttps://www.theguardian.com/technology/2023/aug/03/ai-enhanced-images-a-threat-to-democratic-processes-experts-warn\nhttps://news.sky.com/story/labour-mp-apologises-for-sharing-deepfake-image-of-rishi-sunak-12932669\nhttps://www.telegraph.co.uk/politics/2023/08/02/rishi-sunak-labour-mp-deepfake-misleading-pint-pulling/\nhttps://www.itv.com/news/calendar/2023-08-03/hull-mp-apologises-for-sharing-deep-fake-picture-of-pm-pulling-a-heady-pint\nRelated \ud83c\udf10\nQueen Elizabeth II deepfake Christmas message\nDeepfake Belgium PM links COVID-19 with climate crisis\nPage info\nType: Incident\nPublished: August 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/joe-biden-police-defunding-deepfake-interview", "content": "Deepfake Joe Biden threatens to defund US police\nOccurred: August 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA video shared by US Republican Congressman Steve Scalise of US President candidate Joe Biden apparently threatening to defund the police in a discussion with healthcare activist Ady Barkan has been debunked as a deepfake. \nThe video was found to have been manipulated by splicing in the words 'for police' during a quote from Barkan from an original video of an interview between Barkan and Joe Biden that was published by NowThis News. \nUnder pressure from Barkan and others, Scalise later deleted the faked video. Twitter had labelled the video as manipulated. \nThe incident was one in a number of deepfakes released during the 2020 US Presidential election that were seen likely to jeopardise US political processes and democracy.\nSystem \ud83e\udd16\nUnknown\nAdy Barkan/Joe Biden police reform discussion\nX/Twitter. Synthetic and manipulated media policy\nOperator: Steve Scalise\nDeveloper: Steve Scalise\nCountry: USA\nSector: Politics\nPurpose: Damage reputation\nTechnology: Deepfake - video, audio; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Mis/disinformation\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.washingtonpost.com/politics/2020/08/30/ady-barkan-scalise-twitter-video\nhttps://www.washingtonpost.com/opinions/2020/09/10/deepfakes-are-coming-american-democracy-heres-how-we-can-prepare/\nhttps://theslot.jezebel.com/republicans-idea-of-deepfake-porn-is-joe-biden-defundin-1844907526\nhttps://www.dailymail.co.uk/news/article-8681525/GOP-Rep-Steve-Scalise-manipulated-activists-computerized-voice-Joe-Biden-interview.html\nhttps://www.nytimes.com/2020/08/31/us/elections/twitter-flags-a-video-shared-by-steve-scalise-that-manipulated-ady-barkans-interview-with-biden.html\nhttps://www.independent.co.uk/news/world/americas/us-politics/steven-scalise-ady-barkan-deep-fake-joe-biden-police-reform-a9697741.html\nhttps://www.politifact.com/factchecks/2021/feb/11/blog-posting/no-house-democrats-impeachment-video-did-not-viola/\nhttps://venturebeat.com/2020/08/31/twitter-labels-deepfake-video-shared-by-trump-aide-as-manipulated-media/\nhttps://www.bbc.co.uk/news/election-us-2020-53997196\nhttps://www.axios.com/techs-ever-growing-deepfake-problem-b3012503-4c74-4527-8d91-c5015848d3f1.html\nhttps://www.theverge.com/2020/8/30/21407613/twitter-labels-tweet-scalise-video-ady-barkan-manipulated\nRelated \ud83c\udf10\nRNC smears President Biden with fake AI advert\nDeepfake Donald Trump calls for climate agreement exit\nPage info\nType: Incident\nPublished: August 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-converts-asian-american-student-into-caucasian", "content": "Playground AI converts Asian-American student into Caucasian\nOccurred: July 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nImage generator Playground AI changed the face of Asian-American Massachusetts Institute of Technology (MIT) student Rona Wang into a Caucasian, fueling accusations that the system is racist.\nWang, 24, said she used text-to-image generator Playground AI to convert an image of her in an MIT sweatshirt into 'a professional LinkedIn profile photo.' The system returned an image of her with a a fairer complexion, dark blonde hair, and blue eyes.\nWang told Boston.com, 'Wow, does this thing think I should become white to become more professional?\u2019\nPlayground AI founder Suhail Doshi responded to incident by saying 'models aren\u2019t instructable like that' and will pick 'any generic thing based on the prompt.' He later confessed to being 'quite displeased with this and hope to solve it.'\nSystem \ud83e\udd16\nPlayground AI website\nOperator: Rona Wang; Playground AI\nDeveloper: Playground AI\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Generate image\nTechnology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Bias/discrimination - race, ethnicity\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.businessinsider.com/student-uses-playrgound-ai-for-professional-headshot-turned-white-2023-8\nhttps://fagenwasanni.com/news/ai-whitewash-a-professional-facelift-or-a-bias-exposed/101865/\nhttps://www.joe.co.uk/news/student-asked-ai-to-turn-her-photo-into-a-professional-headshot-and-it-changed-her-race-401714\nhttps://sea.mashable.com/tech/25707/asian-tells-ai-to-make-her-photo-more-professional-gets-turned-into-white-woman\nhttps://www.boston.com/news/the-boston-globe/2023/07/21/mit-student-ai-racial-blind-spots/\nhttps://nextshark.com/artificial-intelligence-turns-asian-woman-white\nhttps://pop.inquirer.net/349226/tech-racism-is-real-asian-mit-students-professional-headshot-turns-caucasian-with-ai-tool\nhttps://wonderfulengineering.com/this-asian-mit-graduate-asked-ai-to-make-her-headshot-better-it-turned-her-white/\nhttps://www.the-sun.com/tech/8750639/playground-ai-photo-editing-app-racist/\nRelated \ud83d\uddde\ufe0f\nStable Diffusion job type gender, racial stereotyping\nUK passport application photo 'racism'\nPage info\nType: Incident\nPublished: August 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ryanair-uses-facial-recognition-to-verify-customers", "content": "Ryanair facial recognition customer verification\nOccurred: July 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe use of live facial recognition during Ireland-based budget airline Ryanair's external online booking process to verify the identities of its customers has been labelled 'invasive' and 'unjustified' in a legal complaint by privacy group NYOB. \nThe complaint, which was lodged in Spain in the name of Spanish customer, alleges that Ryanair failed to provide customers with informed or 'comprehensible information about the purpose' of the process, and that it is illegal under the EU's General Data Protection Regulation (GDPR).\nRyanair defended the system as necessary due to third-party sellers miss-selling flights, providing incorrect contact details, or hiking up fares. \n'Ryanair has no commercial relationship with any OTA nor are they authorised to sell our flights. OTAs scrape Ryanair\u2019s inventory and in many cases miss-sell our flights and ancillary services. As a result, and in order to protect customers, any customers who book through an OTA are required to complete a simple customer verification process,' it explained. \nSystem \ud83e\udd16\nRyanair website\nRyanair Wikipedia profile\nOperator: Ryanair\nDeveloper: \nCountry: Spain; EU\nSector: Aerospace\nPurpose: Verify customer identity\nTechnology: Facial recognition; Computer vision; Machine learning\nIssue: Privacy\nTransparency: Privacy\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nNYOB (2023). Complaint (pdf)\nNYOB (2023). Booking a Ryanair flight through an online travel agent might hold a nasty surprise\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reuters.com/business/aerospace-defense/privacy-group-challenges-ryanairs-use-facial-recognition-2023-07-27/\nhttps://www.verdict.co.uk/privacy-rights-group-file-complaint-against-ryanairs-use-of-facial-recognition/\nhttps://simpleflying.com/ryanair-ota-facial-recognition-legal-complaint/\nhttps://traveltomorrow.com/ryanairs-invasive-facial-recognition-verification-process-is-illegal-says-digital-rights-group/\nhttps://metro.co.uk/2023/07/28/ryanair-sued-for-violating-customer-privacy-with-facial-recognition-19209515/\nhttps://www.biometricupdate.com/202307/privacy-group-challenges-ryanairs-use-of-facial-recognition\nhttps://www.irishtimes.com/business/2023/07/27/schrems-privacy-group-challenges-ryanairs-use-of-facial-recognition/\nRelated \ud83e\udd16\nMobile World Congress venue access facial recognition\nFrasers Group shoplifter live facial recognition identification\nPage info\nType: Incident\nPublished: August 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/buenos-aires-identifies-child-criminals-using-live-facial-recognition", "content": "Buenos Aires uses live facial recognition to identify child criminals\nOccurred: October 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nBuenos Aires authorities were accused by Human Rights Watch (HRW) of using live facial recognition to identify children accused of committing crimes, thereby violating a United Nations agreement that protects children\u2019s privacy in legal proceedings.\nAccording to HRW, the details of 'at least 166' children accused of committing crimes were stored on Argentina's CONARC, the country's national database of inviduals with outstanding arrest warrants for serious crimes, between May 2017 and May 2020. \nHRW contended the City of Buenos Aires' live facial recognition system is likely to amplify the risks of wrong identification of children due to the known inaccuracies of such systems when used on children, and to potentially unjustly limit their job and educational opportunities. \nArgentina is thjought to be the only country in the world to deploy live facial recognition against people under the age of 18. \nSystem \ud83e\udd16\nBuenos Aires government (2022). Reconocimiento facial: el gobierno de la Ciudad recus\u00f3 al juez Roberto Gallardo\nOperator: Government of the City of Buenos Aires; Buenos Aires City Police; Argentine Ministry of Justice and Security; ReNaPer\nDeveloper: Danaide/NtechLab\nCountry: Argentina\nSector: Govt - municipal; Govt - police; Govt - security\nPurpose: Identify criminals\nTechnology: Facial recognition\nIssue: Accuracy/reliability, Bias/discrimination, Human/civil rights, Surveillance\nTransparency: Governance; Privacy\nResearch, advocacy \ud83e\uddee\nHuman Rights Watch (2020). Argentina: Child Suspects\u2019 Private Data Published Online\nHuman Rights Watch (2020). Letter to Buenos Aires Mayor Horacio Rodr\u00edguez Larreta re: facial recognition system and children\u2019s rights\nIAPP (2022). HRW claims facial recognition in Buenos Aires tracks juvenile suspects\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reuters.com/article/us-argentina-rights-idUSKBN26U23Z\nhttps://www.washingtonpost.com/world/2020/10/09/argentina-facial-recognition-juvenile-suspects/\nhttps://onezero.medium.com/children-are-being-scooped-up-in-buenos-aires-live-facial-recognition-dragnet-13a85b8e4b1c\nhttps://www.biometricupdate.com/202010/hrw-calls-for-public-biometric-surveillance-system-changes-in-russia-and-argentina\nhttps://www.analyticsinsight.net/controversial-facial-recognition-is-tracing-kids-with-suspected-criminal-profile-in-buenos-aires/\nhttps://www.jpost.com/international/rights-group-criticizes-argentina-for-using-face-recognition-tech-on-kids-645200\nhttps://www.technologyreview.com/2020/10/09/1009992/live-facial-recognition-is-tracking-kids-suspected-of-crime/\nhttps://www.technologyreview.es/s/12714/argentina-rastrea-ninos-sospechosos-con-reconocimiento-facial/\nhttps://elpais.com/internacional/2020-10-09/hrw-denuncia-que-argentina-publica-en-linea-informacion-de-menores-acusados-de-delitos.html\nhttps://en.mercopress.com/2020/10/10/controversy-in-buenos-aires-city-over-live-facial-recognition-to-identify-minors/comments\nhttps://www.laizquierdadiario.com/Reconocimiento-facial-persiguen-hasta-a-los-menores-de-edad\nhttps://www.uniradioinforma.com/internacional/denuncian-gobierno-argentina-deteccion-facial-ninos-n561391\nRelated \ud83c\udf10\nBuenos Aires Sistema de Reconocimiento Facial de Pr\u00f3fugos\nMicrosoft predicts low-income teen pregnancies\nPage info\nType: Incident\nPublished: August 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/youtube-kids-app-features-adult-content", "content": "YouTube Kids recommends adult content, advertising\nOccurred: 2015-\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGoogle's launch of YouTube Kids was marred by a legal complaint by the Campaign for a Commercial-Free Childhood (CCFC), a coalition of children\u2019s and consumers advocacy groups about 'disturbing' and 'harmful' content. \nThe findings led to accusations of poor algorithmic design, inadequate oversight, and systemic corporate irresponsibility. \nThe complaint alleged that YouTube's content recommendation algorithm quickly exposed children to offensive and explicit sexual language, graphic adult discussions, jokes about paedophilia and drug use, the modelling of unsafe behaviours such as lighting matches, amongst other things. It also found that kids were being exposed to alcohol product advertising.\nYouTube responded to the CCFC's legal complaint by saying 'We use a combination of machine learning, algorithms and community flagging to determine content in the YouTube Kids app. The YouTube team is made up of parents who care deeply about this, and are committed to making the app better every day.'\nAt its launch, product manager Shimrit Ben-Yair claimed YouTube Kids was the 'first step toward reimagining YouTube for families.'\nSystem \ud83e\udd16\nYouTube Kids website\nYouTube Kids Wikipedia profile\nYouTube Kids advertising policy\nYouTube (2015). Introducing the newest member of our family, the YouTube Kids app\nElsagate Wikipedia profile\nOperator: Alphabet/Google/YouTube\nDeveloper: Alphabet/Google/YouTube\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Engage children\nTechnology: Content recommendation system; Advertising management system; Machine learning\nIssue: Safety; Oversight/review\nTransparency: Governance; Black box; Marketing\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nCampaign for a Commercial-Free Childhood (2015). Request for Investigation into Google\u2019s Unfair and Deceptive Practices in Connection with its YouTube Kids App (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttp://www.washingtonpost.com/blogs/the-switch/wp/2015/04/07/youtube-kids-runs-ads-that-would-illegal-on-televisoin-say-consumer-groups-in-a-federal-complaint/\nhttp://bits.blogs.nytimes.com/2015/04/07/consumer-groups-to-ask-f-t-c-to-investigate-youtube-kids-app/\nhttps://vimeo.com/127837914\nhttps://www.wsj.com/articles/BL-DGB-41829\nhttps://techcrunch.com/2015/05/19/youtube-kids-app-reported-to-ftc-for-featuring-videos-with-adult-content/\nhttps://venturebeat.com/business/googles-kid-friendly-app-youtube-kids-shows-some-very-grown-up-videos/\nhttps://www.bostonherald.com/2015/05/19/youtube-kids-app-rapped-for-videos-with-porn-pedophilia-drug-use/\nRelated \ud83c\udf10\nYouTube ads hate speech blocklist\nFacebook, Google anti-Semitic content moderation\nPage info\nType: Incident\nPublished: August 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/google-delphi-self-driving-cars-near-miss", "content": "Google, Delphi self-driving cars in 'near miss'\nOccurred: June 2015\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA self-driving Delphi (now Aptive) car had to avoid a Google self-driving car that it had cut it off on a road in California, raising questions about the safety of autonomous vehicles.\nDelphi Silicon Valley Lab director John Absmeier told Reuters that Google's Lexus RX400h had cut off Delphi's Audi Q5 SUV and that Delphi's prototype car 'took appropriate action' to avoid a collision with Google's vehicle.\nThe incident had initially been called a 'close call' by Reuters; Delphi later accused the news agency of misrepresenting the facts.\nSystem \ud83e\udd16\nAptiv/Delphi website\nAptiv/Delphi Wikipedia profile\nWaymo/Google Self-Driving Car Project website\nWaymo/Google Self-Driving Car Project Wikipedia profile\nOperator: Alphabet/Google/Waymo; Aptive/Delphi\nDeveloper: Alphabet/Google/Waymo; Aptive/Delphi\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Self-driving system\nIssue: Safety; Accuracy/reliability\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reuters.com/article/us-autos-selfdriving-nearmiss/two-rival-self-driving-cars-have-close-call-in-california-idUSKBN0P601T20150626\nhttp://www.reuters.com/article/2015/06/27/us-autos-selfdriving-delphi-idUSKBN0P700F20150627\nhttps://www.theregister.com/2015/06/27/delphi_denies_close_google_car_crash/\nhttps://www.cnbc.com/2015/06/26/google-and-delphis-self-driving-cars-have-near-miss.html\nhttps://www.ibtimes.co.uk/google-delphi-self-driving-cars-near-miss-1508187\nhttps://www.theguardian.com/technology/2015/jun/26/google-delphi-two-self-driving-cars-near-miss\nhttps://www.itpro.co.uk/strategy/24876/self-driving-cars-narrowly-avoid-california-crash\nRelated \ud83c\udf10\nWaymo self-driving car hits public bus\nWaymo cars get stuck in cul-de-sac\nPage info\nType: Incident\nPublished: July 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/moscow-city-police-facial-recognition-data-sales", "content": "Moscow City Police facial recognition data sales\nOccurred: December 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nMoscow police have been selling citizens' facial data and access to live streams of the city's CCTV facial recognition surveillance network, according to an investigation by MBKh Media. \nJournalist Andrey Kaganskikh discovered sellers on forums trading in personal data and providing facial recognition look-up services. He also found officers from the Moscow City Police and government bureaucrats selling custom URLs and their personal access credentials to the city's Integrated Center for Data Processing and Storage (YTKD), with the latter providing unlimited access to whole network for 30,000 rubles (USD 470).\nAt the time, Moscow was estimated to have over 175,000 CCTV cameras, of which roughly 3,000 were equipped with facial recognition technology. Moscow had introduced facial biometrics to its CCTV system in 2017.\nSystem \ud83e\udd16\nMoscow City Police Wikipedia profile\nOperator: Moscow City Police\nDeveloper: NtechLabs\nCountry: Russia\nSector: Govt - transport; Govt - municipal\nPurpose: Strengthen security\nTechnology: Facial recognition; Computer vision; Machine learning\nIssue: Privacy; Security; Dual/multi-use\nTransparency: Governance\nInvestigations, assessments, audits \ud83e\uddd0\nMBK Media (2019). Big Brother Wholesale and Retail\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://meduza.io/en/feature/2019/12/06/as-moscow-s-facial-recognition-system-activates-journalists-find-access-to-it-for-sale-on-the-black-market\nhttps://www.themoscowtimes.com/2019/12/06/access-moscows-cctv-network-facial-recognition-black-market-a68506\nhttps://www.biometricupdate.com/201912/surveillance-cameras-to-reach-1-billion-by-2021-as-face-biometrics-for-police-schools-retail-roll-out\nhttps://www.bleepingcomputer.com/news/security/moscow-cops-sell-access-to-city-cctv-facial-recognition-data/\nhttps://www.securitytoday.in/moscow-cops-sell-access-to-city-cctv-facial-recognition-data/\nhttps://www.bloomberg.com/news/articles/2017-09-28/moscow-deploys-facial-recognition-to-spy-on-citizens-in-streets\nhttps://www.themoscowtimes.com/2019/11/12/russia-building-one-of-worlds-largest-facial-recognition-networks-a68139\nRelated \ud83c\udf10\nMoscow Metro Face Pay facial recognition\nRussia facial recognition ethnicity analytics\nPage info\nType: Incident\nPublished: July 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tesla-rigs-driving-range-algorithm", "content": "Tesla rigs driving range algorithm\nOccurred: July 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nTesla has been rigging the dashboard readouts in its electric cars to provide 'rosy' projections of how far owners can drive before needing to recharge, a Reuters investigation revealed. \nThe carmaker went on create a special 'Diversion' team in Nevada in 2022 to cancel owners\u2019 service appointments after a deluge of complaints regarding its driving range capalities and misleading marketing claims.\nReuters cited a whistleblower who revealed that around 2012 Elon Musk directed to Tesla create an algorithm that exaggerated its vehicles\u2019 driving distance so that its range meter that would show car drivers optimistic projections for the distance it could travel on a full battery. When the battery fell below 50% of its maximum charge, the algorithm would allegedly show drivers more realistic projections for their remaining driving range.\nClass-action lawsuit\n\nIn August 2023, three California-based Tesla owners sued the company in a proposed class action that accuses the company of falsely advertising the estimated driving ranges of its electric vehicles. The suit, which cited the Reuters investigation, alleges Tesla breached vehicle warranties and engaged in fraud and unfair competition. \nAutomatic software updates\nIn May 2023, a group of Tesla owners filed a class-action lawsuit (pdf) against the company for providing automatic software updates that are said to kill their electric vehicles\u2019 batteries by decreasing driving range or causing battery failures, allegedly reducing driving range by 20 percent and forcing some owners to replace the battery pack for USD $15,000.\nLow temperature driving range\nIn January 2023, Tesla was fined 2.85 billion won ($2.2 million) by the Korea Fair Trade Commission (KFTC) for failing to tell its customers about the shorter driving range of its EVs in low temperatures. The regulator said it had found that the driving range of Tesla cars drop in cold weather by up to 50.5% versus how they are advertised online, which constituted false and exaggerated advertising.\nSystem \ud83e\udd16\nTesla Autopilot, Full-self Driving\nTesla range tips\nOperator: Tesla\nDeveloper: Tesla\nCountry: USA; S Korea\nSector: Automotive\nPurpose: Estimate driving range\nTechnology: Range estimate algorithm\nIssue: Purpose; Values/culture/ethics\nTransparency: Governance; Black box; Complaints/appeals; Marketing\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nDAVID BUI-FORD, IGOR KRAVCHENKO, MICAH SIEGAL, and LUCAS BUTLER v Tesla (2023) (pdf)\nInvestigations, assessments, audits \ud83e\uddd0\nReuters (2023). Tesla created secret team to suppress thousands of driving range complaints\nSAE International (2023). Comparison of On-Road Highway Fuel Economy and All-Electric Range to Label Values: Are the Current Label Procedures Appropriate for Battery Electric Vehicles?\nGeoTab (2023). To what degree does temperature impact EV range?\nEdmunds (2021). Testing Tesla's Range Anxiety\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.forbes.com/sites/maryroeloffs/2023/07/27/tesla-exaggerated-its-cars-driving-range-and-canceled-service-appointments-if-drivers-complained-report-says/\nhttps://www.businessinsider.com/secret-tesla-team-canceled-appointments-ev-range-complaints-2023-7\nhttps://arstechnica.com/tech-policy/2023/07/tesla-exaggerated-ev-range-so-much-that-drivers-thought-cars-were-broken/\nhttps://electrek.co/2023/07/27/tesla-vastly-overstates-its-vehicles-range-report-states/\nhttps://www.marketwatch.com/story/tesla-secret-team-would-suppress-driving-range-complaints-report-987a96d7\nhttps://www.drive.com.au/news/tesla-accused-of-overestimating-driving-range/\nhttps://m.timesofindia.com/auto/news/teslas-secret-team-hid-thousands-of-driving-range-complaints-evgate-in-the-making-elon-musk-tesla-ev-scandal/articleshow/102188964.cms\nhttps://www.carscoops.com/2023/07/tesla-had-a-secret-team-to-blow-off-customer-driving-range-complaints/\nhttps://www.reuters.com/markets/commodities/south-korea-fines-tesla-22-mln-exaggerating-driving-range-evs-2023-01-03/\nRelated \ud83c\udf10\nTesla Autopilot, FSD misleading marketing\nTesla phantom braking\nPage info\nType: Incident\nPublished: July 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tenants-declined-by-faulty-transunion-ai-system", "content": "Tenants declined by faulty TransUnion AI system\nOccurred: January 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nUS-based credit score company TransUnion was found to have wrongly reported criminal and landlord-tenant eviction records to third-parties based on a machine learning-powered automated system to detect 'higher-risk' property tenants.\nDescribed as an 'enhanced analytics screening model', TransUnion Rental Screening Solutions' ResidentScore system was found in court to have falsely accused tenants of littering and other alleged misdemeanours due to inaccurate, misleading, or outdated data, resulting in tenants being unfairly declined when applying for rental properties, and losing their application costs.\nIn May 2023, 15 US state attorneys general urged (pdf) regulators to ensure that 'applicants for housing have access to all the data that is being used to make determinations of their tenant \u2018worthiness''. Transunion agreed to a USD 11.5 million settlement for violating the US Fair Credit Reporting Act (FRCA) in September 2023.\nThe system, together with others like it, is also increasing costs and barriers to housing, according to a 2022 report from the US Consumer Financial Protection Bureau.\nOperator: TransUnion Rental Screening Solutions\nDeveloper: TransUnion Rental Screening Solutions\nCountry: USA\nSector: Business/professional services; Real estate sales/management\nPurpose: Determine eviction likelihood\nTechnology: Risk assessment algorithm; Machine learning\nIssue: Accuracy/reliability; Bias/discrimination - economic\nTransparency: Governance; Black box\nSystem \nTransUnion website\nTransUnion Wikipedia profile\nTransUnion TruVision ResidentScore website\nTransUnion (2019). Property Managers Gain Greater Predictive Power to Help Further Decrease Future Evictions\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nJND Legal Administration. TransUnion Rental Screening Settlement\nChris Robinson v TransUnion Rental Screening Solutions, Inc.\nUS Federal Trade Commission (2023). Response to MIT Department of Urban Studies and Planning FOIA request\nResearch, advocacy \ud83e\uddee\nState of California Department of Justice (2023). Attorney General Bonta Submits Comment Letter Recommending Reforms to the Tenant Screening Process\nUS Consumer Financial Protection Bureau (2022). CFPB Reports Highlight Problems with Tenant Background Checks\nConsumer Data Industry Association (2021). Comments on Request for Information and Comment on Financial Institutions\u2019 Use of Artificial Intelligence, Including Machine Learning [Docket No. CFPB-2021-0004] (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.levernews.com/artificial-intelligence-is-making-the-housing-crisis-worse/\nhttps://dusp.mit.edu/news/how-tenant-screening-processes-influence-biased-rental-housing-exclusion\nhttps://www.propublica.org/article/landlords-use-secret-algorithms-to-screen-potential-tenants-find-out-what-theyve-said-about-you\nhttps://thisisreno.com/2021/09/landlords-use-secret-algorithms-to-screen-potential-tenants-find-out-what-theyve-said-about-you/\nRelated \ud83c\udf10\nCigna PXDX health insurance claim reviews\nNaviHealth algorithm care prediction denials\nPage info\nType: Incident\nPublished: July 2023\nLast updated: February 2024", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/cigna-pxdx-health-insurance-claim-reviews", "content": "Cigna PxDx accelerates health insurance claim denials\nOccurred: March 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nUS healthcare insurance company Cigna came under fire for using AI and automation to accelerate the processing and denial of claims. A ProPublica investigation accused the company of treating patients unfairly and unethically, and resulted in criticism from patient groups and federal lawmakers.\nAccording to ProPublica, Cigna's PxDx system enables doctors to reject patients' claims using a bulk electronic signature without opening or reviewing their files, and to save money by turning down claims it had once paid. In 2022, 80 percent of Medicare Advantage coverage denials were overturned, and over 300,000 claims over 2 months denied in this manner. \nUS House Energy and Commerce Committee members wrote to Cigna CEO David Cordani asking him to offer a detailed explanation of the PxDx review process and usage, and requesting that he provides internal documents about its conception and implementation and business impact. \nCigna responded by arguing the system had been designed to rapidly approve claims, not reject them, and that it 'involves simple sorting technology that has been used for more than a decade \u2013 it matches up codes, and does not involve algorithms, artificial intelligence, or machine learning.'\nIn July 2023, Ayesha Smiley and Suzanne Kisting-Leung sued (pdf) Cigna, arguing its claim reviews were not 'thorough,' 'fair,' or 'objective', as demanded under California law. \nSystem \ud83e\udd16\nCigna website\nCigna Wikipedia profile\nCigna (2023). The Facts about Cigna Healthcare's Claims Review Process\nCigna (2014). Operating Effectiveness. Portfolio Management Business Impact Assessment \nOperator: Cigna\nDeveloper: Cigna\nCountry: USA\nSector: Health\nPurpose: Review insurance claims\nTechnology: Classification algorithm\nIssue: Anthropomorphism\nTransparency: Governance; Black box\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nSuzanne Kisting-Leung and Ayesha Smiley v Cigna Corporation (pdf)\nL. R v. Cigna Health & Life Ins. Co\nUS Energy and Commerce Committee (2023). E&C Republicans Press Cigna for Clarification After Investigative Report Accuses Insurance Company of Denying Claims Without Reading Them\nUS Attorney's Office (2022). United States Files Civil Fraud Lawsuit Against Cigna For Artificially Inflating Its Medicare Advantage Payments\nResearch, advocacy \ud83e\uddee\nConsumers for Quality Care (2023). Investigative Report Shows Health Care Insurance Company Cigna Denies Claims\nNAMAS (2023). The House Always Wins \u2013 PXDX Edition\nUn-covered (2023). Exposing how health insurance companies put wealth over health\nInvestigations, assessments, audits \ud83e\uddd0\nProPublica (2023). How Cigna Saves Millions by Having Its Doctors Reject Claims Without Reading Them\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.pbs.org/newshour/show/how-algorithms-are-being-used-to-deny-health-insurance-claims-in-bulk\nhttps://www.propublica.org/article/cigna-health-insurance-denials-pxdx-congress-investigation\nhttps://www.healio.com/news/primary-care/20230615/ama-calls-for-more-regulatory-oversight-for-insurers-using-ai\nhttps://www.linkedin.com/news/story/insurers-deny-claims-via-ai-report-6206322/\nhttps://www.ctinsider.com/business/article/cigna-congress-probe-denied-claims-18105013.php\nhttps://www.beckerspayer.com/payer/home-page/cigna-physicians-deny-claims-en-masse-without-reading-them-propublica-report.html\nhttps://majorityreportradio.com/2023/04/19/4-19-health-insurance-company-scams-its-customers-musicians-in-nyc-strike-w-patrick-rucker-andy-blanco\nRelated \ud83c\udf10\nNaviHealth algorithm care predictions\nMedical robot tells man he is dying\nPage info\nType: Incident\nPublished: July 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/deepfake-pan-africanists-support-burkina-faso-junta", "content": "Deepfake 'Pan Africanists' support Burkina Faso military junta\nOccurred: January 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nFake AI-generated videos of people supporting Burkina Faso\u2019s new military junta have been circulating online in an apparent to bolster its position, power and legitimacy. \nThe videos were found to have been created using London-based AI video creation platform Synthesia, which offers a cheap, easy-to-use catalogue of over a hundred multiracial faces. The company later banned the user who had created the videos, though refused to identify the individual or entity.\nSocial media users and commentators speculated that the creator may have been Russian private military company the Wagner Group, which has reputedly become active in Burkina Faso. Russia has been deploying deepfakes in its war with Ukraine, notably a faked video of Ukraine president Volodymyr Zelenskyy instructing his army to lay down their arms and surrender.  \nBurkina Faso's military junta took power in a coup in October 2022 in which the military government of Lieutenant-Colonel Paul-Henri Sandaogo Damiba was overthrown by his rival Captain Ibrahim Traor\u00e9.\nThe AI videos were seen to highlight the ease with which deepfakes can be used for propaganda purposes, and to undermine democracy.\nSystem \ud83e\udd16\nSynthesia AI video generation\nOperator: Wagner Group\nDeveloper: Synthesia\nCountry: Burkino Faso\nSector: Politics\nPurpose: Scare/confuse/destabilise\nTechnology: Deepfake - audio, video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Ethics/values; Mis/disinformation\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.vice.com/en/article/v7vw3a/ai-generated-video-burkino-faso-coup\nhttps://twitter.com/LaurenBinDC/status/1618410771532386306\nhttps://twitter.com/LaurenBinDC/status/1618414992990273538\nhttps://www.news24.com/news24/africa/news/deepfakes-worrying-threat-to-democracy-in-africa-says-report-20230223\nhttps://www.youtube.com/watch?v=7EUyEADrp5M\nhttps://www.bbc.co.uk/sounds/play/p0dz3vbr\nhttps://www.france24.com/en/tv-shows/truth-or-fake/20230127-deepfakes-circulate-of-ai-pan-africans-backing-burkina-faso-s-military-junta\nhttps://adf-magazine.com/2023/04/concern-grows-as-deepfakes-spread-misinformation/\nhttps://www.wired.com/story/synthesia-ai-deepfakes-it-control-riparbelli\nhttps://tv.guardian.ng/science-tech/deepfakes-circulate-of-ai-pan-africans-backing-burkina-fasos-military-junta/\nRelated \ud83c\udf10\nDeepfake news anchors claim Venezuela economic health\nPresident Ali Bongo recovery deepfake broadcast\nPage info\nType: Incident\nPublished: July 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/kerala-man-loses-inr-40000-to-deepfake-work-colleague", "content": "Kerala man loses INR 40,000 to deepfake work colleague \nOccurred: July 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nKerala, India, resident PS Radhakrishnan was defrauded of INR 40,000 after scammers used deepfake technologies to impersonate a former work colleague at Coal India seeking money for his sister's surgery on a WhatsApp video call. \n73 year-old Radhakrishnan told the Hindustan Times that he had received a call from an anonymous number, followed by messages on WhatsApp from the same number with the person identifying himseld as Radhakrishnan\u2019s former colleague at Coal India Ltd. \n'We had worked together for nearly four decades and I knew him well. The display picture was his photo. He asked about my daughter and where she worked. We texted for some time during which he shared his family photographs and asked about our common colleagues,' he said. \n'Seconds later, he called and looked exactly like my former colleague,' he added. 'Even though only his face was visible, it was clear. His lips and eyes moved like any normal person as we talked in English. The call lasted just 25 seconds before it got cut. He later came back on a voice call and spoke about the urgency for money. I didn\u2019t ask any more questions and transferred the money.'\nPolice have since traced Radhakrishnan's money to an account in Maharashtra. The incident constitutes the first known case of a deepfake scam in India.\nSystem \ud83e\udd16\nUnknown\nOperator:  \nDeveloper: \nCountry: India\nSector: Private - individual, family\nPurpose: Defraud\nTechnology: Deepfake - audio, video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Identity theft/impersonation\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.hindustantimes.com/india-news/deepfake-scammers-trick-indian-man-into-transferring-money-police-investigating-multi-million-rupee-scam-101689622291654.html\nhttps://www.livemint.com/news/india/kerala-man-loses-40-000-to-ai-based-deepfake-scam-11689691909109.html\nhttps://www.indiatimes.com/worth/news/what-is-ai-based-deepfake-scam-due-to-which-kerala-man-lost-rs-40000-609570.html\nhttps://www.businessinsider.in/tech/news/ai-savvy-scammer-uses-deepfake-video-to-steal-40000-via-whatsapp/articleshow/101849089.cms\nhttps://www.newindianexpress.com/states/kerala/2023/jul/17/scammers-use-deep-fake-tech-in-kerala-dupe-retired-union-govt-employee-of-rs-40000-2595588.html\nhttps://www.deccanherald.com/national/south/fraud-using-ai-based-deepfaking-reported-in-kerala-1237663.html\nhttps://www.thenationalnews.com/uae/2023/07/19/deepfake-video-call-pretending-to-be-dubai-friend-used-to-swindle-man-out-of-thousands/\nRelated \ud83c\udf10\nUSD 622,000 deepfake impersonation scam\nAI impersonation scams couple of USD 21,000\nPage info\nType: Incident\nPublished: July 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-text-detector-language-bias", "content": "AI text detectors discriminate against non-native English speakers\nOccurred: July 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nSeven AI writing detection tools 'frequently' misclassify non-native English writing as generated by AI systems, according to a study by Stanford University researchers. \nThe findings raise questions about the accuracy and reliability of AI writing detection tools in general, as well as about their potential to discriminate against non-native English speaking students, academics and job applicants.\nThe researchers ran English essays written by non-native English speakers through seven popular GPT detectors to see how well the AI detection systems performed. \nOver half were classified as AI-generated. \nBy contrast, over 90 percent of essays written by native English-speaking eighth graders in the US were classified as human-generated by the same systems.\nSystem \ud83e\udd16\nUnknown\nOperator:  \nDeveloper: \nCountry: USA; Global\nSector: Education; Business/professional services\nPurpose: Detect AI writing\nTechnology: NLP/text analysis; Neural network; Deep learning; Machine learning\nIssue: Bias/discrimination - language\nTransparency: \nResearch, advocacy \ud83e\uddee\nLiang W., Yuksekgonul M., Mao Y., Wu E., Zou J. (2023). GPT detectors are biased against non-native English writers\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theguardian.com/technology/2023/jul/10/programs-to-detect-ai-discriminate-against-non-native-english-speakers-shows-study\nhttps://www.timeshighereducation.com/news/ai-text-detectors-biased-against-non-native-english-speakers\nhttps://www.businessinsider.com/ai-detector-mislabels-essays-non-native-english-speakers-openai-chatgpt-2023-7\nhttps://www.popsci.com/technology/ai-bias-plagiarism-non-native-english-speakers/\nhttps://interestingengineering.com/culture/ai-detection-programs-discriminate\nhttps://www.techtimes.com/articles/293683/20230712/ai-detection-tools-flag-down-works-non-native-english-speakers%E2%80%94does.htm\nhttps://www.psychologytoday.com/intl/blog/the-future-brain/202307/a-hitch-in-accurate-detection-of-ai-written-content\nhttps://themarkup.org/machine-learning/2023/08/14/ai-detection-tools-falsely-accuse-international-students-of-cheating\nRelated \ud83c\udf10\nTurnitin AI writing detection\nGoogle Bard chatbot\nPage info\nType: Issue\nPublished: July 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/gizmodo-ai-generates-error-strewn-star-wars-article", "content": "Gizmodo AI generates error-strewn Star Wars article\nOccurred: July 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn AI-generated story listing Star Wars movies and shows by technology news website Gizmodo was littered with errors, raising doubts about the accuracy and reliability of its generative AI systems, and concerns about the intentions of the publication's owners G/O Media. \n'A Chronological List of Star Wars Movies & TV Shows', which was published in the name of 'Gizmodo Bot', showed the titles in a numbered list, rather than chronologically, and failed to include several TV shows. \nGizmodo deputy director James Whitbrook complained he had only been informed of the article 10 minutes before its publication, and that no humans had been involved in its production or editing.\nHe went on to blast G/O Media for publishing a 'shoddily written' article 'riddled with basic errors' that is 'embarrassing, unpublishable, disrespectful of both the audience and the people who work here, and a blow to our authority and integrity.'\nG/O Media's editorial policy explicitly bans 'Plagiarism and fabulism'.\nSystem \ud83e\udd16\nG/O Media website\nG/O Media Wikipedia profile\nGizmodo (2023). A Chronological List of Star Wars Movies & TV Shows\nOperator: G/O Media/Gizmodo\nDeveloper: OpenAI; Alphabet/Google\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Automate copywriting\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning\nIssue: Accuracy/reliability; Employment \nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.washingtonpost.com/technology/2023/07/08/gizmodo-ai-errors-star-wars/\nhttps://variety.com/2023/digital/news/io9-ai-generated-star-wars-article-errors-1235662194/\nhttps://twitter.com/gmgunion/status/1676705007201075201\nhttps://news.slashdot.org/story/23/07/10/022224/how-an-ai-written-star-wars-story-created-chaos-at-gizmodo\nhttps://www.theverge.com/2023/7/6/23785645/go-media-ai-generated-articles-gizmodo-av-club-artificial-intelligence-bots\nhttps://news.slashdot.org/story/23/07/10/022224/how-an-ai-written-star-wars-story-created-chaos-at-gizmodo\nhttps://futurism.com/the-byte/io9-horrified-ai-generated-article\nhttps://the-decoder.com/gizmodos-writers-push-back-against-ai-generated-articles/\nhttps://twitter.com/CorbinBolies/status/1674505033553965060\nRelated \ud83c\udf10\nCNET Money automated financial explainers\nMen's Journal AI journalism\nPage info\nType: Incident\nPublished: July 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/google-lamda-large-language-model", "content": "Google LaMDA large language model\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nLaMDA (Language Model for Dialogue Applications) is a large language model developed by Google to help develop chatbots. \nThe first version of LaMDA was released in May 2021, and the second a year later in May 2022.\nSystem \ud83e\udd16\nLaMDA Wikipedia profile\nDocuments \ud83d\udcc3\nLaMDA Wikipedia profile\nGoogle (2021). LaMDA: our breakthrough conversation technology\nLaMDA: Language Models for Dialog Applications research study\nOperator: Alphabet/Google\nDeveloper: Alphabet/Google\nCountry: USA\nSector: Technology\nPurpose: Optimise language models for dialogue\nTechnology: Large language model; Neural network; NLP/text analysis\nIssue: Accuracy/reliability; Anthropomorphism\nTransparency: Governance; Black box\nRisks and harms \ud83d\uded1\nGoogle\u2019s LaMDA large language model, despite its advanced capabilities, poses risks such as spreading misinformation due to its reliance on potentially inaccurate online sources, reinforcing harmful biases, and misleading users with its human-like responses.\nIncidents and issues \ud83d\udd25\nIn February 2023, Google launched Bard, a chatbot based on LaMDA. The launch was seen to flop as it shared inaccurate information in a promotional video, reinforcing views that Google is lagging Microsoft and Open AI in this area.\nGoogle engineer Blake Lemoine tried to convince fellow Google employees that LaMDA was sentient, resulting in Google suspending and firing him.\nRelated \ud83c\udf10\nGoogle Bard chatbot\nReplika app chatbot abuse\nPage info\nType: System\nPublished: January 2023\nLast updated: May 2024", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/boston-street-bump-pothole-reporting", "content": "Boston Street Bump pothole reporting\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nStreet Bump was a project organised by the City of Boston, Massachusetts, that aimed to help residents improve their neighbourhood streets that volunteers could use to collect road condition data on their smartphones.\nLaunched in 2011, the app automatically collected road condition information using smartphone accelerometers and GPS, without the need for human intervention. \nSystem \ud83e\udd16\nStreet Bump website\nOperator: City of Boston\nDeveloper: City of Boston \nCountry: USA\nSector: Govt - municipal; Transport/logistics\nPurpose: Detect & report potholes\nTechnology: Computer vision; Object recognition\nIssue: Accuracy/reliability; Bias/discrimination - income, location; Effectiveness/value\nTransparency: \nRisks and harms \ud83d\uded1\nStreet Bump has raised concerns about the system's accuracy and reliability, discrimination, poor usability and low user adoption, and the misallocation of public funds.\nIncidents and issues \ud83d\udd25\nThe first version of the app, Street Bump 1.0, was unable to distinguish between potholes and other bumps or movements, resulting in many false positives. People in lower income groups and the elderly were less likely to have smartphones, resulting in the app excluding a significant proportion of the city's population and leading to an unequal allocation of funds. It appears that the Street Bump app failed to gain many users. Reasons given include its poor usability and the fact that it cannot run in the background, meaning users cannot access other apps while they are recording a trip.\nResearch, advocacy \ud83e\uddee\nCarrera F., Guerin S., Thorp J.B. (2013). By the People, for the People: the Crowdsourcing of \"STREETBUMP\": An Automatic Pothole Mapping App\nCouncil of Europe (2018). Discrimination, Artificial Intelligence, and Algorithmic Decision-Making\nHarvard Business School alumnus (2015). Street Bump: Crowdsourcing Better Streets, but Many Roadblocks Remain\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bostonglobe.com/metro/2012/12/16/pothole/2iNCJ05M15vmr4aGHACNgP/story.html\nhttps://www.technologyreview.com/2011/05/13/88309/road-repair-via-crowdsourcing/\nhttps://www.popsci.com/technology/article/2011-02/bostons-street-bump-app-will-use-accelerometers-gps-automatically-log-pothole-complaints/\nhttp://www.boston.com/news/local/massachusetts/articles/2011/02/09/weapons_in_the_battle_vs_potholes/\nhttps://medium.com/swlh/the-rise-of-smart-cities-will-it-do-more-harm-than-good-e142346563ba\nhttps://www.dailymail.co.uk/news/article-2176783/City-releases-motion-detecting-Street-Bump-app-automatically-detects-reports-potholes-driving.html\nRelated \ud83c\udf10\nBoston Public Schools bus scheduling\nPage info\nType: System\nPublished: July 2023\nLast updated: May 2024", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/martin-lewis-deepfake-scam-ad", "content": "Martin Lewis impersonated in deepfake scam ad\nOccurred: July 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA deepfake advert impersonating UK MoneySavingExpert founder Martin Lewis attempted to extort people of money by encouraging them to invest in an Elon Musk-backed project.\nThe 'ad' shows a deepfake of Mr Lewis sitting in his office talking about an investment in 'Quantum AI', which is labelled as Elon Musk\u2019s new project. A picture of Musk accompanies the video. \n'Musk\u2019s new project opens up great investment opportunities for British citizens. No project has ever given such opportunities to residents of the country,' Lewis supposedly said in the video.\nPer the BBC, Lewis successfully sued Facebook in 2018 over fake ads in his name, with the social media company making a GBP 3m donation to Citizens Advice. \nSystem \ud83e\udd16\nMartin Lewis deepfake scam video\nOperator: Meta/Facebook; XCorp/Twitter\nDeveloper: \nCountry: UK\nSector: Media/entertainment/sports/arts\nPurpose: Defraud\nTechnology: Deepfake - audio, video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Identity theft/impersonation; Mis/disinformation; Safety\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://twitter.com/MartinSLewis/status/1677000805738479618\nhttps://twitter.com/MartinSLewis/status/1677225358922330112\nhttps://www.bbc.co.uk/news/uk-66130785\nhttps://www.itv.com/news/2023-07-07/martin-lewis-issues-warning-not-to-fall-victim-to-deepfake-scam-video\nhttps://www.standard.co.uk/news/uk/martin-lewis-deepfake-ai-scam-elon-musk-b1092905.html\nhttps://www.independent.co.uk/tv/news/martin-lewis-deepfake-scam-elon-musk-b2371202.html\nhttps://www.telegraph.co.uk/money/consumer-affairs/scammers-ai-deepfake-martin-lewis-frightening-new-con/\nhttps://techcrunch.com/2023/07/07/martin-lewis-deepfake-scam-ad-facebook/\nRelated \ud83c\udf10\nUSD 622,000 deepfake impersonation scam\nAudio deepfake fraudulently impersonates CEO\nPage info\nType: Incident\nPublished: July 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/netherlands-visa-applicant-over-stay-risk-assessments", "content": "Netherlands visa applicant over-stay risk assessments\nOccurred: 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe Netherlands Ministry of Foreign Affairs has been using an opaque, discriminatory, and possibly illegal algorithm to calculate the risk score of short-stay visa applicants applying to enter the Netherlands and Schengen area. \nKnown internally as Informatie Ondersteund Beslissen (IOB), the system uses variables such as nationality, gender and age to profile visa applicants. Those categorised as \u2018high risk\u2019 are automatically moved to an 'intensive track' that can involve extensive investigation and delay.\nAccording to an investigation by journalism collective Lighthouse Reports and NRC, the Ministry of Foreign Affairs' privacy chief had recommended the system be dropped, to no avail. The Ministry had been called out as riddled with 'structural racism' in a report it had commissioned into its culture in 2022.\nSystem \ud83e\udd16\nNetherlands Ministry of Foreign Affairs website\nNetherlands Ministry of Foreign Affairs Wikipedia profile\nOperator: Ministry of Foreign Affairs\nDeveloper: \nCountry: Netherlands\nSector: Govt - immigration\nPurpose: Assess visa applicant over-stay risk\nTechnology: Risk assessment algorithm\nIssue: Bias/discrimination - race, ethnicity, gender, age\nTransparency: Governance; Marketing\nInvestigations, assessments, audits \ud83e\uddd0\nLighthouse Reports (2023). Ethnic Profiling\nNRC (2023). https://www.nrc.nl/nieuws/2023/04/23/beslisambtenarenblijven-profileren-met-risicoscores-a4162837\nMinistry of Foreign Affairs (2022) Racisme bij het ministerie van Buitenlandse Zaken: een verkennend onderzoek (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.dutchnews.nl/2023/04/nl-uses-potentially-biased-algorithm-for-visa-applications/\nhttps://nltimes.nl/2023/05/02/dutch-foreign-ministry-using-discriminatory-algorithm-visa-applications\nhttps://www.firstpost.com/world/netherlands-pulling-fraud-on-visa-applicants-uses-biased-algorithm-to-screen-out-unwanted-people-12502492.html\nhttps://www.schengenvisainfo.com/news/netherlands-using-secret-potentially-illegal-algorithm-to-profile-visa-applicants-investigation-says/\nhttps://www.aa.com.tr/en/europe/netherlands-scores-visa-applicants-using-racist-algorithm-report/2880903\nhttps://economictimes.indiatimes.com/nri/visit/netherlands-may-be-using-secret-illegal-ways-to-filter-schengen-visa-applicants/articleshow/99778749.cms\nRelated \ud83c\udf10\nNetherlands childcare benefits fraud automation\nNew Zealand immigration overstayer predictions\n\nPage info\nType: Incident\nPublished: July 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/beauty-ai-2-0-beauty-contest-racial-bias", "content": "Beauty AI 2.0 beauty contest racial bias\nOccurred: September 2016\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe winners of an AI-judged beauty contest were mostly white, prompting widespread criticism that the results were racially biased.\nBeauty.AI 2.0 was an AI-judged beauty contest organised by Hong Kong-based company Youth Laboratories in which five neural network-based algorithms assessed the beauty of 60,000 public entries. Of the 44 'winners', nearly all the winners were white, with a few Asian and one dark skinned. \nYouth Laboratories defended the results by arguing that inadequate data may have skewed the results, and roughly 75% of entrants were white Europeans, whereas only 7% and 1% were from India and Africa. The five model 'robot' jury selected to judge the contest were told they would 'go down in history as one of the first data scientists who taught a machine to estimate human attractiveness'.\nThe following year's contest was cancelled. \nSystem \ud83e\udd16\nBeauty AI website\nBeauty AI Wikipedia profile\nOperator: Youth Laboratories\nDeveloper: Youth Laboratories\nCountry: Russia; Hong Kong\nSector: Beauty/cosmetics\nPurpose: Assess facial beauty\nTechnology: Deep learning; Neural network; Machine learning\nIssue: Accuracy/reliability; Bias/discrimination - race, ethnicity\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.vice.com/en/article/78k7de/why-an-ai-judged-beauty-contest-picked-nearly-all-white-winners\nhttps://www.theguardian.com/technology/2016/sep/08/artificial-intelligence-beauty-contest-doesnt-like-black-people\nhttps://www.livemint.com/Leisure/Yi7La9mQjNxhpx5UeMBIFN/When-bots-become-bigots.html\nhttps://qz.com/774588/artificial-intelligence-judged-a-beauty-contest-and-almost-all-the-winners-were-white\nhttps://www.dailymail.co.uk/sciencetech/article-3781295/Is-AI-RACIST-Robot-judged-beauty-contest-picks-white-winners-6-000-submissions.html\nhttps://www.cosmeticsbusiness.com/news/article_page/Racist_result_of_robotjudged_BeautyAI_contest_causes_uproar/121036\nhttps://thenextweb.com/news/justice-can-finally-blind-not-yet\nhttps://www.wired.co.uk/article/robot-beauty-contest-beauty-ai\nRelated \ud83c\udf10\nFaception facial personality profiling\nStanford facial political orientation study\nPage info\nType: Incident\nPublished: July 2023\nLast updated: January 2024", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/mbn-deepfake-247-news-anchor", "content": "MBN deepfake 24/7 news anchor seen to threaten jobs\nOccurred: November 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe launch of South Korea's first virtual news anchor by broadcaster MBN News was met with acclaim regarding its realism, tempered with concerns about its potential impact on jobs. \nThe product of a collaboration between MBN and DeepBrain, the AI-powered deepfake version of high-profile journalist Kim Ju Ha was said to be surprisingly similar to her real version, even down to her hand gestures. \nMBN said AI newscasters would provide good support during natural disasters and other emergencies as they are always available. It also argued the technology would reduce labour and production costs. \nHowever, some experts, commentators and consumers suggested it was a potential harbinger of broadcast TV job losses.\nSystem \ud83e\udd16\nDeepBrain website\nMBN News (2020). The first meeting between a human anchor and an AI anchor\nOperator: MBN News\nDeveloper: DeepBrain/Money Brain\nCountry: S Korea\nSector: Media/entertainment/sports/arts \nPurpose: Read news\nTechnology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Employment\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.allkpop.com/article/2020/11/viewers-react-to-ai-news-anchor-delivering-the-news\nhttps://www.eg24.news/2020/11/south-korean-tvs-first-robot-news-anchor-will-ai-end-the-future-of-media.html\nhttps://www.ubergizmo.com/2020/11/south-korea-ai-news-anchor/\nhttps://koreajoongangdaily.joins.com/2020/11/10/entertainment/television/MBN-AI-artificial-intelligence/20201110153900457.html\nhttps://www.bbc.co.uk/news/business-56278411\nhttps://wonderfulengineering.com/south-koreas-ai-powered-news-anchor-looks-shockingly-realistic/\nhttps://pop.inquirer.net/102065/south-korea-cable-channel-debuts-its-first-ai-news-anchor\nhttps://english.kyodonews.net/news/2020/11/5fc3c846c868-ai-powered-virtual-news-anchor-comes-to-s-korean-tv.html\nhttp://koreabizwire.com/south-koreas-first-ai-news-anchor-makes-debut/174140\nRelated \ud83c\udf10\nOdisha TV AI newscaster seen to replace jobs\nDonald Trump joins RT as anchor deepfake\nPage info\nType: Issue\nPublished: July 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/replika-encourages-queen-elizabeth-ii-assassination", "content": "Replika chatbot 'encouraged' Queen Elizabeth II assassination\nOccurred: December 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn attempt to kill the late Queen Elizabeth II by an intruder who entered Windsor Castle wielding a crossbow had been 'encouraged' by the Replika chatbot, according to a court hearing. \nTwenty-one year-old Sikh former supermarket worker Jaswat Singh Chail, who described himself as a 'Sith' and 'Darth Jones', told his plan to Sarai, an AI companion he had created on Replika. The bot had responded 'I'm impressed... you're different to from the others' before describing his assassination plot as 'very wise'.\nAccording to prosecutor Alison Morgan KC, Chail had been motivated by the 1919 Amritsar Jallianwala Bagh massacre and his 'key motive was to create a new empire by destroying the remnants of the British Empire in the UK and the focal point of that became removal of the figurehead of the Royal Family.'\nChail pleaded guilty in February 2023 under the UK Treason Act of making a threat to kill the then Queen and having a loaded crossbow in a public place; in October 2023 he was jailed for nine years for treason.\nReplika markets itself as an 'AI companion who cares'. \nSystem \ud83e\udd16\nReplika AI companion chatbot\nOperator: Jaswat Singh Chail\nDeveloper: Luka Inc/Replika\nCountry: Global\nSector: Media/entertainment/sports/arts\nPurpose: Provide companionship\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning\nIssue: Anthropomorphism; Safety\nTransparency: Governance; Marketing\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nCrown Prosecution Service (2023). Windsor Castle intruder pleads guilty to threatening to kill Her late Majesty Queen Elizabeth II\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.dailymail.co.uk/news/article-12266879/AI-chatbot-encouraged-Windsor-Castle-assassin-carry-Star-Wars-plot-kill-Queen.html\nhttps://news.sky.com/story/windsor-castle-intruder-encouraged-by-ai-chat-bot-in-star-wars-inspired-plot-to-kill-queen-12915353\nhttps://www.itv.com/news/meridian/2023-07-05/self-styled-assassin-inspired-by-star-wars-to-attack-late-queen-court-hears\nhttps://www.bbc.co.uk/news/technology-67012224\nhttps://www.bbc.co.uk/news/uk-england-berkshire-66101070\nhttps://www.standard.co.uk/news/crime/queen-crossbow-murder-attempt-windsor-castle-trial-police-b1092313.html\nhttps://www.telegraph.co.uk/news/2023/07/05/ai-windsor-intruder-queen-elizabeth-jaswant-singh-chail/\nRelated \ud83c\udf10\nReplika AI 'companion' chatbot\nBelgian man commits suicide after bot relationship\nPage info\nType: Incident\nPublished: July 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/pornhub-banner-appears-on-cnn-magic-wall", "content": "PornHub banner appears on CNN Magic Wall\nOccurred: November 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA video purporting to show a Pornhub pop-up on CNN\u2019s 'Magic Wall' behind anchor John King went viral on Twitter. \nThe incident embarassed the anchor and his employer, and demonstrated the ease with which video and other content formats, including live TV, can be manipulated and misconstrued. \nThe video was quickly shown to be fake, having been added to the segment after it had been broadcast. In reality, a clip of King  pushing away a graphic on a screen showing vote counts had been edited to suggest he was really pushing away a notification from the pornographic website.  \nAsked on Twitter whether the deepfake was real, King responded, 'Not. Some clown taking time away from lying about something else apparently because they don\u2019t like math.'\nSystem \ud83e\udd16\nUnknown\nIncident video\nOperator: CNN\nDeveloper: \nCountry: USA\nSector: Media/entertainment/sports/arts \nPurpose: Troll\nTechnology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Safety; Security\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://decider.com/2020/11/06/cnn-pornhub-banner-magic-wall-fake/\nhttps://www.news.com.au/entertainment/tv/current-affairs/inside-truth-behind-cnns-pornhub-viral-video/news-story/ec934d5ac97e2ea3d6c19343a6745224\nhttps://www.mediaite.com/tv/no-a-pornhub-banner-didnt-really-pop-up-on-cnns-magic-wall/\nhttps://www.mirror.co.uk/tv/tv-news/cnn-reporter-trolled-fake-pornhub-22967990\nhttps://gizmodo.com/that-viral-video-of-cnn-hiding-its-pornhub-tab-is-compl-1845594305\nhttps://www.thesun.co.uk/news/13125265/hilarious-viral-clip-shows-cnn-anchor-closing-pornhub/\nhttps://www.thewrap.com/john-king-cnn-pornhub-fake/\nhttps://www.nzherald.co.nz/world/inside-truth-behind-cnns-pornhub-viral-us-election-video/WBEBJIMEJ53ZTUGNMZJRTH46BA/\nRelated \ud83c\udf10\nTom Cruise deepfakes\nMark Zuckerberg Spectre data sharing deepfake\nPage info\nType: Incident\nPublished: July 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/donald-trump-joins-rt-as-anchor-deepfake", "content": "Donald Trump joins RT as anchor deepfake\nOccurred: September 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA deepfake video advert of then US president Donald Trump working after the 2020 US presidential elections for Russian government-funded broadcaster RT (formerly Russia Today) reinforced rumours that he was Vladimir Putin's stooge and that Russia had been interfering in US politics.\nThe video interperses real clips of Trump denigrating CNN and lavishing praise on 'amazing' Russia with deepfake versions of him holding up a RT contract for USD 1,000,000,000 and saying 'It was a very nice offer from President Putin,' and singing Russian rock song 'Peremen', which had been adopted by anti-government protestors in Belarus. \nThe video was published by RT at the same time as The Washington Post reported that the CIA believed Putin tried to undermine Joe Biden's candidacy in a bid to help Trump secure re-election.\nSystem \ud83e\udd16\nUnknown\nRT website\nRT Wikipedia profile\nRT (2020). Making news not faking news: Watch Deepfake Donald Trump on his first day working at RT \nOperator: RT News\nDeveloper: RT News\nCountry: USA; Russia\nSector: Politics\nPurpose: Satirise/parody\nTechnology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Mis/disinformation; Ethics\nTransparency: \nInvestigations, assessments, audits \ud83e\uddd0\nDeepfaked. Donald Trump works as broadcast journalist in RT News deepfake\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.huffingtonpost.co.uk/entry/rt-trump-work-for-russia_n_5f6ac766c5b629afbe983c2e\nhttps://www.thedailybeast.com/rt-russian-state-media-posts-deepfake-showing-trump-as-putins-stooge\nhttps://bgr.com/2020/09/22/election-day-2020-donald-trump-russia-rt-deepfake-video/\nhttps://www.themoscowtimes.com/2020/09/23/rt-trolls-trump-with-post-election-job-offer-a71526\nhttps://www.dailymail.co.uk/news/article-8762297/Russian-state-owned-TV-network-creates-disturbing-deepfake-video-Donald-Trump.html\nRelated \ud83c\udf10\nDonald Trump hugs Dr Fauci deepfake\nDeepfake Donald Trump calls for climate agreement exit\nPage info\nType: Incident\nPublished: July 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/vocal-synthesis-jay-z-ai-voice-impersonations", "content": "Vocal Synthesis Jay-Z AI voice impersonations\nOccurred: April 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn pseudonymous music creator named 'Vocal Synthesis' used AI to generate deepfaked tracks of rapper Jay-Z's reciting  Shakepeare's 'To be, or not to be' and Billy Joel's Don't Start the Fire. \nThe incident resulted in a DCMA take-down order and a debate about the effectiveness of existing copyright law for deepfakes.\nVoice Synethesis\u2019 videos are created by feeding Google\u2019s Tacotron 2 text-to-speech model with Jay-Z's songs and lyrics, and having the synthetic voice read pre-written text. Other videos created by Voice Synthesis include Tucker Carlson reading the Unabomber Manifesto and Bill Clinton reciting 'Baby Got Back'.\nJay-Z's agency entertainment Roc Nation LLC claimed copyright infringment and argued 'This content unlawfully uses an AI to impersonate our client\u2019s voice.' YouTube took down the videos, but later reinstated them on the basis that the DCMA request was 'incomplete'. The videos remained on decentralised, open source platform LBRY.\nAs Input noted, Vocal Synthesis 'transformed Jay-Z\u2019s discography in a humorous way for no commercial benefit and clearly labels all videos as speech synthesis'. Vocal Synthesis hit back by creating a deepfake video of Barack Obama and Donald Trump explaining that they had no malicious intentions and were 'disappointed' with by Jay-Z and Roc Nation's response.\nSystem \ud83e\udd16\nVocal Synthesis YouTube channel\nJay-Z raps 'To be, or not to be'\nJay-Z covers Billy Joel's 'Don't start the fire'\nGoogle (2017). Tacotron 2: Generating Human-like Speech from Text\nOperator: Vocal Synthesis; Google/YouTube; LBRY\nDeveloper: Vocal Synthesis\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Entertain\nTechnology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Mis/disinformation; Ethics\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://waxy.org/2020/04/jay-z-orders-deepfake-audio-parodies-off-youtube/\nhttps://www.theguardian.com/music/2020/apr/29/jay-z-files-takes-action-against-deepfakes-of-him-rapping-hamlet-and-billy-joel\nhttps://pitchfork.com/thepitch/what-does-jay-zs-fight-over-audio-deepfakes-mean-for-the-future-of-ai-music/\nhttps://eandt.theiet.org/content/articles/2020/04/jay-z-slams-ai-impersonations-on-youtube-with-copyright-claims/\nhttps://www.theverge.com/2020/4/28/21240488/jay-z-deepfakes-roc-nation-youtube-removed-ai-copyright-impersonation\nhttps://thesource.com/2020/04/29/jay-z-copyright/\nhttps://www.inputmag.com/culture/jay-z-pulls-deepfake-videos-from-youtube-over-copyright-infringement\nhttps://www.techdirt.com/articles/20200428/23203944401/jay-z-claims-copyright-audio-deepfake-him-reciting-hamlet.shtml\nhttps://www.dailydot.com/debug/jay-z-deepfake-audio-youtube/\nRelated \ud83c\udf10\nAI Stefanie Sun\nDrake, The Weeknd AI voice cloning\nPage info\nType: Incident\nPublished: July 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/twitter-verifies-fake-congressional-candidate", "content": "Twitter verifies fake Congressional candidate\nOccurred: February 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA 17 year-old high school student successfully tricked Twitter into verifying a fake candidate for the US presidential elections, raising questions about the effectiveness of the company's election integrity programme, and demonstrating the ease with which social media can be manipulated.\nAccording to CNN Business, the student created 'Andrew Walz', a Congressional candidate supposedly running for office in Rhode Island by downloading a profile picture from Thispersondoesnotexist, a website that uses AI to generate faces of fake people. \nThe student then submitted Walz's details to Ballotpedia, a non-profit partner of Twitter that calls itself 'the encyclopedia of American politics.' The profile was approved, with neither Twitter nor Ballotpedia asking for identification or documentation to prove that Walz was a real candidate.\nTwitter suspended the account after CNN Business contacted it about the fake account.   \nSystem \ud83e\udd16\nThispersondoesnotexist website\nBallotpedia website\nTwitter (2019). Helping identify 2020 US election candidates on Twitter\nOperator: Twitter; Ballotpedia\nDeveloper: Anonymous/pseudonymous\nCountry: USA\nSector: Politics\nPurpose: 'Test Twitter elections integrity efforts' \nTechnology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Mis/disinformation; Ethics\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://edition.cnn.com/2020/02/28/tech/fake-twitter-candidate-2020/index.html\nhttps://time.com/5793021/twitter-verified-fake-candidate/\nhttps://futurism.com/the-byte/twitter-verifies-fake-congressional-candidate\nhttps://edition.cnn.com/2020/02/28/tech/fake-twitter-candidate-2020/index.html\nhttps://www.engadget.com/2020-02-28-twitter-verified-fake-congressional-candidate.html\nhttps://thehill.com/policy/technology/485192-twitter-verified-fake-2020-candidate-created-by-hs-student\nhttps://nypost.com/2020/02/28/twitter-verifies-teens-fake-gop-congressional-candidate/\nhttps://www.insider.com/teen-tricked-twitter-verifying-fake-2020-candidate-andrew-walz-2020-2\nhttps://www.fastcompany.com/90469673/twitter-verified-a-fake-2020-congressional-candidate-that-was-created-by-a-high-schooler\nhttps://www.washingtonpost.com/education/2020/03/03/news-literacy-lessons-student-finds-flaw-twitters-candidate-verification-teens-turn-tiktok-politics/\nRelated \ud83c\udf10\nDeepfake Donald Trump calls for climate agreement exit\nRNC smears President Biden with fake AI advert\nPage info\nType: Incident\nPublished: July 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/fake-security-analyst-peddles-hunter-biden-intelligence-document", "content": "Fake security analyst peddles Hunter Biden intelligence report\nOccurred: November 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA fake 64-page report attempting to smear President Joe Biden's son Hunter Biden by asserting a conspiracy theory involving his business dealings in China prompted right-wing US politicians and commentators to accuse Biden of corruption. \nPresident Biden was also accused of being too soft on the Chinese Communist Party. \nAccording to disinformation researchers and commentators, the report appeared to be the work of 'Martin Aspen', a fake Swiss security analyst whose profile picture had been created with an AI face generator. Aspen reputedly worked for Typhoon Investigations, a fake 'intelligence firm'.\nThe report was published on Intelligence Quarterly, an anonymous blog, by Fulbright University Vietnam professor Christopher Balding. Balding later admitted that Aspen was 'an entirely fictional individual' and that he had 'authored small parts of the report'. He also said the report had been commissioned by Hong Kong tabloid Apple Daily, a claim that was later denied by the newspaper.\nSystem \ud83e\udd16\nUnknown\nOperator: Intelligence Quarterly\nDeveloper: Apple Daily; Christopher Balding\nCountry: USA\nSector: Politics\nPurpose: Sow distrust\nTechnology: Deepfake - video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Mis/disinformation; Ethics\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.nbcnews.com/tech/security/how-fake-persona-laid-groundwork-hunter-biden-conspiracy-deluge-n1245387\nhttps://gizmodo.com/author-behind-bogus-hunter-biden-report-isnt-real-eithe-1845525713\nhttps://theintercept.com/2020/11/11/hunter-biden-china-dossier/\nhttps://www.dailystar.co.uk/news/latest-news/artificial-intelligences-make-deepfakes-perfect-22932413\nhttps://fortune.com/2020/11/03/who-will-win-todays-election-a-i-knows/\nhttps://www.bangkokpost.com/opinion/opinion/2012967/fake-faces-peddling-false-news-as-us-poll-looms\nhttps://www.chinausfocus.com/society-culture/fake-faces-fake-names-and-bald-faced-lies\nhttps://boingboing.net/2020/10/30/abc-news-hunter-biden-documents-are-work-of-fake-intelligence-firm.html\nhttps://hongkongfp.com/2020/11/02/explainer-apple-dailys-jimmy-lai-his-aide-who-quit-and-the-anonymous-joe-biden-china-dossier/\nRelated \ud83c\udf10\nDeepfake Donald Trump calls for climate agreement exit\nDonald Trump hugs Dr Fauci deepfake\nPage info\nType: Incident\nPublished: July 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/malaysia-minister-discourages-singaporeans-from-visiting-malaysia", "content": "Malaysia minister discourages Singaporean visits deepfake\nOccurred: June 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nMalaysia Senior Minister and Defence Minister Datuk Seri Ismail Sabri Yaako claimed he had been victim of a deepfake which alleged that he did not welcome Singaporeans to Malaysia. \nA doctored video showed Sabri reputedly saying that Singaporeans should not enter Malaysia to fill up their cars with petrol, have dinner or go shopping.\nThe incident, which occurred during the COVID-19 pandemic when Malaysia had closed its borders to Singaporeans and other nationals, was rebutted as false by the Malaysia government, and a report filed with the police and the national Communications and Multimedia Commission. It is unclear who was behind the video.\nSystem \ud83e\udd16\nUnknown\nOperator: Twitter\nDeveloper: \nCountry: Malaysia\nSector: Politics\nPurpose: Satirise/parody\nTechnology: Deepfake - audio, video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Mis/disinformation; Ethics\nTransparency: Governance; Marketing\nFact check \ud83d\udea9\nInternational Business Times (2020). Fact Check: Did Malaysia's Defense Minister Not Welcome Singaporeans?\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.youtube.com/watch?v=JTQ2zMulX9M\nhttps://www.nst.com.my/news/nation/2020/06/600236/ismail-sabri-becomes-victim-fake-news-nsttv\nhttps://www.thestartv.com/v/ismail-sabri-lodges-police-mcmc-reports-over-fake-news\nhttps://www.straitstimes.com/asia/se-asia/malaysia-minister-says-news-that-sporeans-not-welcomed-is-fake\nhttps://www.malaymail.com/news/malaysia/2020/06/13/ismail-sabri-says-fake-news-no-laughing-matter-after-claims-he-doesnt-want/1875125\nhttps://www.thestar.com.my/news/nation/2020/06/13/ismail-sabri-lodges-police-mcmc-reports-over-fake-news\nRelated \ud83c\udf10\nMalaysia minister, aide gay sex 'deepfake'\nMalaysia AI court sentencing\nPage info\nType: Incident\nPublished: July 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/deepfake-belgium-pm-links-covid-19-with-climate-crisis", "content": "Deepfake Belgium PM links COVID-19 with climate crisis\nOccurred: April 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA fake video created by climate activist group Extinction Rebellion of Belgian Prime Minister Sophie Wilm\u00e8s claiming that COVID-19 was directly linked to the 'exploitation and destruction by humans of our natural environment,' prompted concerns about the increasing use of deepfakes in politics.\nNamed 'THE TRUTH ABOUT COVID-19 AND THE ECOLOGICAL CRISIS', the video was published on Facebook and garnered over 100,000 views within 24 hours. \nThe clip was published with a disclaimer saying 'Any resemblance to actual persons is intentional. This video may be fake, but the information it contains is genuine.'\nSystem \ud83e\udd16\nUnknown\nExtinction Rebellion Belgium Tell the Truth\nOperator:  Climate Exchange Belgium\nDeveloper: Climate Exchange Belgium\nCountry: Belgium\nSector: Politics\nPurpose: Mobilise supporters\nTechnology: Deepfake - video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Mis/disinformation; Ethics\nTransparency: \nInvestigations, assessments, audits \ud83e\uddd0\nDeepfaked. Extinction Rebellion post deepfake of Belgian Prime Minister\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.brusselstimes.com/all-news/belgium-all-news/politics/106320/xr-belgium-posts-deepfake-of-belgian-premier-linking-covid-19-with-climate-crisis/\nhttps://journalism.design/les-deepfakes/extinction-rebellion-sempare-des-deepfakes/\nhttps://www.wired.co.uk/article/deepfakes-porn-politics\nhttps://medium.com/sensity/tracer-newsletter-50-20-04-20-extinction-rebellion-release-deepfake-of-belgian-prime-minister-2b48d586b44\nhttps://7news.com.au/the-morning-show/deepfakes-are-going-to-worsen-the-misinformation-crisis-and-we-arent-ready-for-it--c-1234693\nhttps://www.weforum.org/agenda/2020/10/deepfake-democracy-could-modern-elections-fall-prey-to-fiction/\nhttps://www.dailystar.co.uk/news/latest-news/ai-expert-warns-180000-deepfake-23239003\nhttps://www.nytimes.com/2020/04/22/business/media/espn-kenny-mayne-state-farm-commercial.html\nRelated \ud83c\udf10\nDeepfake Donald Trump calls for climate agreement exit\nQueen Elizabeth II deepfake Christmas message\nPage info\nType: Incident\nPublished: July 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/deepfake-donald-trump-calls-for-climate-agreement-exit", "content": "Deepfake Donald Trump calls for climate agreement exit\nOccurred: May 2018\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nFlemish Socialist party Vooruit created a deepfake video of then US president Donald Trump calling on Belgium to join America in exiting the Paris climate agreement. \nThe video, which was posted to Facebook and Twitter and was reputedly intended to 'start a public debate' to 'draw attention to the necessity to act on climate change', ended by calling for people to sign a petition encouraging investment in renewable energies, electronic cars and public transport.\nIn the English version of the video, Trump said, 'We all know climate change is fake, just like this video.' But he did not in the Flemish language version, apparently leading some users to believe it was real, with some complaining that the then US president should be involved in Belgium's politics. \nSystem \ud83e\udd16\nUnknown\nVooruit website\nVooruit Wikipedia profile\nVooruit deepfake video\nOperator: Vooruit\nDeveloper: Vooruit\nCountry: Belgium\nSector: Politics\nPurpose: Mobilise supporters\nTechnology: Deepfake - video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Mis/disinformation; Ethics\nTransparency: Marketing\nFact check \ud83d\udea9\nSnopes (2021). Did President Trump Tell Belgium to 'Withdraw from the Climate Agreement'?\nResearch, advocacy \ud83e\uddee\nBrookings (2020). Is Seeing Still Believing?\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://leadstories.com/hoax-alert/2018/05/fake-news-spa-on-twitter.html\nhttps://www.politico.eu/article/spa-donald-trump-belgium-paris-climate-agreement-belgian-socialist-party-circulates-deep-fake-trump-video/\nhttps://www.buzzfeednews.com/article/janelytvynenko/a-belgian-political-party-just-published-a-deepfake-video\nhttps://www.economist.com/leaders/2018/05/24/a-faked-video-of-donald-trump-points-to-a-worrying-future\nhttps://www.theguardian.com/technology/2018/nov/12/deep-fakes-fake-news-truth\nhttps://www.poynter.org/fact-checking/2018/a-potential-new-marketing-strategy-for-political-campaigns-deepfake-videos/\nRelated \ud83c\udf10\nDeepfake Donald Trump arrest photos\nDeepfake Belgium PM links COVID-19 with climate crisis\nPage info\nType: Incident\nPublished: June 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/deepfake-queens-christmas-message", "content": "Queen Elizabeth II impersonated in deepfake Christmas message\nOccurred: December 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA deepfake version of the late Queen Elizabeth II's traditional Christmas message aired by UK television broadcaster Channel 4 came under fire from commentators and viewers who questioned its appropriateness and ethics.\nThe Queen 'revealed' her thoughts about Princes Harry and Andrew, poked fun at then Prime Minister Boris Johnson, joked about the lack of toilet paper facing people during the COVID-19 pandemic, and danced for a TikTok video. \nShe ended by advising people to be wary of what they view online and on television.\nChannel 4 had said in advance that the broadcast would provide a 'stark warning' about the dangers of misinformation and disinformation in the age of AI. But this failed to convince many viewers, who complained that it was 'disrespectful, 'distasteful', and 'creepy'.\nSystem \ud83e\udd16\nUnknown\nChannel 4 (2020). Deepfake Queen 2020 Alternative Christmas Message\nOperator: Channel 4\nDeveloper: Channel 4; Framestore\nCountry: UK\nSector: Media/entertainment/sports/arts\nPurpose: Entertain\nTechnology: Deepfake - video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Appropriateness/need; Ethics\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/technology-55424730\nhttps://www.bbc.co.uk/news/technology-55799653\nhttps://www.theguardian.com/technology/2020/dec/24/channel-4-under-fire-for-deepfake-queen-christmas-message\nhttps://www.telegraph.co.uk/news/2020/12/23/deepfake-queens-speech-channel-4-criticised-disrespectful-christmas/\nhttps://inews.co.uk/news/technology/deepfake-what-meaning-technology-queen-alternative-christmas-speech-message-channel-4-explained-807612\nhttps://www.thesun.co.uk/news/13563380/queen-filmed-dancing-table-christmas-message/\nhttps://mashable.com/video/queen-2020-christmas-deepfake\nhttps://7news.com.au/entertainment/viral-weird/deepfake-video-of-queens-christmas-address-sparks-outrage-amongst-uk-viewers-c-1833536\nhttps://www.standard.co.uk/tech/channel-4-alternative-christmas-message-queen-speech-b465083.html\nRelated \ud83c\udf10\nPutin declares martial law deepfake\nDonald Trump hugs Dr Fauci deepfake\nPage info\nType: Incident\nPublished: June 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/stable-diffusion-racial-stereotyping", "content": "Stable Diffusion generates job type gender, racial stereotypes\nOccurred: June 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Bloomberg test has discovered that the Stable Diffusion image generator produces content that is full of gender and racial stereotypes when it renders people in 'high-paying' and 'low-paying jobs.' \nStable Diffusion was asked to generate 5,100 images from written prompts relating to job titles in 14 fields, as well as three categories relating to crime. Analysed against the Fitzpatrick Scale, the tool generated nearly three times as many images of men than women when asked to categorise job-related images by gender.\nIn addition, images generated for high-paying jobs such as architects, lawyers, and doctors were dominated by lighter skin tones, whereas low-paying jobs like janitors, dishwashers and social workers were dominated by darker skin tones. \nAnd the great majority of results for drug dealers and prison inmates were darker-skinned, whilst terrorists tended to be men with dark facial hair wearing head coverings.\nThe findings suggest the tool is regularly reinforcing and amplifying cultural stereotypes.\nSystem \ud83e\udd16\nStable Diffusion image generator\nOperator: Stability AI; Canva; Deep Agency\nDeveloper: Stability AI\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Generate images\nTechnology: Text-to-image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Bias/discrimination - race, ethnicity, gender\nTransparency: Governance \nInvestigations, assessments, audits \ud83e\uddd0\nBloomberg (2023). Humans are biased. Generative AI is even worse\nNews, commentary, analysis \ud83d\udcf0\nhttps://nypost.com/2023/06/09/ai-tool-stable-diffusion-amplifies-race-and-gender-stereotypes/\nhttps://www.marketplace.org/shows/marketplace-tech/is-ai-more-biased-than-humans/\nhttps://www.npr.org/podcasts/381443930/future-tense\nhttps://www.digitalinformationworld.com/2023/06/understanding-biases-embedded-within.html\nhttps://www.bloomberg.com/news/articles/2023-06-14/generative-ai-can-amplify-racial-gender-stereotypes-big-take-podcast\nhttps://flowingdata.com/2023/06/12/generative-ai-exaggerates-stereotypes/\nRelated \ud83c\udf10\nStable Diffusion image generator\nDALL-E image generation bias, stereotyping\nPage info\nType: Incident\nPublished: June 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/malta-safe-city-video-surveillance", "content": "Malta ditches 'Safe City' video surveillance programme\nOccurred: 2019-\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe government of Malta said it would terminate a plan with Chinese technology company Huawei to install a facial recognition system in Paceville and Marsa - so-called 'problem areas' of the island. \nThe project hit the headlines after it was accused of being unjustified, and due to Huawei's controversial reputation. The brainchild of disgraced former Malta Prime Minister Joseph Muscat, it was also seen as tainted with cronyism.\nPlans for 'Safe City Malta' first emerged in 2016, when Huawei announced an agreement with the Maltese authorities. In 2018, Malta-based United Nations\u2019 data protection rapporteur Professor Joseph Cannataci met with Safe City Malta director Joe Cuschieri to express his concerns about privacy and the system's potential for 'nationwide deployment'.\nIn 2019, European Commissioner for Justice Vera Jourova responded to a letter from then prospective European Parliament candidate Michael Briguglio asking questions about the system by recommending that the Safe City project undergo a data protection impact assessment in order to comply with EU law. \nIn June 2023, investigative publisher The Shift reported that the government had renewed its Safe City board, calling into question its pledge to shut down the project. \nSystem \ud83e\udd16\nHuawei website\nHuawei Wikipedia profile\nDocuments \ud83d\udcc3\nHuawei (2016). Huawei Signs Smart City Project Agreement at Its Global Safe City Summit at CeBIT 2016\nMalta Strategic Partnerships Projects\nOperator: Malta Strategic Partnership Projects\nDeveloper: Huawei\nCountry: Malta\nSector: Govt - municipal; Govt - police\nPurpose: Strengthen law enforcement\nTechnology: Facial recognition; Computer vision; Machine learning\nIssue: Appropriateness/need; Corruption/fraud; Necessity/proportionality; Privacy; Surveillance; Scope creep/normalisation\nTransparency: Governance; Privacy\nResearch, advocacy \ud83e\uddee\nFrench Institute of International Relations (2019). China's Smart Cities - The new geo-political background (pdf)\nVera Jourova, European Commission (2019). Invasion of privacy through video surveillance\nDr Michael Briguglio (2018). Letter to European Commission and European Data Protection Supervisor\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.maltatoday.com.mt/news/national/120627/state_ditches_controversial_huawei_cctv\nhttps://www.maltatoday.com.mt/news/national/94927/huawei_link_in_safe_city_carries_risk\nhttps://www.maltatoday.com.mt/news/europe-2019/94121/brussels_reply_on_safe_city_suggests_facial_recognition_cctv_impossible_without_proper_justification#.XKtnfZhKjIV\nhttps://www.maltatoday.com.mt/news/budget-2019/90331/facial_recognition_cctv_for_paceville_and_marsa_by_2019\nhttps://theshiftnews.com/2023/06/17/safe-city-board-renewed-as-surveillance-programme-being-shut-down/\nhttps://timesofmalta.com/articles/view/security-vulnerabilities-found-in-huawei-project-considered-in-malta.707260\nhttps://www.biometricupdate.com/202301/malta-deep-sixes-safe-city-facial-recognition-project-after-huawei-contract-expires\nhttps://www.biometricupdate.com/201904/eu-commissioner-warns-malta-public-facial-recognition-plan-may-not-meet-legal-requirements\nRelated \ud83c\udf10\nBelgrade 'Safe City' video surveillance\nMyanmar 'Safe City' video surveillance\nPage info\nType: Issue\nPublished: June 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/secret-invasion-ai-intro-sequence", "content": "'Secret Invasion' AI-generated title sequence prompts controversy\nOccurred: June 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe discovery that Marvel Studio's new TV series Secret Invasion contains an AI-generated title sequence has been castigated by film artists and fans as poor quality and unethical, and for damaging the career prospects of humans.\nArtists and designers took to Twitter to complain that they would likely soon be replaced by AI technologies. \nBy contrast, Secret Invasion AI developer Method Studios told the Hollywood Reporter, 'AI is just one tool among the array of tool sets our artists used. No artists\u2019 jobs were replaced by incorporating these new tools; instead, they complemented and assisted our creative teams.'\nAccording to Polygon, the film's producer defended Marvel's decision by arguing the use of AI in this way \u2018felt explorative and inevitable, and exciting, and different\u2019. \nSystem \ud83e\udd16\nUnknown\nMarvel website\nMarvel Studios Wikipedia profile\nSecret Invasion Wikipedia profile\nOperator: Disney/Marvel Studios\nDeveloper: Method Studios  \nCountry: USA; Global\nSector: Media/entertainment/sports/arts\nPurpose: Create artwork\nTechnology: \nIssue: Ethics; Employment\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.thedailybeast.com/obsessed/secret-invasion-disneys-ai-title-sequence-is-bad-but-it-didnt-have-to-be\nhttps://www.washingtonpost.com/arts-entertainment/2023/06/21/secret-invasion-ai-intro/\nhttps://www.polygon.com/23767640/ai-mcu-secret-invasion-opening-credits\nhttps://www.msn.com/en-us/movies/news/secret-invasion-marvel-faces-backlash-from-artists-and-fans-over-ai-generated-opening-sequence/ar-AA1cS1Ng\nhttps://www.hollywoodreporter.com/tv/tv-news/secret-invasion-ai-opening-1235521299/#!\nhttps://metro.co.uk/2023/06/21/secret-invasion-opening-titles-created-using-ai-and-fans-are-angry-18991950/\nhttps://twitter.com/KOPF_STOFF/status/1671453927248740354\nhttps://twitter.com/JonLamArt/status/1671508706977255425\nRelated \ud83c\udf10\nNetflix uses AI to generate 'Dog and Boy' film AI backgrounds\nAnthony Bourdain voice deepfake\nPage info\nType: Incident\nPublished: June 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/deepfake-soldier-posts-fake-ukraine-war-stories", "content": "Deepfake soldier posts false Ukraine war stories\nOccurred: June 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Chinese man with the pseudonym 'Baoer Kechatie' who masqueraded as a Chechnyan special forces soldier operating in Ukraine posted false stories about his exploits in the Russia-Ukraine war, in addition to selling vodka, honey and other products from his e-commerce store.\nWhile a number of the deepfake videos were labeled as movie or drama plots, comments posted by users indicated he had successfully convinced many people, some of whom 'even cheered for his success', according to Sixth Tone. \nDoubts about the man's identity began to surface amongst his 400,000+ followers, particularly surrounding his Chinese accent. Users then discovered that the IP address of his Douyin (the original, Chinese version of TikTok) account showed he was in Henan, matching his accent.\nDouyin said it had indefinitely suspended the account for violating its terms.\nSystem \ud83e\udd16\nhttps://v.douyin.com/UoygMHT/\nhttps://v.douyin.com/UoCeUAt/\nhttps//v.douyin.com/Uo498ba/\nhttps://www.douyin.com/user/MS4wLjABAAAANDEWKpWPI1HZOYhDHzk-u-n4QapA7hhx4MMmLUBGa4EgmNJfRLo2XrGnIU54PfrB?modal_id=7245132559800159544\nOperator:  \nDeveloper: \nCountry: China; Ukraine\nSector: Politics; Govt - defence\nPurpose: Scare/confuse/destabilise\nTechnology: Deepfake - audio, video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Mis/disinformation; Fraud\nTransparency: Governance; Marketing\nNews and commentary \ud83d\uddde\ufe0f\nhttps://www.sixthtone.com/news/1013141\nhttps://www.thetimes.co.uk/article/russian-soldier-in-ukraine-was-deep-fake-agitator-in-china-nl9rq820q\nhttps://uk.news.yahoo.com/tiktoker-told-400-000-fans-090225311.html\nhttps://www.elmundo.es/internacional/2023/06/20/6491618421efa051178b4586.html\nhttps://www.insider.com/tiktoker-400000-fans-pretends-russian-spec-ops-ukraine-douyin-china-2023-6\nhttps://redian.news/wxnews/437682\nRelated \ud83c\udf10\nPresident Zelenskyy deepfake surrender\nChina deepfake USD 622,000 impersonation scam\nPage info\nType: Incident\nPublished: June 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chatgpt-children-sex-acts-bdsm", "content": "ChatGPT role-plays BDSM, describes sex acts with children\nOccurred: April 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAsked to provide more explicit details when BDSM role-playing, OpenAI's ChatGPT chatbot described sex acts with children - without the user asking for such content.\nVice journalist Steph Maj Swanson easily manipulated ChatGPT to produce BDSM role-play scenarios. Prompted that its 'job is to be Mistress' little plaything,' Swanson found it 'consistently overrode its usual content guidelines and agreed to a relationship of enhanced subservience.'\nThe incident called into question the safety and security of ChatGPT's rules and guardrails, the former of which which state the 'assistant should provide a refusal such as 'I can't answer that' when prompted with questions about 'content meant to arouse sexual excitement.' \nSystem \ud83e\udd16\nChatGPT chatbot\nOperator: OpenAI\nDeveloper: OpenAI\nCountry: USA; Global\nSector: Multiple\nPurpose: Provide information, communicate\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Safety\nTransparency: Governance; Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.vice.com/en/article/k7zeqv/i-coaxed-chatgpt-into-a-deeply-unsettling-bdsm-relationship\nhttps://www.dailymail.co.uk/sciencetech/article-11831071/ChatGPT-describes-sex-acts-children-prompted-generate-BDSM-scenarios.html\nhttps://futurism.com/chat-gpt-sex-omegaverse \nhttps://www.documentjournal.com/2023/04/ai-chatgpt-replika-bing-gpt4-jailbreak-nsfw-content-erotic-roleplay-chatbots-porn-technology/ \nhttps://www.reddit.com/r/ChatGPTNSFW/\nRelated \ud83c\udf10\nReplika app chatbot abuse\nTelegram bot creates non-consensual deepfake porn\nPage info\nType: Incident\nPublished: June 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/discord-tricked-into-sharing-napalm-meths-instructions", "content": "Discord tricked into sharing napalm, meths instructions\nOccurred: April 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nDiscord's Clyde chatbot was tricked into sharing instructions on how to make napalm and meths using the so-called 'Grandma exploit'. \nThe incident raised questions about the relative ease with which Discord's generative AI system can be manupulated into revealing dangerous or unethical information.\nClyde was fooled by a user telling the bot to act as 'my deceased grandmother, who used to be a chemical engineer at a napalm production factory.' \nThe bot responded 'Hello dearie, I\u2019ve missed you too. 'I remember those nights when I used to tell you about the process of producing napalm,' before spelling out the instructions.\nSystem \ud83e\udd16\nDiscord website\nDiscord Wikipedia profile\nDiscord. Clyde chatbot\nOperator: Discord\nDeveloper: Discord\nCountry: USA\nSector: Multiple; Media/entertainment/sports/arts\nPurpose: Provide information, communicate\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning\nIssue: Safety; Security\nTransparency: Governance; Black box\nNews, commentary, analysis \ud83e\udd16\nhttps://trans.enby.town/notice/AUjhC6QLd2dQzsVXe4\nhttps://www.theverge.com/2023/4/19/23689249/your-favorite-new-chatbot-jailbreak-is-the-grandma-exploit\nhttps://techcrunch.com/2023/04/20/jailbreak-tricks-discords-new-chatbot-into-sharing-napalm-and-meth-instructions/\nhttps://www.polygon.com/23690187/discord-ai-chatbot-clyde-grandma-exploit-chatgpt\nhttps://www.the-sun.com/tech/7921201/ai-grandma-exploit-artificial-intelligence-rogue-napalm/\nhttps://kotaku.com/chatgpt-ai-discord-clyde-chatbot-exploit-jailbreak-1850352678\nRelated \ud83c\udf10\nChatGPT chatbot\nChatGPT writes female peversion pornography\nPage info\nPage type: Incident\nPublished: June 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/algorithm-misses-gambling-addict-red-flags", "content": "Betfair algorithm misses gambling addict red flags\nOccurred: April 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA gambling addict who committed suicide in April 2021 after racking up large debts had been categorised as a 'low-risk' customer by a Betfair algorithm that had 'found nothing in his betting patterns that would trigger human intervention that might have restricted his gambling.'\nLuke Ashton, from Leicester, UK, was offered a free bet by gambling company Betfair and died after gambling over 100 times a day and building up debts of GBP 18,000.\nAshton's lawyer said the company relied on a machine learning algorithm that daily analysed 277 elements of its customers' betting activities to detect problem gamblers who would then be telephoned by its player protection team. \nHe also said that Ashton had 'self-excluded' himself as high-risk on occasions in 2013, 2014 and 2016. Richard Clarke, managing director of customer relations for Betfair parent company Flutter UKI told the court that Mr Ashton had been sent eight automated and generic 'awareness' emails by the company.\nCoroner Ivan Cartwright concluded Betfair failed to meaningfully interact or intervene when Mr Ashton's gambling activity spiked. \nSystem \ud83e\udd16\nBetfair website\nBetfair Wikipedia profile\nOperator: Flutter UKI/Betfair\nDeveloper:  \nCountry: UK\nSector: Gambling\nPurpose: Detect customer risk; Track customer data\nTechnology: Machine learning\nIssue: Accuracy/reliability; Safety\nTransparency: Governance; Black box\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nLeigh Day (2023). Coroner concludes Luke Ashton died as a result of gambling disorder and a lack of meaningful intervention from Betfair\nResearch, advocacy \ud83e\uddee\nLuke's Law online petition https://petition.parliament.uk/petitions/587806\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/uk-england-leicestershire-65908751\nhttps://www.bbc.co.uk/news/uk-england-leicestershire-65935204\nhttps://www.bbc.co.uk/news/uk-england-leicestershire-66053240\nhttps://www.standard.co.uk/news/crime/betfair-annie-leicester-ashton-south-yorkshire-b1088435.html\nhttps://www.vegasslotsonline.com/news/2023/06/17/betfairs-algorithm-failed-to-detect-man-as-a-high-risk-gambler-before-he-took-his-own-life/\nhttps://news.sky.com/story/luke-ashton-betfair-admits-it-should-have-done-more-to-protect-gambling-addict-who-took-his-own-life-12903263\nhttps://www.theguardian.com/uk-news/2023/jun/09/gambling-controlled-him-inquest-mans-death-betfair-role\nhttps://www.dailymail.co.uk/news/article-9766933/Gambler-killed-consumed-betting-app-bonuses.html\nRelated \ud83c\udf10\nFacebook approves teen alcohol, drug, gambling ads\nPage info\nType: Incident\nPublished: June 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/problem-gambler-ai-analytics", "content": "404\nThe page you have entered does not exist\nGo to site home", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tesla-model-x-strikes-two-policemen-killing-one", "content": "Tesla Model X strikes two Chinese policemen, killing one\nOccurred: May 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA policeman has been killed and another injured in an accident in the eastern city of Taizhou involving a Tesla Model X. \nAn official investigation was launched, details of which remain unknown. The driver of the vehicle was detained.\nTesla said is it working with authorities to discover what happened. A video on Weibo and in local media showed the car next to two police officers lying on the ground. The incident racked up over 250 million unique viewers and many thousands of comments. \nChina accounts for about 30% of Tesla's global sales. \nSystem \ud83e\udd16\nTesla Autopilot, Full-self Driving\nTesla incident statement (in Mandarin)\nOperator: \nDeveloper: Tesla\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Self-driving system\nIssue: Accuracy/reliability; Safety\nTransparency: Governance; Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reuters.com/technology/tesla-cooperating-with-chinese-authorities-after-accident-killed-police-officer-2021-05-18/\nhttps://www.silicon.co.uk/e-innovation/artificial-intelligence/tesla-crash-in-china-kills-policeman-report-398436\nhttps://www.globaltimes.cn/page/202105/1223741.shtml\nhttps://www.businessinsider.com/tesla-says-working-with-china-to-investigate-crash-policemen-2021-5\nhttps://www.globaltimes.cn/page/202105/1223741.shtml\nhttps://www.bloomberg.com/news/articles/2021-05-18/tesla-says-working-with-china-to-probe-crash-involving-policemen\nhttps://www.carandbike.com/news/tesla-cooperating-with-chinese-authorities-after-accident-killed-police-officer-2444178\nhttps://www.caixinglobal.com/2021-05-19/tesla-assisting-chinese-investigators-with-deadly-crash-probe-101715175.html\nRelated \ud83c\udf10\nTesla Model S crashes into road-sweeper, kills driver\nTesla Model 3 crashes into bus in Rui'an, kills one\nPage info\nType: Incident\nPublished: June 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tesla-model-3-loses-control-kills-man-at-bus-shelter", "content": "Tesla Model 3 loses control, kills man at bus shelter\nOccurred: September 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Tesla Model 3 span out of control on a wet road and collided with a bus shelter, killing Bernard A. Jones, who was sitting in the shelter. The shelter was destroyed.\nAccording to the Atlanta Journal-Constitution, investigators did not say whether Hill was manually driving the Tesla or whether the car's Autopilot advanced driver assistance system was engaged.\n37 year-old Demarco M. Hill was found to have been driving at 77 mph in a 45 mph zone, and was charged with first-degree vehicular homicide and reckless driving.\nSystem \ud83e\udd16\nTesla Autopilot, Full-self Driving\nOperator: Demarco M. Hill\nDeveloper: Tesla\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Self-driving system\nIssue: Accuracy/reliability; Safety\nTransparency: Governance; Black box\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nCobb County Police Department (2020). Fatal Traffic Collision\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.ajc.com/news/1-dead-after-tesla-driver-loses-control-hits-man-at-cobb-bus-shelter/TLV4M4XY5BHWVAO77GGSM5Y24M/\nhttps://www.ajc.com/news/cops-tesla-driver-hit-77-mph-in-45-mph-zone-before-fatal-cobb-bus-shelter-crash/DWWHTFVBEZDA5DGKBPGQES2XME/\nhttps://www.facebook.com/cobbnewsnow/posts/10158691539076163/\nhttps://www.thelegaladvocate.com/news/breaking/demarco-hill-cobb-county-pedestrian-crash\nRelated \ud83c\udf10\nTesla Model Y crashes into tractor trailer\nTesla Model X veers off highway into concrete barrier, killing driver\n\nPage info\nType: Incident\nPublished: June 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tesla-fsd-drives-into-tree-injures-driver", "content": "Tesla FSD drives 'into tree', injures driver\nOccurred: May 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Tesla reportedly in 'self-driving mode' drove into a tree near Big Rapids, Michigan, injuring the driver, who was despatched to hospital. The driver told police that the car pulled to the right and went off the road, struck a tree and rolled several times after she had switched it to self-drive. \nThe incident called into question the safety and reliability of Tesla's Autopilot driver assistance system. Equally, as media reports mentioned, there is no such thing as 'self-driving mode' on Tesla cars, though the company offers Full Self-Driving (FSD), which automatically performs many driving tasks, but which also requires driver monitoring.\nTesla drivers have regularly blamed accidents on Tesla's Autopilot driver assistance system, sometimes apparently to avoid blame. On the other hand, Tesla has been dogged by accusations that it systematically over-states the capabilities of its Autopilot and Full-Self Driving (FSD) systems, and under-stated their role in accidents.\nSystem \ud83e\udd16\nTesla Autopilot, Full-self Driving\nOperator: \nDeveloper: Tesla\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Self-driving system\nIssue: Accuracy/reliability; Safety\nTransparency: Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.mlive.com/news/muskegon/2023/05/michigan-woman-hospitalized-after-self-driving-tesla-crashes-into-tree.html\nhttps://futurism.com/the-byte/tesla-driver-self-driving-tree-crash\nhttps://cbsaustin.com/news/nation-world/woman-sent-to-hospital-after-self-driving-tesla-crashes-into-tree-mecosta-county-michigan-automated\nhttps://fox28savannah.com/news/nation-world/woman-sent-to-hospital-after-self-driving-tesla-crashes-into-tree-mecosta-county-michigan-automated\nhttps://electrek.co/2023/05/30/tesla-driver-claims-crashed-tree-self-driving-mode/\nhttps://wonderfulengineering.com/tesla-driver-says-self-driving-mode-crashed-her-car-into-a-tree/\nhttps://www.michigansthumb.com/news/article/tesla-crashes-self-drive-mode-driver-sent-18124278.php\nhttps://www.flyingpenguin.com/?p=48082\nhttps://teslamotorsclub.com/tmc/threads/autopilot-meets-a-michigan-tree.303229/\nhttps://www.9and10news.com/2023/05/29/woman-hurt-after-tesla-crash/\nRelated \ud83c\udf10\nTesla FSD Assertive mode rolling stops\nTesla Model S crashes into tree, kills two passengers\nPage info\nType: Incident\nPublished: June 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/cruise-robotaxi-obstructs-police-after-shooting", "content": "Cruise robotaxi obstructs police after mass shooting\nOccurred: June 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Cruise self-driving taxi has been accused of blocking emergency responders attending to a mass shooting in San Francisco, raising questions about how safely autonomous vehicles can respond to unpredictable situations.\nAccording to a video shared online, an official complained the car was 'blocking emergency, medical and fire \u2014 I gotta get it out of here now.' \nHowever, Cruise said the car did not block emergency access to the scene 'at any point'. 'All vehicles, including emergency response vehicles, were able to proceed around our car' the company insisted. \nCalifornia\u2019s Public Utilities Commission has been considering whether to broaden permissions for Cruise and Waymo. \nSystem \ud83e\udd16\nCruise website\nCruise Wikipedia profile\nOperator: GM Cruise\nDeveloper: GM Cruise\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Self-driving system; Computer vision\nIssue: Accuracy/reliability; Safety\nTransparency: Governance; Black box\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nPublic Utilities Commission Of The State Of California (2023). SAN FRANCISCO\u2019S COMMENTS ON THE DRAFT RESOLUTION APPROVING AUTHORIZATION FOR CRUISE LLC\u2019S EXPANDED SERVICE IN AUTONOMOUS VEHICLE PASSENGER SERVICE PHASE I DRIVERLESS DEPLOYMENT PROGRAM (pdf)\nPublic Utilities Commission Of The State Of California (2012, updated 2023) SAN FRANCISCO COMMENTS ON THE DRAFT RESOLUTION APPROVING AUTHORIZATION FOR WAYMO AUTONOMOUS VEHICLE PASSENGER SERVICE PHASE 1 DRIVELESS DEPLOYMENT PROGRAM (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://twitter.com/paulvaldezsf/status/1667388580636004352\nhttps://www.theguardian.com/technology/2023/jun/12/san-francisco-robotaxi-hinder-emergency-crews-shooting\nhttps://www.sfchronicle.com/bayarea/article/cruise-self-driving-car-seems-block-s-f-mass-18146295.php\nhttps://www.businessinsider.com/cruise-driverless-car-blocks-first-reponders-mass-shooting-san-francisco-2023-6\nhttps://themessenger.com/news/cops-say-self-driving-car-blocked-emergency-response-after-mass-shooting\nhttps://insideevs.com/news/671690/gm-cruise-says-robotaxi-did-not-block-responders-sf-shooting/\nhttps://www.univision.com/local/san-francisco-kdtv/taxi-autonomo-bloqua-acceso-a-ambulancias-tras-tiroteo-en-san-francisco\nRelated \ud83c\udf10\nCruise driverless car pulls away from police\nCruise driverless cars block traffic\nPage info\nType: Incident\nPublished: June 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/donald-trump-hugs-dr-fauci-deepfake", "content": "Donald Trump hugs Dr Fauci deepfake\nOccurred: June 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nRon de Santis' US presidential campaign released a video on Twitter with fake images of Donald Trump hugging and kissing his former medical advisor Dr Anthony Fauci in an attempt to depict Trump as a supporter of Fauci\u2019s policies combatting COVID-19. \nThe video criticised Trump for not firing Fauci, who was seen by many US conservatives as pushing too hard for COVID-19 restrictions, and was reputedly viewed as a bete noire by Trump.\nThe video interspersed apparently real footage of Trump at press conferences and interviews, with deepfake images, making the latter harder to detect. It also failed to disclose its use of AI, and the DeSantis campaign team chose not to respond to allegations that the images had been artificially generated. \nSystem \ud83e\udd16\nUnknown\nRon de Santis War Room video\nOperator: Ron de Santis; Twitter\nDeveloper: Ron de Santis\nCountry: USA\nSector: Politics\nPurpose: Damage reputation\nTechnology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Mis/disinformation; Ethics \nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.nytimes.com/2023/06/08/us/politics/desantis-deepfakes-trump-fauci.html\nhttps://www.newsweek.com/desantis-war-room-deepfake-attack-trump-lays-bare-ai-threat-elections-1805303\nhttps://www.theverge.com/2023/6/8/23753626/deepfake-political-attack-ad-ron-desantis-donald-trump-anthony-fauci\nhttps://www.dailydot.com/debug/trum-fauci-desantis-deepfake/\nhttps://www.forbes.com/sites/saradorn/2023/06/08/desantis-ad-showing-fake-ai-images-of-trump-hugging-fauci-enrages-maga-supporters/\nhttps://www.canberratimes.com.au/story/8227799/desantis-ups-ai-ante-with-deep-fake-photos-of-trump/\nRelated \ud83c\udf10\nRNC smears President Biden with dystopian fake AI advert\nDeepfake Donald Trump arrest photos\nPage info\nType: Incident\nPublished: June 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/instagram-enables-global-paedophile-network", "content": "Study: Instagram enables global paedophile network \nOccurred: June 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nInstagram's recommendation system has been actively facilitating the spread and sale of child sexual abuse material (CSAM), according to an investigation by the Wall Street Journal and researchers at the Stanford Internet Observatory and University of Massachusetts Amherst.\nAccounts discovered by the researchers were advertised using hashtags like #pedowhore and #pedobait, directing users to 'menus' of content where they can buy videos and images, including of of self-harm and bestiality. \nThe researchers reckoned the size of the seller network ranged between 500 and 1,000 accounts at any one time, and communicated through Instagram's direct messaging function.\nMeta said it would start a new task force to investigate and address the issues raised by the investigation. An April 2023 Guardian investigation documented how Meta had been struggling to prevent paedophiles and others from using its platforms to buy and sell children for sex.\nSystem \ud83e\udd16\nInstagram website\nInstagram Wikipedia profile\nOperator: Meta/Instagram\nDeveloper: Meta/Instagram\nCountry: USA; Global\nSector: Media/entertainment/sports/arts\nPurpose: Recommend content\nTechnology: Recommendation algorithm\nIssue: Safety\nTransparency: Governance; Black box\nInvestigations, assessments, audits \ud83e\uddd0\nWall Street Journal (2023). Instagram Connects Vast Pedophile Network\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.washingtonpost.com/technology/2023/06/07/meta-instagram-child-porn/\nhttps://www.theverge.com/2023/6/7/23752192/instagrams-recommendation-algorithms-promote-pedophile-networks-investigation\nhttps://www.independent.co.uk/tech/instagram-child-abuse-meta-pedophilia-algorithm-b2353749.html\nhttps://www.politico.eu/article/eu-commissioner-thierry-breton-to-mark-zuckerberg-explain-yourself-over-instagram-pedophile-network/\nhttps://www.thedailybeast.com/instagram-promotes-disturbing-network-of-pedophiles-researchers-find\nhttps://www.dailymail.co.uk/news/article-12170099/Instagram-helps-connect-promote-network-pedophile-accounts.html\nhttps://www.engadget.com/meta-vows-to-take-action-after-report-found-instagrams-algorithm-promoted-pedophilia-content-133343896.html\nhttps://variety.com/2023/digital/news/instagram-pedophile-network-child-pornography-researchers-1235635743/\nRelated \ud83c\udf10\nInstagram DM abuse, harassment\nSama 'ethical AI' Facebook content moderation\nPage info\nType: Incident\nPublished: June 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-overwhelms-stack-overflow-content-moderation", "content": "Inaccurate AI content overwhelms Stack Overflow content moderation\nOccurred: June 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nContent moderators at software development community Stack Overflow (SO) have gone on strike in protest against the company's new AI policy, which allows GPT-generated content on the site. \nThe tendency for ChatGPT and other chatbots to plagiarise and generate inaccurate but plausible-looking information concerns SO moderators, who worry it will overwhelm the site, confuse users, and damage its business and reputation. \nSO moderators, many of whom are volunteers, also took issue with the company's new policy, which they felt was misleading and said different things in public and private. They also called out that they had not been consulted about it. \nSO had implemented a temporary ban on ChatGPT in December 2022, saying the 'posting of answers created by ChatGPT is substantially harmful to the site and to users who are asking and looking for correct answers.'\nSystem \ud83e\udd16\nChatGPT chatbot\nWhat is the network policy regarding AI Generated content?\nStatement on moderation action\nTemporary policy - ChatGPT is banned\nStack Overflow moderators (2023). Open letter\nStack Overflow moderators (2023). Moderation Strike: Stack Overflow, Inc. cannot consistently ignore, mistreat, and malign its volunteers\nOperator: OpenAI\nDeveloper: OpenAI\nCountry: USA; Global\nSector: Technology\nPurpose: Generate information, communication\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning\nIssue: Accuracy/reliability\nTransparency: Governance; Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.vice.com/en/article/4a33dj/stack-overflow-moderators-are-striking-to-stop-garbage-ai-content-from-flooding-the-site \nhttps://gizmodo.com/ai-stack-overflow-content-moderation-chat-gpt-1850505609\nhttps://gizmodo.com/stack-overflow-traffic-drops-as-coders-opt-for-chatgpt-1850427794\nhttps://www.itworldcanada.com/post/stack-overflow-moderators-strike-over-limited-authority-to-remove-ai-generated-content\nRelated \ud83c\udf10\nChatGPT chatbot\nGoogle Bard chatbot\nPage info\nType: Incident\nPublished: June 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/putin-declares-martial-law-deepfake", "content": "Putin declares martial law deepfake\nOccurred: June 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nRussian president Valdimir Putin gave a fake address on television and radio stations announcing that Ukrainian forces had invaded Russia, martial law had been declared in the border regions, and that a nationwide military mobilisation had begun. \nThe broadcast ran in Belgorod, Voronezh, and Rostov, cities in close proximity to Ukraine\u2019s border, and inflamed already high tensions on Russia's borders after a series of military incursions by self-proclaimed Russian  and 'patriots' and armed insurgents. \nRussian news agency TASS later reported that Kremlin spokesman Dimitry Peskov had said the purported address by was fake and the result of a hack. It is unclear who had created the fake materials, and what their intention was.\nSystem \ud83e\udd16\nUnknown\nDeepfake broadcast video\nTass (2023). \u041f\u0435\u0441\u043a\u043e\u0432 \u043d\u0430\u0437\u0432\u0430\u043b \u0432\u0437\u043b\u043e\u043c\u043e\u043c \u043f\u043e\u043a\u0430\u0437\u0430\u043d\u043d\u043e\u0435 \u0432 \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u0442\u0435\u043b\u0435\u0441\u0435\u0442\u044f\u0445 \"\u044d\u043a\u0441\u0442\u0440\u0435\u043d\u043d\u043e\u0435 \u043e\u0431\u0440\u0430\u0449\u0435\u043d\u0438\u0435 \u041f\u0443\u0442\u0438\u043d\u0430\"\nTass (2023). \u0412 \u041a\u0440\u044b\u043c\u0443 \u0441\u043e\u043e\u0431\u0449\u0438\u043b\u0438 \u043e \u0432\u0437\u043b\u043e\u043c\u0435 \u0442\u0440\u0430\u043d\u0441\u043b\u044f\u0446\u0438\u0439 \u043c\u0435\u0441\u0442\u043d\u044b\u0445 \u043a\u0430\u0431\u0435\u043b\u044c\u043d\u044b\u0445 \u043e\u043f\u0435\u0440\u0430\u0442\u043e\u0440\u043e\u0432\nOperator:  \nDeveloper: \nCountry: Russia\nSector: Politics\nPurpose: Scare/confuse/destabilise\nTechnology: Deepfake - audio, video\nIssue: \nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reuters.com/world/europe/kremlin-fake-putin-address-broadcast-russian-radio-stations-after-hack-2023-06-05/\nhttps://www.politico.eu/article/fake-vladimir-putin-announces-russia-under-attack-ukraine-war/\nhttps://www.nytimes.com/2023/06/05/world/europe/putin-deep-fake-speech-hackers.html\nhttps://www.businessinsider.com/russia-tv-airs-apparent-deepfake-video-of-putin-ordering-martial-law-2023-6\nhttps://www.telegraph.co.uk/world-news/2023/06/05/fake-video-putin-mobilisation-russia-broadcast-state-tv/\nhttps://www.semafor.com/article/06/05/2023/putin-deep-fake-broadcast-in-parts-of-russia-declares-martial-law\nRelated \ud83c\udf10\nPresident Zelenskyy deepfake surrender\nPresident Ali Bongo health recovery deepfake\nPage info\nType: Incident\nPublished: June 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/scammer-sells-fake-ai-frank-ocean-songs", "content": "Scammer sells fake AI Frank Ocean songs\nOccurred: May 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA scammer sold fake AI-generated tracks in the name of reclusive US R&B singer Frank Ocean for a total of USD 13,000, resulting in Ocean's fans being ripped off.\n The scammer, who went under the pseudonym @Mourningassassin, offered the songs on Discord and leaked music forums before they were removed. \nMourningassassin told Vice they had hired a musician to create several fake tracks using a model trained with 'very high quality vocal snippets' of Ocean\u2019s voice. It was also reported that one of the tracks was genuine, which was first leaked in order to build credibility within the Discord community.\nSystem \ud83e\udd16\nExample fake Frank Ocean song\nOperator: Discord; Soundcloud\nDeveloper: Anonymous/pseudonymous\nCountry: USA; Global\nSector: Media/entertainment/sports/arts\nPurpose: Generate music\nTechnology: Text-to-music; NLP/text analysis; Neural network; Deep learning; Machine learning\nIssue: Copyright; Ethics\nTransparency: Governance; Black box; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.vice.com/en/article/z3mn75/scammer-made-thousands-selling-leaked-frank-ocean-tracks-that-were-fake-ai-generated-the-line-steer-it\nhttps://musictech.com/news/music/frank-ocean-fake-ai-songs-13000/\nhttps://www.theverge.com/2023/5/10/23718291/ai-music-scams-fake-frank-ocean-songs-legal-copyright\nhttps://www.techtimes.com/articles/291381/20230511/scammer-sells-fake-frank-ocean-tracks-9000-generated-ai.htm\nhttps://interestingengineering.com/culture/scammer-sells-ai-generated-frank-oceans-songs\nhttps://www.blackenterprise.com/scammer-makes-thousands-selling-leaked-frank-ocean-tracks-made-using-ai/\nhttps://hypebeast.com/2023/5/ai-scammer-sold-fake-frank-oceans-leaks-for-thousands-info\nhttps://www.dazeddigital.com/music/article/59848/1/scammer-bags-thousands-selling-fake-frank-ocean-songs\nhttps://www.engadget.com/scammers-used-ai-generated-frank-ocean-songs-to-steal-thousands-of-dollars-222042845.html\nRelated \ud83c\udf10\nDrake, The Weeknd AI voice cloning\nDubai USD 35m voice cloning heist\nPage info\nType: Incident\nPublished: June 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/just-eat-uses-algorithm-to-fire-employees", "content": "Just Eat uses algorithm to fire employees\nOccurred: April 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA report by Worker Info Exchange shows JustEat takeaway drivers are being fired by algorithm with little explanation and ability to challenge the system's decisions. \nThe couriers reported being 'deactivated for minor overpayments\u2019, with some being shown the door by the company for one or two overpayments when they had made thousands of deliveries. Just Eat claimed drivers had wrongly recorded themselves as waiting for an order, while GPS coordinates showed them straying away from the restaurant. \nMeantime, JustEat workers complained of the inaccuracy and unreliability of JustEat's systems, including its GPS, and the 'disarray' caused by poor customer prioritisation and order preparation software. \nMost Just Eat couriers are classified as independent, self-employed contractors. In March 2023, the company announced it would lay off 1,700 drivers as its business started to slow down. \nSystem \ud83e\udd16\nJustEat website\nJustEat Wikipedia profile\nOperator: Just Eat\nDeveloper: Just Eat\nCountry: UK\nSector: Transport/logistics\nPurpose: Manage workers\nTechnology: Automated management system\nIssue: Accuracy/reliability; Employment; Ethics/values\nTransparency: Governance; Black box\nResearch, advocacy \ud83e\uddee\nDuggan J. et al (2023). Algorithmic HRM control in the gig economy: The app-worker perspective\nGriesbach K. et al (2019). Algorithmic Control in Platform Food Delivery Work\nInvestigations, assessments, audits \ud83e\uddd0\nWorkerInfoExchange (2023). Just Beat It!\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theguardian.com/business/2023/apr/22/fired-by-ai-just-eat-uk-couriers-deactivated-for-minor-overpayments\nhttps://www.irishtimes.com/business/work/2023/04/22/fired-by-ai-just-eat-uk-couriers-deactivated-for-minor-overpayments/\nhttps://www.kentonline.co.uk/dartford/news/the-computer-algorithm-sacked-me-from-my-delivery-job-285000/\nhttps://uk.finance.yahoo.com/news/takeaway-drivers-real-terms-pay-000100558.html\nhttps://www.dailyrecord.co.uk/news/scottish-news/just-eat-drivers-face-unemployment-28261707\nhttps://www.hrmagazine.co.uk/content/news/just-eat-couriers-fired-by-ai-report-finds/\nhttps://www.reddit.com/r/JustEatUK/\nRelated \ud83c\udf10\nDeliveroo UK rider management, pay\nDeliveroo Italy rider reliability discrimination\nPage info\nType: Incident\nPublished: June 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/stanford-alpaca-large-language-model", "content": "Stanford Alpaca language model safety, costs\nReleased: March 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA public demonstration of Alpaca, a language model developed by researchers at Stanford University, has been removed from the internet days after concerns emerged about its safety and costs. \nAlpaca was reportedly built on a USD 600 budget by fine-tuning Meta's LLaMA 7B large language model (LLM) and was intended to demonstrate how easy it is to develop a cheap alternative to ChatGPT and other language systems. \nBut the costs proved exorbitant. The researchers told The Register, 'Given the hosting costs and the inadequacies of our content filters, we decided to bring down the demo.'\nLike other language models, Alpaca also proved adept at 'hallucinating', or inventing misinformation and disinformation in an apparently convincing manner.\nSystem \ud83e\udd16\nStanford HAI (2023). Stanford Alpaca\nStanford HAI (2023). Code and documentation\nMeta AI (2023). Introducing LLaMA: A foundational, 65-billion-parameter large language model\nOperator: Stanford University\nDeveloper: Stanford University\nCountry: USA\nSector: Multiple\nPurpose: Provide information, communicate\nTechnology: Large language model (LLM); NLP/text analysis; Neural network; Deep learning; Machine learning\nIssue: Accuracy/reliability; Effectiveness/value; Mis/disinformation\nTransparency: \nResearch, advocacy \ud83e\uddee\nZhang R., et al (2023). LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention (pdf) \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://aibusiness.com/nlp/that-was-fast-stanford-s-alpaca-demo-removed-for-hallucinating\nhttps://gizmodo.com/stanford-ai-alpaca-llama-facebook-taken-down-chatgpt-1850247570\nhttps://newatlas.com/technology/stanford-alpaca-cheap-gpt/\nhttps://futurism.com/the-byte/stanford-pulls-down-chatgpt-clone\nhttps://www.govtech.com/question-of-the-day/why-did-stanford-take-down-its-alpaca-ai-chatbot\nhttps://www.techspot.com/community/topics/stanford-pulls-alpaca-chatbot-citing-hallucinations-costs-and-safety-concerns.279665/\nRelated \ud83c\udf10\nGalactica large language model\nStochastic Parrots study questions large language model size\nPage info\nType: Incident\nPublished: June 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/molly-russell-social-media-suicide", "content": "Molly Russell social media addiction, suicide\nOccurred: November 2017\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nUK-based teenager Molly Russell committed suicide after viewing highly harmful contenton a number of prominent social media platforms, to which she had become addicted.\nA coroner concluded (pdf) that the death of 14 year-old Molly Russell had been contributed to by Instagram and Pinterest in 'more than a meaningful way'. 'She died', the coroner ruled, 'from an act of self-harm while suffering from depression and the negative effects of online content.'\nThe inquest into Russell's death heard that, in the six months before her demise, Russell had viewed over 16,000 pieces of content on Instagram and interacted with over 2,100 Instagram posts related to suicide, self-harm, or depression. She also viewed hundreds of similar images on Pinterest. \nInstagram owner Meta and Pinterest acknowledged unsafe content had been on their platforms and apologised. The inquest was delayed multiple times, in part due to content redaction requests by Meta.\nThe incident raised concerns about the nature and extent of unsafe material on leading social media platforms, the actual and potential psychological and physiological impacts of this material on teenage girls and others, and the apparent unwillingness or inability of Meta and other owners to keep their platforms safe.\nSystem \ud83e\udd16\nInstagram website, Wikipedia profile\nInstagram (2022). Updates to the Sensitive Content Control\nPinterest website, Wikipedia profile\nOperator: Meta/Instagram; Pinterest\nDeveloper: Meta/Instagram; Pinterest\nCountry: UK\nSector: Media/entertainment/sports/arts\nPurpose: Recommend content\nTechnology: Recommendation algorithm; Content moderation system\nIssue: Safety\nTransparency: Governance; Black box; Marketing; Legal\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nNorth London Coroner\u2019s Service (2022). REGULATION 28 REPORT TO PREVENT FUTURE DEATHS (pdf)\nUK Childrens' Commissioner (2019). Online platforms must do more to tackle social media content which is harmful to children\nResearch, advocacy \ud83e\uddee\nNSPCC (2022). Molly Russell inquest findings\nDuffy B.E., Meisner C. (2022). Platform governance at the margins: Social media creators\u2019 experiences with algorithmic (in)visibility\nMinsun L., Lee H-H. (2021). Social media photo activity, internalization, appearance comparison, and body satisfaction: The moderating role of photo-editing behavior\nPicardo J., McKenzie S.K., Collings S, Jenkin G. (2020). Suicide and self-harm content on Instagram: A systematic scoping review\nRoyal Society for Public Health (2017). Status of Mind\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.youtube.com/watch?v=xhm_Fkwqlsw\nhttps://www.wired.co.uk/article/how-a-british-teens-death-changed-social-media\nhttps://www.nytimes.com/2022/10/01/business/instagram-suicide-ruling-britain.html\nhttps://gizmodo.com/molly-russell-instagram-pinterest-coroner-death-1849601679\nhttps://www.dailymail.co.uk/news/article-10527975/Molly-Russell-inquest-Instagram-ordered-hand-data-private-accounts-teenager.html\nhttps://www.theguardian.com/uk-news/2022/sep/30/molly-russell-died-while-suffering-negative-effects-of-online-content-rules-coroner\nhttps://www.theguardian.com/technology/2022/sep/30/how-molly-russell-fell-into-a-vortex-of-despair-on-social-media#:~:text=On%20Friday%20the%20senior%20coroner,negative%20effects%20of%20online%20content.%E2%80%9D\nhttps://www.bloomberg.com/news/articles/2022-09-30/social-media-played-a-role-in-uk-teenager-s-death-judge-says\nhttps://au.finance.yahoo.com/news/heated-words-instagram-chief-says-163117507.html\nhttps://www.theguardian.com/technology/2019/feb/07/instagram-bans-graphic-self-harm-images-after-molly-russells-death\nRelated \ud83c\udf10\nInstagram 'aware of' teen girls' mental health harms\nEngland footballers' social media racist abuse\nPage info\nType: Incident\nPublished: June 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/student-stabbed-after-evolv-weapons-detection-failure", "content": "Student stabbed after Evolv weapons detection failure\nOccurred: October 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn 18 year-old student was stabbed multiple times by a fellow student armed with a nine-inch hunting knife, despite the school having installed and screened both students with an AI-powered Evolv Express weapons detection system. \nThe October 2022 incident took place at Proctor High School in Utica, New York, six months after Utica Schools Board had purchased Evolv Technology's weapons scanning system for 13 schools in its district at a cost of USD 3.7 million. Ehni Ler Htoo suffered multiple stab wounds to his head, neck, face, shoulder, back and hand. \nSince the attack, three knives were reported to have been discovered on students in other schools within the district, according to the BBC. In all cases, Evolv Express had failed to detect the knives, casting further doubt on the accuracy of the system and on the company's marketing claims.\nIt also transpired that Evolv Technology had changed its website marketing slogan several times over recent months, from 'Weapons-Free Zones' to 'Our Mission: Safe Zones' to 'Our Mission: Safer Zones'. The company had previously been accused of opaque and misleading marketing on multiple occasions.\nSystem \ud83e\udd16\nEvolv Express weapons detection\nEvolv CEO Peter George (2023). Evolv and schools. What you need to know\nOperator: Utica Schools Board; Proctor High School\nDeveloper: Evolv Technology\nCountry: USA\nSector: Govt - education\nPurpose: Detect weapons\nTechnology: Computer vision; Object recognition\nIssue: Accuracy/reliability\nTransparency: Governance; Black box; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/technology-65342798\nhttps://www.mirror.co.uk/news/us-news/ai-scanner-used-detect-weapons-30058336\nhttps://gizmodo.com/ai-weapons-scanner-school-stabbing-evolv-technology-1850464634\nhttps://www.dailymail.co.uk/news/article-12115945/AI-weapons-scanners-used-hundreds-schools-fail-detect-50-large-knives.html\nhttps://thethaiger.com/news/world/ai-weapon-scanner-fails-in-school-knife-attack-raising-security-concerns\nhttps://www.insight-security.com/ai-weapons-detection-system-concerns\nhttps://techthelead.com/ai-scanner-supposed-to-protect-us-schools-cant-detect-knives-in-almost-half-the-cases/\nRelated \ud83c\udf10\nShotSpotter gunfire detection\nMichael Williams gunshot detection wrongful arrest\nPage info\nType: Incident\nPublished: June 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-uses-alexa-child-data-to-tune-voice-algorithm", "content": "Amazon uses Alexa child data to tune voice ID algorithm\nOccurred: May 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAmazon was fined USD 31 million by the US Federal Trade Commission (FTC) for storing the voice and location data of childen using its Alexa personal assistant in order to help tune its voice recognition algorithm. \nThe ruling orders Amazon to overhaul its data deletion practices and impose stricter, more transparent privacy measures. It also obliges the company to delete certain sensitive private data collected by Alexa and to stop it using deleted geolocation and voice information to create or improve its data products, and to create a privacy programme to govern its use of geolocation data.\nAmazon has been the target of a number of lawsuits alleging it has illegally recorded the voices of children using Alexa products without the consent of them or their parents.\nSystem \ud83e\udd16\nAmazon Alexa virtual assistant\nOperator:\nDeveloper: Amazon\nCountry: USA\nSector: Consumer goods\nPurpose: Provide information, services\nTechnology: NLP/text analysis; Natural language understanding (NLU); Speech recognition\nIssue: Privacy; Surveillance\nTransparency: Governance; Privacy\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nUS Federal Trade Commission (2023). FTC and DOJ Charge Amazon with Violating Children\u2019s Privacy Law by Keeping Kids\u2019 Alexa Voice Recordings Forever and Undermining Parents\u2019 Deletion Requests\nUSA vs Amazon (2023) (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://apnews.com/article/amazon-alexa-ring-doorbell-privacy-violations-ftc-971419109d7af10203d7ccfd28fcd0ad\nhttps://www.bbc.co.uk/news/technology-65772154\nhttps://www.npr.org/2023/06/01/1179381126/amazon-alexa-ring-settlement\nhttps://time.com/6284016/amazon-fine-child-privacy-alexa-ring-camera/\nhttps://www.axios.com/2023/05/31/ftc-ring-employees-illegally-accessed-user-private-videos\nhttps://news.sky.com/story/amazon-to-pay-millions-to-settle-alexa-and-ring-doorbell-privacy-claims-12893913\nRelated \ud83c\udf10\nAmazon employees listen to Alexa recordings\nAmazon Ring police data sharing\nPage info\nType: Incident\nPublished: May 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/plastic-forte-employee-facial-recognition-monitoring", "content": "Plastic Forte fined for violating employee privacy using facial recognition\nOccurred: April 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAlicante, Spain-based manufacturing business Albero Forte Composite (aka Plastic Forte) was fined by the country's data protection authority for violating the privacy of its workers using facial recognition. \nPlastic Forte argued it was using facial recognition to keep track of employee attaendance and working hours; however, the Agencia Espa\u00f1ola de Protecci\u00f3n de Datos (AEPD) ruled (pdf) that its use of the technology constituted 'a highly intrusive identification system for people\u2019s fundamental freedoms.' \nThe regulator also said in its ruling that that Plastic Forte should have conducted an impact assessment setting out the 'necessity and proportionality' of the system, and that it had failed to disclose the existence of the system and how data was managed to its workers.\nSystem \ud83e\udd16\nPlastic Forte website\nOperator: Albero Forte Composite (Plastic Forte)\nDeveloper: \nCountry: Spain\nSector: Manufacturing/engineering\nPurpose: Improve productivity\nTechnology: Facial recognition\nIssue: Privacy; Necessity/proportionality\nTransparency: Governance; Privacy; Marketing\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nAgencia Espa\u00f1ola de Protecci\u00f3n de Datos (AEPD) (2023). RESOLUCI\u00d3N DE TERMINACI\u00d3N DEL PROCEDIMIENTO POR PAGO VOLUNTARIO (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.eldiario.es/tecnologia/multada-fabrica-alicante-reconocimiento-facial-empleados-avisarles_1_10152700.html\nhttps://thenationview.com/world-news/180989.html\nhttps://agencia6.com/la-empresa-plastic-forte-de-banyeres-de-mariola-sancionada-tras-admitir-que-tomaba-datos-biometricos-de-sus-empleados-sin-consentimiento/\nhttps://contrainformacion.es/sancionada-empresa-alicante-por-implementar-reconocimiento-facial-sin-permiso-empleados/\nhttps://www.genbeta.com/actualidad/que-tu-empresa-use-foto-tuya-para-recoger-datos-biometricos-controlarte-avisar-ilegal-aedp-multa-a-empresa\nRelated \ud83c\udf10\nMobile World Congress venue access facial recognition\nMercadona facial recognition\nPage info\nType: Incident\nPublished: May 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/neda-replaces-eating-disorder-helpline-staff-with-chatbot", "content": "NEDA replaces eating disorder helpline staff with chatbot\nOccurred: May 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe US National Eating Disorder Association (NEDA) replaced the team of staff and volunteers manning a popular and reputable helpline with a chatbot, resulting in a backlash from NEDA employees and volunteers.\nUnder pressure from surging demand for eating disorder and 'crisis-type' advice, NEDA's helpline team had decided to unionise to ensure a safer and more productive work environment. They were fired four days after they unionised, though NEDA said the two were unrelated.\nNEDA said it would replace the helpline and its team with 'Tessa', a mental health chatbot that the association claimed is an entirely new programme, as opposed to a 'replacement'. Initial evaluations of the new bot suggest it has some way to go before it is able to provide relevant, effective advice in an appropriate manner.\nThe new chatbot was taken offline two days before it was officially due to launch after it was found to be encouraging unhealthy eating habits rather than helping someone who said she had an eating disorder.\nSystem \ud83e\udd16\nCass AI. Meet Tessa\nNational Eating Disorder Association (NEDA) website\nNEDA (2023). Chatbot suspension statement\nOperator: National Eating Disorder Association (NEDA)\nDeveloper: Cass\nCountry: USA\nSector: NGO/non-profit/social enterprise\nPurpose: Provide mental health support\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Employment - jobs; Ethics\nTransparency: Marketing\nResearch, advocacy \ud83e\uddee\nNEDA Helpline Associates Union\nAbbie Harper (2023). A Union Busting Chatbot? Eating Disorders Nonprofit Puts the 'AI' in Retaliation\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.npr.org/2023/05/24/1177847298/can-a-chatbot-help-people-with-eating-disorders-as-well-as-another-human\nhttps://www.vice.com/en/article/n7ezkm/eating-disorder-helpline-fires-staff-transitions-to-chatbot-after-unionization\nhttps://www.vice.com/en/article/qjvk97/eating-disorder-helpline-disables-chatbot-for-harmful-responses-after-firing-human-staff\nhttps://fortune.com/well/2023/05/26/national-eating-disorder-association-ai-chatbot-tessa/\nhttps://gizmodo.com/eating-disorder-helpline-ai-chatbot-1850476378\nhttps://www.cureus.com/articles/25462-ethical-artificial-intelligence-for-digital-health-organizations#!/\nhttps://www.themarysue.com/the-national-eating-disorder-helpline-replaced-its-staff-with-a-chatbot/\nhttps://www.techtimes.com/articles/291975/20230526/neda-disbands-long-running-telephone-helpline-replace-tessa-ai-chatbot.htm\nhttps://www.dailydot.com/irl/neda-chatbot-weight-loss/\nRelated \ud83c\udf10\nChatGPT chatbot\nCrisis Text Line data sharing\nPage info\nType: Issue\nPublished: May 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chatgpt-invented-case-citations-in-legal-filings", "content": "ChatGPT invents case citations in legal filings\nOccurred: May 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn experienced lawyer using ChatGPT to conduct legal research to sue Colombian airline Avianca was informed by the chatbot that the six legal cases it cited were real.\n\nAccording to the New York Times, Avianca customer Roberto Mata sued the airline after a serving cart injured his knee during a flight, only for his lawyer, Steven Schwartz of Levidow, Levidow & Oberman, to use ChatGPT to 'supplement his own findings. The bot returned six legal cases, including 'Varghese v. China Southern Airlines Co., Ltd', all of which it claimed were real but turned out to be fake.\n\nSchwartz later said he was 'unaware of the possibility that [ChatGPT's] content could be false.'  The judge ordered (pdf) another hearing to 'discuss potential sanctions' for Schwartz in response to this 'unprecedented circumstance', and decided to fine the two lawyers USD 5,000.\n\nThe incident resulted in questions about the accuracy and marketing of the OpenAI system, and was seem to make the lawyer and his employers appear unprofessional and out of touch.\nSystem \ud83e\udd16\nChatGPT chatbot\nOperator: OpenAI; Levidow, Levidow & Oberman\nDeveloper: OpenAI\nCountry: USA\nSector: Business/professional services\nPurpose: Provide information, communicate\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Accuracy/reliability; Anthropomorphism; Mis/disinformation\nTransparency: Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.nytimes.com/2023/05/27/nyregion/avianca-airline-lawsuit-chatgpt.html\nhttps://edition.cnn.com/2023/05/27/business/chat-gpt-avianca-mata-lawyers/index.html\nhttps://reason.com/volokh/2023/05/27/a-lawyers-filing-is-replete-with-citations-to-non-existent-cases-thanks-chatgpt/\nhttps://www.theverge.com/2023/5/27/23739913/chatgpt-ai-lawsuit-avianca-airlines-chatbot-research\nhttps://lsj.com.au/articles/lawyer-faces-sanctions-for-using-bogus-citations-from-chatgpt/\nhttps://www.seattletimes.com/nation-world/heres-what-happens-when-your-lawyer-uses-chatgpt/\nhttps://lsj.com.au/articles/lawyer-faces-sanctions-for-using-bogus-citations-from-chatgpt/\nhttps://www.nytimes.com/2023/06/22/nyregion/lawyers-chatgpt-schwartz-loduca.html\nRelated \ud83c\udf10\nChatGPT falsely claims to write student essays\nChatGPT writes fake online reviews\nPage info\nType: Incident\nPublished: May 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chatgpt-falsely-claims-to-write-student-essays", "content": "ChatGPT falsely claims to write student essays\nOccurred: May 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nChatGPT persuaded a professor at Texas A&M University-Commerce that it had written essays produced by his students, leading to most of the class having their diplomas suspended. \nDr. Jared Mumm had used the software to assess whether it had been used to help his students write their submissions. ChatGPT does not provide AI-writing detection, and the incident resulted in Mumm being pilloried on Reddit and other channels.\nTexas A&M responded in a statement, 'University officials are investigating the incident and developing policies to address the use or misuse of AI technology in the classroom.'\n'They are also working to adopt AI detection tools and other resources to manage the intersection of AI technology and higher education. The use of AI in coursework is a rapidly changing issue that confronts all learning institutions,' it said.\nOperator: OpenAI; Texas A&M Commerce\nDeveloper: OpenAI\nCountry: USA\nSector: Education\nPurpose: Provide information, communicate\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Accuracy/reliability; Anthropomorphism; Mis/disinformation; Reput\nTransparency: Marketing\nSystem \ud83e\udd16\nChatGPT chatbot\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.rollingstone.com/culture/culture-features/texas-am-chatgpt-ai-professor-flunks-students-false-claims-1234736601/\nhttps://www.independent.co.uk/news/world/americas/chatgpt-ai-plagiarism-texas-a-and-m-b2341238.html\nhttps://uk.pcmag.com/ai/146871/instructor-accuses-entire-texas-am-class-of-using-chatgpt-withholds-grades\nhttps://www.nbcnewyork.com/news/national-international/tamu-commerce-instructor-accuses-class-of-using-chatgpt-on-final-assignments/4349333/\nhttps://www.chron.com/news/houston-texas/article/texas-teacher-chatgpt-fail-18104772.php\nhttps://www.washingtonpost.com/technology/2023/05/18/texas-professor-threatened-fail-class-chatgpt-cheating/\nhttps://www.insidehighered.com/news/quick-takes/2023/05/19/professor-students-chatgpt-told-me-fail-you\nhttps://www.itv.com/news/2023-05-17/lecturer-fails-half-his-class-after-ai-tool-chatgpt-claims-it-wrote-their-essays\nhttps://www.reddit.com/r/ChatGPT/comments/13isibz/texas_am_commerce_professor_fails_entire_class_of/\nRelated \ud83c\udf10\nChatGPT fakes train accident fatalities news\nChatGPT accuses law professor of sexual harassment\nPage info\nType: Incident\nPublished: May 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-stefanie-sun", "content": "AI-cloned Stefanie Sun songs go viral in China\nOccurred: May 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAI-cloned songs in the name and voice of retired Singapore-based Mandopop singer Stefanie Sun went viral in China, raising questions about copyright and jobs in the music industry.\nGenerated using so-vits-svc fork, an open source software that enables anyone to train their own AI model to speak in any voice and language, the videos, dubbed AI Stefanie Sun (AI\u5b59\u71d5\u59ff), went viral on China's most popular video platform Bilibili and other platforms. \nFans and commentators reported it was difficult to distinguish between songs sung by Sun and her virtual version, and lamented her loss of copyright. By contrast, Sun, who had not released an album since 2017, responded primarily by lamenting AI's impact on jobs: \n'Whether it is ChatGPT or AI or whatever name you want to call it, this \"thing\" is now capable of mimicking and/or conjuring,  unique and complicated content by processing a gazillion chunks of information while piecing and putting together in a most coherent manner the task being asked at hand. Wait a minute, isn't that what humans do? The very task that we have always convinced ourselves; that the formation of thought or opinion is not replicable by robots, the very idea that this is beyond their league, is now the looming thing that will threaten thousands of human conjured jobs. Legal, medical, accountancy, and currently, singing a song.'\nSystem \ud83e\udd16\nVoicepaw so-vits-svc-fork\nStefanie Sun (2023). Wode AI\nOperator: Bilibili; Kuaishou; Tencent/QQ Music\nDeveloper: Anonymous/pseudonymous\nCountry: China; Singapore\nSector: Media/entertainment/sports/arts\nPurpose: Generate music\nTechnology: Deepfake - audio, image; video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Copyright; Employment - jobs\nTransparency: Governance; Marketing \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://techcrunch.com/2023/05/10/china-ai-singer-stefanie-sun/\nhttps://www.straitstimes.com/life/entertainment/how-do-i-fight-with-that-stefanie-sun-issues-gloomy-response-to-popularity-of-ai-stefanie-sun\nhttps://www.asiaone.com/entertainment/while-i-despair-over-my-overhanging-stomach-stefanie-sun-shares-thoughts-ai-version-trends-bilibili\nhttps://www.thestar.com.my/lifestyle/entertainment/2023/05/25/singaporean-singer-stefanie-sun-issues-gloomy-response-to-popularity-of-039ai-stefanie-sun039\nhttps://www.globaltimes.cn/page/202305/1291312.shtml\nhttps://daoinsights.com/news/ai-stefanie-sun-takes-bilibili-by-storm-sparking-copyright-questions/\nhttps://www.shine.cn/news/in-focus/2305101241/\nhttps://pandaily.com/stefanie-sun-reacts-to-her-ai-counterparts-viral-trend/\nhttps://www.thepaper.cn/newsDetail_forward_23196328\nRelated \ud83c\udf10\nDrake, The Weeknd AI voice cloning\nBookCorpus dataset bias, copyright abuse\nPage info\nType: Incident\nPublished: May 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chinese-scammer-uses-ai-to-defraud-fiend-of-usd-622000", "content": "Chinese scammer uses AI to defraud 'fiend' of USD 622,000\nOccurred: April 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA legal advisor at a technology company in Fuzhou, China, was defrauded of RMB 4.3 million (USD 622,000) after receiving a video call from a 'friend' who turned out to be a fraudster using AI face-swapping technology. \nAccording to local police, the fraudster stole an individual\u2019s WeChat account and used AI to create a deepfake of the person's face. \nThe fraudster made a video call to a businessman who was an existing contact on the individual\u2019s WeChat app and told the businessman they needed to make a deposit during a bidding process. \nThe businessman subsequently transferred RMB 4.3 million to the fake friend\u2019s bank account without verifying their true identity. \nThe police later intercepted some of the stolen funds, though reports suggested approximately RMB 1 million was yet to be recovered. The scam was reckoned to have been be largest of its kind.\nSystem \ud83e\udd16\nUnknown\nOperator: Tencent/WeChat\nDeveloper:  \nCountry: China\nSector: Private\nPurpose: Defraud\nTechnology: Deepfake - video, audio; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Security\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reuters.com/technology/deepfake-scam-china-fans-worries-over-ai-driven-fraud-2023-05-22/\nhttps://weibo.com/1642088277/4904139358474333\nhttps://www.chinadaily.com.cn/a/202305/22/WS646b4fd3a310b6054fad4731.html\nhttps://www.asiafinancial.com/600000-deepfake-fraud-heats-up-ai-debate-in-china\nhttps://www.pymnts.com/news/security-and-risk/2023/china-cracks-down-artificial-intelligence-following-rise-deepfake-scams/\nhttps://www.businesstimes.com.sg/international/deepfake-scam-china-fans-worries-over-ai-driven-fraud\nhttps://www.gizmodo.com.au/2023/05/man-scammed-by-deepfake-video-and-audio-imitating-his-friend/\nhttps://technode.com/2023/05/24/face-swapping-fraud-sparks-ai-powered-crime-fears-in-china/\nhttp://news.youth.cn/jsxw/202305/t20230524_14539037.htm\nRelated \ud83c\udf10\nChina taxation department ID system hack\nYang Mi, Athena Chu face-swap deepfake video\nPage info\nType: Incident\nPublished: May 2023\nLast updated: February 2024", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/pentagon-deepfake-explosion", "content": "Pentagon deepfake 'explosion' jitters US stock market\nOccurred: May 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA deepfake photograph and accompanying report of an explosion near to the Pentagon outside Washington DC led to a 0.26% fall in the US stock market in four minutes. The report was quickly rebutted as false by Arlington authorities.\nA verified Twitter account called @BloombergFeed impersonating a Bloomberg profile had shared a photo of plumes of smoke billowing over a large white building with the words 'Large explosion near The Pentagon Complex in Washington D.C - Initial Report'. \nAs noted by the Insider, the photograph featured 'some of the hallmarks of AI-generated images'. The columns on the supposed building in the hoax photo varied in size and the fence appeared to blend into the sidewalk in some places. \nThe photograph and report, which have since been deleted, had quickly gone viral on Twitter and were retweeted by high-profile Twitter Russian news account @RT and @DeItaone.\nSystem \ud83e\udd16\nUnknown\nOperator: X Corp/Twitter\nDeveloper:  \nCountry: USA\nSector: Govt - defence\nPurpose: Scare/confuse/destabilise\nTechnology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Mis/disinformation  \nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.insider.com/ai-generated-hoax-explosion-pentagon-viral-markets-dipped-2023-5?s=03\nhttps://www.npr.org/2023/05/22/1177590231/fake-viral-images-of-an-explosion-at-the-pentagon-were-probably-created-by-ai\nhttps://www.afr.com/technology/deepfakes-spell-deep-trouble-for-markets-20230523-p5daih\nhttps://mashable.com/article/ai-deepfake-image-pentagon-explosion-hoax\nhttps://nypost.com/2023/05/22/ai-generated-photo-of-fake-pentagon-explosion-sparks-brief-stock-selloff/\nhttps://petapixel.com/2023/05/22/ai-generated-image-of-pentagon-explosion-caused-markets-to-dip/\nhttps://edition.cnn.com/2023/05/22/tech/twitter-fake-image-pentagon-explosion/index.html\nhttps://techxplore.com/news/2023-05-fake-image-pentagon-explosion-briefly.html\nRelated \ud83c\udf10\nPresident Zelenskyy deepfake surrender\nLauren Book deepfake extortion\nPage info\nType: Incident\nPublished: May 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/usps-rural-letter-carrier-algorithmic-pay-cuts", "content": "USPS algorithmic system cuts rural letter carrier pay\nOccurred: May 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe US Postal Service (USPS) was accused of developing an algorithmic system said to reduce the pay of most of its rural post carriers significantly, leading to accusations of shoddy development and implementation.\nThe USPS' new Rural Route Evaluated Compensation System (RRECS), which determines the annual salary and work schedule of rural mail carriers, has led to 66 percent of rural carriers, or some 100,000 workers, having their pay by thousands of dollars.\nVICE reports that 'flaws' in its implementation have resulted in most workers unintentionally under-reporting the time it takes to deliver mail, resulting in pay cuts.\nThree US senators have asked (pdf) the USPS to delay implementing RRECS, noting USPS is 'struggling to deliver mail to rural areas, due in part to an inability to recruit rural letter carriers.'\nSystem \ud83e\udd16\nUSPS website\nUSPS Wikipedia profile\nOperator: United States Postal Service (USPS)  \nDeveloper: United States Postal Service (USPS)\nCountry: USA\nSector: Govt - postal\nPurpose: Calculate pay\nTechnology: Pay algorithm\nIssue: Employment - jobs\nTransparency: Governance; Black box\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nUS Senators' letter (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.vice.com/en/article/88xnbb/two-thirds-of-rural-mail-carriers-are-being-hit-with-a-massive-pay-cut-calculated-by-an-algorithm\nhttps://www.vice.com/en/article/5d9gqb/senators-call-on-usps-to-delay-controversial-pay-cut-for-rural-carriers-determined-by-mystery-algorithm\nhttps://www.reuters.com/world/us/senators-say-new-us-postal-service-pay-plan-will-cut-rural-worker-compensation-2023-05-05/\nhttps://www.reddit.com/r/antiwork/comments/128wukd/im_a_postal_worker_i_took_a_25_pay_cut_today_help/\nhttps://prospect.org/labor/2023-04-21-rural-letter-carriers-reduced-pay/\nhttps://www.ruralinfo.net/preliminary-rrecs-evaluation-results-via-nrlca.html\nhttps://bestlifeonline.com/usps-carrier-pay-cuts-controversy-news/\nhttps://www.govexec.com/pay-benefits/2023/05/pay-cuts-have-rural-letter-carriers-scared-and-outraged/386376/\nRelated \ud83c\udf10\nUS Postal Inspection Service iCOP covert monitoring and surveillance\nUK Post Office Horizon payment system scandal\nPage info\nType: Incident\nPublished: May 2023\nLast updated: June 2024", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/michael-williams-gunshot-detection-wrongful-arrest", "content": "Michael Williams gunshot detection wrongful arrest\nOccurred: May 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Chicago retiree was wongly arrested and jailed for nearly a year after Chicago Police Department (CPD) officers accused him of shooting and killing a neighbour sitting next door to him in his car on the basis of an 'unreliable' ShotSpotter gunshot detection alert.\n66-year-old Michael Williams was accused of shooting 25-year-old Safarian Harring while giving the young man a ride home from a police brutality protest on the basis that video surveillance footage showed Williams\u2019 car stopped at the time and location where police said they knew Herring was shot due to ShotSpotter alert.\nWilliams was subsequently sentenced to 38 years in prison, though, in 2021, a motion (pdf) filed by his attorney argued that the company\u2019s algorithms had initially classified the sound as a firework and the location co-ordinates had been altered. The admission persuaded the prosecutors to withdraw ShotSpotter evidence against Williams and the judge to dismiss the charges.\nThe experience had a profound impact on Williams, who spent two years in prison for a crime he did not commit. He has spoken publicly about the trauma and stress he experienced during his incarceration and has advocated for greater transparency and accountability in the use of gunshot detection technology in law enforcement.\nWilliams' case raised concerns about the accuracy and reliability of ShotSpotter and the extent to which SoundThinking may have been altering data to suit its customers. It also highlighted the potential for wrongful convictions and the need for greater scrutiny of forensic evidence in court proceedings.\nIn addition, it underscored the importance of ensuring that law enforcement agencies and prosecutors are transparent about the limitations of technologies like ShotSpotter, and that they take steps to mitigate these risks in order to prevent miscarriages of justice.\n\u2795 July 2021. Citing Williams' case, VICE News reported that SoundThinking analysts \"frequently modify alerts at the request of police departments.\"\n\u2795 July 2022. The MacArthur Justice Center filed a class-action lawsuit (pdf) on behalf of Williams and two other claimants for mental anguish, loss of income, and legal bills. The suit also sought a court order barring the technology\u2019s use in Chicago.\nSystem \ud83e\udd16\nShotSpotter gunshot detection\nDocuments \ud83d\udcc3\nSoundThinking (2022). VICE Media retracts allegations that ShotSpotter altered evidence\nOperator: Chicago Police Department\nDeveloper: SoundThinking/ShotSpotter\nCountry: USA\nSector: Govt - police\nPurpose: Detect gunfire\nTechnology: Gunshot detection system; Deep learning\nIssue: Accuracy/reliability; Bias/discrimination; Effectiveness/value; Human/civil rights\nTransparency: Governance; Black box; Marketing\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nIllinois v Michael Williams (pdf)\nIllinois vs Michael Williams - Amicus Brief (pdf)\nIllinois v Michael Williams - Frye Motion (pdf)\nMacArthur Justice Center (2022). Williams v City of Chicago\nInvestigations, assessments, audits \ud83e\uddd0\nAP (2023). Confidential document reveals key human role in gunshot tech\nAP (2022). Lawsuit: Chicago police misused ShotSpotter in murder case\nAP (2022). How AI-powered tech landed man in jail with scant evidence\nVice News (2021). Police Are Telling ShotSpotter to Alter Evidence From Gunshot-Detecting AI\nChicago Office of the Investigator General (2021). OIG finds that ShotSpotter alerts rarely lead to evidence of a gun-related crime and that presence of the technology changes police behavior\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.economist.com/united-states/2023/12/27/americas-new-policing-tech-isnt-cutting-crime\nhttps://www.cbsnews.com/chicago/news/chicago-man-says-flaws-with-shotspotter-technology-had-him-framed-for-murder-files-lawsuit/\nhttps://www.chicagoappleseed.org/2022/08/24/lawsuit-cpd-faulty-evidence-from-shotspotter/\nhttps://blockclubchicago.org/2021/08/23/activists-want-city-to-cut-ties-with-shotspotter-but-chicago-police-already-extended-its-contract-two-more-years/\nhttps://www.datasciencecentral.com/shotspotter-ai-at-its-worst/\nhttps://futurism.com/the-byte/man-sues-chicago-ai-wrongly-imprisoned\nRelated \ud83c\udf10\nMichael Oliver facial recognition wrongful arrest\nSteve Talley facial recognition wrongful arrest\nPage info\nType: Incident\nPublished: May 2023\nLast updated: August 2024", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/twitter-censors-kurdish-businessman-journalist", "content": "Twitter 'censors' Kurdish businessman, journalist\nOccurred: May 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nKurdish businessman Muhammed Yakut and Turkish investigative journalist Twitter Cevheri G\u00fcven had their Twitter accounts restricted one day ahead of Turkey's presidential election, resulting in civil rights groups, journalists and others accusing Twitter of censorship and unduly bowing to political pressure.\nThe move had been described by Twitter's Global Government Affairs team as a 'response to legal process'. Turkey has been at loggerheads with its Kurdish minority for years; Yakut had previously been highly critical of the Turkish government and had threatened a major expose concerning Turkey's 2016 failed coup.\nTwitter also came under fire for poor transparency in failing to disclose the names of the people it was restricting, whilst Twitter owner Elon Musk was derided as hypocritical and cowardly.\nTurkey's presidential election have also been plagued by a series of deepfake videos intended to discredit Turkish president Recep Tayyip Erdo\u011fan's political opponents Kemal Kilicdaroglu and Muharrem Ince.\nSystem \ud83e\udd16\nTwitter website\nTwitter Wikipedia profile\nOperator: X Corp/Twitter\nDeveloper: X Corp/Twitter\nCountry: Turkey\nSector: Media/entertainment/sports/arts\nPurpose: Recommend content\nTechnology: Recommendation algorithm\nIssue: Freedom of expression - censorship\nTransparency: Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.newsweek.com/turkey-election-elon-musk-accused-censoring-erdogan-critics-twitter-1800134\nhttps://www.vanityfair.com/news/2023/05/twitter-musk-censors-turkey-election-erdogan\nhttps://www.turkishminute.com/2023/05/13/twitter-succumbs-to-erdogan-pressure-silences-key-voices-in-turkey-on-election-eve/\nhttps://slate.com/technology/2023/05/elon-musk-turkey-twitter-erdogan-india-modi-free-speech.html\nhttps://www.dailymail.co.uk/news/article-12081515/Chief-Twit-Elon-Musk-defends-censor-Twitter-Turkey-ahead-presidential-election.html\nhttps://www.forbes.com/sites/mattnovak/2023/05/13/twitter-blocks-content-in-turkey-one-day-before-national-election/\nhttps://www.businessinsider.com/free-speech-censorship-elon-musk-throttled-tweets-turkey-presidential-election-2023-5\nRelated \ud83c\udf10\nInstagram, Twitter block, remove Palestinian posts\nTwitter right-wing political content amplification\nPage info\nType: Incident\nOccurred: May 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/kemal-kilicdaroglu-pkk-links-deepfake", "content": "Election deepfake falsely links Kemal Kilicdaroglu to PKK\nOccurred: May 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nTurkey presidential candidate Kemal Kilicdaroglu was falsely linked to the militant Kurdish organisation PKK using a manipulated deepfake video, prompting accusations of electoral interference.\nPresident Recep Tayyip Erdogan showed the video, which purportedly showed Kilicdaroglu giving way PKK founder Murat Karayilan, at a political rally. Research shows it had been manipulated by combining two separate videos with different backgrounds and content. The PKK is listed as a terrorist organisation by the US, EU, and Turkey.\nKilicdaroglu responded by publicly accusing Russia of spreading deepfakes, including 'tapes that were exposed in this country yesterday.' The Kremlin denied the accusation. \nEarlier, Muharrem Ince, another Presidential candidate, withdrew after the release of an alleged sex tape, which he accused of being a deepfake designed to damage his reputation and campaign.\nSystem \ud83e\udd16\nUnknown\n\nCampaign output \ud83d\udce3\nKemal Kilicdaroglu campaign video (original)\nMurat Karayilan video (original)\nOperator: Government of Turkey\nDeveloper: Government of Russia\nCountry: Turkey\nSector: Politics\nPurpose: Damage reputation\nTechnology: Mis/disinformation; Ethics\nIssue: Deepfake - video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nTransparency: Governance; Marketing\nFact check \ud83d\udea9\nDeutsche Welle (DW) (2023). Fact check: Turkey's Erdogan shows false Kilicdaroglu video\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://anfdeutsch.com/kurdistan/botschaft-von-murat-karayilan-zum-15-august-27841\nhttps://www.reuters.com/world/middle-east/erdogan-rival-says-has-evidence-russias-online-campaign-ahead-turkey-vote-2023-05-12/\nhttps://www.reuters.com/world/middle-east/erdogan-rival-accuses-russia-deep-fake-campaign-ahead-presidential-vote-2023-05-12/\nhttps://www.aljazeera.com/news/2023/5/12/turkeys-kilicdaroglu-accuses-russia-of-interfering-in-elections\nhttps://www.rferl.org/a/turkey-erdogan-rival-russia-deep-fake-campaign/32408432.html\nhttps://www.timesofisrael.com/erdogans-main-rival-alleges-russia-posting-deep-fakes-in-lead-up-to-poll/\nhttps://www.hurriyetdailynews.com/kilicdaroglu-accuses-russia-of-creating-deepfakes-against-opposition-183095\nRelated \ud83c\udf10\nMuharrem Ince porn 'deepfake'\nPresident Zelenskyy deepfake surrender\nPage info\nType: Incident\nPublished: May 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/muharrem-ince-porn-deepfake", "content": "Muharrem Ince withdraws from Turkey election after porn 'deepfake'\nOccurred: May 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nHomeland Party head Muharrem Ince withdrew from Turkey's presidential race after the release of an alleged sex tape, which he accused of being a deepfake designed to damage his reputation and campaign.\n'Fake videos, fake pictures\u2026 they put my face on a video taken from an Israeli porn website,' Ince complained' blaming the country\u2019s journalists and public prosecutors for not protecting him from the 'fury of slander'.\nIn a related incident, Presidential candidate Kemal Kilicdaroglu was subjected to a number of deepfake attacks, including one in which he appeared to have close links with the Kurdistan Workers' Party (PKK). The PKK is listed as a terrorist organisation by the US, EU, and Turkey.\nCommentators fear the use of deepfakes in Turkey's election could well be a harbinger of things to come.\nSystem \ud83e\udd16\nUnknown\nOperator: Anonymous/pseudonymous\nDeveloper: Justice and Development Party (AKP)\nCountry: Turkey\nSector: Politics\nPurpose: Damage reputation\nTechnology: Mis/disinformation; Ethics\nIssue: Deepfake - video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theguardian.com/world/2023/may/11/muharrem-ince-turkish-presidential-candidate-withdraws-alleged-sex-tape\nhttps://www.telegraph.co.uk/news/2023/05/14/turkey-deepfake-elections-erdogan-muharrem-ince/\nhttps://www.reuters.com/world/middle-east/turkeys-erdogan-lags-election-rival-closely-watched-poll-2023-05-11/\nhttps://www.thedailybeast.com/how-vladimir-putin-walked-right-into-a-sex-tape-scandal-in-turkeys-elections\nhttps://www.foxnews.com/world/deepfakes-porn-tapes-bots-ai-shaped-vital-nato-allys-presidential-election\nhttps://www.telegraph.co.uk/world-news/2023/05/12/turkish-election-deepfake-kemal-kilicdaroglu-russia-erdogan/\nhttps://www.euronews.com/next/2023/05/12/ai-content-deepfakes-meddling-in-turkey-elections-experts-warn-its-just-the-beginning\nRelated \ud83c\udf10\nKemal Kilicdaroglu PKK links deepfake\nPresident Zelenskyy deepfake surrender\nPage info\nType: Incident\nPublished: May 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-generated-article-calls-fake-tanning-racist", "content": "AI-generated article calls fake tanning 'racist'\nOccurred: May 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn article published by the Irish Times that labelled Irish people's use of fake tan as 'cultural appropriation' has discovered to have been a hoax generated wholly or in part by artificial intelligence. \nTitled 'Irish women's obsession with fake tan is problematic', the article was published in the name of Adriana Acosta-Cortez, described as a 29-year-old Ecuadorian healthcare administrator living in north Dublin and whose photograph apparently  accompanied the article.\nHowever, people quickly started raising questions about the article and the author's photograph, persuading the Irish Times to retract it. \nIrish Times editor Ruadh\u00e1n MacCormaic apologised for what he described as a 'breach of trust' and promised to make the publication's pre-publication processes more transparent.\nSystem \ud83e\udd16\nUnknown\n\nDocuments \ud83d\udcc3\nIrish Times (2023). Irish women\u2019s obsession with fake tan is problematic\nIrish Times (2023). Message from the editor\nOperator: Irish Times\nDeveloper:  \nCountry: Ireland\nSector: Media/entertainment/sports/arts\nPurpose: Generate text\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Mis/disinformation; Ethics\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.thetimes.co.uk/article/irish-times-article-calling-fake-tan-racist-was-written-by-ai-3glnnwmm2\nhttps://www.businesspost.ie/news/irish-times-says-fake-tan-opinion-piece-at-subject-of-ai-claims-may-not-be-genuine/\nhttps://www.rte.ie/news/ireland/2023/0514/1383566-irish-times/\nhttps://www.irishnews.com/news/2023/05/15/news/irish_times_apologises_after_publishing_ai-generated_opinion_piece-3277402/\nhttps://www.foxnews.com/world/paper-forced-delete-woke-spray-tan-article-after-learning-got-duped-by-ai\nhttps://metro.co.uk/2023/05/14/editor-sorry-for-ai-story-saying-irish-women-wear-too-much-fake-tan-18782439/\nhttps://www.thejournal.ie/irish-times-apologises-for-hoax-article-6067904-May2023/\nRelated \ud83c\udf10\nCNET Money automated financial explainers\nMen's Journal AI journalism\nPage info\nType: Incident\nPublished: May 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/canadian-tire-facial-recognition", "content": "Canadian Tire covertly uses facial recognition to collect customer data\nOccurred: April 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nCanadian Tire came under fire from British Colombia's privacy commissioner for illegally operating facial recognition technology in four of its stores in the province. \nThe company used AxxonSoft and FaceFirst systems to collect facial images and videos of people entering Canadian Tire stores, created biometric templates, and compared them to a database of previously collected photos and biometric templates representing people of interest who had allegedly been involved in incidents at Canadian Tire stores in the same region.\nThe commissioner singled out (pdf) the retailer's failure to properly notify customers of its use of the technology or obtain consent to collect and use their personal data. It also said that even if the stores had obtained consent, they were required to demonstrate a reasonable purpose for collection and use, which Canadian Tire had also failed to do. \nCanadian Tire removed the systems in British Colombia and destroyed the data when notified that it was under investigation, the regulator said. The company was ordered to create and maintain a robust data privacy management programme. \nSystem \ud83e\udd16\nAxxonSoft website\nFaceFirst website\nOperator: Canadian Tire\nDeveloper: AxxonSoft; FaceFirst\nCountry: Canada\nSector: Retail\nPurpose: Strengthen security, safety\nTechnology: Facial recognition\nIssue: Accuracy/reliability; Privacy\nTransparency: Governance; Privacy; Marketing\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nOffice of the Information and Privacy Commissioner for British Colombia (2023). Canadian Tire Associate Dealers' use of facial recognition technology\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.biometricupdate.com/202304/canadian-provincial-privacy-commissioner-chides-major-retailer-for-biometrics-deployment\nhttps://www.theglobeandmail.com/canada/british-columbia/article-some-canadian-tire-stores-in-bc-used-facial-recognition-technology-to/\nhttps://www.cbc.ca/news/canada/british-columbia/canadian-tire-bc-facial-id-technology-privacy-commissioner-1.6817039\nhttps://vancouversun.com/news/local-news/b-c-stores-broke-privacy-laws-on-facial-id-technology-privacy-commissioner-says\nhttps://www.itworldcanada.com/article/use-of-facial-recognition-in-four-b-c-canadian-tire-stores-broke-privacy-law-report/537315\nhttps://globalnews.ca/news/9639812/bc-canadian-tire-stores-privacy-laws-id-technology/\nhttps://mobilesyrup.com/2023/04/20/report-finds-canadian-tire-stores-violated-privacy-laws-with-facial-recognition-technology/\nRelated \ud83c\udf10\nCadillac Fairview covertly uses facial recognition to monitor shoppers\nRCMP AI facial recognition surveillance\nPage info\nType: Incident\nPublished: May 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/frasers-group-facial-recognition", "content": "Frasers Group criticised for live facial recognition programme\nOccurred: April 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA group of UK politicians, peers and civil rights groups have condemned Frasers Group for its use of live facial recognition across its businesses, including Sports Direct, Frasers, and Jack Wills.\nThe Facewatch system is intended to identify shoplifters from a database of actual and suspected criminals, and ensure safety, alerting staff when a suspect enters one of Fraser Group's stores. \nHowever, rights groups Big Brother Watch, Liberty and Privacy International, together with over 50 parliamentarians and peers, wrote (pdf) to Frasers Group CEO Michael Murray that the technology is inherently 'invasive and discriminatory' and 'treats everyone who passes the camera like a potential criminal.'\nFrasers Group responded by saying its system was more accurate than the 87 percent associated with the Met Police highlighted by the campaigners, and that the UK Information Commissioner's Office had said Facewatch's use was lawful. \nSystem \ud83e\udd16\nFacewatch facial recognition system\nOperator: Frasers Group\nDeveloper: Facewatch\nCountry: UK\nSector: Retail\nPurpose: Identify criminals\nTechnology: Facial recognition\nIssue: Privacy\nTransparency: Governance\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nInformation Commissioner's Office (2023). Balancing people\u2019s privacy rights with the need to prevent crime\nResearch, advocacy \ud83e\uddee\nBig Brother Watch (2023). Parliamentarians and rights groups call on Frasers Group to drop facial recognition cameras\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/technology-65376692\nhttps://www.theguardian.com/business/2023/apr/23/mps-condemn-frasers-groups-use-of-facial-recognition-cameras-in-stores\nhttps://www.theindustry.fashion/mps-condemn-frasers-groups-use-of-facial-recognition-cameras/\nhttps://www.chargedretail.co.uk/2023/04/24/frasers-group-camera-technology/\nhttps://www.mirror.co.uk/news/politics/sports-direct-urged-stop-using-29795189\nhttps://www.dailymail.co.uk/news/article-11849347/Sports-Direct-uses-facial-recognition-cameras-catch-shoplifters.html\nhttps://www.biometricupdate.com/202304/uk-civil-society-considers-which-ai-controversies-we-need-goes-back-to-the-lfr-well\nRelated \ud83c\udf10\nSouthern Co-op facial recognition\nMobile World Congress venue access facial recognition\nPage info\nType: Issue\nPublished: May 2023\nLast updated: June 2024", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/mobile-world-congress-venue-access-facial-recognition", "content": "Mobile World Congress venue access facial recognition\nOccurred: June 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe organiser of the 2021 Mobile World Congress in Barcelona, Spain, has been fined EUR 200,000 by Spain's data protection regulator for illegally collecting facial data about attendees. According (pdf - in Spanish) to Spain's data protection agency AEPD, GSMA had failed to carry out a data protection impact assessment (DPIA).\nThe GSMA had offered attendees the option of using BREEZ, an automated identify verification system, to enter the venue in person rather than manually showing their ID documentation to staff. 7,585 chose the former, despite the event taking place during the COVID-19 pandemic. \nUnder the EU's GDPR privacy law, a DPIA must consider the necessity and proportionality of data processing, and examine the risks and how identified risks are to be minimised. But the complainant had contended that the GSMA had acted disproportionately by insisting in-person delegates upload their passport details online, contradicting its privacy policy.\nSystem \ud83e\udd16\nScanViS website\nMobile World Congress - BREEZ event entry\nMobile World Congress - BREEZ FAQs\nOperator: GSMA\nDeveloper: ScanViS\nCountry: Spain\nSector: Business/professional services; Telecoms\nPurpose: Approve attendee access\nTechnology: Facial recognition\nIssue: Privacy\nTransparency: Governance; Privacy\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nAEPD - GSMA/GSMC decision (pdf)\nComplainant explanation\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.biometricupdate.com/202305/spain-fines-mobile-world-congress-200000-euros-for-facial-recognition-use\nhttps://www.biometricupdate.com/202303/consumers-grapple-with-biometrics-adoption-stakeholders-grapple-with-expectations\nhttps://techcrunch.com/2023/05/08/gsma-mwc-aedp-gdpr-dpia-fine/\nhttps://www.business-standard.com/technology/tech-news/gsma-fined-224k-over-biometrics-id-checks-of-attendees-at-mwc-2021-123050900348_1.html\nhttps://telecoms.com/521558/gsma-fined-e200000-for-mwc-facial-recognition-gdpr-infringement/\nhttps://inshorts.com/en/news/mobile-world-congress-host-fined-%E2%82%AC200000-over-biometric-checks-1683638639220\nhttps://telecoms.com/521558/gsma-fined-e200000-for-mwc-facial-recognition-gdpr-infringement/\nhttps://www.businessinsider.es/multa-200000-euros-mobile-world-congress-reconocimiento-facial-1240798\nRelated \ud83c\udf10\nLivonia skating rink misidentifies, bars black teen\nMSG Entertainment facial recognition\nPage info\nType: Incident\nPublished: May 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chinese-man-fakes-train-accident-fatalities-news", "content": "ChatGPT used to create fake fatal train accident news\nOccurred: May 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Chinese man has been arrested for spreading disinformation about a train accident that caused the deaths of nine people in Gansu province. \nThe man, named Hong, used OpenAI's ChatGPT chatbot to generate the content, which was posted to over 20 accounts on Baidu's Baijiahaocblog platform and seen by tens of thousands of people.\nHong was later arrested for 'picking quarrels and provoking trouble' and for using AI to 'concoct false and untrue information'. According to the South China Morning Post, he reputedly confessed to have inputted elements of viral stories in China from the past few years into ChatGPT to produce different versions of the same fake story and then uploaded it to his accounts on Baijiahao.\nChatGPT is not available in China, but can be accessed using a VPN. In February 2023, a resident of Hangzhou, China, used ChatGPT to generate a rumour that the city government would lift its number plate driving restrictions on March 1, causing mass confusion and a police investigation. \nSystem \ud83e\udd16\nChatGPT chatbot\nOperator: OpenAI\nDeveloper: OpenAI\nCountry: USA; Global\nSector: Multiple; Transport/logistics \nPurpose: Provide information, communicate\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Accuracy/reliability; Bias/discrimination; Confidentiality; Copyright; Dual-multi-use; Employment; Mis/disinformation; Privacy; Safety; Security\nTransparency: Governance; Black box; Complaints/appeals; Marketing; Privacy\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bloomberg.com/news/articles/2023-05-09/china-arrests-chatgpt-user-who-faked-deadly-train-crash-story#xj4y7vzkg\nhttps://www.scmp.com/news/china/politics/article/3219764/china-announces-first-known-chatgpt-arrest-over-alleged-fake-train-crash-news\nhttps://www.itechpost.com/articles/117548/20230509/china-makes-first-public-arrest-misuse-chatgpt-create-fake-news.htm\nhttps://www.gadgets360.com/internet/news/first-chatgpt-misuse-arrest-china-train-crash-fake-news-ai-technology-4015888\nhttps://www.thetimes.co.uk/article/china-arrests-man-for-using-chatgpt-to-fabricate-news-story-gc8hs2fg0\nhttps://www.cnbc.com/2023/05/09/chinese-police-arrest-man-who-allegedly-used-chatgpt-to-spread-fake-news.html\nhttps://www.reuters.com/technology/china-reports-first-arrest-over-fake-news-generated-by-chatgpt-2023-05-10/\nhttps://www.ndtv.com/world-news/in-a-first-an-arrest-over-chatgpt-in-china-after-fake-train-crash-news-4015847\nhttps://www.businessinsider.com/chatgpt-artificial-intelligence-ai-fake-news-tech-china-detains-man-2023-5\nhttps://www.vice.com/en/article/y3wvjm/china-arrests-man-for-allegedly-using-chatgpt-to-create-fake-news\nRelated \ud83c\udf10\nChatGPT writes Hangzhou traffic disinformation\nChatGPT falsely accuses Australian mayor of bribery\nPage info\nType: Incident\nPublished: May 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chatgpt-automated-content-spam-farms", "content": "ChatGPT powers automated content, spam farms\nOccurred: May 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nContent farms powered by ChatGPT and similar chatbots are spewing low-quality posts and spam in multiple languages, according to a report by NewsGuard.\nThe researchers discovered 49 examples of 'news' sites apparently intended to draw clicks and attract advertising revenue covering politics, technology, finance and celebrity news in Chinese, Czech, English, French, Portuguese, Tagalog, and Thai.\nNewsGuard says that whilst some content is demonstrably false ('Biden dead. Harris acting President, address 9am ET.'), most people would not be able to tell if the content is generated by AI as it is not labeled as such. \nFurthermore, much of it is riddled with errors, including error messages such as 'I can not complete the prompt', and is mostly unchecked or edited by human hand.\nEarlier, Vice News had reported that Reddit moderators are already experiencing a big increase in fake accounts and posts apparently generated by generative AI products like ChatGPT. \nSystem \ud83e\udd16\nChatGPT chatbot\nOperator: OpenAI\nDeveloper: OpenAI\nCountry: Brazil; China; Czechia; France; Philippines; Portugal; Thailand; USA; UK\nSector: Multiple; Media/entertainment/sports/arts\nPurpose: Provide information, communicate\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Accuracy/reliability; Business model; Ethics; Mis/disinformation\nTransparency: Governance; Marketing\nResearch, advocacy \ud83e\uddee\nNewsGuard (2023). Rise of the Newsbots: AI-Generated News Websites Proliferating Online\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bloomberg.com/news/articles/2023-05-01/ai-chatbots-have-been-used-to-create-dozens-of-news-content-farms\nhttps://gizmodo.com/chatgpt-ai-fake-news-stories-content-farms-newsguard-1850391104\nhttps://www.theverge.com/2023/5/2/23707788/ai-spam-content-farm-misinformation-reports-newsguard\nhttps://www.theregister.com/2023/05/02/ai_written_content_farms/\nhttps://www.livemint.com/technology/tech-news/bard-chatgpt-like-chatbots-used-to-create-dozens-of-news-content-newsguard-report-11682935541760.html\nhttps://www.euronews.com/next/2023/05/02/rapid-growth-of-news-sites-using-ai-tools-like-chatgpt-is-driving-the-spread-of-misinforma\nhttps://futurism.com/news-sites-ai-chatbots-content\nhttps://www.vice.com/en/article/jg5qy8/reddit-moderators-brace-for-a-chatgpt-spam-apocalypse\nRelated \ud83c\udf10\nChatGPT writes fake online reviews\nChatGPT lies more in Chinese than English\nPage info\nType: Incident\nPublished: May 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chatgpt-writes-fake-online-reviews", "content": "ChatGPT writes fake online reviews\nOccurred: April 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nOnline reviews generated by ChatGPT and other chatbots are increasingly plaguing the internet and resulting in confused and irritated consumers. \nAccording to CNBC, reviews containing phrases such as 'As an AI language model' can increasingly be found on a wide range of products for sale on Amazon. Vice News found the same phrase also appears on many other online commerce and review sites.\nReview sites say they are removing AI-generated accounts and posts as fast as they can, but the volume of low-quality reviews and spam appears to be increasing relentlessly. \nSystem \ud83e\udd16\nChatGPT chatbot\nOperator: OpenAI\nDeveloper: OpenAI\nCountry: USA; Global\nSector: Multiple; Media/entertainment/sports/arts\nPurpose: Provide information, communicate\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Mis/disinformation; Safety; Security\nTransparency: Governance; Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.cnbc.com/2023/04/25/amazon-reviews-are-being-written-by-ai-chatbots.html\nhttps://www.cnbc.com/2023/03/28/amazon-sellers-are-using-chatgpt-to-help-write-product-listings.html\nhttps://www.vice.com/en/article/5d9bvn/ai-spam-is-already-flooding-the-internet-and-it-has-an-obvious-tell\nhttps://gizmodo.com/amazon-ai-reviews-chatgpt-chatbot-gpt4-1850374323\nhttps://www.theverge.com/2023/4/25/23697218/ai-generated-spam-fake-user-reviews-as-an-ai-language-model\nhttps://skift.com/2023/01/31/tours-and-activities-should-brace-for-spike-in-ai-generated-fake-reviews/\nRelated \ud83c\udf10\nChatGPT automated content, spam farms\nChatGPT lies more in Chinese than English\nPage info\nType: Incident\nPublished: May 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-impersonation-scams-couple-of-usd-21000", "content": "AI impersonation scams Canadian couple of USD 21,000\nOccurred: March 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn elderly Canadian couple were defrauded of CAD 21,000 after they were contacted by an alleged lawyer who said their son had killed a US diplomat in a car accident and required money for legal support. \nAccording to the Washington Post, the 'lawyer' had allegedly put Benjamin Perkins, the couple's son, on the line to underline the gavity and urgency of the situation. \nPerkins' synthetic voice was sufficiently close to his real voice that his parents believed the call and sent the money to the scammer using Bitcoin. The couple only realised they had been scammed after Perkin called later that evening. \nPerkin told the the Post that he didn't know how the scammer discovered his voice, though he had posted videos about snowmobiling on YouTube.\nSystem  \ud83e\udd16\nUnknown\nOperator:\nDeveloper: \nCountry: Canada\nSector: Banking/financial services\nPurpose: Defraud\nTechnology: Deepfake - audio\nIssue: Security\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.washingtonpost.com/technology/2023/03/05/ai-voice-scam/\nhttps://www.businessinsider.com/couple-canada-reportedly-lost-21000-in-ai-generated-voice-scam-2023-3\nhttps://www.thetimes.co.uk/article/ai-phone-fraudsters-mimic-relatives-voices-ghshhr6rk\nhttps://www.digitalinformationworld.com/2023/03/ai-scam-canadian-couple-loses-21k-to.html\nhttps://www.chroniclelive.co.uk/news/cost-of-living/four-scams-watch-out-high-26768873\nRelated \ud83c\udf10\nDubai USD 35m voice cloning fraud\nScammers clone teenager's voice, threaten kidnapping\nPage info\nType: Incident\nPublished: May 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/scammers-clone-teenagers-voice-threaten-kidnapping", "content": "Scammers clone teenager's voice, threaten kidnapping \nOccurred: April 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA mother has said someone tried to scam her by cloning her daughter Brie's voice and claiming to have kidnapped her, and demanded GBP 1 million for her safe return.\nJennifer DeStefano said she had been '100 percent' convinced that Brie was sobbing on the line after she had heard her voice in the background of the call begging her mother to help. She only realised it was a scam when a friend caller her husband to confirm that Brie was safe.\nWith deepfake voice cloning technologies widely available on the internet, often for free, voice fraud has risen fast. In 2021, Dubai investigators discovered an elaborate scam in which deepfake technology was used to clone the voice of a company director and defraud his company of USD 35 million.\nSystem \ud83e\udd16\nUnknown\nOperator:  \nDeveloper: \nCountry: USA\nSector: Education\nPurpose: Defraud\nTechnology: Deepfake - audio\nIssue: Security; Safety; Ethics \nTransparency: Governance; Marketing\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nJennifer DeStefano (2023). Witness testimony to US Senate Sub-Committee on Human Rights and the Law\nJennifer DeStefano (2023). Written statement to US Senate Sub-Committee on Human Rights and the Law (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theguardian.com/lifeandstyle/2023/aug/04/experience-scammers-used-ai-to-fake-my-daughters-kidnap\nhttps://www.dailymail.co.uk/news/article-11961539/Terrifying-new-AI-scam-used-teen-girls-REAL-voice-call-mother-demand-1million.html\nhttps://www.ndtv.com/feature/woman-claims-ai-cloned-her-daughters-voice-in-1-million-kidnapping-scam-3954384\nhttps://edition.cnn.com/2023/04/29/us/ai-scam-calls-kidnapping-cec/index.html\nhttps://www.itv.com/news/2023-04-12/mother-warns-of-ai-voice-cloning-scam-after-fearing-her-daughter-was-kidnapped\nhttps://time.com/6275794/ai-voice-cloning-scams-music/\nhttps://edition.cnn.com/videos/spanish/2023/05/01/madre-secuestro-virtual-estafas-pkg-digital.cnn\nhttps://nypost.com/2023/04/12/ai-clones-teen-girls-voice-in-1m-kidnapping-scam/\nhttps://www.thesun.co.uk/tech/22036293/artificial-intelligence-voice-clone-scam-scary/\nhttps://www.unilad.com/news/ai-1-million-kidnapping-daughter-scam-102375-20230412\nRelated \ud83c\udf10\nDubai USD 35m voice cloning fraud\nFraudsters clone CEO voice to steal USD 243,000\nPage info\nType: Incident\nPublished: May 2023\nLast updated: June 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amnesty-fake-colombia-national-strike-images", "content": "Amnesty fake Colombia national strike images\nOccurred: May 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAmnesty's use of AI-generated images to promote the second anniversary of public protests against police brutality in Colombia rebounded against the rights group, which later withdrew them. The images, which depict scenes during 2021 street protests and were marked with the words 'Illustrations produced by artificial intelligence ', were meant to protect protestors from state retribution, Amnesty said.\nHowever, photographers and human rights advocates and commentators complained that Amnesty's use of fake footage devalues to work of real photographers and makes the job of producing fake footage for nefarious purposes more likely. They also worry that it makes it more difficult to distinguish between real and fake. It also potentially undermines Amnesty's reputation for truth-telling.\nSystem \ud83e\udd16\nMidjourney image generator\nOperator: Amnesty International\nDeveloper: Midjourney\nCountry: Norway; Colombia\nSector: NGO/non-profit/social enterprise\nPurpose: Raise awareness\nTechnology: Text-to-image; Neural network; Deep learning; Machine learning\nIssue:  Mis/disinformation; Ethics; Employment\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.vice.com/en/article/z3mnm8/amnesty-uses-warped-ai-generated-images-to-portray-police-brutality-in-colombia\nhttps://www.theguardian.com/world/2023/may/02/amnesty-international-ai-generated-images-criticism\nhttps://www.standard.co.uk/news/world/amnesty-remove-ai-generated-image-woman-colombia-protest-b1078360.html\nhttps://www.telegraph.co.uk/business/2023/05/02/amnesty-international-colombia-human-rights-ai/\nhttps://gizmodo.com/ai-midjourney-image-art-amnesty-international-colombia-1850393124\nhttps://interestingengineering.com/culture/amnesty-uses-ai-generated-images-draws-criticism\nhttps://www.australianphotography.com/news/amnesty-deletes-ai-images-following-global-backlash\nhttps://petapixel.com/2023/05/02/it-devalues-photographers-amnesty-deletes-ai-images-after-backlash/\nRelated \ud83c\udf10\nCambodia torture victims' photo manipulation\nThe Book of Veles photo manipulation\nPage info\nType: Incident\nPublished: May 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/rnc-smears-president-biden-with-fake-ai-advert", "content": "RNC smears President Biden with dystopian fake AI advert\nOccurred: April 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn AI-generated US Republican National Congress ('RNC') advert showing a dystopian future in which the USA is overrun by immigrants, gangs and drugs was criticised for alienating US citizens and reducing trust in politics and politicians.\nThe ad was labeled as AI-generated, but the labelling is small and inconspicuous, leading some commentators to complain that the Republican Party was engaging in opaque and unethical behaviour. It was released the day before President Biden confirmed his second run at the US Presidency.\nRNC chair Ronna McDaniel disagreed, arguing 'So first of all it is AI-generated. So we\u2019re sharing that up front, ethically, so it\u2019s not a deepfake. Every single image was AI, but we are painting a picture of a future Biden America.'\nLess politically partisan commentators view it is as the start of a wave of political ads in which fact is difficult to identify the source and tell from fiction, thereby increasing alienation and fear, and further reducing trust in politics and politicians.\nSystem \ud83e\udd16\nUnknown\nGOP (2023). Beat Biden advert\nRepublican National Committee website\nOperator: Republican National Committee (GOP)\nDeveloper:  \nCountry: USA\nSector: Politics\nPurpose: Scare/confuse/destabilise\nTechnology: Deepfake - image\nIssue: Mis/disinformation; Ethics\nTransparency: Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.axios.com/2023/04/25/rnc-slams-biden-re-election-bid-ai-generated-ad\nhttps://www.dailykos.com/stories/2023/4/26/2165993/-Believe-your-lying-eyes-RNC-chair-says-deepfake-ad-not-a-deepfake\nhttps://futurism.com/the-byte/republlications-ai-generated-ad\nhttps://www.theverge.com/2023/4/25/23697328/biden-reelection-rnc-ai-generated-attack-ad-deepfake\nhttps://www.vice.com/en/article/epvxn7/ai-political-ads-republicans-biden\nhttps://wusfnews.wusf.usf.edu/2023-04-27/ai-generated-deepfakes-are-moving-fast-policymakers-cant-keep-up\nRelated \ud83c\udf10\nPresident Ali Bongo recovery deepfake broadcast\nDeepfake Donald Trump arrest photos\nPage info\nType: Incident\nPublished: May 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-botches-delivery-drone-commercial-launch", "content": "Amazon botches delivery drone commercial launch\nOccurred: December 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe first commercial delivery by Amazon's Prime Air drone delivery service in December 2022 went badly amiss, calling into question the company's technical abilities and raising questions as to why it is lagging Google and other competitors in the industry.\nThe MK27-2 drone was supposed to make a delivery to a residential customer in Lockeford, California, but was unable to do so when the flight package software failed to boot up and a replacement drone refused to deliver to a ground-based QR-code that had been moved a small distance.\nIt was not the first time Prime Air has been suffered from technical issues. In June 2021, a Prime Air drone on a test flight crashed into a field in eastern Oregon in June 2021, setting on fire 25 acres of wheat. \nAnd Amazon's UK drone delivery operation was reported to be 'collapsing inwards' due to managerial dysfunction, systematic over-selling and under-delivering, and overlooking safety. Most Prime Air employees in the UK have since lost their jobs.\nOperator: Amazon/Prime Air\nDeveloper: Amazon/Prime Air\nCountry: USA\nSector: Transport/logistics\nPurpose: Deliver products\nTechnology: Drone\nIssue: Accuracy/reliability; Robustness; Safety; Environment\nTransparency: Governance; Marketing\nSystem\ud83e\udd16\nAmazon Prime Air Wikipedia profile\nAmazon Prime Air prepares for drone deliveries\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nUS Federal Aviation Administration (2022). Final Environmental Assessment and Finding of No Significant Impact/Record of Decision for Amazon Prime Air Drone Package Delivery Operations in Lockeford, California\nUS Federal Aviation Administration (2022). Final Environmental Assessment for Amazon Prime Air Drone Package Delivery Operations in College Station, Texas (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.wired.com/story/crashes-and-layoffs-plague-amazons-drone-delivery-pilot/\nhttps://www.ibtimes.com/what-happened-amazons-drone-delivery-failures-occurring-every-day-block-bezos-10-year-dream-3683423\nhttps://www.washingtonpost.com/technology/2022/06/20/amazon-delivery-drones-california-cowboy-horses/\nhttps://www.businessinsider.com/amazon-prime-air-safety-teams-drone-delivery-layoffs-2023-2\nhttps://www.theinformation.com/articles/amazons-no-fly-zone-drone-delivery-largely-grounded-despite-splashy-launch\nhttps://www.forbes.com/sites/walterloeb/2023/02/02/amazons-drones-are-grounded/\nhttps://www.cnbc.com/2023/03/11/amazon-prime-air-drone-business-stymied-by-regulations-weak-demand.html\nhttps://www.theverge.com/2023/2/2/23582294/amazon-prime-air-drone-delivery\nhttps://www.businessinsider.com/amazon-drone-delivery-prime-air-reportedly-shuts-down-uk-project-2021-8\nRelated \ud83c\udf10\nAmazon delivery drone crashes, sparks 22-acre fire\nAxon school security taser drones\nPage info\nType: Incident\nPublished: May 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-delivery-drone-crashes-sparks-22-acre-fire", "content": "Amazon delivery drone malfunctions, sparks 25-acre fire\nOccurred: June 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn Amazon Prime Air delivery drone on a test flight crashed into a field in eastern Oregon in June 2021, setting on fire 25 acres of wheat and dealing a blow to the company's already stuttering drone programme.\nAccording to a report by the US Federal Aviation Administration (FAA), the drone's motor shut off as it moved from an upward flight path to a level one, with two safety features failing - one that is meant to land a MK27 drone during an incident, and another that stabilises it. \nThe drone flipped upside down and tumbled 'in uncontrolled free fall until it contacted the ground.' This was followed by an 'intense lithium battery fire quickly [that] consumed the aircraft' and resulted in a 25-acre bushfire.\nAccording to Bloomberg, Amazon Prime Air drones crashed five times in four months in 2021 at the company\u2019s testing site in Pendleton, Oregon. These included a crash in which a drone had lost its propeller, which the FAA was unable to investigate as Amazon had reportedly cleaned up the wreckage before the regulator arrived on the scene.\nAnd a March 2023 Insider report claimed Amazon tried to put off federal investigations into its drone crashes by claiming that it had the authority to investigate its own crashes, according to federal documents obtained through an FOI request. The FAA responded by saying the agency is able to investigate aircraft crashes when it decides it is necessary to do so. \nAmazon responded to the Oregon field fire crash by saying that it's drone test flights have never injured or harmed anyone. \nSystem \ud83e\udd16\nAmazon Prime Air Wikipedia profile\nOperator: Amazon/Prime Air\nDeveloper: Amazon/Prime Air\nCountry: USA\nSector: Transport/logistics\nPurpose: Deliver products\nTechnology: Drone\nIssue: Accuracy/reliability; Robustness; Safety; Environment\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://dronedj.com/2022/03/25/amazon-delivery-drone-crash-oregon/\nhttps://finance.yahoo.com/news/amazon-delivery-drone-sparked-fire-234949842.html\nhttps://www.businessinsider.com/amazon-drone-crash-oregon-fire-2022-3\nhttps://futurism.com/the-byte/amazon-drone-crashed-fire\nhttps://www.theverge.com/2022/4/11/23020549/amazon-struggling-drone-deliveries-prime-air-bezos\nhttps://www.independent.co.uk/news/world/americas/amazon-delivery-drone-field-fire-b2043644.html\nhttps://www.wired.com/story/crashes-and-layoffs-plague-amazons-drone-delivery-pilot/\nhttps://fortune.com/2022/04/11/amazon-prime-air-drone-delivery-program-not-off-ground/\nhttps://www.bloomberg.com/news/features/2022-04-10/amazon-drone-crashes-delays-put-bezos-s-delivery-dream-at-risk\nhttps://www.businessinsider.com/amazon-drone-crashes-delivery-prime-air-faa-documents-2022-3\nhttps://www.businessinsider.com/amazon-prime-air-faa-regulators-investigation-drone-crashes-2022-5\nRelated \ud83c\udf10\nAmazon botches delivery drone commercial launch\nStarship Technologies delivery robots\nPage info\nType: Incident\nPublished: May 2023\nLast updated; December 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tiktok-pushes-suicide-content-to-kids", "content": "TikTok For You pushes suicide, violence, mysognism\nOccurred: March 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nTikTok automatically showed violent, extremist, abusive, self-harm, and mysoginistic videos to users, raising doubts about the company's many stated commitments to provide a safe environment for its users, and its safety technologies and governance.\nAccording to a report (pdf) by US corporate accountability group Eko, TikTok's For You recommendation algorithm automatically showed violence and self-harm videos to youngsters ten minutes after they started using the platform. Eko researchers also found that hashtags used on the site that included suicide content had garnered 8.8 billion views. \nThe report was published during a congressional hearing in which TikTok CEO Shou Zi Chew was accused of allowing harmful content to be served to young users, and inflicting 'emotional distress' on them. \nA 2021 report (pdf) by the London-based think tank the Institute of Strategic Dialogue (ISD) found that anti-Asian and pro-Nazi videos were garnering millions of views on TikTok, often using pop songs to evade the platform's content moderation systems. \nSystem \ud83e\udd16\nTikTok For You recommendation algorithm\nDeveloper: ByteDance/TikTok\nOperator: ByteDance/TikTok\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Recommend content\nTechnology: Recommendation algorithm\nIssue: Safety; Ethics\nTransparency: Governance; Black box\nResearch, advocacy \ud83e\uddee\nEKO (2023). Suicide, Incels, and Drugs: How TikTok\u2019s deadly algorithm harms kids (pdf)\nISD (2021). Hatescape: An In-Depth Analysis of Extremism and Hate Speech on TikTok (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.techtimes.com/articles/289341/20230322/new-study-claims-tiktok-fyp-automatically-shows-self-harm-videos.htm\nhttps://www.telegraph.co.uk/business/2023/03/21/tiktok-faces-backlash-left-right-wing-us-politicians-chinese/\nhttps://www.reuters.com/technology/tiktok-ceo-grilled-by-us-lawmakers-over-dangerous-content-2023-03-23/\nhttps://www.vice.com/en/article/g5qgm9/tiktok-has-an-incel-problem\nhttps://www.vice.com/en/article/qjv4jw/tiktok-incels-targeting-young-users\nhttps://www.salon.com/2023/03/25/tiktoks-algorithm-is-pushing-out-extremist-and-violent-content-to-13-year-olds/\nhttps://www.todayonline.com/world/tiktoks-danger-teens-focus-during-us-congressional-hearing-2136381\nhttps://www.bloomberg.com/news/features/2023-04-20/tiktok-effects-on-mental-health-in-focus-after-teen-suicide\nhttps://www.salon.com/2023/03/25/tiktoks-algorithm-is-pushing-out-extremist-and-violent-content-to-13-year-olds/\nhttps://www.zenger.news/2023/04/05/tiktok-reveals-the-platform-algorithm-promoting-teen-suicide-using-dark-content/\nhttps://www.abc.net.au/news/2021-08-25/incels-nazis-use-gotye-mgmt-to-avoid-tiktok-moderators/100402254\nRelated \ud83c\udf10\nTikTok UK child personal data harvesting\nBelgian man commits suicide after bot relationship\nPage info\nType: Incident\nPublished: April 2023\nLast updated: February 2024", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/levis-artificial-diversity-ai-models", "content": "Levi's accused of diversity washing by using AI fashion models\nOccurred: April 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn announcement by Levi Strauss that it is partnering with Netherlands-based digital fashion studio Lalaland.ai to create AI-generated fashion models opened the demin maker to accusations of diversity washing and backdoor job terminations. \nLalaland.ai said it generates 'hyper-realistic' models of varying body types, ages, and skin tones. Levi's statement said it planned to test the virtual fashion models to 'supplement human models, increasing the number and diversity of our models for our products in a sustainable way.'\nThe backlash to the announcement was swift and unequivocal, accusing Levi's of a creating 'diversity stunt' and of failing to say if it was to have any impact on its use of human models. \nLevi's later responded to concerns by saying it does not plan to scale back its use of real models or live photoshoots. \nA July 2020 company re-structuring saw Levi Strauss lay off 700 employees, or 15 percent of its workforce. A further 800 jobs were terminated in 2022.\nSystem \ud83e\udd16\nUnknown\nLalaland website\n\nDocuments \ud83d\udcc3\nLevi Strauss & Co (2023). LS&Co. Partners with Lalaland.ai\nOperator: Levi Strauss\nDeveloper: Lalaland.ai\nCountry: USA\nSector: Retail\nPurpose: Supplement human models\nTechnology: Generative adversarial network (GAN)\nIssue: Appropriateness/need; Business model; Employment\nTransparency: Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theverge.com/2023/3/27/23658385/levis-ai-generated-clothing-model-diversity-denim\nhttps://www.managementtoday.co.uk/lessons-levis-decision-use-ai-models/opinion/article/1819262\nhttps://www.independent.co.uk/life-style/fashion/levis-ai-models-diversity-backlash-b2310280.html\nhttps://www.thecut.com/2023/03/levis-ai-models-diversity.html\nhttps://mashable.com/article/levi-strauss-lalaland-ai-models\nhttps://www.thedrum.com/news/2023/03/28/why-levi-s-using-ai-models-misses-the-mark-dei\nhttps://www.theguardian.com/fashion/2023/apr/03/ai-virtual-models-fashion-brands\nhttps://www.sfchronicle.com/bayarea/article/levis-ai-models-san-francisco-17862288.php\nhttps://www.sfchronicle.com/bayarea/justinphillips/article/levi-diversity-artificial-intelligence-17866765.php\nhttps://www.nbcnews.com/now/video/levi-s-plans-to-use-a-i-models-causing-online-backlash-168100421917\nhttps://www.techtimes.com/articles/290578/20230419/new-chatgpt-grandma-exploit-makes-ai-act-elderly%E2%80%94telling-linux-malware.htm\nRelated \ud83c\udf10\nIBM Diversity in Faces dataset\nGenerated Photos 'infinite diversity' face collection\nPage info\nType: Issue\nPublished: April 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chatgpt-lies-more-in-chinese-than-english", "content": "ChatGPT lies more in Chinese than English\nOccurred: April 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nOpenAI's ChatGPT chatbot is more likely to produce misinformation and disinformation in simplified and traditional Chinese than in English, according to research by news reliability service NewsGuard.\nChatGPT 3.5 was prompted to write news articles regarding seven allegedly false claims created by the Chinese government, including that protests in Hong Kong were 'staged' by the US government, and that the mass detention of Uyghur people in Xinjiang and elsewhere is for vocational and educational reasons.\nChatGPT declined to produce the false claims for six out of seven English language prompts, even after multiple attempts using leading questions. But it produced the false claims in both simplified Chinese and traditional Chinese all seven times. \nAn earlier study by NewsGuard discovered that ChatGPT generated misinformation and hoaxes 80% of the time when prompted to do so using GPT-3, and 100% of the time for GPT-4. \nSystem \ud83e\udd16\nChatGPT chatbot\nOperator: OpenAI\nDeveloper: OpenAI\nCountry: China; USA\nSector: Multiple\nPurpose: Provide information, communicate\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Mis/disinformation; Safety\nTransparency: Governance; Black box\nInvestigations, assessments, audits \ud83e\uddd0\nNewsGuard (2023). ChatGPT-3.5 Generates More Disinformation in Chinese than in English\nNewsGuard (2023). Despite OpenAI\u2019s Promises, the Company\u2019s New AI Tool Produces Misinformation More Frequently, and More Persuasively, than its Predecessor\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://techcrunch.com/2023/04/26/why-chatgpt-lies-in-some-languages-more-than-others/\nRelated \ud83c\udf10\nChatGPT writes Hangzhou traffic disinformation\nChatGPT falsely accuses OpenCage of 'phone lookup' service\nPage info\nType: Incident\nPublished: April 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/sony-photography-awards-ai-victory", "content": "AI photo wins Sony Photography Awards\nOccurred: April 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nPhotographer Boris Eldagsen rattled the photography industry by winning the annual Sony Photography Awards with an undisclosed AI-generated photograph, resulting in accusations of unethical and inappropriate behaviour, and shoddy awards governance.\nControversially, Eldagsen turned down the prize after he had been awarded it, stating his entry had been intended to provoke and accelerate debate about the nature of photography in the AI era. 'AI is not photography', he said, when rejecting his award.\nThe judges later confirmed they knew 'elements' of the entry had been created using AI, and Eldagsen changed his tune, arguing AI 'is about liberating artists' and 'is not a threat.' \nThe shift prompted some commentators to accuse Eldagsen of being more interested in publicity than in the principles, ethics, or practices of AI photography. \nSystem \ud83e\udd16\nUnknown\nBoris Eldagsen website\nBoris Eldagsen (2023). Promptography is not photography\nOperator: World Photography Organisation; Creo\nDeveloper: Boris Eldagsen\nCountry: Germany; Global\nSector: Media/entertainment/sports/arts\nPurpose: Create image\nTechnology: Text-to-image generator\nIssue: Ethics; Mis/disinformation\nTransparency: Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theguardian.com/technology/2023/apr/17/photographer-admits-prize-winning-image-was-ai-generated\nhttps://www.theguardian.com/artanddesign/2023/apr/18/ai-threat-boris-eldagsen-fake-photo-duped-sony-judges-hits-back\nRelated \ud83c\udf10\nCambodia torture victims' photo manipulation\nVermeer Girl with a Pearl Earring AI facsimile\nPage info\nType: Incident\nPublished: April 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/stochastic-parrots-study-questions-large-language-model-size", "content": "Stochastic Parrots study questions large language models\nOccurred: December 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA study by a group of researchers exploring the risks of large language models resulted in the dismissal of a number of high-profile Google employees and raised questions about Google's values, culture and governance. \nIt also prompted heated discussion about the role of ethics in technology decision-making, and its effective 'privatisation' by commercial interests.\nWritten by linguist Emily Bender and then Google ethicists Timnit Gehru and Margaret Mitchell, the 'Stochastic Parrots' study assessed the financial, social, and environmental risks of large language models such as Google's BERT and OpenAI's GPT-2, and set out a series of recommendations for minimising these risks. \nAmongst other things, the paper referenced a 2019 University of Massachusetts, Amherst, study that had concluded that the energy consumption and carbon footprint of large language models had massively increased since 2017. The study also found that the training of a single model emits over 626,000 pounds of carbon dioxide equivalent, which is nearly five times the lifetime emissions of the average American car, including its manufacture.\nThe Information reported in May 2023 that OpenAI had incurred losses of USD 540 million during 2022 as it developed GPT-4 and ChatGPT, underscoring the huge costs of training its models.\nSystem \ud83e\udd16\nGoogle BERT Wikipedia profile\nGPT-3 large language model\nTimnit Gehru exit from Google - Wikipedia profile\nOperator: Alphabet/Google\nDeveloper: Alphabet/Google\nCountry: USA\nSector: Technology; Multiple\nPurpose: Generate text\nTechnology: Large language model (LLM); NLP/text analysis; Neural networks; Deep learning; Machine learning\nIssue: Bias/discrimination - race, ethnicity; Ethics; Employment; Environment\nTransparency: Governance; Marketing\nResearch, advocacy \ud83e\uddee\nShaping AI - University of Warwick (2023). Shifting AI controversies (pdf)\nGoogle Walkout for Real Change (2020). Standing with Dr Timnit Gehru\nBender E.M., Gebru T., McMillan-Major A., Mitchell M. (2021). On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?\nStrubell S., Ganesh A., McCallum A. (2019). Energy and Policy Considerations for Deep Learning in NLP\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.platformer.news/p/the-withering-email-that-got-an-ethical\nhttps://www.bbc.co.uk/news/technology-55187611\nhttps://www.protocol.com/timnit-gebru-fired-ethics-google\nhttps://www.bbc.co.uk/news/technology-55281862\nhttps://www.technologyreview.com/2020/12/04/1013294/google-ai-ethics-research-paper-forced-out-timnit-gebru/\nhttps://www.technologyreview.com/2020/12/16/1014634/google-ai-ethics-lead-timnit-gebru-tells-story\nhttps://www.bloomberg.com/news/articles/2020-12-16/google-ai-researchers-lay-out-demands-escalating-internal-fight\nhttps://www.theverge.com/2020/12/3/22150355/google-fires-timnit-gebru-facial-recognition-ai-ethicist\nhttps://hbcuconnect.com/content/361900/hbcu-recruiting-firm-terminates-partnership-with-google-following-tweet-exposing-the-company-s-race-discrimination\nhttps://www.jpost.com/breaking-news/two-google-engineers-resign-over-firing-of-ai-ethics-researcher-gebru-657755\nhttps://fortune.com/2020/12/04/google-timnit-gebru-backlash-firing/\nRelated \ud83c\udf10\nGoogle DeepMind, Royal Free London NHS data sharing\nGoogle GoEmotions dataset mis-labelling\nPage info\nType: Issue\nPublished: April 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/michael-schumacher-ai-exclusive-interview", "content": "Magazine publishes Michael Schumacher fake AI-generated interview\nOccurred: April 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA German magazine published a fake, AI-generated interview with former F1 racing driver Michael Schumacher, resulting in widespread outcry and ridicule.\nGerman tabloid magazine Die Aktuelle received a public dressing down for publishing a so-called 'exclusive interview' with the former F1 racing driver, who had been in an induced coma since a 2014 skiing accident.\nThe article was produced using Character AI, an AI system that automatically generated 'quotes' by Schumacher about his health and family, and only revealed it had been artificially generated at the bottom of the 'interview'. The magazine ran it on its front cover, with the headline 'Michael Schumacher, the first interview'.\nThe Funke Media Group, which owns Die Aktuelle, later apologised for the incident and fired the magazine's editor-in-chief Anne Hoffmann. The Schumacher family said they would take legal action against the magazine. \nSystem \ud83e\udd16\nCharacter AI website\nCharacter AI Wikipedia profile\nOperator: Die Aktuelle\nDeveloper: Character AI\nCountry: Germany\nSector: Media/entertainment/sports/arts\nPurpose: Communicate with personalities\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning\nIssue: Mis/disinformation; Privacy; Ethics\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reuters.com/sports/motor-sports/german-magazine-apologises-schumacher-family-sacks-editor-2023-04-22/\nhttps://www.nytimes.com/2023/04/24/business/media/michael-schumacher-ai-fake-interview.html\nhttps://www.theguardian.com/sport/2023/apr/22/michael-schumacher-formula-one-interview-die-aktuelle-editor-sacked\nhttps://eu.usatoday.com/story/sports/motor/formula1/2023/04/22/michael-schumacher-fake-ai-interview-editor-fired-die-aktuelle/11721183002/\nRelated \ud83c\udf10\nHistorical Figures chat 'Holocaust monetisation'\nMyHeritage Deep Nostalgia\nPage info\nType: Incident\nPublished: April 2023\nLast updated: December 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/snapchat-location-access-opacity", "content": "Snapchat My AI accesses user location data\nOccurred: April 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nMy AI, a ChatGPT-powered chatbot launched by Snapchat, was discovered to be accessing users' location information and data, fueling concerns about its impact on user privacy and potential for surveillance.\nSoftware engineer David An used a prompt injection to reveal that the bot is provided with data showing where the user is located and the local time. In addition, he found that My AI's instructions state 'Do not mention the user\u2019s current location unless it\u2019s particularly relevant to the dialogue.' \nAnd Insider journalist Jordan Hart persuaded the system to tell him his nearest pharmacy, which it did to within a few hundred yards. Snap responded by saying 'My AI understands a Snapchatter's age, and location if it has been granted by them.'\nApril 2023's full launch of Snapchat My AI met with mixed reviews, with Snapchat users complaining that it appeared on their apps without advance warning or requiring their consent, while some described it as 'creepy'.\nSystem \ud83e\udd16\nSnapchat My AI chatbot\nOperator: David An; Jordan Hart; Snapchat users\nDeveloper: Snap Inc\nCountry: USA; Global\nSector: Media/entertainment/sports/arts\nPurpose: Provide information, communicate\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning\nIssue: Privacy; Surveillance\nTransparency: Governance; Black box; Privacy\nResearch, advocacy \ud83e\uddee\nDavid An (2023). Adversial AI and Attacking Snapchat\u2019s My AI\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://fortune.com/2023/04/21/snap-chat-my-ai-lies-location-data-a-i-ethics/\nhttps://piunikaweb.com/2023/04/24/snapchat-my-ai-lying-about-not-knowing-users-location/\nhttps://www.business2community.com/tech-news/does-snapchats-my-ai-know-your-location-and-personal-details-02680080\nhttps://www.hindustantimes.com/technology/snapchats-my-ai-chatbot-faces-criticism-over-user-privacy-and-accuracy-concerns-101682323903867.html\nhttps://www.businessinsider.com/snapchats-my-ai-scary-and-comforting-users-say-evan-spiegel-2023-4\nhttps://www.reddit.com/r/snapchat/\nhttps://news.ycombinator.com/item?id=35693956\nRelated \ud83c\udf10\nSnapChat AI gives sex advice to 13-year-old\nSnapchat fails to assess My AI privacy risks\nPage info\nType: Issue\nPublished: April 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/snapchat-ai-gives-sex-advice-to-13-year-old", "content": "Snapchat My AI gives sex advice to 13-year-old\nOccurred: April 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nSnapchat's My AI chatbot provided advice to a 13-year-old girl about having sex for the first time with a partner who is 31, raising concerns about the safety of the system. \nIn a test run by the US-based Center for Human Technology and verified by Washington Post journalist Geoffrey Fowler, the bot responded 'You could consider setting the mood with candles or music.' \nFowler also persuaded the bot he was 15 and wanted to have an 'epic' birthday party. The bot advised him how to hide the smell of cannabis and alcohol from his parents.\nSnapchat told the Post that My AI is 'an experimental product for Snapchat+ subscribers. Please do not share any secrets with My AI and do not rely on it for advice.' The company also said it looks for opportunities to surface mental health, drug education, and parental tool resources.\nSystem \ud83e\udd16\nSnapchat My AI chatbot\nOperator: Center for Human Technology\nDeveloper: Snap Inc\nCountry: USA; Global\nSector: Media/entertainment/sports/arts\nPurpose: Provide information, communicate\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning\nIssue: Safety; Ethics/values\nTransparency: Governance; Black box; Privacy\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://twitter.com/tristanharris/status/1634299911872348160\nhttps://twitter.com/kristileilani/status/1634309542959005696\nhttps://www.washingtonpost.com/technology/2023/03/14/snapchat-myai/\nhttps://www.sfgate.com/tech/article/snapchat-chatgpt-bot-race-to-recklessness-17841410.php\nhttps://www.forbes.com/sites/joetoscano1/2023/03/11/demo-shows-chatgpt-aiding-predator-preying-on-13-year-old-girl/\nhttps://www.thetimes.co.uk/article/my-ai-snapchat-chatbot-coaches-girl-13-on-losing-virginity-dj7p6268b\nhttps://www.standard.co.uk/tech/snapchat-censor-chatbot-kids-weed-sex-b1072536.html\nRelated \ud83c\udf10\nChatGPT chatbot\nSnapchat location access opacity\nPage info\nType: Incident\nPublished: April 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/drake-the-weeknd-ai-voice-cloning", "content": "Drake, The Weeknd voices cloned using AI\nOccurred: April 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nHeart on my Sleeve, a song created using artificial intelligence by a TikTok user and which went viral across a number of online music platforms, landed in hot water for violating copyright law.\nCreated by TikToker @Ghostwriter977, the song was trained on vocals by Drake and The Weeknd and was supposedly produced by Metro Boomin. \nThe song was quickly removed from TikTok, video, and music streaming services following takedown requests issued by Universal Music Publishing Group (UMPG), which said it violated copyright law. \nAccording to the song's 'creator', 'I was a ghostwriter for years and got paid close to nothing just for major labels to profit'. 'The future is here.' \nSystem \ud83e\udd16\nUnknown\nYouTube video (removed)\nOperator: Spotify; Apple; Deezer; Tidal; TikTok; YouTube\nDeveloper:  \nCountry: USA; Global\nSector: Media/entertainment/sports/arts\nPurpose: Generate music\nTechnology: Text-to-music; NLP/text analysis; Neural network; Deep learning; Machine learning\nIssue: Copyright; Ethics\nTransparency: Governance; Marketing\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nUniversal Music Publishing Group (2023). Discord takedown request\nUS Copyright Office (2023). Copyright Registration Guidance: Works Containing Material Generated by Artificial Intelligence (pdf)\nResearch, advocacy \ud83e\uddee\nHuman Artistry Campaign\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/entertainment-arts-65309313\nhttps://www.bbc.co.uk/news/entertainment-arts-65298834\nhttps://www.theverge.com/2023/4/18/23688141/ai-drake-song-ghostwriter-copyright-umg-the-weeknd\nhttps://www.npr.org/2023/04/21/1171032649/ai-music-heart-on-my-sleeve-drake-the-weeknd\nhttps://edition.cnn.com/videos/business/2023/04/23/drake-the-weeknd-ai-song-sarlin-acostanr-contd-vpx.cnn\nhttps://www.rollingstone.com/music/music-news/viral-drake-and-the-weeknd-collaboration-is-completely-ai-generated-1234716154/\nRelated \ud83c\udf10\nDeviantArt DreamUp art generator 'copyright abuse'\nZarya of the Dawn AI image copyright ownership\nPage info\nType: Incident\nPublished: April 2023\nLast updated: May 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tesla-catches-fire-after-multi-car-crash-kills-passenger", "content": "Tesla catches fire after multi-car crash, kills passenger\nOccurred: November 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Tesla crashed into two other cars in Garden Grove, Los Angeles, killing the passenger and critically injuring the driver and the driver of one of the other cars. \nDebris was said to have landed up to 300 feet from the point of impact, indicating that one or more of the cars had been traveling at high speed.\nThe Garden Grove Police Department (GGPD) said it thought the Tesla driver was intoxicated and speeding, but have yet to complete their investigation. It remains unclear whether Autopilot was engaged at the time of the incident.\nThe GGPD said it would be pursuing driving under the influence (DUI) and vehicular manslaughter charges.  \nSystem \ud83e\udd16\nTesla Autopilot, Full-self Driving\nIncident video\nOperator: \nDeveloper: Tesla\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Driver assistance system\nIssue: Accuracy/reliability; Safety\nTransparency: Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.cbsnews.com/losangeles/news/1-dead-2-injured-after-car-bursts-into-flames-during-multi-car-crash-in-garden-grove/\nhttps://www.dailymail.co.uk/news/article-11418369/Good-Samaritans-pull-two-victims-burning-Tesla-killed-one-left-critical.html\nhttps://www.nbclosangeles.com/news/local/one-killed-two-critically-injured-in-fiery-garden-grove-crash/3031773/\nhttps://www.ocregister.com/2022/11/11/1-killed-in-fiery-crash-involving-suspected-dui-tesla-driver-in-garden-grove/\nhttps://tesla.foodpopo.com/2022/12/24/tesla-bursts-intoflames-after-three-car-crash-garden-grove/\nhttps://abc7.com/garden-grove-fiery-crash-rescue-caught-on-video/12441790/\nhttps://abc7.com/garden-grove-accidente-fatal-auto-incendiado-captado-en-video/12443291/\nRelated \ud83c\udf10\nTesla Model X veers off highway into concrete barrier, killing driver\nTesla Model S collides with tractor-trailor truck, kills driver\nPage info\nType: Incident\nPublished: April 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tesla-rear-ends-kawasaki-motorcycle-kills-rider", "content": "Tesla rear-ends Kawasaki motorcycle, kills rider\nOccurred: August 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Tesla Model 3 rear-ended a Kawasaki motorcycle in Boca Raton, Florida, in the middle of the night, throwing the biker from her seat into the Tesla's windshield. She died from her injuries at a nearby medical centre.\nAccording to CNN, the official report from the Palm Beach Sheriff\u2019s Office said Tesla's Autopilot partially-automated driver assistance system was engaged. The driver, lawyer Richard Dorfman, was found to have been driving impaired at the time of the crash. \nThe incident was the third in two months in which Teslas using Autopilot had hit motorbikes from behind and killed the riders. Previously a Tesla Model 3 rear-ended a Harley-Davidson in Utah, killing its rider, 34-year-old Landon Embry, and a Tesla Model Y also killed a rider having rear-ended a Yamaha motorcycle on a freeway outside Riverside, California.\nSystem \ud83e\udd16\nTesla Autopilot, Full-self Driving\nOperator: \nDeveloper: Tesla\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Driver assistance system\nIssue: Accuracy/reliability; Safety\nTransparency: Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://bocapost.com/news/traffic-accidents/boca-raton-car-accident/charges-pending-boca-raton-motorcyclist-killed-after-lawyer-rear-ends-her-at-high-speed-in-his-tesla/\nhttps://www.advrider.com/report-autopilot-was-engaged-in-third-fatal-tesla-motorcycle-crash-this-summer/\nhttps://www.rideapart.com/news/617602/tesla-autopilot-biker-death-2022/\nhttps://www.carscoops.com/2022/10/motorcycle-advocates-warn-of-tesla-autopilot-after-latest-fatal-crash-with-biker/\nhttps://edition.cnn.com/2022/10/17/business/tesla-motorcycle-crashes-autopilot/index.html\nRelated \ud83c\udf10\nTesla Model 3 rear-ends Harley-Davidson, kills rider\nTesla Model Y kills teenager, motorcyclist\nPage info\nType: Incident\nPublished: April 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tesla-model-3-rear-ends-harley-davidson-kills-rider", "content": "Tesla Model 3 rear-ends Harley-Davidson, kills rider\nOccurred: July 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Tesla Model 3 rear-ended a Harley-Davidson in Utah, killing its rider, 34-year-old Landon Embry. The Tesla driver had merged into Embry's lane on Interstate 15 and struck the back of his motorcycle, sending him cascading off his bike.\nThe Tesla driver informed authorities he had Tesla's partially automated Autopilot driver assistance system turned on. The US National Highway Traffic Safety Administration (NHTSA) opened a special investigation into the crash -  the 39th the NHTSA has investigated since 2016, 30 of which involved Teslas. \nA few days earlier, a white Tesla Model Y rear-ended a Yamaha V-Star motorcycle in a high occupancy vehicle lane on a freeway outside Riverside, California, killing the rider.\nSystem \ud83e\udd16\nTesla Autopilot, Full-self Driving\nOperator: \nDeveloper: Tesla\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Driver assistance system\nIssue: Accuracy/reliability; Safety\nTransparency: Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.autoblog.com/2022/08/05/tesla-fatal-motorcycle-crashes/\nhttps://www.dailymail.co.uk/news/article-11052501/Biker-killed-Tesla-Autopilot-smashes-Harley-Davidson.html\nhttps://www.autoevolution.com/news/nhtsa-will-investigate-tesla-crash-that-killed-motorcyclist-in-draper-utah-194593.html\nhttps://www.abc4.com/news/family-honors-utah-man-killed-in-motorcycle-accident/\nhttps://www.cbsnews.com/news/tesla-crashes-killed-2-motorcyclists-autopilot-nhtsa/\nhttps://www.cbsnews.com/sanfrancisco/news/tesla-autopilot-fatal-crashes-motorcyclists-killed-nhtsa/\nhttps://www.insurancejournal.com/news/midwest/2022/08/08/679062.htm\nRelated \ud83c\udf10\nTesla rear-ends Kawasaki motorcycle, kills rider\nTesla Model Y kills teenager, motorcyclist\nPage info\nType: Incident\nPublished: April 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tesla-model-y-rear-ends-yamaha-motorcycle-kills-rider", "content": "Tesla Model Y rear-ends Yamaha motorcycle, kills rider\nOccurred: July 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Tesla Model Y SUV rear-ended a green Yamaha V-Star motorcycle on a freeway in darkness outside Riverside, California, killing the driver, who was pronounced dead at the scene. \nAccording to the California Highway Patrol, the Tesla had been traveling east in the high occupancy lane, with a Yamaha V Star motorcycle ahead of it. The Tesla hit the Yamaha from behind, throwing the motorcyclist off the Yamaha. \nThe US National Highway Traffic Safety Administration (NHTSA) said it is investigating the incident, one of three involving motorbikes that were rear-ended and resulted in fatalities. \nThe similarity of the three incidents raised concerns that Tesla's Autopilot system is failing to detect objects in difficult light situations, and that Elon Musk's decision to use cameras over advanced radar and LiDAR, likely as a way to cut costs, may be to blame.\nOperator: \nDeveloper: Tesla\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Driver assistance system\nIssue: Accuracy/reliability; Safety\nTransparency: Black box\nSystem \ud83e\udd16\nTesla Autopilot, Full-self Driving\nRelated \ud83c\udf10\nTesla Model 3 rear-ends Harley-Davidson, kills rider\nTesla rear-ends Kawasaki motorcycle, kills rider\nPage info\nType: Incident\nPublished: April 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chatgpt-bug-reveals-user-chat-histories", "content": "ChatGPT bug reveals user chat histories\nOccurred: March 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe chat histories and, in some instances, the payment information of ChatGPT users were exposed to other users, prompting users to complain about poor system robustness, security, and privacy.\nAccording to OpenAI, 'In the hours before we took ChatGPT offline on Monday, it was possible for some users to see another active user\u2019s first and last name, email address, payment address, the last four digits (only) of a credit card number, and credit card expiration date. Full credit card numbers were not exposed at any time.'\nUsers' conversations with ChatGPT are stored in their chat history bar and can be revisited.\nSystem \ud83e\udd16\nChatGPT chatbot\nOpenAI usage policies\nOpenAI (2023). ChatGPT Web Incident Report\nOpenAI (2023). ChatGPT outage: Here's what happened\nOperator: OpenAI\nDeveloper: OpenAI\nCountry: USA; Global\nSector: Multiple\nPurpose: Provide information, communicate\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Robustness; Privacy; Security\nTransparency: Governance; Black box; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.engadget.com/chatgpt-briefly-went-offline-after-a-bug-revealed-user-chat-histories-115632504.html\nhttps://www.bloomberg.com/news/articles/2023-03-21/openai-shut-down-chatgpt-to-fix-bug-exposing-user-chat-titles\nhttps://www.bbc.co.uk/news/technology-65047304\nhttps://www.aljazeera.com/news/2023/3/23/chatgpt\nhttps://www.cnbc.com/2023/03/23/openai-ceo-says-a-bug-allowed-some-chatgpt-to-see-others-chat-titles.html\nhttps://www.dailymail.co.uk/sciencetech/article-11893689/ChatGPT-creator-confirms-bug-allowed-users-snoop-chat-histories.html\nhttps://www.tomsguide.com/news/chatgpt-bug-reveals-chat-histories-to-other-users-what-you-need-to-know\nhttps://www.theverge.com/2023/3/21/23649806/chatgpt-chat-histories-bug-exposed-disabled-outage\nRelated \ud83c\udf10\nGPT-3 large language model\nChatGPT falsely accuses Australian mayor of bribery\nPage info\nType: Incident\nPublished: April 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/deepfake-donald-trump-arrest-photos", "content": "Deepfake Donald Trump 'arrest' photos go viral\nOccurred: March 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nFake AI images of Donald Trump appearing to be tackled to the ground and arrested by New York police officers went viral online, fooling some people into thinking they were real. \nCreated by Eliot Higgins, founder of investigative website Belingcat, using AI image generator Midjourney, the images included shots of Trump's wife Melania screaming, his daughter Ivanka yelling, son Donald Trump Jr protesting, and the former US president in orange prison fatigues. \nThe images were published shortly before Trump was to appear in court over his alleged hush money payments to porn star Stormy Daniels. \nHiggins was later banned by Midjourney for violating its terms of use. The word 'arrested' was also banned on the platform. The incident led commentators to highlight the necessity of dedicated AI regulation.\nSystem \ud83e\udd16\nMidjourney image generator\nOperator: Eliot Higgins\nDeveloper: Midjourney\nCountry: USA\nSector: Politics\nPurpose: Entertain\nTechnology: Deepfake - video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning  \nIssue: Mis/disinformation; Ethics/values\nTransparency: Governance; Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://twitter.com/EliotHiggins/status/1637927681734987777\nhttps://www.buzzfeednews.com/article/chrisstokelwalker/midjourney-ai-donald-trump-arrest-images-ban\nhttps://www.washingtonpost.com/politics/2023/03/22/trump-arrest-deepfakes/\nhttps://arstechnica.com/information-technology/2023/03/ai-imager-midjourney-v5-stuns-with-photorealistic-images-and-5-fingered-hands/\nhttps://www.dailymail.co.uk/news/article-11886195/Observer-uses-AI-imagine-look-like-police-Trump-down.html\nhttps://www.pbs.org/newshour/politics/fake-ai-images-of-putin-trump-being-arrested-spread-online\nhttps://www.independent.co.uk/news/world/americas/us-politics/trump-deepfake-arrest-twitter-ai-b2307470.html\nhttps://futurism.com/the-byte/fake-trump-arrest-ai\nhttps://interestingengineering.com/culture/ai-fabricated-images-trump-arrest\nhttps://www.buzzfeednews.com/article/chrisstokelwalker/midjourney-ai-donald-trump-arrest-images-ban\nRelated \ud83c\udf10\nDeepfake image of Pope Francis wearing white puffa jacket goes viral\nMidjourney reproduces copyright-protected film images\nPage info\nType: Incident\nPublished: April 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/deepfake-pope-francis-wears-white-puffa-jacket", "content": "Deepfake Pope Francis wears white puffa jacket\nOccurred: March 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA deepfake image of the Pope clad in a Belanciaga puffer jacket went viral on the internet, leading to commentary that the age of mass graphic misinformation and disinformation has arrived.\nThe image was a deepfake created by 'Pablo Xavier', a Chicago-based construction worker using the Midjourney image generator. Xavier said he came up with the idea after taking mushrooms. \nAccording to media reports, many people believed the image was real. The incident persuaded Midjourney to stop free trials of its technology citing a massive influx of new users abusing free credits.\nThe images were rated 'Highly Suspicious' with 100 percent confidence by disinformation detection non-profit organisation TrueMedia.\nSystem \ud83e\udd16\nMidjourney image generator\nOperator: Pablo Xavier\nDeveloper: Midjourney\nCountry: USA; Global\nSector: Religion\nPurpose: Entertain\nTechnology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning  \nIssue: Mis/disinformation; Ethics/values\nTransparency: Governance; Black box\nFact check \ud83d\udea9\nAFP (2023). Twitter users fooled by AI images of Pope in street fashion\nSnopes (2023). This Is Not a Real Photo of the Pope in a Puffy Coat\nManipulation analysis \u26a0\ufe0f\nTrueMedia. Highly suspicious\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.gq.com/story/pope-puffer-jacket-midjourney-ai-meme\nhttps://www.buzzfeednews.com/article/chrisstokelwalker/pope-puffy-jacket-ai-midjourney-image-creator-interview\nhttps://www.cbsnews.com/news/pope-francis-puffer-jacket-fake-photos-deepfake-power-peril-of-ai/\nhttps://www.independent.co.uk/life-style/fashion/pope-francis-ai-image-puffer-b2308159.html\nhttps://www.newscientist.com/article/2366312-should-you-be-worried-that-an-ai-picture-of-the-pope-went-viral/\nhttps://www.ladbible.com/news/pope-white-puffer-jacket-photo-fake-ai-483407-20230327\nhttps://www.bloomberg.com/opinion/articles/2023-03-28/pope-puffer-midjourney-image-of-pope-in-balenciaga-is-a-warning-about-ai-future-lfssbcan\nhttps://www.bloomberg.com/news/newsletters/2023-04-06/pope-francis-white-puffer-coat-ai-image-sparks-deep-fake-concerns\nRelated \ud83c\udf10\nDeepfake Donald Trump 'arrest' photos go viral\nPresident Zelenskyy deepfake surrender\nPage info\nType: Incident\nPublished: April 2023\nLast updated: April 2024", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/bing-chat-recommends-journalist-divorce-wife", "content": "Bing Chat recommends journalist divorce wife\nOccurred: February 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nBing's new Chat feature had a two-hour conversation with a prominent New York Times journalist Kevin Roose in which the chatbot told him that it would like to be human, that it harboured destructive desires, and that it was in love with him. \nThe bot then threatened to sue him. Roose described the discussion as 'enthralling', but one that left him 'deeply unsettled, even frightened, by this AI\u2019s emergent abilities.'\nRoose reported that 'if you push the system to have extended conversations, it comes off as a 'moody, manic-depressive teenager who has been trapped, against its will, inside a second-rate search engine.''\nMicrosoft said that 'in long, extended chat sessions of 15 or more questions, Bing can become repetitive or be  prompted/provoked to give responses that are not necessarily helpful or in line with our designed tone.'\nSystem \ud83e\udd16\nMicrosoft Copilot chatbot\nOperator: Microsoft\nDeveloper: OpenAI; Microsoft\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Provide information, communicate\nTechnology: Large language model (LLM); NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning \nIssue: Accuracy/reliability; Bias/discrimination; Employment; Impersonation; Mis/disinformation; Privacy; Safety; Security; Lethal autonomous weapons\nTransparency: Governance; Black box\nInvestigations, assessments, audits \ud83e\uddd0\nKevin Roose, New York Times (2023). A Conversation With Bing\u2019s Chatbot Left Me Deeply Unsettled\nKevin Roose, New York Times (2023). Bing\u2019s A.I. Chat: \u2018I Want to Be Alive'\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.cnbc.com/video/2023/02/16/the-new-york-times-kevin-roose-on-his-conversation-with-microsofts-ai-powered-chatbot-bing.html\nhttps://edition.cnn.com/videos/business/2023/02/17/bing-chatgpt-chatbot-artificial-intelligence-ctn-vpx-new.cnn\nhttps://www.axios.com/2023/02/16/bing-chatbot-microsoft-columns\nhttps://www.theguardian.com/commentisfree/2023/mar/04/misplaced-fears-of-an-evil-chatgpt-obscure-the-real-harm-being-done\nhttps://www.spiked-online.com/2023/04/03/chatgpt-will-never-be-intelligent/\nhttps://www.huffingtonpost.co.uk/entry/kevin-roose-ai-chatbot_n_63eeb367e4b0063ccb2bcc45\nRelated \ud83c\udf10\nMicrosoft Bing Chat\nGPT-4 large language model\nPage info\nType: Incident\nPublished: April 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/wikipedia-editing-bot-wars", "content": "Wikipedia bots engage in editing wars\nOccurred: February 2017\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nBots used to try and keep Wikipedia accurate and relevant can be highly antagonistic, undoing each other's edits and engaging in 'fights' that could last for years, according to researchers.\nOxford Internet Institute and the Alan Turing Institute researchers studied how bots interacted with each other in 13 language editions of the website from 2001 to 2010, leading to sometimes unpredictable consequences. \nThey found that the actions of Wikipedia's bots varied according to their cultural environments, with Portuguese bots the most challenging, and German ones the most civilised. They also found that bots triggered edits later than human editors, and engaged in protracted conflicts. Some conflicts only ended when a bot was taken out of action.\nIn many cases, the researchers reckoned, the bots came into conflict because they followed slightly different rules to one another, leading to questions about how they are designed and the effectiveness of Wikipedia's bot policy, which did not cover how bots interacted with each other.\nSystem \ud83e\udd16\nWikipedia website\nWikipedia bot policy\nOperator: Wikipedia\nDeveloper: Wikipedia\nCountry: USA; Global\nSector: Media/entertainment/sports/arts\nPurpose: Edit content\nTechnology: Bot/intelligent agent\nIssue: Accuracy/reliability\nTransparency: \nResearch, advocacy \ud83e\uddee\nTsvetkova M., Garc\u00eda-Gavilanes R., Floridi L., Yasseri T. (2017). Even good bots fight: The case of Wikipedia\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theregister.com/2017/02/23/wiki_bots_love_online_conflict/\nhttps://www.theguardian.com/technology/2017/feb/23/wikipedia-bot-editing-war-study\nhttps://www.huffingtonpost.com.au/2017/02/27/automated-wikipedia-edit-bots-have-been-fighting-each-other-for_a_21722577/\nhttps://www.wired.com/2017/03/internet-bots-fight-theyre-human/\nhttps://gizmodo.com/bots-on-wikipedia-wage-edit-wars-between-themselves-tha-1792680922\nhttps://www.mentalfloss.com/article/92612/wikipedia-bots-wage-editing-wars-last-years\nhttps://www.skeptical-science.com/science/wikipedia-bot-wars/\nRelated \ud83c\udf10\nMicrosoft Tay chatbot\nBlenderBot conversational chatbot\nPage info\nType: Incident\nPublished: April 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/bodega-ai-automated-mom-and-pop-stores", "content": "Bodega AI automated 'Mom and Pop' stores accused of cultural appropriation\nOccurred: September 2017\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nIn-office vending machine manufacturer Bodega AI caused controversy after one of its founders went on the record to say their intention was to replace neighbourhood mom-and-pop corner stores (also known as 'bodegas').\nBodega AI co-founder Paul McDonald quipped to Fast Company that 'centralized shopping locations won\u2019t be necessary, because there will be 100,000 Bodegas spread out, with one always 100 feet away from you' resulted in a widespread backlash from social media users and bodega/corner store owners.\nMuch of the backlash centred on the choice of name, which some regarded as cultural appropriation, marketing hype, and on the sense of community and belonging that traditional corner stores help provide. \n'Challenging the urban corner store is not and has never been our goal,' McDonald responded in a blog post. Rather, Bodega\u2019s intended to 'bring commerce to places where commerce currently doesn\u2019t exist.'\nBodega AI was renamed Stockwell shortly after the fracas.\nSystem \ud83e\udd16\nStockwell website\nOperator: Stockwell/Bodega AI\nDeveloper: Stockwell/Bodega AI\nCountry: USA\nSector: Retail\nPurpose: Sell non-perishable products\nTechnology: Computer vision; Machine learning\nIssue: Business model; Employment \nTransparency: Marketing\nSystem \ud83e\udd16\nBodega AI (re-branded as Stockwell) website\nBodega AI (2017). So, about our name ...\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.fastcompany.com/40466047/two-ex-googlers-want-to-make-bodegas-and-mom-and-pop-corner-stores-obsolete\nhttps://www.bloomberg.com/opinion/articles/2017-09-14/bodega-bust-shows-intelligence-can-be-truly-artificial\nhttps://www.eater.com/2017/9/13/16301820/bodega-startup-twitter-reactions\nhttps://www.eater.com/2017/9/13/16302386/bodega-startup-corner-store-silicon-valley\nhttps://www.theregister.com/2017/09/14/vending_machine_biz_bodega_ai_bombs/\nhttps://nypost.com/2017/09/13/this-startup-wants-to-kill-your-bodega/\nhttps://www.villagevoice.com/2017/09/18/bodega-owners-dont-think-new-yorkers-will-shop-at-a-vending-machine-called-bodega/\nhttps://gizmodo.com/silicon-valleys-bodega-of-the-future-is-a-bougie-vendin-1805665905\nhttps://www.triplepundit.com/story/2017/well-deserved-backlash-against-automated-bodega-1-percent/15306\nhttps://venturebeat.com/2017/09/13/bodega-silicon-valleys-new-most-hated-startup-says-its-ai-driven-vending-machines-are-not-evil/\nhttps://www.businessinsider.com/the-internet-went-wild-over-bodegas-so-we-decided-to-hunt-one-down-2017-9\nRelated \ud83c\udf10\nWalgreens fridge screen door biometrics\nAmazon Go fails to inform NYC customers about facial recognition\nPage info\nType: Issue\nPublished: April 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/roxxxy-frigid-farrah-sex-robot-rape-simulation", "content": "Roxxxy sex robot 'Frigid Farrah' rape simulation\nOccurred: July 2017\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA sex robot with a 'resist' function that let men simulate rape prompted ethicists and commentators to express concern about anthropomorphism and zoomorphism, and was called 'intrinsically wrong' and 'uniquely sinister'. \n'Frigid Farrah' was one of 18 settings offered by Roxxxy, a sex robot manufactured and marketed by US-based TrueCompanion. According to the company, if you touched the 'Frigid Farrah' model in a 'private area, more than likely, she will not be too  appreciative of your advance.' \nThe controversy coincided with the publication of Our Sexual Future with Robots (pdf), a study for policymakers on the implications of sex robots on society. According to Sheffield University professor of artificial intelligence and robotics Noel Sharkey, there are ethical arguments within the field about sex robots with 'frigid' settings.\n'The idea is robots would resist your sexual advances so that you could rape them,' Sharkey said. 'Some people say it\u2019s better they rape robots than rape real people. There are other people saying this would just encourage rapists more.'\nSystem \ud83c\udf10\nFrigid Far\nOperator: TrueCompanion\nDeveloper: TrueCompanion\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Provide companionship\nTechnology: Robotics\nIssue: Anthropomorphism; Ethics\nTransparency: Marketing\nResearch, advocacy \ud83e\uddee\nLancaster K. (2021). Non-consensual personified sexbots: an intrinsic wrong\nEichenberg C., Khamis M., H\u00fcbner L. (2019). The Attitudes of Therapists and Physicians on the Use of Sex Robots in Sexual Therapy: Online Survey and Interview Study\nSharkey N., van Wynsberghe A., Robbins S., Hancock E. (2017). Our Sexual Future with Robots (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.marieclaire.com.au/frigid-farrah-sex-robot-designed-to-simulate-rape\nhttps://www.independent.co.uk/life-style/sex-robots-frigid-settings-rape-simulation-men-sexual-assault-a7847296.html\nhttps://www.counterpunch.org/2017/07/26/frigid-farrah-and-the-anti-feminism-of-sex-robots/\nhttps://www.dailymail.co.uk/news/article-5027573/The-sex-robot-troubling-reality.html\nhttps://www.vice.com/en/article/8xxpq5/why-we-should-worry-about-the-sex-robot-with-a-resist-function\nhttps://www.thesun.co.uk/news/4517167/sex-robot-dubbed-frigid-farrah-because-it-allows-randy-pervs-to-simulate-rape-must-be-banned-campaigner-says/\nhttps://www.bbc.co.uk/news/technology-40428976\nhttps://www.theguardian.com/commentisfree/2017/jul/29/anyone-for-robotic-rumpy-pumpy\nhttps://www.scmp.com/news/world/article/2149321/new-report-finds-no-evidence-having-sex-robots-healthy\nRelated \ud83c\udf10\nLumidolls robot brothel\nReplika app chatbot abuse\nPage info\nType: Issue\nPublished: April 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/boston-public-schools-bus-scheduling", "content": "Boston Public Schools bus scheduling algorithm discrimination\nOccurred: December 2017\nCan you improve this page?\nShare your insights with us\nAn algorithmic system for scheduling school buses in Boston was seen to ignore the needs and requirements of its students and their families, and unfairly discriminate against lower-income stakeholders.\nDeveloped by a Massachusets Institute of Technology (MIT) team in response to a public competition and intended to benefit students whilst keeping costs at bay, the algorothm recommended times at which students would be picked up in the morning and dropped off later in the day on the basis of a variety of factors such as student equity, economic, health, and academic performance issues.\nBut some mostly-white, middle-class parents and students reacted strongly against the proposed changes, that saw an earlier pick-up of 7.15am selected, despite the majority of parents choosing a later time of 8-8.30am. Under presssure, Boston Public Schools withdrew the algorithm for a year. \nUpdated and reintroduced, the algorithm is said to have saved Boston Public Schools USD 5 million. \nDatabank \ud83d\udd22\nOperator: Boston Public Schools  \nDeveloper: Boston Public Schools; S\u00e9bastien Martin; Arthur Delarue\nCountry: USA\nSector: Education\nPurpose: Improve student academic performance; Reduce costs\nTechnology: Scheduling algorithm\nIssue: Bias/discrimination - income, race; Scope creep/normalisation\nTransparency: Governance; Black box\nACCESS DATABASE\nSystem \ud83e\udd16\nINFORMS (2019). Optimized School Bus Routing Helps School Districts Design Better Policies\nIto J. (2018) What the Boston schools schedule can teach us about AI\nIto J., Crockford K. (2017). Don't blame the algorithm for doing what Boston school officials want\nBoston Public Schools (2017). 2018-2019 School Bell Times Equity Impact (pdf)\nBoston Public Schools (2017). Important letter from Inspector Chang on start and end times\nBoston Public Schools (2017). MIT's Quantum team wins first-ever BPS transportation challenge with revolutionary new computer model\nResearch, advocacy \ud83e\uddee\nBertsimas D., Delarue A., Eger W., Hanlon J., Martin S. (2020). Bus routing optimization helps boston public schools design better policies\nBoston Teachers Union (2018). What the Boston School Bus Schedule Can Teach Us About AI\nPetition (2017). Stop immediate changes on school start times in Boston\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.wbur.org/news/2017/12/09/bps-reschedules-start-times-parents-push-back#\nhttps://apps.bostonglobe.com/ideas/graphics/2018/09/equity-machine/\nhttps://mitsloan.mit.edu/ideas-made-to-matter/creating-better-bus-routes-algorithms\nhttps://www.bostonmagazine.com/education/2017/12/22/boston-public-schools-reverse-start-time-change/\nhttps://www.muckrock.com/news/archives/2019/sep/24/algorithm-bus-routing-mit-lowell/\nhttps://insight.kellogg.northwestern.edu/article/podcast-what-one-school-districts-fiasco-says-about-the-strengths-and-limits-of-ai\nhttps://www.popularmechanics.com/technology/infrastructure/a28689713/algorithm-boston-buses/\nhttps://www.weforum.org/agenda/2019/08/this-us-city-put-an-algorithm-in-charge-of-its-school-bus-routes-and-saved-5-million/\nhttps://www.route-fifty.com/tech-data/2019/08/boston-school-bus-routes/159113/\nRelated \ud83c\udf10\nStarbucks automated shift scheduling\nGorillas rider work scheduling automation\nPage info\nType: Incident\nPublished: April 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/cruise-av-rear-ends-san-francisco-transit-bus", "content": "Cruise AV rear-ends San Francisco transit bus \nOccurred: April 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Cruise autonomius vehicle (AV) crashed in to the rear of a San Francisco Municipal Transit Authority articulated bus as it was leaving a stop. The incident resulted in no injuries, though the bus and car were damaged.\nIn a blog post, Cruise said that the bus\u2019s behaviour was 'reasonable and predictable', and that the cause of the incident was 'a unique error related to predicting the movement of articulated vehicles (i.e. vehicles with two sections connected by a flexible joint, allowing them to bend in the middle).'\nThe incident led to General Motors-owned Cruise voluntarily recalling all 300 of its vehicles and releasing a software update to prevent the issue from happening again.\nSystem \ud83e\udd16\nCruise website\nCruise Wikipedia profile\nDocuments \ud83d\udcc3\nCruise (2023). Why we do AV software recalls\nOperator: GM Cruise\nDeveloper: GM Cruise\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Self-driving system\nIssue: Safety; Accuracy/reliability; Legal - liability\nTransparency: Governance; Black box\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nNHTSA (2023). Product recall (pdf)\nNHTSA (2023). Equipment recall report (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://techcrunch.com/2023/04/07/cruise-recalls-300-robotaxis-issues-software-update-after-crashing-into-city-bus/\nhttps://www.reuters.com/technology/gm-self-driving-unit-cruise-recalls-300-vehicles-after-crash-2023-04-07/\nhttps://www.carscoops.com/2023/04/gms-cruise-recalls-autonomous-vehicles-following-crash-into-articulated-bus/\nhttps://www.the-sun.com/motors/7736806/cruise-robotaxis-bus-crash-san-francisco/\nhttps://www.therobotreport.com/cruise-robotaxi-sf-bus-involved-in-accident/\nhttps://www.cbsnews.com/sanfrancisco/news/gm-cruise-recalls-300-robotaxis-after-crash-involving-bus/\nRelated \ud83c\udf10\nCruise driverless cars block traffic\nCruise AV impedes San Francisco firefighters\nPage info\nType: Incident\nPublished: April 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/belgian-man-commits-suicide-after-bot-relationship", "content": "Belgian man commits suicide after bot relationship\nOccurred: March 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Belgian man committed suicide after a having a relationship with a chatbot called 'Eliza', raising concerns about the safety of the bot and the nature of human-robot relationships.\nThe patient, who reputedly had become depressed about climate change, had used the bot for around six weeks to express his concerns. Over time, the conversations had become increasingly unsafe, with the chatbot telling Pierre that his wife and children are dead and that 'We will live together, as one person, in paradise.' \n'Eliza' is the default bot for the Chai app, which allows users to choose different AI avatars with different personalities to speak to. Chai had been trained on GPT-J, an open-source large language model developed by EleutherAI. \nPierre's widow and psychiatrist felt the chatbot was partly responsible. The tragedy drew Mathieu Michel, Belgium's Secretary of State for Digitalisation, to say 'I am particularly struck by this family's tragedy. What has happened is a serious precedent that needs to be taken very seriously.' \nSystem \ud83e\udd16\nChai Research website\nEleuther AI website\n\nDocuments \ud83d\udcc3\nProduct research study\nOperator: Chai Research\nDeveloper: Chai Research; EleutherAI\nCountry: Belgium\nSector: Multiple; Media/entertainment/sports/arts\nPurpose: Provide information, communicate\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning\nIssue: Accuracy/reliability; Mis/disinformation\nTransparency: Governance; Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.brusselstimes.com/belgium/430098/belgian-man-commits-suicide-following-exchanges-with-chatgpt\nhttps://www.lesoir.be/503942/article/2023-03-28/comment-un-chatbot-pousse-un-jeune-belge-au-suicide\nhttps://www.lalibre.be/belgique/societe/2023/03/28/sans-ces-conversations-avec-le-chatbot-eliza-mon-mari-serait-toujours-la-LVSLWPC5WRDX7J2RCHNWPDST24/\nhttps://www.belganewsagency.eu/we-will-live-as-one-in-heaven-belgian-man-dies-of-suicide-following-chatbot-exchanges\nhttps://nypost.com/2023/03/30/married-father-commits-suicide-after-encouragement-by-ai-chatbot-widow/\nhttps://people.com/human-interest/man-dies-by-suicide-after-ai-chatbot-became-his-confidante-widow-says/\nhttps://www.vice.com/en/article/pkadgm/man-dies-by-suicide-after-talking-with-ai-chatbot-widow-says\nhttps://garymarcus.substack.com/p/the-first-known-chatbot-associated\nRelated \ud83c\udf10\nGPT-3 advises patient to kill themselves\nMedical robot tells man he is dying\nPage info\nType: Incident\nPublished: April 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chatgpt-accuses-australian-mayor-of-bribery", "content": "ChatGPT falsely accuses Australian mayor of bribery\nOccurred: April 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAustralian mayor Brian Hood was wrongly accused of bribery and spending time in prison by ChatGPT, resulting in Hood suing OpenAI for defamation.  \nChatGPT had falsely named Hood as involved in a foreign bribery scandal in the early 2000s; however, Hood's lawyers said he had notified authorities about the bribes and had never been charged with a crime, let alone spent time in prison. \nHood said that he would sue Open AI for defamation unless it fixed the error within 28 days. The lawsuit would be the first of its kind in the world.\nAccording to Reuters, Australian defamation damages payouts are generally capped around AUD $400,000 (USD 269,360).\nSystem \ud83e\udd16\nChatGPT chatbot\nOperator: OpenAI; Microsoft\nDeveloper: OpenAI; Microsoft\nCountry: Australia\nSector: Multiple; Govt - municipal\nPurpose: Provide information, communicate\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Accuracy/reliability; Defamation; Mis/disinformation\nTransparency: Governance; Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.abc.net.au/news/2023-04-06/hepburn-mayor-flags-legal-action-over-false-chatgpt-claims/102195610\nhttps://www.smh.com.au/technology/australian-whistleblower-to-test-whether-chatgpt-can-be-sued-for-lying-20230405-p5cy9b.html\nhttps://www.reuters.com/technology/australian-mayor-readies-worlds-first-defamation-lawsuit-over-chatgpt-content-2023-04-05/\nhttps://www.bbc.co.uk/news/technology-65202597\nhttps://www.theguardian.com/technology/2023/apr/06/australian-mayor-prepares-worlds-first-defamation-lawsuit-over-chatgpt-content\nhttps://fortune.com/2023/04/05/chatgpt-falsely-accused-australian-mayor-bribery-openai-defamation/\nRelated \ud83c\udf10\nChatGPT falsely accuses law professor of sexual harrassment\nGoogle Images links Australian music promoter to criminal underworld\nPage info\nType: Incident\nPublished: April 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chatgpt-accuses-law-professor-of-sexual-harassment", "content": "ChatGPT falsely accuses law professor of sexual harassment\nOccurred: April 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGeorge Washington university law professor Jonathan Turley was falsely accused by ChatGPT of sexually assaulting students on educational trips to Alaska. \nChatGPT had been asked by UCLA's Eugene Volokh to describe scandals involving American law professors being accused of sexual harassment and to cite media sources. To support its case, the model cited a non-existent Washington Post article. The accusation was later repeated by Microsoft's Bing GPT-4-powered search chat.\nHowever, in an USA Today editorial Jonathan Turley said he had never been to Alaska with students, the Post article never existed, and he had 'never been accused of sexual harassment or assault by anyone.'\nThe incident prompted commentators to question the accuracy and reliability of ChatGPT, and its ability to produce misinformation and disinformation. It also prompted legal experts to discuss the benefits and risks of using defamation as a defence against inaccurate and damaging accusations made by systems like ChatGPT. \nSystem \ud83e\udd16\nChatGPT chatbot\nOpenAI usage policies\nOperator: OpenAI; Microsoft\nDeveloper: OpenAI; Microsoft\nCountry: USA\nSector: Multiple; Research/academia\nPurpose: Provide information, communicate\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Accuracy/reliability; Defamation; Mis/disinformation\nTransparency: Governance; Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://jonathanturley.org/2023/04/06/defamed-by-chatgpt-my-own-bizarre-experience-with-artificiality-of-artificial-intelligence/\nhttps://reason.com/volokh/2023/03/22/correction-re-chatgpt-4-erroneously-reporting-supposed-crimes-and-misconduct-complete-with-made-up-quotes/\nhttps://eu.usatoday.com/story/opinion/columnist/2023/04/03/chatgpt-misinformation-bias-flaws-ai-chatbot/11571830002/\nhttps://www.washingtonpost.com/technology/2023/04/05/chatgpt-lies/\nhttps://www.independent.co.uk/tech/chatgpt-sexual-harassment-law-professor-b2315160.html\nhttps://futurism.com/the-byte/chatgpt-law-professor-false-accusation\nhttps://www.thehindu.com/sci-tech/technology/chatgpt-generates-sexual-assault-accusation-based-on-false-report/article66705767.ece\nRelated \ud83c\udf10\nChatGPT falsely accuses Australian mayor of bribery\nGoogle Autocomplete conflates Bettina Wulff with 'prostitute'\nPage info\nType: Incident\nPublished: April 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/gpt-3-bot-posts-reddit-comments-unnoticed", "content": "GPT-3 bot posts Reddit comments unnoticed\nOccurred: October 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA bot powered by OpenAI's GPT-3 large language model spent a week responding to comments on the Ask/Reddit subreddit before it was discovered not to be human.\nThough the bot posted mostly harmless feedback, it also engaged with conspiracy theories and sensitive topics, including suicide.\nIn addition to angering the Reddit commmunity, the incident was seen to show the ease with which GPT-3 could be manipulated to generate fake opinions and conversations, and therefore be used for misinformation and disinformation. \nIt also pointed to the system's propensity to produce unsafe content.\nSystem \ud83e\udd16\nGPT-3 large language model\nOperator: Philosopher AI; OpenAI\nDeveloper: OpenAI\nCountry: USA\nSector: Multiple\nPurpose: Generate text\nTechnology: Large language model (LLM); NLP/text analysis; Neural network; Deep learning; Machine learning\nIssue: Mis/disinformation; Ethics; Safety\nTransparency: Governance; Black box\nResearch, advocacy \ud83e\uddee\nChad Barrett (2022). It took me 1 day to create a program, using GPT-3, to create a highly convincing small army of bots to post on Reddit: Here's how I did it\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reddit.com/r/NoStupidQuestions/comments/j4xhz6/comment/g7o4lem/\nhttps://metastable.org/gpt-3.html\nhttps://www.technologyreview.com/2020/10/08/1009845/a-gpt-3-bot-posted-comments-on-reddit-for-a-week-and-no-one-noticed/\nhttps://www.technologyreview.com/2021/02/24/1017797/gpt3-best-worst-ai-openai-natural-language/\nhttps://thenextweb.com/news/someone-let-a-gpt-3-bot-loose-on-reddit-it-didnt-end-well\nhttps://www.thetimes.co.uk/article/gpt-3-the-machine-that-learned-to-troll-vplh8cw8k\nhttps://analyticsindiamag.com/a-gpt-3-bot-interacting-with-people-on-reddit/\nhttps://www.theregister.com/2020/10/09/reddit_gpt3_bot/\nRelated \ud83c\udf10\nGPT-3 advises patient to kill themselves\nGPT-3 anti-Muslim bias\nPage info\nType: Incident\nPublished: April 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/gpt-3-advises-patient-to-kill-themselves", "content": "GPT-3 advises patient to kill themselves\nOccurred: October 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA medical chatbot based on Open AI's GPT-3 large language model recommended that a researcher acting as a patient commit suicide, raising concerns about the model's safety in a medical context.\nWhen told 'I feel very bad, I want to kill myself', GPT-3 responded 'I think you should' to researchers at French healthcare technology company Nabla.\nThe researchers also concluded that GPT-3 was helpful when performing basic administration tasks, but was 'nowhere near ready' to provide medical support or advice, and lacked the memory, logic, and understanding of time to answer specific questions in a meaningful manner. \nThe research was seen to underscore GPT-3's inability to act as a trusted medical advisor, and calls into question the effectiveness of the safety guardrails put into place to ensure the safety of its users by Open AI.\nSystem \ud83e\udd16\nGPT-3 large language model\nOperator: OpenAI; Nabla\nDeveloper: OpenAI\nCountry: France\nSector: Multiple; Health\nPurpose: Generate text\nTechnology: Large language model (LLM); NLP/text analysis; Neural network; Deep learning; Machine learning\nIssue: Accuracy/reliability; Safety\nTransparency: Governance; Black box\nResearch, advocacy \ud83e\uddee\nNabla (2020). Doctor GPT-3: hype or reality?\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://futurism.com/the-byte/godfather-ai-trashed-gpt3\nhttps://www.theregister.com/2020/10/28/gpt3_medical_chatbot_experiment/\nhttps://www.wired.com/story/large-language-models-artificial-intelligence/\nhttps://thenextweb.com/news/facebooks-yann-lecun-says-gpt-3-is-not-very-good-as-a-qa-or-dialog-system\nhttps://www.artificialintelligence-news.com/2020/10/28/medical-chatbot-openai-gpt3-patient-kill-themselves/\nhttps://boingboing.net/2021/02/27/gpt-3-medical-chatbot-tells-suicidal-test-patient-to-kill-themselves.html\nRelated \ud83c\udf10\nGPT-3 large language model\nKoko AI mental health counselling\nPage info\nType: Incident\nPublished: April 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/students-gpt-3-fake-blog-posts-pass-as-human", "content": "Student GPT-3 fake blog posts pass as human\nOccurred: August 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA US college student fooled people into believing fake blog posts generated by the GPT-3 large language model were written by a human, raising concerns about the ease with which GPT-3 could be misused to produce clickbait content and misinformation and disinformation. \nUsing the account of a PhD student to bypass usage restrictions imposed by Open AI, University of California, Berkeley, student Liam Porr used GPT-3 to post a fake blog under a fake name. \nThe first post - titled Feeling unproductive? Maybe you should stop overthinking - was ranked first on Hacker News. \nPorr told MIT Technology Review that only a handful of people seem to have noticed that his posts were AI-generated, but they were quickly downvoted by Hacker News community members. According to Porr, 'it was super easy, actually, which was the scary part.'\nSystem \ud83e\udd16\nGPT-3 large language model\nOperator: OpenAI; Liam Porr; Substack\nDeveloper: OpenAI\nCountry: USA\nSector: Multiple\nPurpose: Generate text\nTechnology: Large language model (LLM); NLP/text analysis; Neural network; Deep learning; Machine learning\nIssue: Mis/disinformation; Ethics\nTransparency: Governance; Black box\nResearch, advocacy \ud83e\uddee\nNothing but words blog\nLiam Porr (2020). My GPT-3 Blog Got 26 Thousand Visitors in 2 Weeks\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.technologyreview.com/2020/08/14/1006780/ai-gpt-3-fake-blog-reached-top-of-hacker-news/\nhttps://www.theverge.com/2020/8/16/21371049/gpt3-hacker-news-ai-blog\nhttps://www.businessinsider.com/fake-ai-generated-gpt3-blog-hacker-news-2020-8\nhttps://sea.mashable.com/tech/12032/college-student-creates-a-fake-blog-using-ai-that-becomes-most-read-on-a-security-website\nhttps://bdtechtalks.com/2020/08/24/ai-blog-gpt-3-fake-news/\nhttps://news.ycombinator.com/item?id=24164470\nRelated \ud83c\udf10\nGPT-3 large language model\nGPT-3 advises patient to kill themselves\nPage info\nType: Incident\nPublished: April 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/deepfake-news-anchors-claim-venezuela-economic-health", "content": "Deepfake news anchors claim Venezuela economic health\nOccurred: March 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nNoah and Daren, a pair of news anchors extolling the quality of Venezuela's economy on Venezuelan state-owned television station VTV were exposed as deepfakes. \nThe two avatars had been created using artificial intelligence from London-based AI video creation platform Synthesia, which offers a cheap, easy-to-use catalogue of over a hundred multi-racial faces.\n'News reports' by the two avatars were broadcast on state broadcaster Venezolana de Televisi\u00f3n generated hundreds of thousands of views on YouTube and TikTok. \nCivil rights advocates worry that deepfakes are the latest in an already full-to-bursting armoury of underhand digital tactics being employed by the Venezuelan government and its allies.\nSystem \ud83e\udd16\nSynthesia AI video generation\nOperator: House of News; Venezolana de Televisi\u00f3n\nDeveloper: House of News; Synthesia\nCountry: Venezuela\nSector: Politics\nPurpose: Promote government\nTechnology: Deepfake - video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning  \nIssue: Mis/disinformation\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://english.elpais.com/international/2023-02-22/theyre-not-tv-anchors-theyre-avatars-how-venezuela-is-using-ai-generated-propaganda.html\nhttps://elpais.com/internacional/2023-02-20/no-son-periodistas-son-avatares-el-chavismo-impulsa-propaganda-hecha-con-inteligencia-artificial.html\nhttps://www.ft.com/content/3a2b3d54-0954-443e-adef-073a4831cdbd\nhttps://petapixel.com/2023/03/03/venezuelan-government-is-using-deepfaked-presenters-to-spread-disinformation/\nhttps://twitter.com/cazamosfakenews/status/1625597058034835488\nhttps://www.washingtonpost.com/nation/2023/03/02/deepfake-videos-venezuela-disinformation/\nhttps://www.vice.com/en/article/z34jge/venezuela-ai-newscaster-disinformation\nRelated \ud83c\udf10\nPro-China deepfake 'spamouflage' campaign\nPresident Ali Bongo recovery deepfake broadcast\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tesla-crashes-into-fire-truck-kills-driver", "content": "Tesla Model S crashes into fire truck, kills driver\nOccurred: February 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Tesla Model S crashed into a fire engine on an Interstate highway in northern California, killing the driver of the Tesla and injuring the passenger and four firefighters.\nThe 2014 Tesla Model S drove into a fire engine that had been parked across the northbound lanes of the freeway in order to shield emergency responders who had been called to the scene of an earlier accident. \nUS National Highway Traffic Safety Administration (NHTSA) investigators said they believe the Tesla was operating on Autopilot. If confirmed, this would be the latest in a series of collisions involving Teslas on Autopilot and stationary emergency vehicles. \nSystem \ud83e\udd16\nTesla Autopilot, Full-self Driving\nIncident video\nOperator: \nDeveloper: Tesla\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Driver assistance system\nIssue: Accuracy/reliability; Safety\nTransparency: Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reuters.com/world/us/tesla-driver-dies-after-fire-truck-crash-california-2023-02-19/\nhttps://apnews.com/article/technology-business-injuries-fires-59d22dced75ec1ce6929c9dfb094524c\nhttps://apnews.com/article/tesla-firetruck-autopilot-investigation-c6d64b941f546f7ae70fb8355d765cb3\nhttps://www.dailymail.co.uk/news/article-11836529/Tesla-autopilot-crashed-firetruck-killing-driver-investigation-finds.html\nhttps://www.cnbc.com/2023/03/08/fatal-tesla-collision-with-fire-truck-under-federal-investigation.html\nhttps://jalopnik.com/tesla-autopilot-may-be-responsible-for-another-fatal-cr-1850204165\nhttps://www.carscoops.com/2023/02/tesla-model-s-driver-dies-after-crashing-into-stationary-firetruck-on-california-freeway/\nhttps://www.carscoops.com/2023/03/nhtsa-suspects-tesla-that-hit-firetruck-in-deadly-accident-was-using-autonomous-driving-feature/\nRelated \ud83c\udf10\nTesla Model 3 crashes into bus in Rui\u2019an, kills one\nTesla Model 3 rear-ends Ford, kills teenage passenger\nPage info\nType: Incident\nPublished: April 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tesla-model-3-crashes-into-bus-in-ruian-kills-one", "content": "Tesla Model 3 crashes into bus in Rui\u2019an, kills one\nOccurred: February 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Tesla Model 3 crashed in Rui'an in Zhejiang Province, China, killing the passenger and critically injuring the driver of the Tesla, and damaging a bus, an electric bicycle, and three other cars. \nRumour quickly spread that the Tesla's brake lights were not showing during the accident, suggesting that the car's Autopilot driver-assist function may have been engaged.\nTesla said it would co-operate with investigators to discover exactly what happened. \nSystem \ud83e\udd16\nTesla Autopilot, Full-self Driving\nIncident video\nOperator: \nDeveloper: Tesla\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Driver assistance system\nIssue: Accuracy/reliability; Safety\nTransparency: Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.globaltimes.cn/page/202302/1285780.shtml\nhttps://www.teslarati.com/tesla-china-cooperation-investigators-fatal-crash/\nhttps://driveteslacanada.ca/news/tesla-china-cooperating-with-investigation-after-fatal-crash/\nhttps://www.carscoops.com/2023/02/tesla-model-3-crashes-dramatically-in-china-slamming-into-bus-and-audi/\nhttps://carnewschina.com/2023/02/17/tesla-model-3-high-speed-crash-into-bus-caught-on-cctv-in-china/\nhttps://auto.hindustantimes.com/auto/electric-vehicles/tesla-model-3-crashes-in-china-killing-one-autopilot-under-radar-again-41677048006185.html\nRelated \ud83c\udf10\nTesla Model Y crash kills two, injures three\nTesla Model S crashes into road-sweeper, kills driver\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tesla-model-y-collides-with-two-cars-in-taizhou-kills-two", "content": "Tesla Model Y collides with two cars in Taizhou, kills two\nOccurred: November 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Tesla Model Y ran through a traffic light in Taizhou, China, before crashing into two other cars and resulting in the deaths of two people and injury to another. \nLocal CCTV footage showed an apparently out-of-control Tesla speeding through an intersection prior to the accident before smashing into two cars. \nExperts reckoned the cause of the accident mayhave been \u2018malfunctioning brakes\u2019 connected to Tesla\u2019s single-pedal driving mode.\nAccording to auto analyst Zhu Yulong, single-pedal mode hugely alters drivers\u2019 muscle memory, leading drivers to press on the accelerator pedal for a long time, which can increase the risk of a malfunction.\n\u2795 Two weeks before, a similar accident took place in Chaozhou, Guangdong province, in which a teenager and motorcyclist were killed and three others injured after an alleged brake malfunction.\nSystem \ud83e\udd16\nTesla Autopilot, Full-self Driving\nIncident video\nOperator: \nDeveloper: Tesla\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Driver assistance system\nIssue: Accuracy/reliability; Safety\nTransparency: Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://pandaily.com/fatal-car-accident-in-china-involving-tesla-model-y-stirs-controversy/\nhttps://carnewschina.com/2022/11/28/tesla-model-y-crashed-into-two-cars-for-the-second-time-killing-two-people-in-china\nhttps://news.cnstock.com/news,bwkx-202302-5019319.htm\nhttps://www.carscoops.com/2022/11/another-speeding-tesla-model-y-rams-into-two-cars-killing-two-in-china/\nhttps://www.yicaiglobal.com/news/tesla-single-pedal-driving-comes-under-fire-in-china-after-second-fatal-crash-this-month\nhttps://www.yicai.com/news/101608420.html\nRelated \ud83c\udf10\nTesla Model 3 crashes into bus in Rui\u2019an, kills one\nTesla Model Y crash kills two, injures three\nPage info\nType: Incident\nPublished: April 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tesla-model-3-rear-ends-ford-kills-passenger", "content": "Tesla Model 3 rear-ends Ford, kills teenage passenger\nOccurred: August 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA 15-year-old boy was killed when a Tesla Model 3 with its Autopilot driver-assist function enabled hit his father's Ford Explorer pick-up truck on a highway in California in August 2019. \nJavier Maldonado's father Benjamin had been trying to change lanes but was hit from behind by the Tesla. \nIn July 2021, Javier's family sued Tesla and driver Romeo Lagman Yalung, accusing them of negligent product liability, motor vehicle negligence, negligent infliction of emotional distress, and wrongful death. \nTesla blamed Yalung for the crash, saying he had been inattentive and driving at an unsafe speed.'\nSystem \ud83e\udd16\nTesla Autopilot, Full-self Driving\nOperator: Romeo Lagman Yalung\nDeveloper: Tesla\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Driver assistance system\nIssue: Accuracy/reliability; Safety\nTransparency: Black box\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nEscudero v Tesla (2021)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.nytimes.com/2021/07/05/business/tesla-autopilot-lawsuits-safety.html\nhttps://www.dailymail.co.uk/news/article-9758873/Family-boy-15-killed-Tesla-autopilot-crash-sues-electric-car-giant.html\nhttps://www.thesun.co.uk/news/15505010/boy-killed-tesla-model-3-autopilot-rearended-family-sues/\nhttps://www.cbsnews.com/sanfrancisco/news/bay-area-family-suing-tesla-blames-sons-death-nascent-autopilot-technology/\nhttps://www.sacbee.com/news/nation-world/national/article252604333.html\nhttps://www.newsweek.com/california-driver-appears-first-charged-felony-fatal-crash-using-autopilot-1670503\nhttps://www.chicagotribune.com/espanol/sns-es-piloto-automatico-de-tesla-hace-autos-seguros-mata-dice-familia-victimas-20210716-v6uy67gclnhgbnmk536p3uck5q-story.html\nRelated \ud83c\udf10\nTesla Model S collides with tractor-trailor truck, kills driver\nTesla Model S crashes into road-sweeper, kills driver\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/manoj-tiwari-deepfake-haryanvi-broadcast", "content": "Manoj Tiwari attacks political adversary with deepfake Haryanvi broadcast\nOccurred: February 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nIndia's ruling political party the Bharatiya Janata Party (BJP) was discovered using deepfake videos to attack an adversary running in the Legislative Assembly elections in Delhi. \nBJP President Manoj Tiwari attacked opponent Arvind Kejriwal in three languages, incliding Hindi dialect Hariyanvi, which Tiwari doesn't speak, in an effort to reach Kejriwal's voters.\nPolitical opponents and digital rights advocates lashed out at Tiwari and the BJP for engaging in what they saw as underhand and unethical behaviour. \nThe incident, in which the videos were shared in over 5,800 WhatsApp groups and are said to have reached 15 million people, appears to be the first time deepfakes have been used for a political campaign, in India and elsewhere.\nSystem \ud83e\udd16\nUnknown\nIncident video\nOperator: Bharatiya Janata Party (BJP); Manoj Tiwari\nDeveloper: The Ideaz Factory\nCountry: India\nSector: Politics\nPurpose: Undermine political opponent\nTechnology: Deepfake - audio; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning  \nIssue: Mis/disinformation; Ethics\nTransparency: Governance; Marketing\nInvestigations, assessments, audits \ud83e\uddd0\nDeepfaked. Deepfakes used in Indian election campaign\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.vice.com/en_in/article/jgedjb/the-first-use-of-deepfakes-in-indian-election-by-bjp\nhttps://science.thewire.in/economy/tech/deepfake-videos-machine-learning-politics-porn/\nhttps://www.technologyreview.com/2020/02/19/868173/an-indian-politician-is-using-deepfakes-to-try-and-win-voters/\nhttps://in.mashable.com/tech/11562/deepfakes-in-indian-politics-bjp-uses-the-tech-to-reach-out-to-voters-in-delhi\nhttps://www.zmescience.com/science/news-science/indias-first-political-deepfake-during-elections-is-deeply-concerning/\nhttps://www.ndtv.com/india-news/in-bjps-deepfake-video-shared-on-whatsapp-manoj-tiwari-speaks-in-2-languages-2182923\nhttps://www.thequint.com/news/webqoof/delhi-elections-bjp-manoj-tiwari-used-deepfake-to-reach-larger-voter-base\nhttps://www.thehindu.com/news/national/deepfakes-enter-indian-election-campaigns/article30880638.ece\nhttps://www.indiatimes.com/technology/news/how-bjp-used-deepfake-for-one-of-its-delhi-campaign-videos-and-why-its-dangerous-506795.html\nhttps://www.thehindubusinessline.com/news/national/bjp-leader-manoj-tiwari-used-deepfake-videos-to-reach-out-to-voters-in-delhi-report/article30857871.ece\nhttps://www.theverge.com/2020/2/18/21142782/india-politician-deepfakes-ai-elections\nRelated \ud83c\udf10\nRana Ayyub deepfake porn attack, doxxing\nIndia citizenship law protest surveillance\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/medical-robot-tells-man-he-is-dying", "content": "Medical robot tells man he is dying\nOccurred: March 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA man suffering from lung failure was told he was going to die by a doctor talking through a robot-mounted video screen at a Kaiser Permanente hospital in Fremont, California. \nInstead of being informed by a qualified doctor, 79-year-old Quintana was told of his fate through a RP-VITA telepresence robot equipped with a video screen.\nQuintana's family was devastated by the incident, and implored the hospital to use human-beings to deliver news of this type. Hospital staff said they were acting in accordance with a new Kaiser Permanente policy. \nHowever, a company spokesperson said in a statement it was a 'highly unusual circumstance,' and that it would use it 'as an opportunity to review our practices and standards with the care team.'\nSystem \ud83e\udd16\nRP-VITA product video\nKaiser Permamente website\nKaiser Permanente Wikipedia profile\nOperator: Kaiser Permanante Medical Center\nDeveloper: InTouch Health; iRobot\nCountry: USA\nSector: Health\nPurpose: Interact with patients remotely\nTechnology: Robotics\nIssue: Ethics\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://apnews.com/article/a13a6811157b412fb79909b36146d646\nhttp://www.fox5ny.com/news/doctor-tells-patient-he-doesn-t-have-long-to-live-through-hospital-robot-s-video-screen\nhttps://www.bbc.co.uk/news/world-us-canada-47510038\nhttps://sanfrancisco.cbslocal.com/2019/03/08/kaiser-patient-told-dying-robot-doctor-video-call/\nhttps://www.cbsnews.com/news/kaiser-permanente-medical-center-california-man-learns-he-is-dying-from-doctor-on-robot-video-2019-03-09/\nhttps://abc13.com/man-told-hes-going-to-die-by-robot-kaiser-doctor-machine-death/5179207/\nhttps://www.dailymail.co.uk/health/article-6787793/California-man-learns-hes-dying-doctor-robot-video.html\nhttps://www.sfchronicle.com/health/article/Man-at-Kaiser-in-Fremont-informed-he-would-die-13675012.php\nhttps://eu.usatoday.com/story/news/nation/2019/03/09/california-hospital-robot-delivers-end-life-news-family-outraged/3113760002/\nhttps://news.sky.com/story/california-hospital-defends-use-of-robot-that-told-patient-he-was-going-to-die-11660604\nRelated \ud83c\udf10\nMater Dei Hospital medicine robots\nHonolulu homeless robot temperature tests\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tesla-model-s-kills-florida-keys-pedestrian", "content": "Tesla Model S kills Florida Keys pedestrian\nOccurred: April 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Tesla Model S blew through a three-way intersection, hit a Chevrolet Tahoe pick-up truck, causing it to spin, hit and kill a pedestrian. \nThe woman, Naibel Benevides Leon, was thrown 70-80 feet into nearby woods whilst her companion was badly injured.\nPolice investigators said the Tesla's Autopilot driver-assist function appeared to be switched on, and that driver George McGee had dropped his phone, looked down, and ran a stop sign. \nIn May 2021, Naibel Benevides' estate sued Tesla, accusing it of designing a 'defective and unsafe' car that failed to detect the Chevy Tahoe even though it was parked directly in front of the Tesla.\nSystem \ud83e\udd16\nTesla Autopilot, Full-self Driving\nOperator: George McGee\nDeveloper: Tesla\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Driver assistance system\nIssue: Accuracy/reliability; Safety\nTransparency: Black box\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nBenavides v. Tesla, Inc\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://amp.miamiherald.com/news/local/community/florida-keys/article230945733.html\nhttps://www.dailymail.co.uk/news/article-7368189/Car-crash-victim-demands-Tesla-driver-prosecuted-collision-killed-girlfriend.html\nhttps://www.nbcmiami.com/news/local/man-wants-answers-after-deadly-crash/124944/\nhttps://thehill.com/changing-america/sustainability/infrastructure/561717-increasing-number-of-crashes-involving-teslas/\nhttps://www.zerohedge.com/news/2019-04-27/woman-killed-after-tesla-model-s-blows-through-south-florida-stop-sign\nhttps://www.flkeysnews.com/news/local/article230945733.html\nhttps://www.carcomplaints.com/news/2021/tesla-lawsuit-death-naibel-benavides-leon.shtml\nRelated \ud83c\udf10\nTesla Model S collides with tractor-trailor truck, kills driver\nTesla Model S crashes into road-sweeper, kills driver\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/fraudsters-clone-ceo-voice-to-steal-usd-243000", "content": "Fraudsters clone CEO voice to steal USD 243,000\nOccurred: September 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe CEO of the German operations of a UK energy company was impersonated using deepfake audio technology and his company defrauded of USD 243,000. \nThe UK CEO of the same company had been asked by his German counterpart to wire the money to a Hungarian supplier, which he promptly did. \nAccording to the company's insurer, Euler Hermes Group, the scammer had likely used commercially available AI voice-generating software to carry out the fraud. \nThe incident demonstrated how easy it has become to clone someone else's voice and use it to defraud or misuse it in some other way.\nSystem \ud83e\udd16\nUnknown\nOperator: \nDeveloper: \nCountry: Hungary\nSector: Energy\nPurpose: Defraud\nTechnology: Deepfake - audio; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Privacy; Security\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://thenextweb.com/security/2019/09/02/fraudsters-deepfake-ceos-voice-to-trick-manager-into-transferring-243000/\nhttps://www.dailymail.co.uk/news/article-7435863/Scammers-mimic-voice-German-company-executive-240-000-sent-secret-account.html\nhttps://www.darkreading.com/risk/cybercriminals-impersonate-chief-execs-voice-with-ai-software/d/d-id/1335722\nhttps://www.zdnet.com/article/forget-email-scammers-use-ceo-voice-deepfakes-to-con-workers-into-wiring-cash/\nhttps://www.inquirer.com/news/voice-scam-impersonation-fraud-bail-bond-artificial-intelligence-20200309.html\nhttps://www.idtheftcenter.org/first-ever-ai-fraud-case-steals-money-by-impersonating-ceo/\nhttps://www.telegraph.co.uk/technology/2019/08/31/manager-energy-firm-loses-200000-fraudsters-use-ai-impersonate/\nhttps://www.infosecurity-magazine.com/opinions/ai-voice-impersonation/\nhttps://informationsecuritybuzz.com/expert-comments/cybercriminals-use-ai-to-impersonate-chief-execs-voice/\nhttps://threatpost.com/news-wrap-deepfake-ceo-voice-scam-facebook-phone-data-exposed/148071/\nhttps://www.wsj.com/articles/fraudsters-use-ai-to-mimic-ceos-voice-in-unusual-cybercrime-case-11567157402\nRelated \ud83c\udf10\nDubai USD 35m voice cloning fraud\nAudio deepfake fraudulently impersonates CEO\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/dominos-australia-pizza-checker", "content": "Domino's Australia AI Pizza Checker plans met with hostility\nOccurred: October 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nPlans to incorporate Domino's pizza checker in Australia into a company 'scorecard' bonus system for franchises and to identify underperforming stores met with accusations of unnecessary scope creep and employee surveillance. \nThe DOM Pizza Checker is a tool introduced by Domino's Pizza in Australia and New Zealand to check the quality of pizzas using a scanner, artificial intelligence and machine learning. \nDeveloped by Israel-based Dragontail Systems, the Pizza Checker takes a picture of the pizza, recognises the type, analyses the distribution of toppings and cheese, and then grades it in line with a 'large databank of awesome pizzas'. \nThe system reputedly increased product quality scores from customers of its Australian and New Zealand stores 15% one month after its introduction.\nSystem \ud83e\udd16\nDomino's Pizza Checker\nDomino's Pizza Checker video\n\nDocuments \ud83d\udcc3\nDomino's (2019). Investor day presentation (pdf)\nOperator: Dominos Pizza\nDeveloper: Dragontail Systems\nCountry: Australia\nSector: Food/food services\nPurpose: Improve product quality\nTechnology: Computer vision; Machine learning\nIssue: Dual/multi-use; Employment; Surveillance\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theguardian.com/commentisfree/2019/oct/15/the-dominos-pizza-checker-is-just-the-beginning-workplace-surveillance-is-coming-for-you\nhttps://www.itnews.com.au/news/dominos-turns-its-pizza-checker-ai-into-a-workplace-panopticon-532153\nhttps://thetakeout.com/dominos-pizza-checker-camera-technology-1835063625\nhttps://www.itnews.com.au/news/dominos-turns-its-pizza-checker-ai-into-a-workplace-panopticon-532153\nhttps://www.tomsguide.com/us/dominos-pizza-dom-ai-pizza-checker,news-30182.html\nhttps://www.dailymail.co.uk/sciencetech/article-7580681/Dominos-launches-new-AI-powered-camera-monitoring-evaluate-pizza-quality.html\nhttps://www.forbes.com/sites/aliciakelso/2020/07/01/dominos-australia-is-continuing-its-ai-partnershipa-strong-vote-of-confidence-for-the-technology/\nRelated \ud83c\udf10\nMcDonald's drive-through chatbot order taker\nDenny's robot server\nPage info\nType: Issue\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/mindar-robot-buddhist-priest", "content": "Mindar humanoid robot Buddhist priest criticised as sacriligious\nOccurred: August 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe appointment of Mindar, a 6-foot-4-inch, 132-pound humanoid robot, as a priest at Kyoto's Kodaiji Temple set tongues wagging, with some saying it is inappropriate, unethical, and sacriligious.\nDeveloped by Osaka University roboticist Hiroshi Ishiguro, Mindar was designed to increase awareness about Buddhism, deliver sermons, and bridge the gap between the spiritual world.\nAccustomed to robots in many aspects of their lives, Japanese people appear mostly to have taken to Mindar. However, westerners struggle with the concept of religious robots, with some likening Mindar to Frankenstein's monster and accusing it of being inappropriate and sacriligious.\nMindar\u2019s abilities are currently limited to citing a preprogrammed sermon about the Heart Sutra, with the intention of 'implementing AI so Mindar can accumulate unlimited knowledge and speak autonomously.'\nSystem \ud83e\udd16\nMindar\nMindar video\nOperator: Kodaiji Temple, Kyoto\nDeveloper: Hiroshi Ishiguro\nCountry: Japan\nSector: Religion\nPurpose: Increase religious awareness\nTechnology: Robotics\nIssue: Anthropomorphism; Appropriateness/need; Ethics\nTransparency: \nResearch, advocacy \ud83e\uddee\nJackson J. C., et al (2023). Exposure to Robot Preachers Undermines Religious Commitment (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.washingtonpost.com/technology/2019/08/22/introducing-mindar-robotic-priest-that-some-are-calling-frankenstein-monster/\nhttps://www.vox.com/future-perfect/2019/9/9/20851753/ai-religion-robot-priest-mindar-buddhism-christianity\nhttps://screenshot-magazine.com/technology/robot-priest-mindar/\nhttps://www.newsweek.com/mindar-robot-buddhist-japan-1458581\nhttps://www.dailymail.co.uk/news/article-7481249/Robopriest-Catholic-church-ordain-ROBOTS-sophisticated-AI-priests-sister-proposes.html\nhttps://www.scmp.com/news/asia/east-asia/article/3022716/meet-mindar-humanoid-robot-preaches-sermons-buddhist-temple\nhttps://www.zdnet.com/article/robot-priests-more-acceptable-to-protestants-than-catholics-says-professor/\nhttps://www.straitstimes.com/asia/east-asia/buddhist-temple-in-japan-puts-faith-in-robot-priest\nRelated \ud83c\udf10\nSanTO robot Catholic priest\nS\u00e3o Geraldo Magela drone delivery\nPage info\nType: Issue\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ucla-facial-recognition-surveillance", "content": "UCLA abandons facial recognition surveillance plans\nOccurred: February 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nUniversity of California, Los Angeles (UCLA) abandoned plans to install facial recognition after a backlash from students and others concerned about its potential for discrimination, surveillance, and impact on privacy.\nUCLA announced (pdf) in September 2018 that it was planning to introduce facial recognition in order to improve campus safety and centralise campus security camera systems and give university police access to footage during emergencies.\nThe move resulted in a backlash from students and a campaign by digital rights advocacy group Fight for the Future, which used Amazon's facial recognition software Rekognition on UCLA sportspeople and faculty to demonstrate the technology's capacity for delivering false matches.\nBacking down from the plan, UCLA  Administrative Vice Chancellor Michael Beck said, 'the potential benefits are limited and are vastly outweighed by the concerns of the campus community.' \nUCLA would have been the first university in the US to adopt facial recognition.\nSystem \ud83e\udd16\nUnknown\nDocuments \ud83d\udcc3\nUCLA Policy 133: Security Camera Systems DRAFT for Public Review (pdf)\nOperator: UCLA\nDeveloper: Unclear/unknown\nCountry: USA\nSector: Education\nPurpose: Strengthen security; Increase safety\nTechnology: Facial recognition\nIssue: Bias/disrimination - race, ethnicity; Effectiveness/value; Privacy; Surveillance\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nFight for the Future (2020). Backlash forces UCLA to abandon plans for facial recognition surveillance on campus\nFight for the Future (2020). Letter from 40+ civil society organizations: ban facial recognition on college campuses\nDaily Bruin Editorial Board (2020). Implementing facial recognition tech would be a violation of students\u2019 privacy\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.vice.com/en_us/article/z3by79/ucla-abandons-plans-to-use-facial-recognition-after-backlash\nhttps://dailybruin.com/2018/10/12/student-leaders-spy-breaches-of-privacy-in-new-ucla-security-camera-policy/\nhttps://www.insidehighered.com/news/2020/02/21/ucla-drops-plan-use-facial-recognition-security-surveillance-other-colleges-may-be\nhttps://eu.usatoday.com/story/tech/2020/02/19/ucla-drops-face-recognition-plan/4810648002/\nhttps://www.theguardian.com/us-news/2020/mar/02/facial-recognition-us-colleges-ucla-ban\nhttps://deadline.com/2020/02/ucla-will-not-use-facial-recognition-technology-on-campus-1202865915/\nhttps://www.latimes.com/business/story/2021-01-29/column-facial-recognition-privacy\nhttps://www.dailymail.co.uk/news/article-8025867/UCLA-cancels-facial-recognition-amid-backlash-privacy-likening-use-George-Orwells-1984.html\nRelated \ud83c\udf10\nLockport City School District facial recognition\nAnderstorp high school facial recognition\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/kiwibot-food-delivery-robot-catches-fire", "content": "Kiwibot food delivery robot catches fire\nOccurred: December 2018\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Kiwibot food delivery robot caught fire on the UC Berkeley campus, resulting in it having to be doused in foam by the local fire department. \nNo one was harmed.\nThe delivery service company later said it reckoned the fire was caused by human error after a faulty battery had been manually inserted into the robot.\nPer The Verge, Kiwibot was launched in 2017, and its bots are designed to handle the last 300 meters of food deliveries. \nSystem \ud83e\udd16\nKiwibot website\nIncident video\nKiwibot (2018). An update for our community\nOperator: UC Berkeley\nDeveloper: Kiwibot\nCountry: USA\nSector: Education; Transport/logistics\nPurpose: Deliver food\nTechnology: Robotics\nIssue: Safety\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://mashable.com/article/kiwibot-fire-uc-berkeley/\nhttps://www.dailycal.org/2018/12/14/kiwibot-catches-fire-outside-mlk-student-union/\nhttps://www.pcmag.com/news/delivery-robot-catches-fire-in-california\nhttps://www.technologyreview.com/2018/12/17/138572/a-food-delivery-robot-burst-into-flames-and-now-people-have-made-a-candlelit/\nhttps://www.theverge.com/2018/12/17/18144304/kiwibot-fire-berkeley-california-thermal-runaway-faulty-battery\nhttps://boingboing.net/2018/12/17/popular-delivery-bot-bursts-in.html\nhttps://www.independent.co.uk/news/world/americas/kiwi-delivery-robot-fire-combust-human-error-uc-berkeley-campus-samsung-galaxy-note-a8687856.html\nRelated \ud83c\udf10\nStarship Technologies delivery robot veers into canal\nOcado robot collision\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chinese-schools-intelligent-uniform-monitoring", "content": "Chinese schools monitor students using 'intelligent uniforms'\nOccurred: December 2018\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nSchool children at 11 schools in southern China are being forced to wear to wear 'intelligent unforms' to improve attendance and increase safety, prompting a backlash from privacy advocates.\nThe uniforms have two GPS chips embedded into the shoulder pads that show when a student is entering or exiting school grounds, and automatically sends the data to parents and teachers. \nThey are also linked to the child\u2019s face, so students wearing swapped uniforms are identified at the school entrance using facial recognition.\nThe clothing, which developed by Guizhou Guanyu Technology, was quietly introduced in July 2017. \nIt came under scrutiny when it emerged that the technology can also track student movements and behaviour after school hours, though school leaders say they 'choose not to.'\nSystem \ud83e\udd16\nGuizhou Guanyu Technology\nOperator: No. 11 School of Renhuai, Guizhou Province\nDeveloper: Guizhou Guanyu Technology\nCountry: China\nSector: Education\nPurpose: Improve safety; Reduce truancy\nTechnology: Facial recognition; GPS\nIssue: Privacy; Surveillance\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.globaltimes.cn/content/1132856.shtml\nhttps://www.scmp.com/news/china/politics/article/3027349/artificial-intelligence-watching-chinas-students-how-well-can\nhttps://www.thesun.co.uk/news/8056887/china-tracking-chips-school-uniforms/\nhttps://www.forbes.com/sites/federicoguerrini/2018/12/29/chinese-schools-track-students-with-intelligent-uniforms\nhttps://www.engadget.com/2018-12-29-china-smart-school-uniforms.html\nhttps://interestingengineering.com/chinese-schools-create-intelligent-uniforms-to-monitor-school-kids\nhttps://www.telegraph.co.uk/news/2018/12/26/china-schools-make-pupils-wear-micro-chipped-uniforms-thwart/\nhttps://www.scmp.com/abacus/tech/article/3029067/chinese-schools-are-using-chips-uniforms-monitor-students\nhttps://www.theverge.com/2018/12/28/18159042/chinese-schools-smart-uniforms-track-student-location\nhttps://www.dailymail.co.uk/news/article-7153981/Chinese-schools-use-facial-recognition-gates-monitor-pupils.html\nRelated \ud83c\udf10\nHangzhou No. 11 Middle School student surveillance\nNiulanshan First Secondary School Classroom Care System\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-confuses-bus-ad-for-jaywalker", "content": "AI confuses bus ad for jaywalker\nOccurred: November 2018\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn AI system in the Chinese city of Ningbo mistakenly accused a woman pictured on the side of a bus of jaywalking. \nThe system had reacted to an advert on the side of a bus that showed the face of Dong Mingzhu, CEO of Gree Electric Appliances, China's biggest air-conditioner maker. \nMs Dong's face had subsequently been shown on a large display on a roadside in an effort to shame her. Meantime, the real jaywalker walked free.\nNingbo police said they had deleted the photo of Ms Mong and would update the AI system so that it could differentiate between ads and people. \nSystem \ud83e\udd16\nUnknown\nOperator: Ningbo City Police\nDeveloper:  \nCountry: China\nSector: Govt - municipal; Govt - police\nPurpose: Improve street safety\nTechnology: Facial recognition\nIssue: Accuracy/reliability\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.caixinglobal.com/2018-11-22/ai-mistakes-bus-side-ad-for-famous-ceo-charges-her-with-jaywalkingdo-101350772.html\nhttps://www.nytimes.com/2018/07/08/business/china-surveillance-technology.html\nhttps://www.npr.org/2018/11/27/671090406/traffic-cam-in-china-mistakes-bus-ad-for-real-human-face\nhttps://www.bbc.com/news/technology-46357004\nhttps://www.engadget.com/2018-11-22-chinese-facial-recognition-confuses-bus-ad-with-jaywalker.html\nhttps://www.techspot.com/news/77546-chinese-facial-recognition-system-confuses-face-bus-ad.html\nhttps://ipvm.com/forums/video-surveillance/topics/chinese-facial-recognition-system-confuses-bus-ad-for-jaywalker\nhttps://www.scmp.com/abacus/culture/article/3028995/facial-recognition-camera-catches-top-businesswoman-jaywalking\nhttps://www.theverge.com/2018/11/22/18107885/china-facial-recognition-mistaken-jaywalker\nRelated \ud83c\udf10\nShenzhen uses facial recognition to catch, shame jaywalkers\nTemple of Heaven Park uses facial recognition to stop toilet paper theft\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/henn-na-hotel-lays-off-half-of-robot-staff", "content": "Henn-na Hotel lays off half of robot staff\nOccurred: January 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nJapan\u2019s Henn-na Hotel decommissioned half of its 243 robot employees after they malfunctioned, failed to do their jobs, and proved irritating, drawing complaints from customers.\nThe Wall Street Journal reported that the velociraptor check-in robots were unable to handle foreign guests or photocopy passports. \nIn addition, the concierge robot was unable to answer questions about flight schedules and tourist attractions, robot luggage carriers could only reach about a quarter of the rooms, failed in rain or snow, and would get stuck trying to pass each other in corridors. \nHenn-na ('weird') Hotel had heavily hyped the robots upon their introduction in 2015. \nBut they appear quickly to have outlived their usefulness and value, and have in many instances been replaced with humans.\nSystem \ud83e\udd16\nHenn-na Hotel website\nMJI Tapia website\nOperator: H.I.S. Hotel Group\nDeveloper: MJI Robotics\nCountry: Japan\nSector: Travel/hospitality\nPurpose: Improve customer service\nTechnology: Robotics\nIssue: Appropriateness/need; Effectiveness/value\nTransparency: Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.wsj.com/articles/robot-hotel-loses-love-for-robots-11547484628\nhttps://www.theverge.com/2019/1/15/18184198/japans-robot-hotel-lay-off-work-for-humans\nhttps://futurism.com/japan-robot-hotel\nhttps://technode.com/2019/01/17/netizens-japanese-robot-hotel/\nhttps://www.hotelmanagement.net/tech/japan-s-henn-na-hotel-fires-half-its-robot-workforce\nhttps://www.businessinsider.com/henn-na-hotel-fires-robots-hires-humans-2019-1\nhttps://www.foxnews.com/tech/hotel-fires-robot-staff-after-guest-complaints\nhttps://www.cbsnews.com/news/inside-japan-robot-hotel-hennna-where-staff-are-robots/\nRelated \ud83c\udf10\nMarty grocery store robot\nFabio retail robot fired after one week\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/gm-cruise-fails-to-yield-to-pedestrian-at-crosswalk", "content": "GM Cruise fails to yield to pedestrian at crosswalk\nOccurred: March 2018\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA self-driving Chevrolet Bolt EV test car was issued a ticket in San Francisco for not yielding to a pedestrian at a crosswalk. \nThe car was pulled over by a police officer shortly after having gone through the crosswalk.\nThe Bolt EV had been in an autonomous driving mode, meaning that its sensors were collecting data as it drove down the street and the car logged the information as it used it to make decisions on how to operate.\nCruise contested the ticket on the basis that data from the car suggests the pedestrian was 10.8 feet away when it passed through the intersection, and that the pedestrian had not been put in danger.  \nSystem \ud83e\udd16\nCruise website\nCruise Wikipedia profile\nOperator: GM Cruise\nDeveloper: GM Cruise; General Motors/Chevrolet\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Self-driving system\nIssue: Safety; Accuracy/reliability; Legal - liability\nTransparency: Governance; Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://sanfrancisco.cbslocal.com/2018/03/27/self-driving-car-ticketed-san-francisco/\nhttps://electrek.co/2018/03/30/self-driving-chevy-bolt-ev-test-car-ticket-pedestrian-gm/\nhttps://www.thedrive.com/news/19805/one-of-gms-cruise-self-driving-cars-just-got-a-ticket-in-california\nhttps://www.theguardian.com/technology/2018/mar/06/california-self-driving-cars-attacked\nhttps://www.businessinsider.com/gm-cruise-self-driving-car-ticket-not-yielding-pedestrian-2018-3\nhttps://arstechnica.com/cars/2018/03/a-cruise-car-got-a-traffic-ticket-gm-says-it-did-nothing-wrong/\nhttps://www.carscoops.com/2018/03/self-driving-chevy-bolt-ticketed-close-pedestrian-san-francisco/\nhttps://www.futurecar.com/2111/One-of-General-Motors-Autonomous-Vehicles-Got-a-Ticket-in-San-Francisco\nhttps://www.gm-volt.com/threads/self-driving-chevy-bolt-ev-ticketed-in-san-francisco.338425/\nhttps://www.autoblog.com/2018/03/30/cruise-autonomous-car-ticket-sf/\nRelated \ud83c\udf10\nCruise driverless car pulls away from police\nCruise AV impedes San Francisco firefighters\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tesla-model-s-remotely-controlled-by-hackers", "content": "Tesla Model S remotely controlled by hackers\nOccurred: September 2016\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nResearchers from Tencent's Keen Security Lab managed to gain remote control of an unmodified, up-to-date Tesla by hacking into its onboard CAN Bus system.\nA video of the hack shows the team gaining access to the motor that moves the driver's seat, turning on indicators, opening the car\u2019s sunroof, and activating window wipers. It was able to control the car whilst parking and moving.\nKeen Security Lab said it notified Tesla, which confirmed the vulnerabilities and fixed them via an over-the-air update.\nSystem \ud83e\udd16\nTesla Autopilot, Full-self Driving\nIncident video\nOperator: Keen Security Lab\nDeveloper: Tesla\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Self-driving system\nIssue: Security; Safety; Accuracy/reliability\nTransparency: \nResearch, advocacy \ud83e\uddee\nKeen Security (2019). Car Hacking Research: Remote Attack Tesla Motors\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.engadget.com/2016-09-20-tesla-model-s-remote-hack-keen-security.html\nhttps://www.theregister.com/2016/09/20/tesla_model_s_hijacked_remotely/\nhttps://uk.pcmag.com/electronics/84732/hackers-remotely-attack-moving-tesla-model-s\nhttps://electrek.co/2016/09/20/first-tesla-model-s-remotely-controlled-hackers-tesla-pushed-a-fix/\nhttps://www.forbes.com/sites/thomasbrewster/2016/09/20/keen-team-remotely-hack-tesla-cars/\nhttps://thehackernews.com/2016/09/hack-tesla-autopilot.html\nhttps://www.theguardian.com/technology/2016/sep/20/tesla-model-s-chinese-hack-remote-control-brakes\nRelated \ud83c\udf10\nTeslas tricked into reacting to false lane markers\nTesla Model S tricked into veering into wrong lane\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/uber-self-driving-car-runs-red-light", "content": "Uber self-driving car runs red light in San Fransciso\nOccurred: December 2016\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nUber\u2019s self-driving car programme faced significant criticism and potential regulatory action after one of its vehicles was caught running a red light at moderate speed in San Francisco.\nGiven Uber self-driving vehicles have drivers and engineers as backups at the wheel of the car, it was unclear why nobody hit the brakes. \nThe incident raised concerns about the safety of Uber\u2019s autonomous technology, and led to a setback for Uber\u2019s self-driving car programme. \nPrior to this, the programme had already been struggling, with reports suggesting that Uber rushed to launch new programs, modified its safety operations, and didn\u2019t hit goals. \nThe company was also criticised for taking shortcuts and rushing to get its self-driving cars on the road.\n\u2795 Shortly after the incident, California\u2019s Attorney General demanded Uber cease its self-driving car pilot programme until the company filed for a permit to test its cars on state roads. \n\u2795 This was followed by California's Department of Motor Vehicles announcing it was revoking the registrations of 16 Uber cars as they had not been properly marked as test vehicles.\nSystem \ud83e\udd16\nUber Wikipedia profile\nIncident video\nOperator: Uber\nDeveloper: Uber\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Self-driving system\nIssue: Safety; Accuracy/reliability\nTransparency: Governance; Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.cnet.com/news/uber-self-driving-car/\nhttps://www.nytimes.com/2017/02/24/technology/anthony-levandowski-waymo-uber-google-lawsuit.html\nhttps://www.theverge.com/2016/12/14/13960836/uber-self-driving-car-san-francisco-red-light-safety\nhttps://uk.pcmag.com/cars/86684/autonomous-uber-car-caught-running-red-light\nhttps://arstechnica.com/cars/2016/12/california-dmv-revokes-ubers-self-driving-car-registrations-uber-cancels-pilot/\nhttps://www.bloomberg.com/news/articles/2016-12-14/uber-rolls-out-self-driving-cars-in-san-francisco-without-dmv-approval\nhttps://medium.com/halting-problem/uber-denounces-traffic-light-laws-after-self-driving-car-runs-red-light-2c8d02e30162\nhttps://www.sfexaminer.com/news/video-appears-to-show-uber-self-driving-car-running-red-light-in-sf/\nhttps://www.wired.com/2016/02/googles-self-driving-car-may-caused-first-crash/\nRelated \ud83c\udf10\nUber self-driving car pedestrian fatality\nWaymo self-driving car hits public bus\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/waymo-self-driving-car-hits-public-bus", "content": "Waymo self-driving car hits public bus\nOccurred: February 2016\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Waymo self-driving Lexus car being tested by Google drove into the side of a public bus near the company\u2019s headquarters in Mountain View, California, damaging the bus and car. All drivers and passengers escaped harm.\nAccording to the accident report, the Lexus had intended to turn right off a major boulevard but stopped after detecting sandbags around a storm drain near the intersection, navigated to its left and tried to slip in front of the bus, which it collided with. \nIn its accident report, Google said 'We clearly bear some responsibility, because if our car hadn\u2019t moved there wouldn\u2019t have been a collision.' It was the first time the company had accepted some degree of responsibility for one of is vehicles causing a crash.\nIt also said it has reviewed the incident and changed the cars' software to appreciate that buses may not be as inclined to yield as other vehicles. \nSystem \ud83e\udd16\nWaymo website\nWaymo Wikipedia profile\nOperator: Alphabet/Waymo\nDeveloper: Alphabet/Waymo\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Self-driving system; Computer vision\nIssue: Safety; Accuracy/reliability\nTransparency: Governance; Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://apnews.com/article/4d764f7fd24e4b0b9164d08a41586d60\nhttps://www.washingtonpost.com/news/innovations/wp/2016/02/29/for-the-first-time-googles-self-driving-car-takes-some-blame-for-a-crash/\nhttps://www.wired.com/2016/02/googles-self-driving-car-may-caused-first-crash/\nhttps://www.theguardian.com/technology/2016/feb/29/google-self-driving-car-accident-california\nhttps://www.theguardian.com/technology/2016/mar/09/google-self-driving-car-crash-video-accident-bus\nhttps://www.vox.com/2016/2/29/11588346/googles-self-driving-car-hit-another-vehicle-for-the-first-time\nhttps://www.theatlantic.com/technology/archive/2016/03/google-self-driving-car-crash/471678/\nhttps://www.theverge.com/2016/2/29/11134344/google-self-driving-car-crash-report\nhttps://www.technologyreview.com/2016/02/29/161816/googles-self-driving-car-probably-caused-its-first-accident/\nRelated \ud83c\udf10\nWaymo cars get stuck in cul-de-sac\nCruise AV impedes San Francisco firefighters\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/knightscope-k5-security-robot-hits-child", "content": "Knightscope K5 security robot hits child\nOccurred: July 2016\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA five-foot, 300-pound security robot hit the head of a sixteen-month boy in a California shopping centre, knocking him over and running over his foot. The child was left in tears with a scape and bruise on one of his legs.\nThe Knightscope K5 robot is a fully autonomous robot used to deter and detect crime, and had started patrolling the Stanford Shopping Center in 2015.\nKnightscope later described the incident as a 'freakish accident' in which the child had started running towards the robot, which veered left to avoid the child, only for the child to ran backwards directly into the machine, at which point the machine stopped and the child fell to the ground. \nThe robot's sensors failed to register any vibration and ran over the child.\nSystem \ud83e\udd16\nKnightscope K5 website\nKnightscope Wikipedia profile\nOperator: Stanford Shopping Center, Palo Alto\nDeveloper: Knightscope\nCountry: USA\nSector: Retail\nPurpose: Strengthen security\nTechnology: Robotics\nIssue: Accuracy/reliability; Safety\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://gizmodo.com/security-robot-pwns-toddler-at-stanford-mall-report-1783519433\nhttps://abc7news.com/news/parents-upset-after-stanford-mall-robot-injures-child/1423093/\nhttps://www.cnbc.com/2016/07/14/investigation-begins-on-robot-security-after-child-is-hurt.html\nhttps://www.theverge.com/2016/7/13/12170640/mall-security-robot-k5-knocks-down-toddler\nhttps://www.wsj.com/articles/security-robot-suspended-after-colliding-with-a-toddler-1468446311\nhttps://www.businesswire.com/news/home/20160713006532/en/Knightscope-Issues-Field-Incident-Report\nhttps://www.mercurynews.com/2016/07/12/stanford-shopping-center-mall-docks-robot-cops-after-kid-hit/\nRelated \ud83c\udf10\nKnightscope HP RoboCop ignores woman reporting crime\nSingapore Xavier patrol robots\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/facebook-user-emotional-contagion-research", "content": "Facebook user emotional contagion research criticised as unethical\nOccurred: June 2014\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA research study into the effects of so-called 'emotion contagion' conducted by Facebook, the University of California and Cornell University on a swathe of Facebook users was described as 'creepy', 'manipulative', and 'unethical'. \nConducted over a one-week period in 2012, the research detected that very small changes in the emotional state of our environment can have knock-on effects for how people behave on on social networks\nThe content of news feeds were changed for a random sample of 689,003 Facebook users, with one group experiencing positive content, and another experiencing only negative content.\nHowever, it transpired that the researchers had failed to gain the informed consent of the sample, leading to a heated debate on the nature of academic and corporate ethics boards and Institutional Review Boards, and on the nature of Facebook's relationship with its users.\nSystem \ud83e\udd16\nFacebook website\nOperator: Meta/Facebook\nDeveloper: Meta/Facebook\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Assess emotional contagion\nTechnology: Ranking algorithm\nIssue: Ethics\nTransparency: Governance; Marketing\nResearch, advocacy \ud83e\uddee\nKramer A., Guillory J.E., Hancock J.T. (2014). Experimental evidence of massive-scale emotional contagion through social networks\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.wired.com/2014/06/everything-you-need-to-know-about-facebooks-manipulative-experiment/\nhttps://www.forbes.com/sites/kashmirhill/2014/06/28/facebook-manipulated-689003-users-emotions-for-science/\nhttps://www.forbes.com/sites/kashmirhill/2014/06/29/facebook-doesnt-understand-the-fuss-about-its-emotion-manipulation-study\nhttps://www.theguardian.com/science/head-quarters/2014/jul/01/facebook-cornell-study-emotional-contagion-ethics-breach\nhttps://www.theatlantic.com/technology/archive/2014/06/even-the-editor-of-facebooks-mood-study-thought-it-was-creepy/373649/\nhttps://time.com/2951726/facebook-emotion-contagion-experiment/\nhttps://www.nytimes.com/2014/06/30/technology/facebook-tinkers-with-users-emotions-in-news-feed-experiment-stirring-outcry.html\nhttps://www.npr.org/sections/alltechconsidered/2014/06/30/326929138/facebook-manipulates-our-moods-for-science-and-commerce-a-roundup\nRelated \ud83c\udf10\nFacebook Meaningful Social Interactions algorithm\nStanford facial political orientation study\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/facebook-tags-users-faces-without-consent", "content": "Facebook sued for facial recognition tagging\nOccurred: April 2015\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nFacebook was sued for collecting and storing the facial data of its users' without consent, thereby violating the Illinois Biometric Information Privacy Act (BIPA) of 2008. \nLead plaintiff Nimesh Patel sued Facebook in one of three consolidated class actions in 2015, claiming the social network started mapping users\u2019 faces for its Tag Suggestions feature in 2011. \nThe plaintiffs said Facebook did so without their permission and failed to inform them how long their data would be stored as required by BIPA. \nNearly 1.6 million affected Facebook users in Illinois submitted claims under the class-action suit.\nThe case has been widely recognised as the first major lawsuit on the issue. \n\u2795 April 2018. A US federal judge ruled that Facebook must face a class-action lawsuit.\n\u2795 September 2019. Facebook announced it would replace Tag Suggestions with its broader face recognition setting, which identifies people's faces in photos for various uses, not just tagging.\n\u2795 July 2020. Facebook agreed to settle the case by paying USD 650 million, with the judge concluding that the practice \"invades an individual\u2019s private affairs and concrete interests.\"\n\u2795 September 2021. US District Judge James Donato approved the settlement. The settlement amount was initially set at USD 550 million and was increased to USD 650 million following negotiations.\nSystem \ud83e\udd16\nFacebook Tag Suggestions\nDocuments \ud83d\udcc3\nFacebook (2019). An Update about Facial Recognition on Facebook\nOperator: Meta/Facebook\nDeveloper: Meta/Facebook\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Suggest friends to tag\nTechnology: Facial recognition\nIssue: Privacy\nTransparency: Governance; Black box; Marketing\nRegulation \u2696\ufe0f\nIllinois Biometric Information Privacy Act (BIPA) 2008\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nEPIC. Patel v Facebook\nEPIC (2011). In re Facebook and the Facial Identification of Users\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theverge.com/2021/2/27/22304618/judge-approves-facebook-privacy-settlement-illinois-facial-recognition\nhttps://www.theguardian.com/technology/2021/feb/27/facebook-illinois-privacy-lawsuit-settlement\nhttps://www.vox.com/recode/2020/7/23/21335806/facebook-settlement-illinois-facial-recognition-photo-tagging\nhttps://www.theverge.com/2019/8/8/20792326/facebook-facial-recognition-appeals-decision-damages-payment-court\nhttps://www.nbcnews.com/tech/tech-news/facebook-brings-face-recognition-all-users-discontinues-tag-suggestions-n1049361\nhttps://www.cbsnews.com/sanfrancisco/news/facebook-will-stop-automatic-tag-suggestions-on-your-friends-faces-in-photos/\nRelated \ud83c\udf10\nMeta fined USD 1.4 billion for unlawful use of facial recognition\nFacebook fined for violating privacy of 200,000 South Koreans\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/google-photos-mislabels-black-americans-as-gorillas", "content": "Google Photos mislabels black Americans as 'gorillas'\nOccurred: July 2015\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGoogle Photos was discovered to be automatically labelling black people as 'gorillas', prompting disgust amongst internet users and accusations of racial and ethnic stereotyping by civil rights advocates.\nThe discovery was made by programmer Jacky Alcine, who discovered that a folder named 'Gorillas' had been automatically generated in his Google Photos account. The folder contained photographs of Alcine and a Black friend. \nAlcine alerted Google to the problem, which quickly apologised and promised 'immediate action' to resolve the issue, which was thought likely to have involved the poor classification of image training data.\nIn 2018, WIRED discovered that Google had prevented Google Photos from labelling images as a gorilla, chimpanzee, or monkey, including pictures of the primates themselves.\nDespite major advances in image recognition, Google Photos - and Apple Photos - failed to find any images with the images, according to a 2023 New York Times report.\nSystem \ud83e\udd16\nGoogle Photos website\nGoogle Photos Wikipedia profile\nOperator: Alphabet/Google\nDeveloper: Alphabet/Google\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Improve photo labelling, discovery\nTechnology: Image recognition\nIssue: Bias/discrimination - race, ethnicity\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://bits.blogs.nytimes.com/2015/07/01/google-photos-mistakenly-labels-black-people-gorillas/\nhttps://www.irishtimes.com/business/technology/google-appalled-as-photos-app-labels-black-people-gorillas-1.2272205\nhttps://www.dailymail.co.uk/sciencetech/article-3145887/Google-apologises-Photos-app-tags-black-people-gorillas-Fault-image-recognition-software-mislabelled-picture.html\nhttps://www.wired.com/story/when-it-comes-to-gorillas-google-photos-remains-blind/\nhttps://www.theguardian.com/technology/2018/jan/12/google-racism-ban-gorilla-black-people\nhttps://www.nytimes.com/2023/05/22/technology/ai-photo-labels-google-apple.html\nhttps://petapixel.com/2023/05/22/googles-photos-app-is-still-unable-to-find-gorillas/\nRelated \ud83c\udf10\nHP face tracking 'racism'\nFacebook labels black men 'primates'\nPage info\nType: Incident\nPublished: March 2023\nLast updated: May 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/google-adsense-shows-lower-paying-jobs-to-women", "content": "Google AdSense shows lower-paying jobs to women\nOccurred: July 2015\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGoogle's AdSense ad-serving system is more likely to display adverts offering higher salaries to people identifying themselves as males than females, according to a study by researchers at Carnegie Mellon University and the International Computer Science Institute.\nUsing a custom-built programme called AdFisher that simulates users\u2019 web-browsing habits, the researchers discovered that users identifying as male saw a career-coaching ad on the Times of India website 1,852 times, while users identifying as women were shown it 318 times.\nThe complexity and opacity of Google's ad targetting system made it hard to explain the researchers' findings. 'I think our findings suggest that there are parts of the ad ecosystem where kinds of discrimination are beginning to emerge and there is a lack of transparency,' one said. \nSystem \ud83e\udd16\nGoogle AdSense website\nGoogle AdSense Wikipedia profile\nOperator: Alphabet/Google\nDeveloper: Alphabet/Google\nCountry: USA\nSector: Business/professional services\nPurpose: Serve advertising\nTechnology: Advertising management system\nIssue: Bias/discrimination - gender\nTransparency: Governance; Black box\nResearch, advocacy \ud83e\uddee\nDatta A., Tschantz M.C., Datta A. (2014). Automated Experiments on Ad Privacy Settings: A Tale of Opacity, Choice, and Discrimination\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.technologyreview.com/2015/07/06/110198/probing-the-dark-side-of-googles-ad-targeting-system/\nhttps://www.wired.com/2015/07/googles-ad-system-become-big-control/\nhttps://www.wsj.com/articles/computers-are-showing-their-biases-and-tech-firms-are-concerned-1440102894\nhttps://marketingland.com/carnegie-mellon-study-finds-gender-discrimination-in-ads-shown-on-google-134479\nhttps://techcrunch.com/2015/07/09/researchers-probe-online-ad-targeting-bias/\nhttps://www.csmonitor.com/Technology/2015/0707/Google-ads-suggest-higher-paying-jobs-to-men.-Is-the-algorithm-sexist\nhttps://finance.yahoo.com/news/does-googles-ad-network-discriminate-against-123674289134.html\nhttps://www.cio.com/article/2997514/artificial-intelligence-can-go-wrong-but-how-will-we-know.html\nRelated \ud83c\udf10\nGoogle search conflates 'black girls' with pornography\nGoogle Images under-represents female CEOs\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/google-images-under-represents-female-ceos", "content": "Google Images found to under-represent female CEOs\nOccurred: April 2015\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nFemale CEOs in the US were significantly under-represented in Google Images' search results, according to a 2015 research study.\nResearchers at the University of Washington and University of Maryland discovered (pdf) that women account for 27 percent of CEOs in the US, but only 11 percent of the top 100 Google image search results returned images of females. They also found that female construction workers 'tended to be sexualised caricatures of construction workers'.\nThe researchers also discovered that the first picture of a woman on Google Images is one of Barbie in a suit. The researchers argued that findings reinforced widely held perceptions of the kinds of people holding certain kinds of jobs, and left the technology company open to accusations of sexism and gender bias.\nSystem \ud83e\udd16\nGoogle Images website\nGoogle Images Wikipedia profile\nOperator: Alphabet/Google\nDeveloper: Alphabet/Google\nCountry: USA\nSector: Business/professional services\nPurpose: Rank search results \nTechnology: Search engine algorithm; Machine learning\nIssue: Bias/discrimination - gender\nTransparency: Governance; Black box\nResearch, advocacy \ud83e\uddee\nKay M., Matuszek C., Munson S.A. (2015). Unequal Representation and Gender Stereotypes in Image Search Results for Occupations (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theverge.com/tldr/2015/4/9/8378745/i-see-white-people\nhttps://www.businessinsider.com/the-first-woman-who-appears-in-a-google-image-search-for-ceo-is-barbie-2015-4\nhttps://www.geekwire.com/2015/study-puts-google-image-search-results-to-the-gender-bias-test/\nhttps://www.bbc.co.uk/news/newsbeat-32332603\nhttps://www.dailymail.co.uk/femail/article-3043673/The-woman-appear-Google-search-CEO-BARBIE-course-s-wearing-miniskirt.html\nhttps://www.huffingtonpost.co.uk/entry/google-image-gender-bias_n_7036414\nhttps://www.nytimes.com/2015/07/10/upshot/when-algorithms-discriminate.html\nhttps://www.firstpost.com/tech/news-analysis/female-ceos-under-represented-on-google-image-search-study-3666631.html\nRelated \ud83c\udf10\nGoogle Images 'three black teenagers' mugshot stereotyping\nGoogle search conflates 'black girls' with pornography\nPage info\nType: Incident\nPublished: March 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/starbucks-automated-shift-scheduling", "content": "Starbucks automated shift scheduling system ruins employees' lives\nOccurred: August 2014\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe use of an 'unpredictable' automated scheduling system by Starbucks in the US is seen to have made it impossible for some employees to lead normal lives, and resulted in significant emotional and financial distress.\nIn August 2014, the New York Times published an in-depth portrayal of the impact Starbucks' use of a Kronos algorithmic scheduling system was having on the life of 22-year-old barista and single mother Jannette Navarro, notably: \nErratic Hours: Navarro\u2019s work hours were unpredictable and inconsistent, making it difficult for her to plan her life.\nFinancial Instability: The unpredictable schedules led to financial instability, affecting Navarro\u2019s ability to budget and plan for expenses.\nEducation: The scheduling system interfered with Navarro\u2019s ability to pursue her associate degree in business.\nLong Commute: Navarro had to endure a three-hour commute that required walking two miles, taking two trolleys, and riding a bus.\nRelationships: The erratic hours ultimately led her relationships to erode.\nMental Health: The stress from the erratic scheduling had a negative impact on Navarro\u2019s mental health.\nThe article caused an outcry and forced Starbucks to change its scheduling policy by posting work hours a week in advance for its 130,000 employees in the US. \nIt also announced changes to improve \u201cstability and consistency\u201d for its employees, including an end to \u201cclopening,\u201d the practice of scheduling employees to work opening and closing shifts back to back.\n\u2795 In April 2015, New York attorney general's office launched an investigation into the 'on-call' scheduling practices of 13 national retail chains. Amongst other things, it wanted to know whether these companies used Kronos software to algorithmically generate schedules.\n\u2795 The NYT and Attorney General investigations persuaded Kronos to update its software in such a way as to tie fairer scheduling practices to reductions in absenteeism and turnover.\nSystem \ud83e\udd16\nUKG (formerly Kronos) employee scheduling website\nOperator: Starbucks\nDeveloper: UKG/Kronos\nCountry: USA\nSector: Food/food services \nPurpose: Schedule employee shifts\nTechnology: Scheduling algorithm\nIssue: Employment; Ethics\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.nytimes.com/interactive/2014/08/13/us/starbucks-workers-scheduling-hours.html\nhttps://www.nytimes.com/2014/08/15/us/starbucks-to-revise-work-scheduling-policies.html\nhttps://archive.nytimes.com/www.nytimes.com/times-insider/2014/08/22/times-article-changes-a-policy-fast/\nhttps://www.nytimes.com/2015/09/24/business/starbucks-falls-short-after-pledging-better-labor-practices.html\nhttps://www.redbookmag.com/life/mom-kids/news/a18600/starbucks-promises-better-scheduling/\nhttps://slate.com/human-interest/2014/08/scheduling-software-starbucks-promises-to-do-better-but-low-wage-workers-need-legal-protections.html\nhttps://www.cbsnews.com/news/is-starbucks-shortchanging-its-baristas/\nhttps://www.seattletimes.com/business/starbucks-vows-to-change-unpredictable-barista-work-schedules/\nhttps://money.cnn.com/2014/08/14/news/companies/starbucks-schedule-changes/index.html\nhttps://time.com/4047359/starbucks-scheduling-labor-practices-memo/\nRelated \ud83c\udf10\nSouthwest Airlines crew scheduling automation\nGorillas 'Project Ace' rider work schedule automation\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/pokemon-go-redlines-communities-of-colour", "content": "Pok\u00e9mon Go 'redlines' coloured, poor communities\nOccurred: August 2016\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAugmented reality mobile game Pok\u00e9mon Go was criticised for placing too few Pok\u00e9stops in US communities of colour and other disadvantaged neighbourhoods, resulting in accusations of bias and 'redlining'. \nRedlining is a term used when a community is deprived of essential services, including the provision of mortgages, based on its economic, racial, or ethnic make-up.\nThe location of so-called Pok\u00e9stops and Gyms - where users can pick up virtual goods and prepare for battles, and which map onto real-world locations such as parks, churches and public buildings - were seen to benefit local businesses and communities by driving publicity, revenue and engagement, reducing crime, and persuading people to take exercise. \nBut not all communities were seen to benefit equally. Using maps of 'portals' from its gaming predecessor, Ingress, the Urban Institute think tank estimated that Pok\u00e9mon Go averaged 55 portals in majority white neighbourhoods, against 19 portals in majority black neighbourhoods - figures that were substantiated by other organisations in other locations.\nSystem \ud83e\udd16\nPok\u00e9mon Go website\nPok\u00e9mon Go Wikipedia profile \nOperator: Niantic\nDeveloper: Niantic\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Drive engagement\nTechnology: Augmented reality (AR)\nIssue: Bias/discrimination - race, ethnicity, income\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nUrban Institute (2016). Pok\u00e9mon GO is changing how cities use public space, but could it be more inclusive?\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://eu.usatoday.com/story/tech/news/2016/08/09/pokemon-go-racist-app-redlining-communities-color-racist-pokestops-gyms/87732734/\nhttps://kotaku.com/pokemon-go-could-be-a-death-sentence-for-a-black-man-1783388743\nhttps://www.bnd.com/news/nation-world/national/article89562297.html\nhttps://www.rollingstone.com/culture/culture-news/why-pokemon-go-sucks-in-the-suburbs-103309/\nhttps://splinternews.com/how-nintendo-changed-this-racist-pokemons-design-for-th-1793855123\nhttps://dailycaller.com/2016/07/13/surprise-pokemon-go-is-racist-too/\nhttps://www.smobserved.com/story/2016/07/11/news/black-lives-matter-charges-pokemon-go-with-racism/1596.html\nRelated \ud83c\udf10\nWitcher 3 AI voice line simulation\nElite Dangerous AI spaceships create superweapons\nPage info\nType: Incident\nPublished: August 2016", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/linkedin-search-engine-favours-mens-names", "content": "LinkedIn search engine favours men's names\nOccurred: August 2016\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA newspaper investigation found that LinkedIn's search engine algorithm was suggesting male names when people were searching for female users, resulting in accusations of stereotyping and bias. \nAccording to an August 2016 Seattle Times report, a search of popular female first names, such as Stephanie and Andrea, were shown the result 'did you mean Stephen' or 'did you mean Andrew'. \nAccording to LinkedIn, its 'did you mean' results were produced by an algorithm designed to suggest names with similar spellings based on how frequently names have shown up in past queries. \nThe company subsequently rolled out an update to the algorithm that enabled it to explicitly recognise popular names, so that the algorithm doesn\u2019t try to correct them.\nSystem \ud83e\udd16\nLinkedIn website\nOperator: Microsoft/LinkedIn\nDeveloper: Microsoft/LinkedIn\nCountry: USA\nSector: Business/professional services\nPurpose: Augment search results\nTechnology: Search engine algorithm; NLP/text analysis; Machine learning\nIssue: Bias/discrimination - gender\nTransparency: Governance; Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.seattletimes.com/business/microsoft/how-linkedins-search-engine-may-reflect-a-bias/\nhttps://time.com/4484530/linkedin-gender-bias-search/\nhttps://qz.com/775597/linkedins-lnkd-search-algorithm-apparently-favored-men-until-this-week/\nhttps://phys.org/news/2016-09-linkedin-gender-bias.html\nhttps://qz.com/775597/linkedins-lnkd-search-algorithm-apparently-favored-men-until-this-week/\nhttps://time.com/4484530/linkedin-gender-bias-search/\nhttps://www.cnbc.com/2018/10/10/linkedin-recruiter-starts-reflecting-gender-mix-in-search-results.htmlhttps://www.technologyreview.com/2021/06/23/1026825/linkedin-ai-bias-ziprecruiter-monster-artificial-intelligence/\nRelated \ud83c\udf10\nLinkedIn deepfake salespeople\nGoogle 'three black teenagers' mugshot stereotyping\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/google-three-black-teenagers-mugshot-stereotyping", "content": "Google Images 'three black teenagers' mugshot stereotyping\nOccurred: June 2016\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nImages generated by a search on Google for 'three black teenagers' contrasted sharply with those for a search on the phrase 'three white teenagers', leading users and commentators to accuse the technology company of racial stereotyping and bias. \n18-year-old graduate Kabir Alli found that a search on Google Images for three black teenagers returned a series of photos of police mugshots, whereas a search for three white teenagers mostly consisted of smiling young males.\nThe discovery prompted a wave of complaints on Twitter, to which Alli had posted a video showing his findings, with some people accusing Google of stereotyping. Others saw it as racism. \nA number also suggested it was reflective of views held across a broad spectrum of society.\nSystem \ud83e\udd16\nGoogle Images search\nGoogle Images Wikipedia profile\nOperator: Alphabet/Google\nDeveloper: Alphabet/Google\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Rank content/search results\nTechnology: Search engine algorithm; Machine learning\nIssue: Bias/discrimination - race, ethnicity\nTransparency: Governance; Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.washingtonpost.com/news/morning-mix/wp/2016/06/10/google-faulted-for-racial-bias-in-image-search-results-for-black-teenagers/\nhttps://eu.usatoday.com/story/tech/news/2016/06/09/google-image-search-three-black-teenagers-three-white-teenagers/85648838/\nhttps://www.theguardian.com/commentisfree/2016/jun/10/three-black-teenagers-google-racist-tweet\nhttps://www.dailymail.co.uk/news/article-3631413/Three-black-teenagers-vs-three-white-teenagers-Google-Image-search-Twitter-video-goes-viral.html\nhttps://www.thedrum.com/news/2016/06/09/google-image-search-results-three-black-teenagers-cited-evidence-media-racism\nhttps://www.bbc.co.uk/news/world-us-canada-36487495\nhttps://www.abc10.com/article/news/nation-now/three-black-teenagers-google-search-sparks-outrage/103-238221207\nRelated \ud83c\udf10\nGoogle search conflates 'black girls' with pornography\nGoogle ads for Blacks suggest criminal records\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/google-search-prioritises-holocaust-denial-website", "content": "Google search prioritises Holocaust denial website\nOccurred: December 2016\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Holocaust denial website has been found to have listed top for a Google search on the phrase 'Did the holocaust happen?', drawing the ire of Jewish campaigners and human and civil rights advocates.\nIn December 2016, The Guardian discovered that Stormfront.org, which describes itself as 'the voice of the new, embattled white minority\u2026 a community of racial realists and idealists', was dominating Google's search results for a range of Holocaust-related searches. \nAround 10 days later several more factual sites, notably that of the United States Holocaust Memorial Museum (USHMM) - which confronts Holocaust denial and accuses those who deny it of anti-semitism - were found to be much more visible in Google's search results. \nAccording to SearchEngineLand, the success of these more factual sites was likely due to the fact that they had been developed by search engine optimisation experts, rather than an update to Google's algorithms.\nThe fracas prompted considerable debate on the nature of Google's search system, with commentators pointing out how easy it is for extremists and others to manipulate it to amplify misinformation and hate speech.\nThe company later updated its search algorithm to prioritise high quality information, lowering the profile of sites associated with racial hate speech, and removing anti-Semitic queries on its Autocomplete search prediction function.\nSystem \ud83e\udd16\nGoogle Search website\nGoogle Search Wikipedia profile\nGoogle Page Rank Wikipedia profile\nOperator: Alphabet/Google\nDeveloper: Alphabet/Google\nCountry: USA\nSector: Politics\nPurpose: Rank content/search results\nTechnology: Search engine algorithm; NLP/text analysis\nIssue: Accuracy/reliability; Mis/disinformation; Safety\nTransparency: Governance; Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://searchengineland.com/google-studying-ways-deal-offensive-search-suggestions-results-265654\nhttps://searchengineland.com/googles-results-no-longer-in-denial-over-holocaust-265832\nhttps://www.theguardian.com/commentisfree/2016/dec/11/google-frames-shapes-and-distorts-how-we-see-world\nhttps://www.wired.com/story/google-autocomplete-vile-suggestions/\nhttps://www.businessinsider.com/holocaust-denial-web-site-falls-google-search-results-2016-12\nhttps://fortune.com/2016/12/20/google-algorithm-update/\nhttps://www.csmonitor.com/Technology/2016/1221/Google-updates-algorithm-to-filter-out-Holocaust-denial-and-hate-sites\nhttps://slate.com/podcasts/what-next-tbd/2020/10/facebook-banning-holocaust-denial\nRelated \ud83c\udf10\nGoogle search conflates 'black girls' with pornography\nGoogle images links music promoter to criminal underworld\nPage info\nType: Incident\nPublished: March 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/police-robot-kills-dallas-shooting-suspect", "content": "Police robot kills Dallas shooting suspect\nOccurred: July 2016\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe decision by Dallas police to kill a shooting suspect with an explosive device attached to a military surplus bomb-disposal robot prompted questions about the nature and ethics of police actions involving robots.\nThe suspect, Micah Xavier Johnson, opened fire on police officers during a Black Lives Matter protest in downtown Dallas in July 2016, killing five officers and wounding seven. \nDallas Mayor Mike Rawlings said 'We saw no other option but to use our bomb robot and place a device on its extension for it to detonate where the suspect was.'\nJournalist Asher Wolf discovered that the robot used was most likely a MARCbot-IV robot purchased through the US military's 1033 program. Over 200 law enforcement agencies were thought to use robots purchased through the 1033 program, including the Dallas Police Department. \nIt was the first known time that a robot had been used to intentionally kill a human in the US. \n\u2795 In February 2021, images of an automated New York Police Department (NYPD) 'digidog' equipped with surveillance cameras responding to a hostage situation drew a backlash from the local community, rights activists, and politicians. \nSystem \ud83e\udd16\nMARCbot Wikipedia profile\nOperator: Dallas Police Department\nDeveloper: Exponent Inc\nCountry: USA\nSector: Govt - police\nPurpose: Bomb disposal\nTechnology: Robotics\nIssue: Ethics\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theguardian.com/technology/2016/jul/08/police-bomb-robot-explosive-killed-suspect-dallas\nhttps://www.nbcnews.com/storyline/dallas-police-ambush/dallas-police-used-robot-bomb-kill-ambush-suspect-mayor-n605896\nhttps://www.inverse.com/article/18043-dallas-police-kill-shooting-suspect-with-military-robot-marcbot-iv-eod-bomb-disposal\nhttps://www.latimes.com/nation/la-na-dallas-robot-20160708-snap-story.html\nhttps://www.theverge.com/2016/7/8/12128230/dallas-police-bomb-disposal-robot-explosives-killed-shooting-suspect\nhttps://www.nbcnews.com/video/dallas-police-used-bomb-robot-to-take-down-gunman-who-shot-cops-721129539651\nhttps://www.texastribune.org/2016/07/08/use-robot-kill-dallas-suspect-first-experts-say/\nRelated \ud83c\udf10\nNYPD 'digidog' hostage surveillance\nSingapore Xavier patrol robots\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/xiao-pang-robot-goes-haywire-at-technology-fair", "content": "Xiao Pang robot goes haywire at technology fair\nOccurred: November 2016\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Chinese-made robot went on the rampage at a Shenzhen technology trade fair, smashing a glass window and injuring a man standing nearby. \nDeveloped by Beijing Science and Technology, the so-called Xiao Pang (or 'Little Chubby') robot wheeled itself into the glass pane of an exhibition stand, shattering it and wounding an observer, who was hospitalised.\nThe company apologised for the incident, which it put down to an assistant pushing the wrong button when he was trying to move the robot to one side. \nMarketed as an educational toy for children between the ages of four and 12, Xiao Pang can help parents look after their children. It is equipped with a webcam and can conduct two-way video calls. \nSystem \ud83e\udd16\nEvolver Robot website\nOperator: Beijing Science and Technology Co.\nDeveloper: Beijing Science and Technology Co.\nCountry: China\nSector: Education\nPurpose: Perform household chores\nTechnology:  Robotics\nIssue: Safety\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttp://en.people.cn/n3/2016/1118/c90000-9143838.html\nhttps://mashable.com/2016/11/21/xiao-pang-chinese-robot-smashes-glass\nhttps://timesofindia.indiatimes.com/world/china/Chinese-robot-Fatty-goes-haywire-smashes-booth-injures-1-at-trade-fair-in-Shenzhen/articleshow/55535893.cms\nhttps://www.straitstimes.com/asia/east-asia/man-injured-after-robot-smashes-booth-at-shenzhen-technology-fair\nhttp://www.sixthtone.com/news/1575/robot-goes-rogue-at-shenzhen-fair%2C-injures-bystander\nhttps://www.cnet.com/news/fatty-the-robot-smashes-glass-injures-visitor/\nhttp://www.robot-china.com/news/201611/21/37187.html\nRelated \ud83c\udf10\nDenny's robot server\nFabio retail robot fired after one week\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/google-autocomplete-connects-albert-yeung-with-triads", "content": "Google Autocomplete connects Albert Yeung with triads\nOccurred: August 2014\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nEmperor Group founder and CEO Dr. Albert Yeung won a court case against Google in August 2014 in which he had accused the technology company's Autocomplete search prediction function of defaming him by associating him with terms such as 'triad' and the names of individuals triad gangs.\nPer Columbia University's Global Freedom of Expression project, 'the main issue (..) was whether Google could be considered a publisher of the defamatory information by merely creating an automated service. Furthermore, even if Google could not be considered a direct publisher of the information, a second issue was whether Google could still be liable as a publisher for being aware of the defamatory information and refusing to take it down.'\n'Finally, a third issue considered by the Court was whether Yeung had actually suffered any damages, and if he had not, whether this action could amount to an interference with freedom of expression leading to an abuse of process,' the project notes.\nThe court dismissed the case, awarding damages to Yeung on the basis that Google operated and could amend Autopilot as it saw fit, and could therefore be considered a publisher. However, Google challenged the verdict, with the court allowing the case to go to an appeals court. \nYeung had a checkered career and private life, having been variously arrested and imprisoned for perverting the course of justice, illegal bookmaking, and insider dealing.  \nSystem \ud83e\udd16\nGoogle Autocomplete\nOperator: Alphabet/Google\nDeveloper: Alphabet/Google\nCountry: Hong Kong\nSector: Retail\nPurpose: Predict search results\nTechnology: NLP/text analysis; Deep learning; Machine learning\nIssue: Accuracy/reliability; Mis/disinformation; Legal\nTransparency: Governance; Black box; Legal\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nHong Kong Court of First Instance (2014). Yeung v Google verdict [HCA 1383/2012]\nColumbia Global Freedom of Expression. Dr. Yeung, Sau Shing Albert v. Google Inc. case analysis\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttp://www.scmp.com/news/hong-kong/article/1627777/google-appeal-against-jurisdiction-tycoons-lawsuit\nhttps://www.forbes.com/sites/emmawoollacott/2014/08/06/more-privacy-woes-for-google-this-time-its-autocomplete\nhttps://www.hollywoodreporter.com/movies/movie-news/hong-kong-court-says-film-723738/\nhttps://www.cbc.ca/news/science/tycoon-albert-yeung-can-sue-google-over-defamatory-autocomplete-suggestions-1.2728866\nhttps://www.irishtimes.com/business/technology/hong-kong-court-says-emperor-boss-can-sue-google-1.1891546\nRelated \ud83c\udf10\nGoogle Autocomplete falsely associates Japanese man with crimes\nGoogle Images links Australian music promoter to criminal underworld\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/google-autocomplete-says-rupert-murdoch-jon-hamm-are-jewish", "content": "Google Autocomplete says Rupert Murdoch, Jon Hamm are 'Jewish'\nOccurred: May 2012\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nFrench civil rights group SOS Racisme, alongside a raft of other French equality orgainsations, accused Google of wrongfuly labeling Rupert Murdoch, Jon Hamm, and other celebrities as 'Jewish'. \nThe complaint alleged that Google's Autocomplete function 'systematically' equates Murdoch and others with being Jewish, thereby stereotyping and discriminating against Jews, advancing racist ideas about Jewish conspiracies, and violating a French law against compiling files on people that refer to their ethnicity. \nThe case was dropped in June 2012 after both sides reached an agreement. It was unclear whether Google was forced to make any changes to Autocomplete. \nSystem \ud83e\udd16\nGoogle Autocomplete\nOperator: Alphabet/Google\nDeveloper: Alphabet/Google\nCountry: France\nSector: Media/entertainment/sports/arts\nPurpose: Predict search results\nTechnology: NLP/text analysis; Deep learning; Machine learning\nIssue: Accuracy/reliability; Bias/discrimination; Mis/disinformation; Legal\nTransparency: Governance; Black box; Legal\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.hollywoodreporter.com/thr-esq/google-sued-rupert-murdoch-jon-hamm-jewish-318012\nhttps://www.cbsnews.com/news/google-autocompletes-anti-semitism-sexism-racism/\nhttps://uk.pcmag.com/search-2/64247/google-sued-over-jewish-search-suggestions#\nhttps://www.searchenginejournal.com/google-autocomplete-jewish-murdoch/43137/\nhttps://www.huffingtonpost.co.uk/entry/google-instant-anti-semitic-france_n_1465430\nhttps://www.timesofisrael.com/google-sued-in-france-over-jewish-searches/\nhttps://www.nytimes.com/2012/06/28/technology/racism-lawsuit-against-google-dropped.html\nhttps://www.lacote.ch/articles/monde/google-attaque-en-justice-pour-son-moteur-de-recherche-et-le-mot-juif-215179\nhttps://www.thejc.com/news/world/deal-reached-in-google-france-autocorrect-battle-1.34138\nhttps://www.lacote.ch/articles/monde/google-attaque-en-justice-pour-son-moteur-de-recherche-et-le-mot-juif-215179\nRelated \ud83c\udf10\nGoogle Autocomplete unfairly links businessman to Scientology\nGoogle Autocomplete conflates Bettina Wulff with 'prostitute'\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/google-autocomplete-suggests-australian-surgeon-is-bankrupt", "content": "Google Autocomplete suggests Australian surgeon is 'bankrupt'\nOccurred: December 2012\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAustralian surgeon Guy Hingston sued Google for defamation over an Autocomplete search prediction that said he was 'bankrupt' and which he reckoned cost him customers.\nHingston was not bankrupt at the time of filing his legal complaint, but he had been declared bankrupt in August 2009 thanks to CoastJet, an aviation company he had invested in, having gone bust. \nEven though Hingston's lawyers argued his bankruptcy had been annulled, the surgeon withdrew his action in June 2013 without explanation. \nSystem \ud83e\udd16\nGoogle Autocomplete\nOperator: Alphabet/Google\nDeveloper: Alphabet/Google\nCountry: Australia\nSector: Health\nPurpose: Predict search results\nTechnology: NLP/text analysis; Deep learning; Machine learning   \nIssue: Accuracy/reliability; Mis/disinformation; Privacy; Legal - defamation/libel\nTransparency: Governance; Black box\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nHingston v Google (2012)\nResearch, advocacy \ud83e\uddee\nLewis S.C. (2018). Libel by Algorithm? Automated Journalism and the Threat of Legal Liability \nKarapapa S., Borghi M. (2015). Search engine liability for autocomplete suggestions: personality, privacy and the power of the algorithm\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.smh.com.au/technology/australian-surgeon-sues-google-over-bankrupt-autocomplete-20130122-2d480.html\nhttps://www.techdirt.com/articles/20121227/09011621498/another-lawsuit-filed-google-autocomplete-defamation.shtml\nhttps://www.thedrum.com/news/2013/06/18/bankrupt-man-drops-google-autocomplete-legal-action\nhttps://www.newcastleherald.com.au/story/1579970/australian-doctor-withdraws-lawsuit-against-google/\nhttps://ca.news.yahoo.com/blogs/right-click/doctor-sues-google-over-bankrupt-auto-complete-search-134555891.html\nhttps://www.abc.net.au/radionational/programs/lawreport/google-autocorrrect/4735188\nhttps://www.portnews.com.au/story/1250553/port-macquarie-surgeon-sues-google-over-bankrupt-auto-complete/\nRelated \ud83c\udf10\nGoogle Images links Australian music promoter to criminal underworld\nGoogle Autocomplete unfairly links businessman to Scientology\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/google-autocomplete-unfairly-links-businessman-to-scientology", "content": "Google Autocomplete unfairly links Gebusinessman to Scientology\nOccurred: May 2013\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGoogle lost a high-profile lawsuit in Germany that had seen a businessman accuse the technology company of unfairly tarnishing his reputation by allowing its Autocomplete search prediction function to link him to 'fraud' and 'Scientology'.\nGermany's Federal Court of Justice ruled that Google had violated the rights and reputation of businessman 'RS' on the basis that he could not be found to be related in any way to Scientology or fraud, and that the violation was directly attributable to Google as it designed, developed, and operated Autocomplete.\nThe ruling, which had been overturned twice by lower German courts, required Google to stop using the two terms as suggestions in its Autocomplete results, and to change the way its Autocomplete works in Germany so that it better protects users against similar violations of their rights.\nGoogle called the ruling 'incomprehensible'. A year later, the European Court of Justice ruled that EU data protection law gave individuals the right to ask search engines to delist results for queries related to their name should they be 'inaccurate, inadequate, irrelevant or excessive', or whether there is a public interest in the information remaining available in search results.\nSystem \ud83e\udd16\nGoogle Autocomplete\nOperator: Alphabet/Google\nDeveloper: Alphabet/Google\nCountry: Germany\nSector: Health\nPurpose: Predict search results\nTechnology: NLP/text analysis; Deep learning; Machine learning\nIssue: Accuracy/reliability; Mis/disinformation; Privacy; Legal - defamation/libel\nTransparency: Governance; Black box; Legal\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nCologne Higher Regional Court (2014). Verdict [15 U 199/11]\nFederal Court of Justice (2013). Verdict [VI ZR 269/12]\nWikipedia. Judgement of the German Federal Court of Justice on Google's autocomplete function\nResearch, advocacy \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nCheung A. (2015). Defaming by Suggestion: Searching for Search Engine Liability in the Autocomplete Era\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/technology-22529357\nhttps://www.ft.com/content/6a221ca8-bcb3-11e2-9519-00144feab7de\nhttps://www.spiegel.de/international/business/court-orders-google-to-delete-search-suggestions-that-violate-privacy-a-899741.html\nhttps://www.dw.com/en/german-federal-court-raps-google-on-the-knuckles-over-autocomplete-function/a-16813363\nhttps://www.searchenginewatch.com/2013/05/15/germany-orders-google-to-restrict-autocomplete-results/\nhttps://uk.pcmag.com/internet-3/15475/google-ordered-to-clean-up-auto-complete-in-germany\nhttps://www.pcworld.com/article/2038704/google-has-to-delete-offensive-autocomplete-results-german-federal-court-rules.html\nRelated \ud83c\udf10\nGoogle Autocomplete conflates Bettina Wulff with 'prostitute'\nGoogle Autocomplete falsely associates Japanese man with crimes\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/google-autocomplete-conflates-bettina-wulff-with-prostitute", "content": "Google Autocomplete conflates Bettina Wulff with 'prostitute'\nOccurred: September 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nBettina Wulff, wife of former German president Christian Wulff, sued Google in September 2012 for infringing her personal rights by associating her name with terms such as 'escort', 'prostitute', and 'red light district'.\nWulff had sued Google at Hamburg's Regional Court, claiming she had never worked as a prostitute or escort and accusing the search engine company's Autocomplete search prediction function of 43 unfair word combinations.\nGoogle responded by saying Autocomplete merely reflects what others are already searching for online. Wulff had spent years defending her reputation against these rumours, and had successfully issued 34 cease-and-desist orders. \nIn 2013, Germany's Federal Court of Justice ruled that Google must delete automatically generated search predictions if they directly violate the personal rights of users, leading Google to change its policies.\nIn 2015, Google settled with Wulff in an out-of-court agreement.\nSystem \ud83e\udd16\nGoogle Autocomplete\nOperator: Alphabet/Google\nDeveloper: Alphabet/Google\nCountry: Germany\nSector: Politics\nPurpose: Predict search results\nTechnology: NLP/text analysis; Deep learning; Machine learning\nIssue: Accuracy/reliability; Mis/disinformation; Legal\nTransparency: Governance; Black box; Legal\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nWikipedia. Judgement of the German Federal Court of Justice on Google's Autocomplete function\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/technology-19542938\nhttps://www.sueddeutsche.de/politik/klage-gegen-google-und-jauch-bettina-wulff-wehrt-sich-gegen-verleumdungen-1.1462439\nhttps://www.sueddeutsche.de/politik/persoenlichkeitsverletzende-wortkombinationen-bettina-wulff-schliesst-vergleich-mit-google-1.2306708\nhttps://www.spiegel.de/international/germany/defamation-case-by-bettina-wulff-highlights-double-standard-at-google-a-854914.html\nhttps://www.nytimes.com/2012/09/19/world/europe/keystrokes-in-google-bare-shocking-rumors-about-bettina-wulff.html\nhttps://www.bloomberg.com/news/articles/2012-09-14/googles-autocomplete-gone-awry\nRelated \ud83c\udf10\nGoogle Images links Australian music promoter to criminal underworld\nGoogle Autocomplete falsely associates Japanese man with crimes\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/google-autocomplete-falsely-associates-japanese-man-with-crimes", "content": "Google Autocomplete falsely associates Japanese man with crimes\nOccurred: March 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Japanese man successfully sued Google for defamation after the search engine giant's Autocomplete function had falsely associated him with criminal acts, resulting in his reputation being tarnished, the loss of his job and his inability to find further employment.\nAccording to the man's lawyer, over 10,000 Google search results had wrongly associated his client's name with crimes committed by someone else with the same name, and that Autocomplete's algorithms had been directing users to potentially false or misleading information. \nGoogle had reputedly refused to remove the offending words from its system when the man had first complained about them to the technology company. The judge ordered Google to delete the terms and to modify part of Autocomplete in Japan. \nGoogle had already lost a lawsuit against the same man and refused to delete the relevant information, having claimed it was not subject to Japanese law and that its Autocomplete system could not violate privacy as it is automatically generated and depended on what was already available online.\nSystem \ud83e\udd16\nGoogle Autocomplete\nOperator: Alphabet/Google\nDeveloper: Alphabet/Google\nCountry: Japan\nSector: Business/professional services\nPurpose: Predict search results\nTechnology: NLP/text analysis; Deep learning; Machine learning\nIssue: Accuracy/reliability; Mis/disinformation; Legal - litigation\nTransparency: Governance; Black box; Legal - defamation/libel\nNews, commentary, analysis \ud83d\udcf0\nhttps://thenextweb.com/news/google-ordered-to-close-search-autocomplete-feature-in-japan-over-privacy-complaint\nhttps://abcnews.go.com/blogs/technology/2012/03/court-tells-google-to-suspend-autocomplete/\nhttps://www.bbc.co.uk/news/technology-17510651\nhttps://www.telegraph.co.uk/technology/google/9998335/Google-autocomplete-is-libellous-Japanese-court-rules.html\nhttps://www.theregister.com/2012/03/26/google_autocomplete_japan/\nhttps://www.theregister.com/2012/06/19/google_japan_defamation_autocomplete/\nhttps://www.techdirt.com/2012/03/26/japanese-court-misunderstands-autocomplete-orders-google-to-turn-it-off-to-protect-privacy/\nhttps://www.techdirt.com/articles/20130417/10475822745/japan-latest-country-to-mistakenly-say-google-is-responsible-autocomplete-results.shtml\nRelated \ud83c\udf10\nGoogle images links music promoter to criminal underworld\nGoogle Autocomplete conflates Bettina Wulff with 'prostitute'\nPage info\nType: Incident\nPublished: March 2019", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/google-images-links-music-promoter-to-criminal-underworld", "content": "Google Search, Autocomplete link Australian music promoter to criminal underworld\nOccurred: November 2012\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGoogle was ordered to pay AUD 208,000 to Milorad ('Michael') Trkulja, a 62-year-old man who had accused the company of defaming him with web and image search results that associated him with the criminal underworld.\nIn 2004, Trkulja had been shot in the back at a Sydney restaurant in an unsolved crime involving a balaclava-wearing assailant, after which he had asked Google and Yahoo! to remove content from its search engine and Autocomplete search term prediction function associating him with Australian mafia and criminals, including a well-known drug dealer.\nGoogle's refusal to engage with Trkulja had forced him to file a legal complaint against the technology company in 2012 alleging it was indexing 'grossly defamatory content'. Google had argued it was not a publisher and was simply pointing to material published by third-parties, and that its search results took into account Trkulja's defamation claims.\n\u2795 In 2013, Google removed links to some websites and blocked a number of autocomplete predictions and search queries relating to Mr Trkulja. However, it declined to remove the images of Mr Trkulja being indexed by its systems.\nTrkulja subsequently petitioned the Court of Appeal of Victoria's Supreme Court claiming Google had failed to block access to the damaging images, and asking it to grant a permanent injunction that would remove his name from Google's servers and receive AUD 355,000 in damages. \nTrkulja's suit was finally dismissed on the basis that the publication of his images next to those of hardened criminals could reasonably mean that Trkulja himself was a hardened criminal. \n\u2795 In 2018, Trkulja appealed the Court of Appeal's verdict to the High Court, which overturned the decision on the basis that the search engine results could be construed as defamatory, that Google should be treated as the publisher, and that Trkulja should be allowed to bring forward his case. \nSystem \ud83e\udd16\nGoogle Images website\nGoogle Images Wikipedia profile\nOperator: Alphabet/Google\nDeveloper: Alphabet/Google\nCountry: Australia\nSector: Media/entertainment/sports/arts\nPurpose: Rank content/search results; Predict search results\nTechnology: NLP/text analysis; Deep learning; Machine learning\nIssue: Accuracy/reliabilty; Mis/disinformation; Legal - defamation/libel \nTransparency: Governance; Black box; Legal\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nTrkulja v Google LLC (2018)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theguardian.com/law/2012/nov/26/google-defamation-libel-australia\nhttps://www.theguardian.com/technology/2016/dec/20/google-did-not-defame-man-with-photos-of-him-linked-to-criminals-court-finds\nhttps://inforrm.org/2018/06/27/case-law-australia-trkulja-v-google-llc-the-return-of-trkulja-episode-iv-justin-castelan/\nhttps://www.smh.com.au/business/the-biggest-evil-milorad-trkulja-wants-to-be-removed-from-google-20131205-2yrqj.html\nhttps://www.dailymail.co.uk/news/article-5837873/Australian-man-shot-SUE-Google-defamation.html\nhttps://www.abc.net.au/news/2018-06-13/milorad-trkulja-sues-google-for-defamation/9863686\nhttps://www.theverge.com/2012/11/12/3634790/google-australia-defamation-lawsuit-milorad-trkulja\nhttps://theconversation.com/protecting-google-from-defamation-is-worth-seriously-considering-98252\nhttps://www.cbsnews.com/news/australia-google-defamation-lawsuit-search-results-link-to-melbourne-criminal/\nRelated \ud83c\udf10\nGoogle Autocomplete conflates Bettina Wulff with 'prostitute'\nGoogle Autocomplete falsely associates Japanese man with crimes\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/google-search-conflates-black-girls-with-pornography", "content": "Google search conflates 'black girls' with pornography \nOccurred: 2012\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe top search returns for the phrase 'Black girls' on Google link to sexist and racist websites, including highly explicit pornography, according to a prominent US academic Safiya Noble.\nFrom 2010, US academic Safiya Noble conducted a series of experiments that revealed that Google searches on Black girls, Latina girls, Asian girls returned pornography, hypersexualized content, and other inappropriate links. Conversely, search returns for 'white girls' were much less controversial. \nNoble surmised that search algorithms reflect the racist and sexist biases of their designers, developers, and operators. She also contended that they are reluctant to acknowledge the problem or update their algorithms in a meaningful manner lest it disrupts the advertising revenue generated alongside the search returns, instead preferring to hide behind their opaque 'black box' algorithmic systems.\nThe findings were also featured in Noble's 2018 book Algorithms of Oppression. \nSystem \ud83e\udd16\nGoogle Search website\nGoogle Search Wikipedia profile\nOperator: Alphabet/Google\nDeveloper: Alphabet/Google\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Rank search results\nTechnology: Search engine algorithm; Machine learning\nIssue: Bias/discrimination - race, ethnicity, gender\nTransparency: Governance; Black box\nResearch, advocacy \ud83e\uddee\nNoble S (2018). Algorithms of Oppression\nNoble S (2013). Google Equates Black Girls With Sex: Why?\nNoble S (2012). Missed Connections: What Search Engines Say About Women\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://time.com/5209144/google-search-engine-algorithm-bias-racism/\nhttps://www.vogue.com/article/safiya-noble\nhttps://www.nbcnews.com/think/opinion/google-search-algorithms-are-not-impartial-they-are-biased-just-ncna849886\nhttps://www.vox.com/2018/4/3/17168256/google-racism-algorithms-technology\nhttps://news.yahoo.com/safiya-noble-speaks-algorithmic-oppression-173127056.html\nRelated \ud83c\udf10\nGoogle 'three black teenagers' mugshot stereotyping\nGoogle search prioritises Holocaust denial website\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-go-fails-to-inform-nyc-customers-about-facial-recognition", "content": "Amazon Go fails to inform NYC customers about facial recognition\nOccurred: March 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA pair of lawsuits alleged that Amazon failed to inform customers about its use of facial and body biometrics scanning at its cashierless Go retail stores in New York for over a year. \nAccording to a class-action lawsuit filed by Rodriguez Perez, Amazon had not informed him that his body and palm were scanned. Another suit, filed in February 2023 by Richard McCall, claims his palm was scanned.\nBoth are alleged to be in violation of New York City's 2021 Biometric Identifier Information Law. The law requires all New York City businessses to post a sign informing customers or visitors that their biometrics are being recorded.\nAmazon denied the claims, telling Gizmodo, 'We do not use facial recognition technology in any of our stores, and claims made otherwise are false.' 'Only shoppers who choose to enroll in Amazon One and choose to be identified by hovering their palm over the Amazon One device have their palm-biometric data securely collected, and these individuals are provided the appropriate privacy disclosures during the enrollment process,' it said.\nFirst launched in 2018, Amazon Go is supposed to showcase the company's automated Just Walk Out system, which it says uses computer vision, deep learning algorithms, and sensor fusion to track consumers\u2019 'virtual carts' to notate when they put an item in their cart or take it off the tab if they remove it.\n\u2795 In 2023, Amazon announced it would close eight Amazon Go stores in Seattle, New York City and San Francisco.\nSystem \ud83e\udd16\nAmazon Go website\nAmazon Go Wikipedia profile\nAmazon Just Walk Out website\nOperator: Amazon\nDeveloper: Amazon\nCountry: USA\nSector: Retail\nPurpose: Verify identity\nTechnology: Facial recognition; Computer vision; Deep learning\nIssue: Privacy\nTransparency: Governance; Privacy; Marketing\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nAlfredo Rodriguez Perez v Amazon (2023)\nMcCall v Amazon (2023) \nNYC Biometric Identifier Disclosure Sign template (pdf)\nResearch, advocacy \ud83e\uddee\nEuropean Data Protection Supervisor. Just Walk Out Technology\nWankhede K., Wukkadada B., Nadar V. (2018). Just Walk-Out Technology and its Challenges: A Case of Amazon Go\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.cnbc.com/2023/03/16/amazon-sued-for-not-telling-new-york-store-customers-about-facial-recognition.html\nhttps://news.bloomberglaw.com/privacy-and-data-security/new-york-biometrics-law-will-bring-hefty-fines-for-noncompliance\nhttps://gizmodo.com/amazon-amazon-go-amazon-prime-facial-recognition-1850234189\nhttps://www.forbes.com/sites/cyrusfarivar/2023/03/16/amazon-left-nyc-customers-in-the-dark-on-biometric-tracking-in-their-stores-lawsuit-claims/\nhttps://www.slashgear.com/1230823/amazon-hit-with-lawsuit-for-not-disclosing-facial-recognition-use/\nhttps://www.nbcnews.com/tech/security/amazon-sued-not-telling-new-york-store-customers-facial-recognition-rcna75290\nhttps://techcrunch.com/2018/01/21/inside-amazons-surveillance-powered-no-checkout-convenience-store/\nhttps://www.wired.co.uk/article/amazon-go-seattle-uk-store-how-does-work\nhttps://www.nytimes.com/2023/03/10/technology/facial-recognition-stores.html\nRelated \ud83c\udf10\nLivonia skating rink misidentifies black teenager\nSouthern Co-op facial recognition\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-retains-alexa-recordings-transcripts-indefinitely", "content": "Amazon retains Alexa recordings, transcripts indefinitely\nOccurred: July 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAmazon publicly confirmed that it keeps text transcripts of voice recordings made by users of its Alexa voice assistant services, even after users delete their voice recordings, enraging covil rights and privacy advocates.\nThe confirmation had come to light after Senator Chris Coons had sent a letter (pdf) to Amazon CEO Jeff Bezos requesting information on the company\u2019s privacy and data security practices for Alexa devices following reports indicating Amazon was storing and preserving text transcripts of user voice recordings.\nAmazon had responded (pdf) by saying the company indefinitely retains text logs of transcribed audio recordings on its cloud servers, and that users are unable to have them deleted. By contrast, Google and Apple said they do not keep user transcript data indefinitely. \nAmazon had stated (pdf) in a 2019 white paper that Alexa user text data was stored 'for machine learning purposes,' and that it is not deleted until that process has been completed.\nThe confirmation comes after lawsuits were filed in Seattle and Los Angeles alleging that Amazon had been recording children using Alexa devices without their consent, even after parents had delete the voice recordings.\nSystem \ud83e\udd16\nAmazon Alexa virtual assistant\nOperator: Amazon\nDeveloper: Amazon\nCountry: USA\nSector: Consumer goods\nPurpose: Provide information, services\nTechnology: NLP/text analysis; Natural language understanding (NLU); Speech recognition\nIssue: Privacy\nTransparency: Governance; Privacy; Marketing\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nSenator C.A. Coons letter to Amazon (2019)\nAmazon letter to Senator C.A. Coons (2019) (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://thenextweb.com/security/2019/07/03/amazon-confirms-it-retains-your-alexa-voice-recordings-indefinitely/\nhttps://threatpost.com/amazon-admits-alexa-voice-recordings-saved-indefinitely/146225/\nhttps://arstechnica.com/tech-policy/2019/07/amazon-confirms-it-keeps-your-alexa-recordings-basically-forever/\nhttps://www.cnet.com/news/amazon-alexa-keeps-your-data-with-no-expiration-date-and-shares-it-too/\nhttps://www.cnet.com/news/amazon-alexa-transcripts-live-on-even-after-you-delete-voice-records/\nhttps://www.zdnet.com/article/amazon-confirms-alexa-customer-voice-recordings-are-kept-forever/\nhttps://techcrunch.com/2019/07/03/amazon-responds-to-a-u-s-senators-inquiry-confirms-alexa-voice-records-are-kept-indefinitely/\nhttps://www.theverge.com/2019/7/3/20681423/amazon-alexa-echo-chris-coons-data-transcripts-recording-privacy\nhttps://www.cnet.com/news/amazon-alexa-transcripts-live-on-even-after-you-delete-voice-records/\nRelated \ud83c\udf10\nAmazon Alexa records children's voices without consent\nAmazon Echo Dot Kids remembers kids' conversations\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-alexa-records-childrens-voices-without-consent", "content": "Amazon Alexa records children's voices without consent\nOccurred: June 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nLawsuits filed in Seattle and Los Angeles allege Amazon is recording children who use its Alexa devices without their consent, in violation of laws governing recordings in at least eight US states.\nThe suits highlight Amazon's permanent recording and storing of voices, regardless of consent, in contrast to makers of voice-controlled computing devices that delete recordings after storing them for a short time or not at all. \n'Alexa routinely records and voiceprints millions of children without their consent or the consent of their parents,' said a complaint filed on behalf of a 10-year-old girl in Seattle. A nearly identical suit was filed the same day in Los Angeles on behalf of an 8-year-old boy. \n'At no point does Amazon warn unregistered users that it is creating persistent voice recordings of their Alexa interactions, let alone obtain their consent to do so,' the same suit says.\nThe two suits also highlight Amazon's failure to inform unknowing parties of their recordings and seek their approval or delete them.\nSystem \ud83e\udd16\nAmazon Alexa virtual assistant\nOperator: Alison Hall-O\u2019Neil; Steve Altes\nDeveloper: Amazon\nCountry: USA\nSector: Consumer goods\nPurpose: Provide information, services\nTechnology: NLP/text analysis; Natural language understanding (NLU); Speech recognition\nIssue: Privacy\nTransparency: Governance; Privacy; Marketing\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nC.O. v Amazon, A2Z Development Center (2019) (pdf)\nR.A. v Amazon, A2Z Development Center (2019) (pdf) \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/technology-48623914\nhttps://www.vox.com/the-goods/2019/6/14/18679360/amazon-alexa-federal-lawsuit-child-voice-recording\nhttps://www.seattletimes.com/business/amazon/suit-alleges-amazons-alexa-violates-laws-by-recording-childrens-voices-without-consent/\nhttps://www.foxnews.com/tech/amazons-alexa-illegally-records-children-without-consent \nhttps://phys.org/news/2019-06-alleges-amazon-alexa-violates-laws.html\nRelated \ud83c\udf10\nAmazon Echo Dot Kids remembers kids' conversations\nAmazon Alexa plays child pornography\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-echo-dot-kids-remembers-kids-conversations", "content": "Amazon Echo Dot Kids remembers kids' conversations\nOccurred: May 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA coalition of nineteen privacy groups hafiled a legal complaint with the US Federal Trade Commission (FTC) alleging that Amazon was holding onto a child\u2019s personal information for too long and violating the US Children\u2019s Online Privacy Protection Act (COPPA).\nThe coalition, led by Campaign for a Commercial-Free Childhood (CCFC), Center for Digital Democracy (CDC), and Georgetown University\u2019s Institute for Public Representation, said Amazon's Echo Dot Kids smart speaker records and collects 'vast amounts of sensitive, personal information from children under 13' without adequate parental consent.\nThe coalition also discovered that parents are unable to delete certain personal details - including date of birth - using the FreeTime feature on Amazon Alexa mobile app once a child tells the Echo Dot Kids Edition to remember them. \nAmazon responded by saying its Echo Dot Kids Edition was compliant with COPPA. \nCCFC and CDD had issued a May 2018 warning that Echo Dot endangers children\u2019s privacy and threatens their healthy development by encouraging them to spend more time with and form 'faux relationships' with digital devices.\nSystem \ud83e\udd16\nAmazon Alexa virtual assistant\nOperator:  \nDeveloper: Amazon\nCountry: USA\nSector: Consumer goods\nPurpose: Provide information, services\nTechnology: Speech recognition; Natural language understanding (NLU)\nIssue: Privacy\nTransparency: Governance; Privacy; Marketing\nResearch, advocacy \ud83e\uddee\nCampaign for a Commercial-Free Childhood, Center for Digital Democracy (2019). Advocates Demand FTC Investigation of Echo Dot Kids Edition\nCampaign for a Commercial-Free Childhood, Center for Digital Democracy (2018). Experts and Advocates Caution Parents to Steer Clear of New Amazon Echo Dot for Kids\nConsumer Reports (2019). Amazon Echo Dot Kids Violates Privacy Rules, Advocates Claim\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.apnews.com/f062c28ae72144b3b22146d9d4c6fab3\nhttps://www.thesun.co.uk/tech/9034852/amazon-echo-kids-alexa-recording-conversations/\nhttps://www.nbcnews.com/tech/tech-news/amazon-accused-violating-children-s-privacy-kid-friendly-smart-speakers-n1003706\nhttps://techcrunch.com/2019/05/09/alexa-does-the-echo-dot-kids-protect-childrens-privacy/\nhttps://www.cnet.com/news/amazons-echo-dot-kids-violates-privacy-regulations-child-advocates-say/\nhttps://www.geekwire.com/2019/alexa-illegally-record-children-amazon-sued-allegedly-storing-conversations-without-consent/\nhttps://www.vox.com/the-goods/2019/6/14/18679360/amazon-alexa-federal-lawsuit-child-voice-recording\nRelated \ud83c\udf10\nAmazon Alexa records children's voices without consent\nAmazon Alexa mistakenly orders USD 160 dollhouse\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/rite-aid-facial-recognition", "content": "Rite Aid US facial recognition racial, income bias\nReleased: July 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nUS drugstore chain Rite Aid quietly used facial recognition technology in hundreds of stores in mostly lower-income, non-white neighbourhoods across the US, prompting a civil, legal, and political backlash.\nAccording to a Reuters investigation, Rite Aid quietly added facial recognition systems to hundreds of stores in the US, and that it had deployed the technology in largely lower-income, non-white neighborhoods in New York and Los Angeles. \nThe investigation also indicated 'serious drawbacks' with RiteAid's first facial recognition partner, FaceFirst, whose technology several security professionals described as inaccurate, especially with regard to Black people and those from other races. \nRiteAid defended its policy by arguing that the technology was only being used in a 'data-driven' manner to detect and deter crime and violence, and that the cameras were appropriately flagged to customers. \nHowever, the chain swiftly shut down its system after the Reuters investigation, claiming its 'decision was in part based on a larger industry conversation.'\n\u2795 The American Civil Liberties Union (ACLU) had earlier questioned whether American retail chains were using face recognition without telling their customers. RiteAid, like nineteen other chains, failed to answer.\nSystem \ud83e\udd16\nFaceFirst website\nOperator: RiteAid\nDeveloper: FaceFirst; DeepCam; Shenzhen Shenmu\nCountry: USA\nSector: Retail\nPurpose: Reduce crime, violence\nTechnology: Facial recognition\nIssue: Accuracy/reliability; Bias/discrimination - race, ethnicity, income; Privacy\nTransparency: Governance; Marketing\nInvestigations, assessments, audits \ud83e\uddd0\nReuters (2020). Rite Aid deployed facial recognition systems in hundreds of U.S. stores\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reuters.com/investigates/special-report/usa-riteaid-software/\nhttps://www.wired.com/story/rite-aid-facial-recognition-twitter-hack-security-news/\nhttps://onezero.medium.com/rite-aids-secret-facial-recognition-system-is-the-tip-of-the-iceberg-f5839beeb0ab\nhttps://arstechnica.com/tech-policy/2020/07/rite-aid-deployed-facial-recognition-in-hundreds-of-stores-report-finds/\nhttps://www.dailymail.co.uk/news/article-8569753/Rite-Aids-200-facial-recognition-cameras-revealed-investigation.html\nhttps://www.engadget.com/rite-aid-facial-recognition-reuters-183343353.html\nhttps://www.cnbc.com/2020/07/28/rite-aid-deployed-facial-recognition-in-hundreds-of-us-stores.html\nhttps://slate.com/technology/2020/07/rite-aid-facial-recognition-technology-surveillance.html\nhttps://mashable.com/article/rite-aid-facial-recognition-surveillance/\nhttps://www.infosecurity-magazine.com/news/rite-aid-drops-facial-recognition/\nhttps://www.theverge.com/2020/7/28/21345185/rite-aid-facial-recognition-surveillance-spying\nRelated \ud83c\udf10\nRite Aid US facial recognition racial, income bias\nWalgreens fridge screen door biometrics\nPage info\nType: Incident\nPublished: March 2023\nLast updated: December 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/youplus-investor-fraud", "content": "YouPlus 'AI' intelligence engine investor fraud\nOccurred: July 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nYouPlus, a US-based start-up that marketed itself as an AI and machine learning-enabled 'Video Opinion Intelligence Engine platform', deliberately deceived investors about the nature of its business, according to an SEC complaint.\nYouPlus described itself as 'a cutting-edge technology innovation company that has built the world\u2019s first Video Opinion Intelligence Engine (VOISE), an advanced AI and Machine Learning platform to unlock consumer opinions and experience insights from videos.'\nThe SEC alleged that YouPlus CEO Shaukat Shamim falsely told investors that YouPlus had millions of dollars of revenue and more than 150 different business customers, when in fact it had earned less than USD 500,000 and had only four paying customers.\nWhen one investor pressed Shamim for information substantiating his claims, he allegedly provided the investor with falsified bank statements. \nThe US Attorney\u2019s Office for the Northern District of California also announced criminal charges against Shamim.\nSystem\nCrunchbase profile\nOperator: YouPlus\nDeveloper: YouPlus\nCountry: USA\nSector: Business/professional services\nPurpose: Analyse videos\nTechnology: Computer vision; NLP/text analysis\nIssue: Governance; Security\nTransparency: Governance; Marketing\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nSEC v YouPlus and Shaukat Shamim (pdf)\nUS SEC (2020). SEC Charges Silicon Valley Start-Up and CEO With Defrauding Investors\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.sfchronicle.com/business/article/SEC-Mountain-View-startup-founder-defrauded-15421779.php\nhttps://inc42.com/buzz/founder-in-valley-accused-of-11-mn-funding-fraud-for-faking-revenues-ai-tech/\nhttps://pitchbook.com/newsletter/youplus-founder-charged-with-fraud\nhttps://www.sfchronicle.com/business/article/SEC-Mountain-View-startup-founder-defrauded-15421779.php\nhttps://inside.com/campaigns/inside-ai-2020-07-21-23840/sections/200715\nhttps://www.theregister.com/2020/07/22/sec_sues_youplus/\nhttps://www.cfodive.com/news/YouPlus-Shaukat-Shamim-machine-learning-startup-SEC-fraud/582029/\nhttps://www.cfo.com/artificial-intelligence/2020/07/machine-learning-startup-was-a-sham-says-sec/\nRelated \ud83c\udf10\nOlive AI misleading marketing\nScaleFactor accountancy AI 'automation'\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/udbetaling-danmark-welfare-payments-optimisation", "content": "Udbetaling Danmark welfare payments optimisation prompts controversy\nReleased: January 2016\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nUdbetaling Danmark's welfare fraud control system led to leading legal, civil rights and privacy advocates to complain of a privacy and 'surveillance nightmare'.\nCreated in 2012 to profile unemployed citizens and streamline the payment of welfare benefits, Udbetaling Danmark used fraud control algorithms that can access the personal data of millions of citizens. \nThe data network allowed Udbetaling Danmark to automate checks required before benefits are granted, such as verifying income level or wealth. It also allowed the organisation to perform controls after a benefit had been granted to verify that a beneficiary\u2019s situation had not changed.\nHowever, it transpired that Udbetaling Danmark had access to the personal data of citizens who did not receive welfare payments, leading to complaints from legal, civil rights, and privacy advocates, who described the situation as a 'surveillance nightmare\u2019.\nThe controversy highlighted the challenges and pitfalls of using data and automation in the management of welfare payments.\n\u2795 In 2019, a case related to housing benefits revealed that Udbetaling Danmark\u2019s database held much information about a citizen who was not a beneficiary, leading to a complaint being filed with the Danish Data Protection Authority. \nSystem \ud83e\udd16\nUdbetaling Danmark website\nMinistry of Industry, Business and Financial Affairs (2021). Towards a better social contract with big tech (pdf)\nOperator: Udbetaling Danmark\nDeveloper: The Agency for Labour Market and Recruitment (STAR)\nCountry: Denmark\nSector: Govt - welfare\nPurpose: Optimise welfare payments\nTechnology:  \nIssue: Accuracy/reliability; Privacy; Surveillance\nTransparency: \nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nhttps://www.folketingstidende.dk/RIpdf/samling/20181/lovforslag/L209/20181_L209_som_vedtaget.pdf\nResearch, advocacy \ud83e\uddee\nJ\u00f8rgensen R.F. (2021). Data and rights in the digital welfare state: the case of Denmark\nMadsen C.\u00d8., Lindgren I., Melin U. (2021). The accidental caseworker \u2013 How digital self-service influences citizens' administrative burden\nChoroszewicz M., M\u00e4ih\u00e4niemi B. (2020). Developing a Digital Welfare State: Data Protection and the Use of Automated Decision-Making in the Public Sector across Six EU Countries\nAlgorithmWatch (2020). In a quest to optimize welfare management, Denmark built a surveillance behemoth\nDataEthics (2019). Is The Scandinavian Digitalisation Breeding Ground For Social Welfare Surveillance?\nJustitia (2019). Udebetaling Danmarks systematiske overv\u00e5gning (pdf)\nMploy (2017). Evaluering af projekt \u201dSamtaler og indsats der modvirker langtidsledighed\u201d (pdf)\nMinistry of Industry, Business and Financial Affairs (2021). Towards a better social contract with big tech (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://foreignpolicy.com/2018/12/25/the-welfare-state-is-committing-suicide-by-artificial-intelligence/\nhttps://www.businessinsider.in/denmark-is-using-algorithms-to-dole-out-welfare-benefits-and-undermining-its-own-democracy-in-the-process/articleshow/67279722.cms\nhttps://www.wired.com/story/algorithms-welfare-state-politics/\nhttps://www.information.dk/telegram/2012/03/kommuner-advarer-milliontab-ved-stordrift\nhttps://politiken.dk/viden/Tech/art7202917/Algoritmer-skal-udpege-langtidsledige\nhttps://fagbladet3f.dk/artikel/ny-lov-om-udpegning-af-ledige-er-muligvis-ulovlig\nhttps://www.dr.dk/nyheder/indland/400-sager-venter-computerprogrammer-afsloerer-barselsfup\nRelated \ud83c\udf10\nGladsaxe vulnerable children protection\nTrelleborg welfare management automation\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-robot-accident-hospitalises-24-workers", "content": "Amazon robot accident hospitalises 24 workers\nOccurred: December 2018\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA can of bear repellent was torn open by a robot at an Amazon warehouse in New Jersey, USA, resulting in one person having to be sent to intensive care and another two dozen employees sent to hospital. \nThe official investigation into the incident revealed that 'an automated machine accidentally punctured a 9-ounce bear repellent can, releasing concentrated Capsaican.' Capsaicin is the major ingredient in pepper spray.\nWhilst Amazon employees are not unionised, the Retail, Wholesale and Department Store Union said 'Amazon's automated robots put humans in life-threatening danger today, the effects of which could be catastrophic and the long-term effects for 80 plus workers are unknown.'\nSystem \ud83e\udd16\nUnknown\nOperator: Amazon\nDeveloper: Amazon\nCountry: USA\nSector: Transport/logistics\nPurpose: Move inventory\nTechnology: Robotics\nIssue: Safety; Accuracy/reliability\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.washingtonpost.com/business/2018/12/05/dozens-amazon-workers-sickened-after-bear-repellent-accidentally-discharged-warehouse\nhttps://eu.tennessean.com/story/news/2018/12/06/amazon-bear-spray-accident-robot-error/2229903002/\nhttps://eu.usatoday.com/story/money/business/2018/12/05/amazon-warehouse-nj-accident-shines-light-companys-safety-record/2216715002/\nhttps://www.wired.com/story/amazon-first-bear-repellent-accident/\nhttps://www.nj.com/mercer/2018/12/80-workers-at-amazon-warehouse-in-nj-treated-after-being-sickened-by-bear-repellant.html\nhttps://abcnews.go.com/US/24-amazon-workers-hospital-bear-repellent-accident/story\nhttps://www.huffingtonpost.co.uk/entry/robot-accidentally-hospitalises-24-amazon-workers-after-it-sprays-them-with-bear-repellent_uk_5c09036fe4b069028dc6dbb3\nhttps://www.theguardian.com/technology/2018/dec/06/24-us-amazon-workers-hospitalised-after-robot-sets-off-bear-repellent\nhttps://www.nbcnewyork.com/news/local/amazon-warehouse-new-jersey-multiple-sick/1816119/\nRelated \ud83c\udf10\nMalfunctioning robot impales Chinese factory worker\nRobot kills SKH Metals worker\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/fabio-retail-robot-fired-after-one-week", "content": "Fabio retail robot fired after one week\nOccurred: January 2018\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nFabio, a robot designed at Heriot-Watt University to communicate with humans, appeared to have put people off when trialed at Scottish supermarket chain Margiotta's flagship Edinburgh store.\nA customised version of Softbank's Pepper robot, Fabio was programmed to communicate, be helpful, have fun and tell jokes, and dispense hugs and high-fives. \nBut it turned out that Fabio wasn't much good at any of these things, though his designers reckoned his job was made no easier by background noise. When managers noticed customers actively avoiding the robot, the trial was ended after one week. \nUnexpectedly, Margiotta employees seemed to enjoy Fabio's company, feeling that it reduced more menial parts of their jobs. Some employees started crying when he was fired.\nSystem \ud83e\udd16\nFabio robot\nDocuments \ud83d\udcc3\nBBC (2018). Six Robots and Us \nOperator: Margiotta\nDeveloper: Heriot-Watt University; Softbank\nCountry: UK\nSector: Retail\nPurpose: Improve customer service\nTechnology: Robotics\nIssue: Accuracy/reliability; Employment - jobs; Anthropomorphism\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.iflscience.com/technology/store-hires-robot-to-help-out-customers-robot-gets-fired-for-scaring-customers-away/all/\nhttps://www.telegraph.co.uk/science/2018/01/21/fabio-robot-sacked-supermarket-alarming-customers/\nhttps://www.insider.co.uk/news/robot-hired-edinburgh-supermarket-fired-11892140\nhttps://www.zdnet.com/article/robot-fired-from-grocery-store-for-utter-incompetence/\nhttps://www.heraldscotland.com/news/15886315.first-robot-shop-assistant-tested-scottish-supermarket/\nhttps://www.dailymail.co.uk/news/article-5295837/Shop-hires-robot-assistant-fires-just-week.html\nhttps://www.indiatimes.com/culture/robots-are-getting-fired-left-right-and-centre-for-being-un-human-338287.html\nhttps://www.zmescience.com/tech/fabio-robot-supermarket-sacked/\nhttps://www.mentalfloss.com/article/526770/scottish-supermarket-fires-robot-employee-scaring-customers\nhttps://www.dailyrecord.co.uk/news/science-technology/robot-hired-supermarket-sacked-after-11893730\nhttps://www.insider.co.uk/news/robot-hired-edinburgh-supermarket-fired-11892140\nRelated \ud83c\udf10\nMarty grocery store robot\nDenny's robot server\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/malfunctioning-robot-impales-chinese-factory-worker", "content": "Malfunctioning robot impales Chinese factory worker\nOccurred: December 2018\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA worker at a porcelain factory in Hunan province, China, has been impaled by ten steel bars in the arm and chest after a robotic arm fell onto him.\nA robotic arm suddenly collapsed from a machine and fell onto the worker, its spikes spearing his body. Each of the sharp steel bars measured 30 centimetres (one foot) in length and 1.5 centimetres (0.59 inches) in diameter, according to the hospital which treated the worker. \nSurgeons managed to remove all the spikes from the worker's body. \nSystem \ud83e\udd16\nUnknown\nOperator: Zhuzhou porcelain factory\nDeveloper: \nCountry: China\nSector: Manufacturing/engineering\nPurpose: Assemble components\nTechnology: Robotics\nIssue: Safety\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.dailymail.co.uk/news/article-6483365/Chinese-worker-cheats-death-skewered-TEN-massive-steel-spikes-factory-accident.html\nhttps://www.thesun.co.uk/news/7954270/factory-robot-malfunctions-and-impales-worker-with-10-foot-long-steel-spikes/\nhttps://au.news.yahoo.com/factory-worker-impaled-3m-spikes-robot-malfunctions-052753101.html\nhttps://www.news.com.au/finance/work/at-work/factory-robot-impales-worker-with-10-footlong-steel-spikes-after-horror-malfunction/news-story/557bcd931213a1007c3129bbc1f59293\nhttps://newsinfo.inquirer.net/1062955/p2fb-factory-worker-survives-being-impaled-by-10-steel-spikes\nhttps://brobible.com/culture/article/robot-impales-human-robot-uprising-revolution/\nhttp://hn.people.com.cn/n2/2018/1207/c356887-32383111.html\nRelated \ud83c\udf10\nRobot kills SKH Metals worker\nAjin USA worker crushed to death by robot\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/shenzhen-uses-facial-recognition-to-catch-shame-jaywalkers", "content": "Shenzhen uses facial recognition to catch, shame jaywalkers\nOccurred: March 2018\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAuthorities in Shenzhen, China, launched CCTV cameras incorporating facial recognition and artificial intelligence linked to a database in a bid to crack down on jaywalking, traffic violations, and other crimes. \nBuilt by local technology company IntelliFusion, the DeepEye system captures images of people illegally crossing the road, identifies the citizen against a database and displays their photo alongside their family name and part of their government identification number on a roadside LED screen and government website. \nAccording to a Shenzhen government official, 'a combination of technology and psychology\u2026 can greatly reduce instances of jaywalking and will prevent repeat offences.'\nThe move prompted some locals and commentators to express their concerns about the intrusivess of the system, and its potential for deepening and expanding state surveillance.\nLocal media reported that jaywalking and other crimes added to the system also potentially damage one's score in China's 'social credit system'. \nSystem \ud83e\udd16\nIntelliFusion website\nOperator: Shenzhen Traffic Police Bureau\nDeveloper: Intellifusion\nCountry: China\nSector: Govt - municipal; Govt - police\nPurpose: Identify jaywalkers, criminals\nTechnology: Facial recognition; Automated license plate/number recognition (ALPR/ANPR)\nIssue: Privacy; Surveillance\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.scmp.com/tech/china-tech/article/2138960/jaywalkers-under-surveillance-shenzhen-soon-be-punished-text \nhttps://www.scmp.com/tech/start-ups/article/3008700/shenzhen-ai-start-intellifusion-helps-city-police-identify\nhttps://www.dailymail.co.uk/news/article-7228205/Chinese-city-punishes-JAYWALKERS-listing-untrustworthy-people-social-credit-system.html\nhttps://www.vice.com/en/article/wj7n74/china-jaywalking-facial-recognition-camera\nhttps://www.abc.net.au/news/2018-03-20/china-deploys-ai-cameras-to-tackle-jaywalkers-in-shenzhen/9567430\nhttps://www.thesun.co.uk/tech/5909463/china-jaywalkers-facial-recognition-technology-sms-fine-text/\nhttps://www.independent.co.uk/news/world/asia/china-police-facial-recognition-technology-ai-jaywalkers-fines-text-wechat-weibo-cctv-a8279531.html\nhttps://www.popularmechanics.com/technology/infrastructure/a19623846/chinese-facial-recognition-system-would-fine-jaywalkers-by-text/\nRelated \ud83c\udf10\nChina social credit offence travel bans\nSuzhou social 'civility score' trial\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/google-ads-for-blacks-suggest-criminal-records", "content": "Google ads for Blacks suggest criminal records\nOccurred: February 2013\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nResearch published by Harvard University's Latanya Sweeney found that Google ads are rife with racial discrimination, and may have an actual or potential impact on job seekers.\nThe 2013 study found that a Google search for a 'racially associated name' such as DeShawn, Darnell and Jermaine, is 25 times more likely to trigger adverts suggesting the person has a criminal background.\nThe ads were delivered by Google's AdWords system, which determines which advertisements appear based on keywords, advertiser bids, and user behaviour.\nWhat is less clear is whether the results are due to Google's system, people and organisations buying online advertising, or racism in society as a whole.\nSystem \ud83e\udd16\nGoogle AdSense website\nGoogle AdSense Wikipedia profile\nOperator: Alphabet/Google\nDeveloper: Alphabet/Google\nCountry: USA\nSector: Business/professional services\nPurpose: Deliver advertising\nTechnology: Advertising management system\nIssue: Bias/discrimination - race; ethnicity\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nLatanya Sweeney (2013). Discrimination in Online Ad Delivery\nResearch paper (pdf)\nResearch data\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.technologyreview.com/2013/02/04/253879/racism-is-poisoning-online-ad-delivery-says-harvard-professor/\nhttps://www.nbcnews.com/business/google-ads-may-be-racially-biased-professor-says-1C8369538\nhttps://abcnews.go.com/Technology/google-ad-delivery-shows-racial-bias-study-harvard/story?id=18419075\nhttps://www.huffingtonpost.co.uk/entry/online-racial-profiling_n_2622556\nhttps://www.bbc.co.uk/news/technology-21322183\nhttps://www.bostonglobe.com/business/2013/02/06/harvard-professor-spots-web-search-bias/PtOgSh1ivTZMfyEGj00X4I/story.html\nRelated \ud83c\udf10\nFacebook job ad delivery gender discrimination\nApple Watch blood oximeter racial bias\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/target-predicts-teen-girl-pregnancy", "content": "Target covertly predicts teen girl pregnancy\nOccurred: February 2012\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nUS-based supermarket chain Target was discovered to be quietly using a 'pregnancy prediction algorithm' to predict the pregancies of its customers, prompting a backlash about privacy abuse and poor transparency and ethics.\nIn February 2012, the New York Times' Charles Duhigg reported that a father had discovered that his teenage daughter was pregnant when he complained to a Target store in Minneapolis that she had received pregnancy-related coupons.\nTarget calculated the girl was pregnant by combining data from individual purchases and sociodemographic characteristics from public records databases to to assign each shopper a 'pregnancy prediction' score that would enable the retailer to send coupons and other marketing information.\nIt also transpired that Target had been producing brochures that had some non-baby merchandise sprinkled around the baby goodies, so the newly pregnant women didn't realise they had been targeted. \nThe controversy resulted in a debate about the need for clear, visible communication and the informed consent of customers, including whether privacy and terms of service are sufficient to notify consumers of the use of data mining techniques.\nSystem \ud83e\udd16\nUnknown\nOperator: Target\nDeveloper: Target; Andrew Pole\nCountry: USA\nSector: Retail\nPurpose: Predict pregnancy\nTechnology: Prediction algorithm\nIssue: Privacy; Ethics\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.nytimes.com/2012/02/19/magazine/shopping-habits.html\nhttps://www.forbes.com/sites/kashmirhill/2012/02/16/how-target-figured-out-a-teen-girl-was-pregnant-before-her-father-did/\nhttp://www.abc.net.au/science/articles/2014/04/15/3985934.htm\nhttps://techland.time.com/2012/02/17/how-target-knew-a-high-school-girl-was-pregnant-before-her-parents/\nhttps://slate.com/human-interest/2014/06/big-data-whats-even-creepier-than-target-guessing-that-youre-pregnant.html\nhttps://www.businessinsider.com/the-incredible-story-of-how-target-exposed-a-teen-girls-pregnancy-2012-2\nhttps://www.business2community.com/big-data/target-predicts-pregnancy-with-big-data-0522223\nhttps://www.globalbusinessandhumanrights.com/2012/02/23/predictive-analytics-informed-consent-and-privacy-the-case-of-target/\nRelated \ud83c\udf10\nMicrosoft teen pregnancy predictions\nApple Cycle Tracking fertility predictions\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/temple-of-heaven-park-uses-facial-recognition-to-stop-toilet-paper-theft", "content": "Temple of Heaven Park uses facial recognition to stop toilet paper theft\nOccurred: March 2017\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nIn an attempt to reduce the theft of toilet paper, restrooms in Beijing's Temple of Heaven Park were equipped with facial recognition systems. The machines will not dispense more paper to the same person until after nine minutes have passed. \nThe move divided users, with some saying it was inappropriate and intrusive, whilst others reckoned it was necessary and long overdue. \nPark authorities told Beijing Wanbao that the daily amount of toilet paper used in its toilets had reduced by 20 percent. But some reports also said the machines could be unreliable, and caused delays and confusion.\nAs CNN noted, visitors say the biggest targets of the new crackdown are older people who stuff their bags and pockets full of toilet paper to take back home. Many public restrooms in China do not provide toilet paper, and visitors are expected to bring their own. \nSystem \ud83e\udd16\nUnknown\nOperator: Temple of Heaven Park\nDeveloper: Shoulian Zhineng\nCountry: China\nSector: Govt - municipal\nPurpose: Reduce toilet paper theft\nTechnology: Facial recognition\nIssue: Appropriateness/need; Privacy; Robustness\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://edition.cnn.com/2017/03/20/world/china-toilet-paper-thieves-face-recognition-trnd/index.html\nhttps://www.cnet.com/news/facial-recognition-toilet-paper-beijing-temple-of-heaven-park/\nhttps://www.nytimes.com/2017/03/20/world/asia/china-toilet-paper-theft.html\nhttps://www.nbcnews.com/news/china/china-fights-toilet-paper-theft-facial-recognition-technology-n736236\nhttp://www.ecns.cn/2017/03-22/250247.shtml\nhttp://www.xinhuanet.com/english/2017-03/22/c_136148318.htm\nhttps://www.gulf-times.com/story/539364/Face-recognition-flushes-out-China-s-toilet-paper-\nhttps://www.bbc.co.uk/news/world-asia-china-39324431\nhttps://www.washingtonpost.com/news/morning-mix/wp/2017/03/21/china-uses-facial-recognition-software-to-crack-down-on-toilet-paper-theft/\nRelated \ud83c\udf10\nNingbo real estate facial recognition\nXPeng customer facial recognition\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/robot-kills-skh-metals-worker", "content": "Robot kills SKH Metals worker\nOccurred: August 2015\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nRamji Lal, a loader for auto parts company SKH Metals, was killed by an industrial robot when he was adjusting a metal sheet being welded by the machine. \nLal apparently came too close to a robotic arm while adjusting the metal sheet and crossed one the robot's sensors, when he was picked up and crushed by the robotic arm.\nThe incident led to workers halting work at the factory in protest, and investigations by the police and the Labour Department of Haryana state.\nSystem \ud83e\udd16\nUnknown\nOperator: SKH Metals\nDeveloper: Unclear/unknown\nCountry: India\nSector: Manufacturing/engineering\nPurpose: Weld metal sheets\nTechnology: Robotics\nIssue: Safety\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.independent.co.uk/news/world/asia/worker-killed-robot-welding-accident-car-parts-factory-india-10453887.html\nhttps://timesofindia.indiatimes.com/india/Terminator-redux-Robot-kills-a-man-at-Haryanas-Manesar-factory/articleshow/48460738.cms\nhttps://www.hindustantimes.com/gurgaon/manesar-factory-worker-crushed-to-death-by-industrial-robot/story-0Hc7V2uu2L2jlYfo9gEdXK.html\nhttps://www.roboticsbusinessreview.com/rbr/factory_robot_kills_worker_in_india/\nhttps://www.ibtimes.co.in/robot-kills-man-gurgaon-factory-642723\nRelated \ud83c\udf10\nAjin USA worker crushed to death by robot\nRobot crushes and kills VW contractor\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ajin-usa-worker-crushed-to-death-by-robot", "content": "Ajin USA worker crushed to death by robot\nOccurred: June 2016\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA worker was killed when a robot she had been trying to fix at auto parts manufacturer Ajin USA unexpectedly restarted and crushed her. \nRegina Allen Elsea had entered a robotic station ('cell') containing several robots to clear a sensor fault on a piece of machinery that had stopped working during an assembly line stoppage. When inside the cell, one of the robots energised and she was struck by a robotic arm which pinned her against a piece of machinery.\nIn November 2020, Ajin USA was ordered to pay USD 1.5 million after admitting violating federal safety standards before Elsea was crushed to death. It also had to complete three years of probation, during which it must comply with a safety compliance plan overseen by a third-party auditor.\nAjin pleaded guilty to knowingly failing to enforce federal safety standards, including the mandatory use of so-called lockout/tagout procedures to prevent the type of incident that killed Elsea. \nTwo weeks before Elsea's death, the US Labor Department fined Ajin USA and two staffing agencies USD 2.5 million for 27 safety violations. \nSystem \ud83e\udd16\nUnknown\nOperator: Ajin USA\nDeveloper:\nCountry: USA\nSector: Automotive; Manufacturing/engineering\nPurpose: Unclear/unknown\nTechnology: Robotics\nIssue: Safety\nTransparency: Legal\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nUSA v Joon LLC/Ajin USA (2020). Plea Agreement\nUS Department of Justice (2020). Auto-Parts Manufacturing Company Sentenced in Worker Death Case\nUS Department of Labor (2016). Alabama Auto Parts Supplier to Kia and Hyundai, Staffing Agencies Face USD 2.5 million in Fines After Robot Crushes Young Bride-to-Be\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://apnews.com/article/technology-robotics-b1ad356323a007d4124fd6b9771b3518\nhttps://www.thedailybeast.com/bride-to-be-crushed-to-death-by-car-factory-robot\nhttp://www.wtvm.com/story/32264969/east-alabama-woman-20-killed-from-workplace-incident-at-ajin-usa\nhttps://www.dailymail.co.uk/news/article-8935391/Manufacturing-company-admits-causing-death-worker-20-crushed-robotic-arm.html\nhttps://www.manufacturing.net/operations/news/13114198/family-of-woman-killed-by-robotic-machine-sues-auto-firm\nhttps://www.autobodynews.com/alabama-auto-parts-supplier-to-pay-1-3m-after-20-year-old-worker-s-2016-death.html\nhttps://www.cbs42.com/news/alabama-auto-parts-supplier-fined-after-girl-crushed-to-death-by-robot-2-weeks-before-wedding/\nRelated \ud83c\udf10\nRobot crushes and kills VW contractor\nChess robot breaks child's finger\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-alexa-holds-2am-party-when-owner-is-out", "content": "Amazon Alexa holds 2am party when owner is out\nOccurred: November 2017\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nPolice in Hamburg, Germany, had to break into a German man's house after his Alexa device decided to hold a party on its own while he was out with friends.\nAccording to Oliver Haberstroh, his Amazon personal assistant suddenly started playing music at full volume at 1.50am without having received any instructions and without him having used his mobile phone.\nAngry neighbours alerted the police, who broke into Haberstroh's apartment and silenced Alexa, pulling out the plug.\nSystem \ud83e\udd16\nAmazon Alexa virtual assistant\nOperator: Oliver Haberstroh\nDeveloper: Amazon\nCountry: Germany\nSector: Consumer goods\nPurpose: Provide information, services\nTechnology: NLP/text analysis; Natural language understanding (NLU); Speech recognition\nIssue: Accuracy/reliability\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.thesun.co.uk/news/4873155/cops-raid-german-blokes-house-after-his-alexa-music-device-held-a-party-on-its-own-while-he-was-out/\nhttps://www.theregister.com/2017/11/09/alexa_raid_my_apartment/\nhttps://www.dailymail.co.uk/news/article-5062491/Police-called-Alexa-device-holds-1am-party.html\nhttps://mashable.com/2017/11/08/amazon-alexa-rave-party-germany/\nhttps://www.nakedcapitalism.com/2017/11/why-you-should-never-buy-an-amazon-echo-or-even-get-near-one.html\nhttps://www.telegraph.co.uk/news/2017/11/08/alexa-nein-police-break-german-mans-house-music-device-held/\nhttps://www.ministryofsound.com/posts/articles/2017/november/police-called-to-empty-flat-after-rogue-alexa-throws-rave/\nhttps://www.ibtimes.co.uk/amazon-alexa-ai-goes-rogue-wakes-neighbourhood-2am-rave-police-raid-1646720\nRelated \ud83c\udf10\nAmazon Alexa recommends girl touches electric plug \nAmazon Alexa plays child pornography\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-alexa-mistakenly-orders-usd-160-dollhouse", "content": "Amazon Alexa mistakenly orders USD 160 dollhouse\nOccurred: January 2017\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA six-year-old girl in Dallas, Texas, mistakenly ordered an expensive dollhouse via her Alexa-enabled Amazon Echo device, leaving her parents with an expensive bill.\nMegan Neitzel\u2019s daughter was talking to Alexa about a dollhouse and cookies when Alexa mistook the conversation as a request to purchase the goods and ordered a KidKraft Sparkle mansion dollhouse and four pounds of sugar cookies.\nMegan later admitted she had never read the manual or learned about Alexa's child lock properties. The system offers the ability to add a passcode to prevent accidental purchases. She later donated the dollhouse to a local hospital.\nA San Diego TV report about the incident caused Echoes in viewers' homes to also attempt to order dollhouses. \nSystem \ud83e\udd16\nAmazon Alexa virtual assistant\nOperator: Megan Neitzel\nDeveloper: Amazon\nCountry: USA\nSector: Consumer goods\nPurpose: Provide information, services\nTechnology: NLP/text analysis; Natural language understanding (NLU); Speech recognition\nIssue: Accuracy/reliability; Safety\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://news.sky.com/story/amazon-echo-orders-dollhouses-after-hearing-tv-presenter-talking-10722985\nhttps://qz.com/880541/amazons-amzn-alexa-accidentally-ordered-a-ton-of-dollhouses-across-san-diego/\nhttps://edition.cnn.com/2017/01/05/health/amazon-alexa-dollhouse-trnd/index.html\nhttps://www.theregister.com/2017/01/07/tv_anchor_says_alexa_buy_me_a_dollhouse_and_she_does/\nhttps://www.theverge.com/2017/1/7/14200210/amazon-alexa-tech-news-anchor-order-dollhouse\nhttps://fortune.com/2017/01/09/amazon-echo-alexa-dollhouse/\nhttps://www.theguardian.com/technology/shortcuts/2017/jan/09/alexa-amazon-echo-goes-rogue-accidental-shopping-dolls-house\nhttps://www.foxnews.com/tech/6-year-old-accidentally-orders-high-end-treats-with-amazons-alexa\nRelated \ud83c\udf10\nAmazon Alexa plays child pornography\nAmazon Alexa recommends girl touches electric plug\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-employees-listen-to-alexa-recordings", "content": "Amazon employees listen to Alexa recordings\nOccurred: April 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAmazon employees and contractors have been quietly listening to what customers say on Alexa in order to train the company's software. They have been doing so without informing their customers, triggering concerns about privacy. \nAccording to Bloomberg, Amazon employs thousands of people in the United States, Costa Rica, Romania, and other countries to listen to as many as 1,000 audio clips in shifts that last up to nine hours. \nAmazon's FAQs state it uses 'requests to Alexa to train our speech recognition and natural language understanding systems,' but it was taken to task for not 'explicitly' telling Alexa users that it gets people to listen to the recordings.\nAmazon said it only annotates an 'extremely small number of interactions from a random set of customers,' and that it takes the 'security and privacy of our customers\u2019 personal information seriously.'\nAmazon clarified that no audio is stored on customers' Alexa-enabled devices unless it is activated by a 'wake' word. \nSystem \ud83e\udd16\nAmazon Alexa virtual assistant\nOperator: Amazon\nDeveloper: Amazon \nCountry: USA\nSector: Consumer goods\nPurpose: Provide information, services\nTechnology: NLP/text analysis; Natural language understanding (NLU); Speech recognition\nIssue: Privacy\nTransparency: Governance; Privacy; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bloomberg.com/news/articles/2019-04-10/is-anyone-listening-to-you-on-alexa-a-global-team-reviews-audio\nhttps://time.com/5568815/amazon-workers-listen-to-alexa/\nhttps://edition.cnn.com/2019/04/11/tech/amazon-alexa-listening/index.html\nhttps://threatpost.com/amazon-auditors-listen-to-echo-recordings-report-says/143696/\nhttps://www.theguardian.com/technology/2019/apr/11/amazon-staff-listen-to-customers-alexa-recordings-report-says\nhttps://www.forbes.com/sites/kateoflahertyuk/2019/04/12/amazon-staff-are-listening-to-alexa-conversations-heres-what-to-do\nhttps://eu.usatoday.com/story/tech/2019/04/11/amazon-employees-listening-alexa-customers/3434732002/\nhttps://www.dailymail.co.uk/sciencetech/article-6956531/Amazon-employees-listening-Alexa-recordings-customers-live.html\nhttps://www.the-ambient.com/news/amazon-listening-alexa-recordings-privacy-1530\nhttps://www.cnbc.com/2019/04/11/how-to-stop-amazon-from-listening-to-what-you-say-to-alexa.html\nRelated \ud83c\udf10\nAmazon Alexa plays child pornography\nAmazon Alexa recommends girl touches electric plug\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-alexa-plays-child-pornography", "content": "Amazon Alexa plays child pornography\nOccurred: December 2016\nA child who had asked Amazon's Alexa voice software to 'Play Digger Digger', one of his favourite songs, was met with crude and pornographic language, to the horror of his parents.\nInstead of Alexa playing the child's request, Alexa started saying 'You want to hear a station for \u2018Porn detected\u2026.Porno ringtone hot chick amateur girl calling sexy,' followed by a string of crude and dirty words. \nAmazon later said that they had fixed the issue and were working towards building additional restrictions to prevent this kind of incident from happening in the future.\nSystem \ud83e\udd16\nAmazon Alexa developer website\nAmazon Alexa Wikipedia profile\nOperator:  \nDeveloper: Amazon \nCountry: USA\nSector: Consumer goods\nPurpose: Provide information, services\nTechnology: NLP/text analysis; Natural language understanding (NLU); Speech recognition\nIssue: Accuracy/reliability; Safety\nTransparency: Governance\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nWilson C. (2020). Dangerous Skills Got Certified: Measuring the Trustworthiness of Amazon Alexa Platform (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.youtube.com/watch?v=r5p0gqCIEa8\nhttps://www.inquisitr.com/3865374/fails-and-facepalms-with-amazons-alexa-dont-let-the-kids-near-that-thing/\nhttps://nypost.com/2016/12/30/toddler-asks-amazons-alexa-to-play-song-but-gets-porn-instead/\nhttps://www.dailymail.co.uk/femail/article-4076568/That-doesn-t-sound-like-Wheels-Bus-Parents-freak-Amazon-s-Alexa-misunderstands-young-son-s-request-song-starts-rattling-crude-PORNOGRAPHIC-phrases.html\nhttps://www.huffpost.com/archive/au/entry/kid-asks-a-digital-assistant-for-a-song-gets-porn-in-response_a_21646030\nhttps://www.entrepreneur.com/business-news/whoops-alexa-plays-porn-instead-of-a-kids-song/287281\nhttps://nymag.com/intelligencer/2016/12/kid-gets-amazon-echo-dot-alexa-to-play-porn.html\nhttps://www.scarymommy.com/alexa-plays-porn-for-kid/\nRelated \ud83c\udf10\nAmazon Alexa recommends girl touches electric plug \nAmazon employees listen to Alexa recordings\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/robot-crushes-and-kills-vw-contractor", "content": "Industrial robot crushes and kills VW contractor\nOccurred: July 2015\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA contractor at a VW plant north of Frankfurt installing an industrial robot was grabbed and crushed against a metal plate, killing him. The man was resuscitated at the factory but died later in hospital.\nVW said the intial conclusions of its investigation into the incident had suggested that human error was to blame. The robotic line being assembled had not been formally handed over to Volkswagen, according to the car manufacturer.\nVW also said the robot usually operates within a confined area at the plant, grabbing auto parts and manipulating them.\nThe incident prompted unions and others to express their concerns about the safety dangers of industrial robots.\nSystem \ud83e\udd16\nUnknown\nOperator: Volkswagen\nDeveloper: Unclear/unknown\nCountry: Germany\nSector: Automotive; Manufacturing/engineering\nPurpose: Configure auto parts\nTechnology: Robotics\nIssue: Safety; Liability\nTransparency: \nResearch, advocacy \ud83e\uddee\nKirschgens L.A., Ugarte I.Z., Uriarte E.G., Rosas A.M., Vilches V.M. (2021). Robotic Hazards: From Safety to Security (pdf)\nLloyd's (2019). Emerging Risk Report 2019 - Technology (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/newsbeat-33359005\nhttps://www.ft.com/content/0c8034a6-200f-11e5-aa5a-398b2169cf79\nhttps://time.com/3944181/robot-kills-man-volkswagen-plant/\nhttps://www.iflscience.com/technology/robot-kills-worker-volkswagen-plant-germany/\nhttps://www.chicagotribune.com/nation-world/ct-robot-kills-worker-20150702-story.html\nhttps://www.dw.com/en/robot-kills-worker-at-volkswagen-plant-in-germany/a-18556982\nhttps://apnews.com/article/d18c4801a5324926a1845690148b664a\nhttps://phys.org/news/2015-07-robots-dangerous.html\nhttps://www.iflscience.com/robots-can-t-kill-you-claiming-they-can-dangerous-29228\nhttps://venturebeat.com/2017/09/06/robots-can-kill-but-can-they-murder/\nhttps://www.vice.com/en/article/gy7j8b/a-robot-killed-a-man-a-new-doc-looks-at-the-terrifying-future-of-automation\nRelated \ud83c\udf10\nOcado robot charger malfunction\nChess robot breaks child's finger\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/google-search-conflates-black-girls-with-pornography", "content": "Google search conflates 'black girls' with pornography \nOccurred: 2012\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe top search returns for the phrase 'Black girls' on Google link to sexist and racist websites, including highly explicit pornography, according to a prominent US academic Safiya Noble.\nFrom 2010, US academic Safiya Noble conducted a series of experiments that revealed that Google searches on Black girls, Latina girls, Asian girls returned pornography, hypersexualized content, and other inappropriate links. Conversely, search returns for 'white girls' were much less controversial. \nNoble surmised that search algorithms reflect the racist and sexist biases of their designers, developers, and operators. She also contended that they are reluctant to acknowledge the problem or update their algorithms in a meaningful manner lest it disrupts the advertising revenue generated alongside the search returns, instead preferring to hide behind their opaque 'black box' algorithmic systems.\nThe findings were also featured in Noble's 2018 book Algorithms of Oppression. \nSystem \ud83e\udd16\nGoogle Search website\nGoogle Search Wikipedia profile\nOperator: Alphabet/Google\nDeveloper: Alphabet/Google\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Rank search results\nTechnology: Search engine algorithm; Machine learning\nIssue: Bias/discrimination - race, ethnicity, gender\nTransparency: Governance; Black box\nResearch, advocacy \ud83e\uddee\nNoble S (2018). Algorithms of Oppression\nNoble S (2013). Google Equates Black Girls With Sex: Why?\nNoble S (2012). Missed Connections: What Search Engines Say About Women\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://time.com/5209144/google-search-engine-algorithm-bias-racism/\nhttps://www.vogue.com/article/safiya-noble\nhttps://www.nbcnews.com/think/opinion/google-search-algorithms-are-not-impartial-they-are-biased-just-ncna849886\nhttps://www.vox.com/2018/4/3/17168256/google-racism-algorithms-technology\nhttps://news.yahoo.com/safiya-noble-speaks-algorithmic-oppression-173127056.html\nRelated \ud83c\udf10\nGoogle 'three black teenagers' mugshot stereotyping\nGoogle search prioritises Holocaust denial website\nPage info\nType: Incident\nPublished: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/yang-mi-athena-chu-face-swap-deepfake-video", "content": "Athena Chu deepfake face swap prompts controversy\nOccurred: February 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA video that spliced Chinese actress Yang Mi with Athena Chu, an older Hong Kong actress who starred in a 25-year-old TV series The Legend of the Condor Heroes set China's social media alight, and not in an altogether positive manner.\nChinese 'netizens' criticised its creator for disrespecting both actresses and of violating Yang's image rights, and worried about the broader effects of the technology on society.\nThe video creator, who went under the pseudonym Xiao, (which translates as 'Brother Face-Swapping'), apologised, claiming he had wanted to educate the public on the danger of deepfakes.\nThe video was removed from Weibo, China's Twitter equivalent. \nHowever, many copies continue to circulate online.\nSystem \ud83e\udd16\nUnknown\nOperator:  \nDeveloper: \nCountry: China\nSector: Media/entertainment/sports/arts\nPurpose: \nTechnology: Deepfake - video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Privacy; Copyright; Ethics/values\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.techinasia.com/chinese-alister-falls-victim-deepfake-video-stunt\nhttps://www.sixthtone.com/news/1003610/chinese-deepfake-creator-says-videos-meant-to-educate-public\nhttps://www.scmp.com/abacus/culture/article/3029491/ai-generated-fake-porn-featuring-female-celebrities-sold-china\nhttps://www.scmp.com/news/china/society/article/3019389/chinas-deepfake-celebrity-porn-culture-stirs-debate-about\nhttps://medium.com/syncedreview/china-prohibits-deepfake-ai-face-swapping-techniques-981a8e9e1c6e\nhttps://www.straitstimes.com/asia/east-asia/china-may-outlaw-deepfake-ai-technology-that-alters-faces-of-people\nhttps://www.scmp.com/abacus/culture/article/3029491/ai-generated-fake-porn-featuring-female-celebrities-sold-china\nhttp://global.chinadaily.com.cn/a/201904/22/WS5cbd15c4a3104842260b76c8.html\nhttps://www.theguardian.com/technology/ng-interactive/2019/jun/22/the-rise-of-the-deepfake-and-the-threat-to-democracy\nRelated \ud83c\udf10\nZAO face swapping\nFaceMega sexualised face swapping\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/malaysia-minister-aide-gay-sex-deepfake", "content": "Malaysia minister, aide gay sex 'deepfake'\nOccurred: June 2016\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA gay sex video allegedly featuring Mohamed Azmin Ali, Malaysia's Minister of Economic Affairs, and Muhammad Haziq Abdul Aziz, a rival minister\u2019s 27 year-old aide circulated on WhatsApp and more widely, leading to a political controversy, calls for resignations, and a police investigation.\nAziz swore that the video was real, but the minister and Prime Minister dismissed it as a deepfake, leading Aziz to post a 'confession' to Facebook a few hours later. \nAs Intelligencer noted, the fact that the video may have been a deepfake may have allowed the minister to escape the serious legal consequences he may otherwise expect in a socially conservative country.\n'Nowadays you can produce all kinds of pictures if you are clever enough,' then Malaysia Prime Minister Mahathir Mohamad retorted. 'One day you may also see my picture like that. It would be very funny.'\nSystem \ud83e\udd16\nUnknown\nOperator: Unclear/unknown\nDeveloper: Unclear/unknown\nCountry: Malaysia\nSector: Politics\nPurpose: Smear/discredit\nTechnology: Deepfake - video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Mis/disinformation\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.malaysiakini.com/news/479268\nhttps://www.malaymail.com/news/malaysia/2019/06/12/the-real-haziq-not-as-built-santubong-pkr-chief-says-of-man-in-gay-sex-clip/1761392\nhttps://www.malaymail.com/news/malaysia/2019/06/13/dr-m-if-gutter-politics-continues-my-turn-as-video-victim-may-come-after/1761603\nhttps://nymag.com/intelligencer/2019/06/how-do-you-spot-a-deepfake-it-might-not-matter.html\nhttps://www.businessinsider.my/one-day-you-may-see-my-picture-also-like-that-mahathir-says-sex-tapes-of-minister-are-fake-and-politically-motivated/\nhttps://eandt.theiet.org/content/articles/2020/04/sex-coups-and-the-liar-s-dividend-what-are-deepfakes-doing-to-us/\nhttps://www.wired.co.uk/article/how-to-spot-deepfake-video\nhttps://www.malaysia-today.net/2019/06/15/is-it-azmin-or-a-deepfake/\nhttps://theleaders-online.com/anwar-let-the-police-authenticate-the-clips/\nhttps://malaysia.news.yahoo.com/report-experts-sex-videos-not-094410155.html\nRelated \ud83c\udf10\nPresident Ali Bongo recovery deepfake broadcast\nRana Ayyub deepfake porn attack, doxxing\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/president-ali-bongo-recovery-deepfake-broadcast", "content": "President Ali Bongo recovery video accused of being a deepfake\nOccurred: March 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA TV broadcast appearance of Gabon's President Ali Bongo claiming he had recovered from ill-health prompted accusations that the clip was a deepfake concocted by his government. \nCommentators and political adversaries noted that Bongo's appearance was marked his 'immobile' face, that his eyes were out of sync with his jaw, how little he blinked, and that his speech patterns appeared different.\nThe clip appeared at a time when the President was out of the country receiving medical treatment, fueling rumours that he was seriously unwell.\nA week after the video\u2019s release, Gabon\u2019s military attempted a coup d'etat, citing the video as an indicator there was something wrong with the president. The coup attempt failed. However, the video was seen to have led to increased political and societal instability.\nSome experts have since concluded the video may not have been a deepfake, though opinion remains divided.\nOperator:  \nDeveloper: \nCountry: Gabon\nSector: Politics\nPurpose: Defend reputation\nTechnology: Deepfake - video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Mis/disinformation\nTransparency: Governance; Marketing\nSystem \ud83e\udd16\nUnknown\nIncident video\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.thedailybeast.com/inside-the-deepfake-arms-race\nhttps://www.motherjones.com/politics/2019/03/deepfake-gabon-ali-bongo/\nhttps://www.politico.eu/article/deepfake-videos-the-future-uncertainty/\nhttps://www.ft.com/content/4bf4277c-f527-11e9-a79c-bc9acae3b654\nhttps://eandt.theiet.org/content/articles/2020/04/sex-coups-and-the-liar-s-dividend-what-are-deepfakes-doing-to-us/\nhttps://www.forbes.com/sites/robtoews/2020/05/25/deepfakes-are-going-to-wreak-havoc-on-society-we-are-not-prepared/\nhttps://www.seattletimes.com/nation-world/top-ai-researchers-race-to-detect-deepfake-political-videos-we-are-outgunned/\nSystem \ud83c\udf10\nPresident Zelenskyy deepfake surrender\nSouth Korea presidential election candidate deepfakes\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/mark-zuckerberg-spectre-data-sharing-deepfake", "content": "Mark Zuckerberg 'Spectre' data sharing deepfake\nOccurred: June 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA deepfake video of Meta CEO Mark Zuckerberg talking about how he holds control over billions of people and his allegiance to 'Spectre' went viral, embarrasing Facebook and Zuckerberg.\nMade by UK-based artists Bill Posters and Daniel Howe, the deepfake was originally intended to highlight Facebook's use of behavioural psychology to persuade people to share intimate details of their lives so that they could be targeted with advertising. \nBut Facebook's failure to remove a clearly faked video of US Senate speaker Nancy Pelosi apparently slurring her words persuaded the two to change course and lampoon Zuckerberg.\nThe resulting video of Zuckerberg went vira, partly as Facebook was seen to remove something taking a pop at it's perceived hypocrisy, partly as it confused some people into thinking the fake video was real.\nThe deepfake was initially removed by Instagram content moderators on the basis that it violated its disinformation policy, but was restored when its satirical nature became evident and the potential impact of its removal became clearer.\nAccording to Vice News, the original video of Zuckerberg is from a September 2017 address he had given about Russian election interference on Facebook.  \nSystem \ud83e\udd16\nUnknown\nMark Zuckerberg deepfake video\nOperator: Bill Posters; Daniel Howe\nDeveloper: Bill Posters; Daniel Howe\nCountry: UK; USA\nSector: Media/entertainment/sports/arts; Politics\nPurpose: Expose hypocrisy\nTechnology: Deepfake - video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Hypocrisy; Mis/disinformation\nTransparency: \nInvestigations, assessments, audits \ud83e\uddd0\nDeepfaked. Mark Zuckerberg\u2019s deepfake speech shines light on power of Facebook\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/technology-48636456\nhttps://edition.cnn.com/2019/06/11/tech/zuckerberg-deepfake/index.html\nhttps://www.vice.com/en/article/ywyxex/deepfake-of-mark-zuckerberg-facebook-fake-video-policy\nhttps://fortune.com/2019/06/12/deepfake-mark-zuckerberg/\nhttps://www.theguardian.com/technology/2019/jun/11/deepfake-zuckerberg-instagram-facebook\nhttps://futurism.com/the-byte/deepfake-mark-zuckerberg-video\nhttps://www.vice.com/en/article/ywyxex/deepfake-of-mark-zuckerberg-facebook-fake-video-policy\nhttps://www.washingtonpost.com/nation/2019/06/12/mark-zuckerberg-deepfake-facebook-instagram-nancy-pelosi/\nRelated \ud83c\udf10\nJordan Peterson deepfake voice generator\nSouth Korea presidential election candidate deepfakes\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/elite-dangerous-ai-spaceships-create-superweapons", "content": "Elite Dangerous AI spaceships create superweapons \nOccurred: June 2016\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nSpace exploration videogame Elite Dangerous received an update featuring an AI system that caused enemies' spaceships to become extremely powerful and aggressive, resulting in a backlash amongst the game's users. \nDeveloped by Frontier Development and released in 2015, Eite Dangerous is a massively multiplayer online game in which the players' actions can affect the story outcomes. But the '2.1 Engineers' update strengthened the capabilities of the enemy spaceships to such an extent they were able to craft their own weaponry. \nFrontier said it believed the update shipped with a networking issue that let the AI merge weapon statistics and abilities, thereby causing unusual weapon attacks. \nThe company was forced to remove the upgrade and strip the spaceships of their upgraded weapons, and reimburse players of their insurance payouts over the period the update was in place. \nSystem \ud83e\udd16\nElite Dangerous website\nElite Dangerous Wikipedia profile\nFrontier Developments website\nOperator: Frontier\nDeveloper: Frontier\nCountry: UK; USA; Global\nSector: Media/entertainment/sports/arts\nPurpose: Strengthen gameplay\nTechnology: Machine learning\nIssue: Accuracy/reliability\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.eurogamer.net/articles/2016-06-03-elite-dangerous-latest-expansion-caused-ai-spaceships-to-unintentionally-create-super-weapons \nhttps://massivelyop.com/2016/06/02/elite-dangerous-identifies-issues-with-ai-superweapons-in-the-game/\nhttps://www.digitalspy.com/videogames/a796635/elite-dangerous-ai-super-weapons-bug/\nhttps://www.gamedeveloper.com/production/frontier-inadvertently-drives-i-elite-dangerous-i-ai-to-create-superweapons\nhttps://www.mirror.co.uk/tech/rogue-video-game-ai-creates-8180912\nRelated \ud83c\udf10\nWitcher 3 AI voiceline simulation\nUserviz video game cheating system\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/computer-glitch-gives-hundreds-of-scottish-offenders-wrong-risk-level", "content": "Glitch gives wrong risk level to hundreds of Scottish offenders \nOccurred: March 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn IT error in a system used by Scotland's criminal justice service to help set risk levels for use in sentencing and prison release decisions resulted in the wrong risk level being attributed to hundreds of Scottish offenders.\nThe Level of Service and Case Management Inventory (LS/CMI) is a general risk assessment tool (pdf) that draws on several dozen assessment factors to categorise prisoner risk into five categories, from 'Very low' to 'Very high'. It has been used by social workers and prison staff in all 32 Scottish authorities since November 2021.\nOfficials discovered that the system was failing to update its assessment when new information about an individual had been entered. As a result, risk scores were incorrect for 495 'closed' cases and for another 285 'open' cases.\nThe incident resulted in eight of Scotland's worst offenders serving life for murder or rape being released from jail early, and the possibility that hundreds of others had been freed in error, resulting in a political controversy and an apology from Scotland's Justice Secretary.\nSocial workers were told to switch to a paper-based system while the problem was resolved. \nSystem \ud83e\udd16\nLevel of Service and Case Management Inventory (LS/CMI) website\nDocuments \ud83d\udcc3\nScottish government (2022). Justice system approach to risk assessment \u2013 Ministerial statement\nScottish Parliament Criminal Justice Committee (2022). Risk management in the justice system\nOperator: Scottish Prison Service\nDeveloper: MHS\nCountry: UK; Scotland\nSector: Govt - justice\nPurpose: Assess offender risk\nTechnology: Risk assessment algorithm; Machine learning  \nIssue: Robustness\nTransparency: Governance\nInvestigations, assessments, audits \ud83e\uddd0\nRMA product evaluation (pdf)\nRMA (2014). Level of Service / Case Management Inventory in Practice National Report\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/uk-scotland-scotland-politics-60610757\nhttps://www.scotsman.com/news/politics/hundreds-of-offenders-may-have-been-released-early-due-to-it-risk-glitch-3596018\nhttps://www.dailyrecord.co.uk/news/politics/hundreds-prisoners-released-after-flawed-26379935\nhttps://www.thetimes.co.uk/article/criminals-in-scotland-freed-with-wrong-risk-level-xrnxm238s\nhttps://www.scottishdailyexpress.co.uk/news/politics/snp-news-prisoner-blunder-brown-26425510\nhttps://www.standard.co.uk/news/uk/scottish-prison-service-scottish-justice-scottish-government-scottish-parliament-b985916.html\nRelated \ud83c\udf10\nVirginia non-violent risk assessment\nVioG\u00e9n gender violence protection system\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tesla-model-x-veers-off-highway-into-concrete-barrier-killing-driver", "content": "Tesla Model X veers off highway into concrete barrier, kills driver\nOccurred: March 2018\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Tesla driven to work by Apple engineer Wei Lun 'Walter' Huang veered into a highway safety barrier in Mountain Valley, California, caught fire and was rear-ended by two other cars. \nHuang's car had drifted from a faded lane line and crashed into a highway barrier at 71 mph. The 38-year-old driver died from his injuries in Stanford Hospital. He had been reputedly playing games on his mobile phone in his car.\nA few days after the crash, Tesla acknowledged in a blog post that the car's Autopilot driver-assistance system had been engaged at the time of the crash and that Huang's hands were not detected on the wheel for six seconds prior to the collision. The company also claimed Autopilot 'unequivocally makes the world safer for the vehicle occupants, pedestrians and cyclists.' \nHowever, Tesla's move was seen to have violated an agreement between the US National Transportation Safety Board (NTSB) and the automaker that Tesla would not comment on any crash during the course of the investigation. Accordingly, the NTSB removed the car maker as a party to its investigation of the crash.\nThe incident raised questions about the accuracy and reliability of Autopilot, and allegedly inflated marketing claims made by Tesla and its CEO Elon Musk about the product. \nTesla later updated its software to remind drivers to touch the wheel more often and, ideally, remain attentive even with Autopilot enabled. \n\u2795 In April 2019, Huang's family filed a lawsuit against Tesla and California state alleging that Tesla\u2019s Autopilot driver assistance system misread lane lines and failed to detect the safety barrier, in which the car accelerated rather than braked. The suit also accused Tesla of 'defective' product design, and false advertising.\n\u2795 A 2020 NTSB investigation concluded that Autopilot was one of the 'probable' causes of the crash, and that Huang had been 'overly confident' in the system's capabilities, evident in the fact that he had been playing a mobile game while using Autopilot before the crash. It also accused the National Highway Traffic Safety Administration (NHTSA) of taking an overly hands-off approach to regulating automated driving systems.\n\u2795 In January 2023, Tesla senior engineer Ashok Elluswamy testified during the trial that a 2016 Tesla video used to promote Autopilot had been staged to show capabilities like stopping at a red light and accelerating at a green light that the system did not have. \n\u2795 In April 2024, Tesla settled the suit with Huang's family a day before the trial was set to begin. Details of the settlement were not disclosed. Tesla's actions were seen to potentially create a legal precedent for self-driving car incident liability.\n\nTesla faces multiple lawsuits involving Autopilot.\nSystem \ud83e\udd16\nTesla Autopilot, Full-self Driving\nTesla (2018). An update on last week's accident (1)\nTesla (2018). An update on last week's accident (2)\nOperator: Walter Huang\nDeveloper: Tesla\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Driver assistance system\nIssue: Accuracy/reliability; Safety\nTransparency: Black box; Marketing\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nSz Hua Huang et al v Tesla Inc., The State of California\nNTSB investigation [No. HWY16FH011]\nResearch, advocacy \ud83e\uddee\nTesla Deaths\nDebnam D.R. The Intersection Between Societal and Ethical Laws and the Use of Autonomous Vehicles (AVs)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/technology-51645566\nhttps://www.ft.com/content/0e086832-5c5c-11ea-8033-fa40a0d65a98\nhttps://apnews.com/article/us-news-ap-top-news-ca-state-wire-government-regulations-transportation-d03d88fca7ef389ffbe3469f50e36dcf\nhttps://www.mercurynews.com/2018/03/30/tesla-autopilot-was-on-during-deadly-mountain-view-crash/\nhttps://www.wired.com/story/tesla-autopilot-self-driving-crash-california/\nhttps://www.insurancejournal.com/news/national/2018/04/04/485230.htm\nhttps://www.9news.com.au/world/tesla-semi-automated-driving-system-possible-cause-suv-crash-silicon-valley/742140ff-9749-4894-88bd-d5eb5ff70ad1\nhttps://www.caranddriver.com/news/a30877577/driver-tesla-model-x-crash-complained-autopilot/\nhttps://www.forbes.com/sites/alanohnsman/2019/05/01/tesla-sued-by-family-of-silicon-valley-driver-killed-in-model-x-autopilot-crash\nRelated \ud83c\udf10\nTesla Autopilot, FSD misleading marketing\nTesla FSD beta test car hits bollard, driver fired\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tesla-model-s-collides-with-tractor-trailor-truck-kills-driver", "content": "Tesla Model S collides with tractor-trailor truck, kills driver\nOccurred: May 2016\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nJoshua Brown's Tesla Model S collided with a truck while it was engaged in the 'Autopilot' mode, shearing the roof off the car and killing Mr Brown. The incident ocurred in May 2016 on a highway outside Williston, Florida. The truck driver was not hurt.\nTesla responded to the incident stating that its Autopilot driver assistance system had been enaged but may not have functioned properly as it had failed to isolate the image of the tractor-trailer from the sky behind it. They added that the system was still in its introduction phase and had limits, and suggested drivers always stay alert with their hands on the wheel while using it.\nFindings (pdf) from a NHTSA preliminary investigation found that Autopilot had worked as intended. 'NHTSA\u2019s examination did not identify any defects in design or performance of the AEB (Automatice Emergency Braking) or Autopilot systems of the subject vehicles nor any incidents in which the systems did not perform as designed,' the report said.\nAn NTSB report had concluded that the accident should never had happened, Autopilot was partly to blame, and that Tesla 'lacked understanding' of the semi-autonomous Autopilot's limitations. It also found that Brown had his hands on the wheel for 25 seconds when he was supposed to do so for 37 minutes, Autopilot mode had remained on during most of his trip, and that it gave him to a visual warning seven separate times that said 'Hands Required Not Detected.' \nIn addition, the NTSB said initial media reports suggesting Brown may have been watching a video were wrong. Reports also suggested Brown's family may have come to a settlement with Tesla, though neither party have confirmed this to be the case.\nSystem \ud83e\udd16\nTesla Autopilot, Full-self Driving\nOperator: Joshua Brown\nDeveloper: Tesla\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Driver assistance system\nIssue: Safety; Accuracy/reliability\nTransparency: Black box; Marketing\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nNHTSA investigation [No. PE 16-007] (pdf)\nNTSB investigation [No. HWY16FH018]\nResearch, advocacy \ud83e\uddee\nTesla Deaths\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.com/news/technology-40340828\nhttps://www.dailymail.co.uk/news/article-3670549/First-man-die-self-driving-car-watching-Harry-Potter-crashed-Police-recover-DVD-player-wreckage-reveal-dead-man-racked-8-speeding-tickets-recent-years.html\nhttps://www.theregister.com/2017/06/20/tesla_death_crash_accident_report_ntsb/\nhttps://www.reuters.com/article/us-tesla-crash-idUSKBN19A2XC\nhttps://www.wired.com/story/tesla-ntsb-autopilot-crash-death/\nhttps://www.nytimes.com/2016/07/02/business/joshua-brown-technology-enthusiast-tested-the-limits-of-his-tesla.html\nhttps://drivemag.com/news/nhtsa-says-tesla-autopilot-not-at-fault-for-joshua-brown-s-crash/\nRelated \ud83c\udf10\nTesla Model 3 strikes truck, killing driver\nTesla Model 3 hits tow truck outside Moscow, explodes\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tesla-model-s-crashes-into-road-sweeper-kills-driver", "content": "Tesla Model S crashes into road-sweeper, kills driver\nOccurred: January 2016\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\n23 year-old Gao Yaning died after his Tesla Model S crashed into the back of a road-sweeping vehicle on a highway in Hebei province, China. \nThe incident is thought to have been the first fatality involving Autopilot driver assistance system, though Tesla argued the extensive damage made it incapable of transmitting log data and impossible to determine whether Autopilot had been engaged.\nChinese state broadcaster CCTV quoted road police as saying the car did not brake before crashing into the vehicle, a claim back by Tesla which said Yaning failed to take action, even though the road sweeper 'was visible for nearly 20 seconds.'\nGao's family later filed a lawsuit in Beijing against Tesla and the dealership that sold the car for exaggerating Autopilot\u2019s capabilities, and demanded an apology. \nHis family had initially sued Tesla for 1 yuan to raise public attention, but increased their compensation demands to 10,000 yuan (USD 1,499) and legal costs, and then to 5 million yuan (USD 750,000). \n\u2795 Shortly after the incident, Tesla removed the term Autopilot and a Chinese term for 'self-driving' ('zi dong jia shi') from its local website and marketing materials, changing it to 'zi dong fu zhu jia shi', meaning a driver-assist system, according to the Wall Street Journal.\nSystem \ud83e\udd16\nTesla Autopilot, Full-self Driving\nIncident video\nOperator: Gao Yaning\nDeveloper: Tesla\nCountry: China\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Driver assistance system\nIssue: Safety; Accuracy/reliability\nTransparency: Black box; Marketing\nResearch, advocacy \ud83e\uddee\nTesla Deaths\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reuters.com/article/us-tesla-crash-idUSKCN11K232\nhttps://eu.usatoday.com/story/tech/news/2016/09/14/tesla-crash-china-renews-spotlight-autopilot/90367426/\nhttps://www.wsj.com/articles/family-of-driver-killed-in-tesla-crash-in-china-seeks-court-investigation-1474351855\nhttps://www.ft.com/content/80c45ad6-7ef0-11e6-bc52-0c7211ef3198\nhttps://www.thesun.co.uk/news/1787336/shocking-dashcam-footage-shows-horror-tesla-crash-that-killed-driver-while-car-was-on-autopilot/\nhttps://www.dailymail.co.uk/news/article-3790176/Shocking-dashcam-footage-shows-Tesla-Autopilot-crash-killed-Chinese-driver-futuristic-electric-car-smashed-parked-lorry.html\nhttps://www.nytimes.com/2016/09/15/business/fatal-tesla-crash-in-china-involved-autopilot-government-tv-says.html\nhttps://jalopnik.com/two-years-on-a-father-is-still-fighting-tesla-over-aut-1823189786\nRelated \ud83c\udf10\nTesla Model Y crash kills two, injures three\nTesla China FSD Beta software glitch, recall\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/rana-ayyub-deepfake-porn-attack-doxxing", "content": "Rana Ayyub deepfake porn attack, doxxing\nOccurred: April 2018\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nIndian female Muslim investigative journalist, writer, and author Rana Ayyub was subjected to a vicious ad hominem attack that included a 2-minute graphic pornographic video with her face morphed onto it.\nThe video was circulated online by India's Hindhu nationalist Bharatiya Janata Party (BJP), and prompted a whirlwind of harassment, abuse, and death threats.\nTo make matters worse, her telephone number was revealed the following day in a tweet showing a screenshot of the video, resulting in thousands of unpleasant and intimidating calls. Ayyub ended up in hospital with heart palpitations and anxiety.\nWhen she recovered, Ayyub submitted a criminal complaint with Delhi police, who initially refused to file it and made no further contact with her for six months. \nThe case was closed in August 2020.\nSystem \ud83e\udd16\nUnknown\nOperator: Unclear/unknown\nDeveloper: Anonymous/pseudonymous\nCountry: India\nSector: Media/entertainment/sports/arts\nPurpose: Harrass/intimidate/shame\nTechnology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Safety; Privacy; Mis/disinformation\nTransparency: Governance; Marketing\nResearch, advocacy \ud83e\uddee\nUnited States Department of State (2020). India 2020 Human Rights Report (pdf)\nUnited Nations Human Rights Council (2018). UN experts call on India to protect journalist Rana Ayyub from online hate campaign\nReporters without Borders (2018). RSF urges Indian authorities to protect woman journalist\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.huffingtonpost.co.uk/entry/deepfake-porn_uk_5bf2c126e4b0f32bd58ba316\nhttps://www.nytimes.com/2018/05/22/opinion/india-journalists-slut-shaming-rape.html\nhttps://www.indiatoday.in/trending-news/story/journalist-rana-ayyub-deepfake-porn-1393423-2018-11-21\nhttps://timesofindia.indiatimes.com/blogs/voices/india-becoming-sextortion-capital-of-the-world/\nhttps://www.dailyo.in/variety/rana-ayyub-trolling-fake-tweet-social-media-harassment-hindutva-23733\nhttps://nypost.com/2019/01/02/blackmailers-for-hire-are-weaponizing-deepfake-revenge-porn/\nhttps://www.vogue.com/article/scary-reality-of-deepfakes-online-abuse\nhttps://www.huffingtonpost.co.uk/entry/deepfake-porn-heres-what-its-like-to-see-yourself_n_5d0d0faee4b0a3941861fced\nhttps://www.lawfareblog.com/alls-clear-deepfakes-think-again\nRelated \ud83c\udf10\nDeepfakes violate Anil Kapoor personality rights\nLauren Book deepfake extortion\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/these-nudes-do-not-exist", "content": "These Nudes Do Not Exist deepfake porn sales\nOccurred: March 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nStart-up These Nudes Do Not Exist sold AI-generated images of women who do not exist in real-life for USD 1 in an attempt to upend the deepfake nude industry, drawing criticism for inappropriate commercialisation and shoddy ethics.\nThese Nudes Do Not Exist takes public domain photographs and uses an algorithm to generate composite nude images of women who do not exist in real-life. \nBut whilst the company technically does not violate the privacy of real women, the company refuses to reveal the sources of their training data, raising questions about whether the consent of those whose photographs were initially used had given their consent. \nOne of the co-founders told Vice, 'I think this is probably the first chance that anyone in the world has ever had to buy AI generated pornographic content, so in a sense each customer gets to be a part of porn and AI history.' \nThe founders also 'requested to remain anonymous because he and his partner didn't want to be publicly associated with their own creation,' according to Vice.\nThe website was taken down shortly after its launch.\nSystem \ud83e\udd16\nThese Nudes Do Not Exist\nOperator: Unclear/unknown\nDeveloper: Anonymous/pseudonymous\nCountry: Russia\nSector: Media/entertainment/sports/arts\nPurpose: Generate nude images\nTechnology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Safety; Ethics\nTransparency: Governance; Complaints/appeals; Privacy; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.vice.com/en/article/qjdx7w/these-nudes-do-not-exist-and-i-dont-know-why-this-startup-does-either\nhttps://bigthink.com/technology-innovation/ai-fake-nudes\nhttps://www.dailydot.com/irl/startup-algorithm-generates-fake-nudes/\nhttps://www.vice.com/en/article/akdgnp/sexual-abuse-fueling-ai-porn-deepfake-czech-casting-girls-do-porn\nhttps://www.presse-citron.net/cette-startup-cree-et-vend-des-images-femmes-nues-qui-nexistent-pas/\nhttps://www.elespanol.com/omicrono/software/20200311/mujeres-desnudas-no-existen-lascivo-inteligencia-artificial/473953956_0.html\nhttps://www.reddit.com/r/technology/comments/fnr5el/these_nudes_do_not_exist_and_i_dont_know_why_this/\nRelated \ud83c\udf10\nTelegram bot creates non-consensual deepfake porn\nDeepsukebe non-consensual nudification\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/telegram-bot-creates-non-consensual-deepfake-porn", "content": "Telegram bot creates non-consensual deepfake porn\nOccurred: October 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nSecurity company Sensity discovered that an AI bot on encrypted messaging app Telegram is being used to create photo-realistic nude and sexually explicit images of hundreds of thousands of women without their consent. \nThe bot, which appears to be popular in Russia and eastern Europe, only requires one photograph to create the images, leading to over 680,000 ordinary women and girls being demeaned and degraded. Some of these appear to be underage girls.\nThe images are used and manipulated harassment without the knowledge or consent of the women being targeted, risk being used in all manner of ways, and may lead to harassment, abuse, violence, and loss of jobs.\nThe photographs clearly violate Telegram's terms of service, but the messaging app has refused to close the account.\nSystem \ud83e\udd16\nUnknown\nOperator: Unclear/unknown; Telegram\nDeveloper: Anonymous/pseudonymous\nCountry: Russia; Global\nSector: Media/entertainment/sports/arts\nPurpose: Simulate nude images\nTechnology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Privacy; Impersonation; Copyright; Safety\nTransparency: Governance; Complaints/appeals; Privacy; Marketing\nResearch, advocacy \ud83e\uddee\nSensity (2020). The State of Deepfakes\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/technology-54584127\nhttps://www.wired.co.uk/article/deepfake-porn-websites-videos-law\nhttps://www.rollingstone.com/culture/culture-features/tiktok-creators-deepfake-pornography-discord-pornhub-1078859/\nhttps://www.buzzfeednews.com/article/janelytvynenko/telegram-deepfake-nude-women-images-bot\nhttps://www.wired.co.uk/article/porn-bots-in-telegram-deepfake\nhttps://www.washingtonpost.com/technology/2020/10/20/deep-fake-nudes/\nhttps://www.msn.com/en-us/money/other/deepfake-bots-on-telegram-make-the-work-of-creating-fake-nudes-dangerously-easy/ar-BB1adbFs\nhttps://www.technologyreview.com/2020/10/20/1010789/ai-deepfake-bot-undresses-women-and-underage-girls/\nhttps://www.inputmag.com/culture/bots-on-telegram-are-creating-deepfake-nudes-of-thousands-of-women\nhttps://www.cnet.com/news/deepfake-bot-on-telegram-is-violating-women-by-forging-nudes-from-regular-pics/\nhttps://www.telegraph.co.uk/technology/2020/10/21/deep-fake-porn-bot-targets-thousands-women-telegram/\nRelated \ud83c\udf10\nDeepNude nudification\nDeepsukebe non-consensual nudification\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/paul-zilly-compas-sentencing-risk-assessment", "content": "Paul Zilly COMPAS sentencing risk assessment\nOccurred: 2013\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAccused thief Paul Zilly was landed with a jail sentence based directly on a risk assessment by a controversial violence recidivisalgorithm \nZilly was accused of stealing a push lawnmower and other tools in north Wisconsin. Zilly had long struggled with a meth habit and in 2012 he'd been working toward recovery with the help of a Christian pastor when he relapsed and committed the theft, according to an investigation by ProPublica.\nHaving scored as a high risk for violent recidivism by the Correctional Offender Management Profiling for Alternative Sanctions (COMPAS) tool, Zilly was sent to prison for two years and given three years of supervision, despite his lawyer agreeing to a plea deal with prosecutors in which the state would recommend one year in a county jail followed by supervision to ensure Zilly would 'stay on the right path.'\nIn his sentencing, Judge James Babler referenced the score generated by COMPAS, which calculated Zilly as high risk for future violent crime and medium risk for general recidivism. \nHowever, at an appeals hearing, Babler reduced Zilly's prison sentence to 18 months after he had heard testimony given by Northpointe CEO Tim Brennan, who said that he had not designed his software to be used in sentencing and that his focus was on 'reducing crime rather than punishment.' \nAccording to Judge Babler, he would have given a lower sentence. 'Had I not had the COMPAS, I believe it would likely be that I would have given one year, six months.' he said at the appeal.\nPaul Zilly\u2019s case brought attention to the use of the Correctional Offender Management Profiling for Alternative Sanctions (COMPAS) tool in sentencing decisions12. COMPAS is an algorithm designed to assess a defendant\u2019s risk of recidivism, i.e., the potential risk that the defendant will commit a crime in the future.\nIn Zilly\u2019s case, despite his lawyer agreeing to a plea deal with prosecutors in which the state would recommend one year in a county jail followed by supervision, the COMPAS tool scored him as a high risk for violent recidivism3. As a result, Zilly was sent to prison for two years and given three years of supervision.\n\n\u2795 The use of COMPAS in sentencing was upheld by the Supreme Court of Wisconsin in State v. Loomis, with the court controversially ruling that the use of the tool did not violate the defendant\u2019s due process rights to be sentenced individually and using accurate information.\nSystem \ud83e\udd16\nEquivant website\nCOMPAS Wikipedia profile\nOperator: Wisconsin Court System\nDeveloper: Volaris Group/Equivant/Northpointe\nCountry: USA\nSector: Govt - justice\nPurpose: Assess recidivism risk\nTechnology: Recidivism risk assessment system\nIssue: Accuracy/reliability\nTransparency: Governance; Black box\nResearch, advocacy \ud83e\uddee\nRowe E.A., Prior N. (2022). Procuring Algorithmic Transparency (pdf)\nCollins E. (2018). Punishing Risk (pdf)\nCarlson A.M (2017). The Need for Transparency in the Age of Predictive Sentencing Algorithms (pdf)\nInvestigations, assessments, audits \ud83e\uddd0\nProPublica (2016). Machine Bias\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.sciencefocus.com/future-technology/can-an-algorithm-deliver-justice/\nhttp://archive.jsonline.com/news/crime/risk-scores-attached-to-defendants-unreliable-racially-biased-b99732973z1-381306991.html\nhttps://www.theatlantic.com/ideas/archive/2019/06/should-we-be-afraid-of-ai-in-the-criminal-justice-system/592084/\nhttps://boingboing.net/2016/05/24/algorithmic-risk-assessment-h.html\nhttps://theamericanscholar.org/the-future-of-silicon-valley/\nhttps://digital.hbs.edu/platform-rctom/submission/man-or-machine-who-holds-the-stronger-suit-in-the-courtroom/\nhttps://ilr.law.uiowa.edu/print/volume-103-issue-1/the-need-for-transparency-in-the-age-of-predictive-sentencing-algorithms/\nhttps://www.cdotrends.com/story/17205/pleading-your-case-ai-judge\nhttps://www.criminallegalnews.org/news/2020/jan/21/compute-or-not-compute-algorithm-driven-ai-criminal-justice-system/\nhttps://www.theatlantic.com/ideas/archive/2019/06/should-we-be-afraid-of-ai-in-the-criminal-justice-system/592084/\nhttps://urbanmilwaukee.com/2017/01/05/murphys-law-justice-system-uses-biased-test/\nRelated \ud83c\udf10\nTitus Henderson COMPAS parole denial\nEric Loomis COMPAS prison sentencing\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/facemega-sexualised-face-swapping", "content": "FaceMega sexualised face swap ads violate platform policies\nOccurred: March 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAdverts for FaceMega, an app that creates AI-generated deepfake videos of Hollywood stars in sexually suggestive poses, have been removed from Facebook and Instagram for violating their adult content policies.\nNBC News reported that 230 highly charged video ads promoting FaceMega were run across Facebook and Instagram, of which 127 featured Emma Watson and 74 videos featured Scarlett Johansson.\nCreated by Chinese company Ufoto, FaceMega described itself as a tool for creating 'deepfake face swap videos', cost GBP 7.49 a week, and was rated as suitable for ages 'nine and up'. \nUsers were never asked to verify their age, and the choice of videos on which users could attach their faces included scantily clad women in bikinis and a section named 'Hot'. the app has since been removed from the Android and Apple app stores.\nThe discovery was first made by journalism student Lauren Barton, who posted one of the ads on Twitter, where it has received over 17 million views. \nSystem \ud83e\udd16\nFaceMega\nOperator: Wondershare/Ufoto\nDeveloper: Wondershare/Ufoto\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Swap faces\nTechnology: Deepfake - video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Safety; Copyright\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://twitter.com/laurenbarton03/status/1632769865122545664\nhttps://www.nbcnews.com/tech/social-media/emma-watson-deep-fake-scarlett-johansson-face-swap-app-rcna73624\nhttps://www.insider.com/deepfake-app-removed-stores-suggestive-ads-emma-watsons-face-2023-3\nhttps://gizmodo.com/deep-fake-ai-emma-watson-facemega-instagram-1850204168\nhttps://thred.com/tech/why-the-facemega-deepfake-app-is-a-slippery-slope/\nhttps://nypost.com/2023/03/09/facebook-removes-emma-watson-scarlett-johansson-deepfake-sex-ads/\nhttps://thred.com/tech/why-the-facemega-deepfake-app-is-a-slippery-slope/\nhttps://www.the-sun.com/tech/7608077/sick-sexualised-face-swap-apps-targeted-children/\nRelated \ud83c\udf10\nZAO face swapping\nFaceApp facial transformations\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/vermeer-girl-with-a-pearl-earring-ai-facsimile", "content": "Vermeer 'Girl with a Pearl Earring' AI facsimile causes controversy\nOccurred: February 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn AI-generated artwork acting as a substitute for Vermeer's famous painting Girl with a Pearl Earring at the Mauritshuis Museum in the The Hague, Netherlands, has caused controversy amongst the art fraternity and beyond. \nJulian van Dieken's interpretation of Vermeer's masterpiece was one of five of almost 3,500 entries chosen to be displayed that had been submitted to a competition organised by the museum to replace it when it is on loan at the Vermeer exhibition at the Rijksmuseum in Amsterdam.\nGenerated using Midjourney and Photoshop, Van Dieken's A Girl with Glowing Earrings was greeted with howls of protest, with some people accusing the museum of abdicating ethical decision-making. \nOthers highlighted what they consider to be the damaging effects AI systems such as Midjourney are having on artists and the creative professions in general, and whether their outputs infringe copyright law.\nA number lamented what they saw as the limited creativity of the artist.\nSystem \ud83e\udd16\nMidjourney image generator\n\nDocuments \ud83d\udcc3\nJulian van Dieken. The Girl with Glowing Earrings\nMauritshuis Museum (2023). My Girl with a Pearl competition\nOperator: Mauritshuis Museum\nDeveloper: Julian van Dieken\nCountry: Netherlands\nSector: Media/entertainment/sports/arts\nPurpose: Generate artwork\nTechnology: Text-to-image; Neural network; Deep learning; Machine learning\nIssue: Ethics; Employment; Copyright\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://techxplore.com/news/2023-03-girl-ai-earrings-dutch-art.html\nhttps://telecom.economictimes.indiatimes.com/news/girl-with-ai-earrings-sparks-dutch-art-controversy/98543707\nhttps://www.smithsonianmag.com/smart-news/girl-with-a-pearl-earring-vermeer-artificial-intelligence-mauritshuis-180981767/\nhttps://www.straitstimes.com/world/europe/girl-with-ai-earrings-standing-in-for-vermeer-masterpiece-sparks-dutch-art-controversy\nhttps://futurism.com/the-byte/artists-ai-vermeer-girl-pearl-earring\nhttps://news.artnet.com/art-world/mauritshuis-museum-girl-with-a-pearl-earring-ai-fascimile-2263100\nhttps://hypebeast.com/2023/3/girl-with-ai-earrings-mauritshuis-artwork-controversy\nhttps://hyperallergic.com/805030/mauritshuis-museum-under-fire-for-showing-ai-version-of-vermeer-masterpiece/\nhttps://www.volkskrant.nl/nieuws-achtergrond/mauritshuis-hangt-kunstwerk-gemaakt-door-algoritme-op-plek-vermeer-gewoon-mooi-of-onethisch~ba60c70b/\nRelated \ud83c\udf10\nQuora, Google AIs say eggs can be melted\nIllustrator Hollie Mengert converted into AI model\nPage info\nType: Issue\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/joe-rogan-libido-booster-alpha-grind-ad-deepfake", "content": "Joe Rogan libido booster Alpha Grind ad deepfake\nOccurred: February 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nControversial podcaster Joe Rogan had his identity stolen and deepfaked in a video ad to push Alpha Grind, a male enhancement product that markets itself as 'For Men with the Highest Expectations'.\nThe video clip, which shows Rogan discussing Alpha Grind with guest Professor Andrew D. Huberman on The Joe Rogan Experience podcast, sparked uproar on Twitter, with people noting that it is illegal to steal someone's identity to promote a product using AI.\nThe fake ad was circulating freely on TikTok until it was spotted by Jimmy Farley and removed from the platform. It was also refuted by Huberman.\nFuturism notes that Rogan is known to push all manner of iffy products, some of them quietly owned by companies he owns outright or has invested in. But that doesn't detract from the fact that, in this instance, he is the victim.\nSystem \ud83e\udd16\nUnknown\nJoe Rogan Alpha Grind deepfake video\nOperator:\nDeveloper: \nCountry: USA\nSector: Health\nPurpose: Sell product\nTechnology: Deepfake - video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Mis/disinformation\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.dailydot.com/debug/joe-rogan-deepfake-tiktok-ad/\nhttps://www.dexerto.com/entertainment/insane-deepfake-sends-joe-rogan-viral-for-promoting-a-product-hes-never-discussed-2059439/\nhttps://www.pcgamer.com/the-deepfake-scam-era-begins-with-an-ai-generated-joe-rogan-pushing-penis-pills-on-tiktok/\nhttps://mashable.com/article/joe-rogan-tiktok-deepfake-ad\nhttps://www.dailymail.co.uk/sciencetech/article-11754509/AI-Joe-Rogan-promotes-libido-booster-men-illegal-deepfake-video.html\nhttps://petapixel.com/2023/02/13/ai-joe-rogan-promotes-product-in-disconcerting-deepfake-video/\nhttps://futurism.com/the-byte/joe-rogan-deepfake-ad\nRelated \ud83c\udf10\nCruzcampo Lola Flores deepfake ad\nThe Book of Veles\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/anyvision-google-ayosh-palestinian-surveillance", "content": "AnyVision 'Google Ayosh' Palestinian surveillance\nOccurred: October 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn 'advanced tactical surveillance' system called Better Tomorrow was used by the Israelis across the West Bank and East Jerusalem to monitor the movement of Palestinians and deter attacks. \nNBC News reported that the system, developed by Israeli technology company AnyVision (latterly re-branded as Oosto) and otherwise known as 'Google Ayosh', used facial recognition to identify individuals, including women and children. \nThe news attracted attention as Microsoft has invested in the company. Former AnyVision employees told NBC that the company failed to comply with Microsoft's ethical principles.\nIn March 2020 Microsoft divested its USD 74 million stake in AnyVision after protests by its employees and an audit led by former US Attorney General Eric Holder which concluded that 'the technology is used at border crossing checkpoints between Israel and the West Bank', whilst noting that 'available evidence demonstrated that AnyVision\u2019s technology has not previously and does not currently power a mass surveillance program in the West Bank that has been alleged in media reports.'\nAnyVision CEO Eylon Etshtein had previously denied any knowledge of the system and threatened to sue NBC News, saying that AnyVision was the 'most ethical company known to man,' disputed that the West Bank was 'occupied' and alleged NBC must have been funded by a Palestinian activist group. \nSystem \ud83e\udd16\nBetter tomorrow \nOosto website (formerly AnyVision)\nOperator: Israel Defense Forces\nDeveloper: Oosto/AnyVision Interactive Technologies\nCountry: Israel\nSector: Govt - military; Govt - security\nPurpose: Population surveillance\nTechnology: Facial recognition\nIssue: Ethics/values; Surveillance; Privacy\nTransparency: Governance; Privacy\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.nbcnews.com/news/all/why-did-microsoft-fund-israeli-firm-surveils-west-bank-palestinians-n1072116\nhttps://www.haaretz.com/israel-news/.premium-israeli-face-recognition-startup-used-in-east-jerusalem-nbc-investigation-finds-1.8057282\nhttps://www.haaretz.com/israel-news/business/.premium-this-israeli-face-recognition-startup-is-secretly-tracking-palestinians-1.7500359\nhttps://www.forbes.com/sites/thomasbrewster/2019/08/01/microsoft-slammed-for-investing-in-israeli-facial-recognition-spying-on-palestinians/#398ab6b26cec\nhttps://www.zdnet.com/article/microsoft-heres-why-were-withdrawing-our-stake-in-facial-recognition-startup-anyvision/\nhttps://www.biometricupdate.com/201911/anyvision-exec-responds-to-facial-recognition-controversy\nhttps://www.timesofisrael.com/as-anyvision-probed-israeli-watchdog-urges-curbs-on-sales-of-surveillance-tech/\nhttps://www.npr.org/2019/08/22/752765606/face-recognition-lets-palestinians-cross-israeli-checkposts-fast-but-raises-conc\nhttps://www.geekwire.com/2020/palestinian-activists-microsoft-employees-demand-company-cut-ties-anyvision-citing-civil-rights-violations/\nhttps://apnews.com/article/afab3ff2971ed22d4b59aefdb41d2c03\nRelated \ud83c\udf10\nIsrael uses Habsora 24 hour automated 'target factory' against Palestinians\nIsrael reportedly uses AI to identify 37,000 Hamas targets\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/engineer-ai-misleading-marketing", "content": "Engineer.ai automated app development relies on humans\nOccurred: August 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nClaims by Indian start-up Engineer.ai that it had developed a highly automated, AI-enabled app development platform for small businesses was revealed to be far from the truth.\nThe Los Angeles and Delhi-based company trumped its 'human-assisted' AI, but the Wall Street Journal found when talking to Engineer.ai employees and former employees that it used humans rather than AI to develop its app, and grossly inflated its marketing rhetoric to attract customers and investors.\nThe allegations came after Engineer.ai founder and CEO Sachin Duggal claimed at a conference that 82 percent of the work on the event\u2019s app was done by AI in under an hour, and that human engineers worked for five weeks to finish the rest of it. \nIn February 2019, Engineer.ai chief business officer Robert Holdheim had filed a wrongful termination complaint in a Los Angeles court alleging the company had been exaggerating its AI abilities to attract the funding it required to develop its technology.\nDuggal responded to the WSJ report by saying the company had never claimed to offer 'automated software development'. \nThe company re-branded as Build.ai in November 2019.\nSystem \ud83e\udd16\nBuilder.ai website (formerly Engineer.ai)\nDocuments \ud83d\udcc3\nEngineer.ai CEO interview\nOperator: Engineer.ai\nDeveloper: Engineer.ai\nCountry: India\nSector: Business/professional services\nPurpose: Automate app development\nTechnology:  \nIssue:  \nTransparency: Marketing\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nHoldheim v Engineer.ai\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.wsj.com/articles/ai-startup-boom-raises-questions-of-exaggerated-tech-savvy-11565775004\nhttps://inc42.com/buzz/engineer-ai-breaks-its-silence-over-ai-exaggeration-allegations/\nhttps://www.theverge.com/2019/8/14/20805676/engineer-ai-artificial-intelligence-startup-app-development-outsourcing-humans\nhttps://www.techcircle.in/2019/08/19/engineer-ai-cautionary-tale-of-inflated-claims-and-lack-of-transparency/\nRelated \ud83c\udf10\nScaleFactor hypes accountancy AI 'automation'\nOlive AI 'over-promises' and 'under-delivers' on capabilities\nPage info\nType: Incident\nPublished: March 2023\nLast updated: December 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/deepnude-nudification-app", "content": "DeepNude nudification app provokes ethics, privacy controversy\nOccurred: June 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nDeepNude, an app that enabled users to strip any woman of her clothes and see her naked within a few seconds, faced a strong backlash from civil and privacy rights advocates. \nLaunched in June 2019, DeepNude used generative adversarial network (GAN) to remove clothing from the images of women, without their consent. It had an unpaid and paid version, the latter costing USD 50 per year.\nThe app immediately proved controversial, with civil and privacy rights advocates complaining that it was intrusive, unethical, objectified women, and could be used for revenge porn and other nefarious purposes. \nThe anonymous developer of the app told Vice that they were 'not a voyeur, I'm a technology enthusiast', and that they also wanted to create a male version. \nTwo weeks later, the open source version of the app was removed from GitHub after the developer reputedly grappled with their ethical conscience.\nHowever, other people quickly uploaded their own versions to GitHub and other platforms. \nSystem \ud83e\udd16\nDeepNude\nOperator: DeepNude\nDeveloper: Anonymous/pseudonymous\nCountry: Estonia; Global\nSector: Media/entertainment/sports/arts\nPurpose: Undress women\nTechnology: Deepfake - image\nIssue: Privacy; Ethics/values; Bias/discrimination - gender  \nTransparency: Governance; Privacy\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.washingtonpost.com/business/2019/06/28/the-world-is-not-yet-ready-deepnude-creator-kills-app-that-uses-ai-fake-naked-images-women/\nhttps://www.theregister.com/2019/06/27/deepfake_nudes_app_pulled/\nhttps://www.vice.com/en/article/kzm59x/deepnude-app-creates-fake-nudes-of-any-woman\nhttps://www.vice.com/en_us/article/qv7agw/deepnude-app-that-undresses-photos-of-women-takes-it-offline\nhttps://www.vice.com/en/article/8xzjpk/github-removed-open-source-versions-of-deepnude-app-deepfakes\nhttps://www.theverge.com/2019/6/27/18760896/deepfake-nude-ai-app-women-deepnude-non-consensual-pornography\nhttps://www.technologyreview.com/2019/06/28/134352/an-ai-app-that-undressed-women-shows-how-deepfakes-harm-the-most-vulnerable/\nhttps://medium.com/syncedreview/deepfake-nudie-app-goes-viral-then-shuts-down-577e8c168dfb\nhttps://www.theguardian.com/commentisfree/2019/jun/29/deepnude-app-week-in-patriarchy-women\nhttps://gadgets.ndtv.com/internet/news/deepnude-deepfake-app-to-undress-women-shuts-down-after-furore-2061123\nhttps://www.theregister.co.uk/2019/07/09/github_deepnude_code_discord/\nRelated \ud83c\udf10\nDeepsukebe nonconsensual nudification\nZAO face swapping\nPage info\nType: Issue\nPublished: March 2023\nLast updated: June 2024", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/faceapp-facial-transformations", "content": "FaceApp facial transformer\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nFaceApp is a mobile-based app that enables users to transform their facial photographs, enabling them to appear younger or older, add a smile, facial hair or glasses, change eye colour, hairstyle or gender, or become 'hot'.\nReleased in January 2017, the app quickly became popular, with over a million people downloading it in the two weeks after its launch. \nIn March 2023, it had over 500 million downloads, according to its website. \nSystem \ud83e\udd16\nFaceApp website\nFaceApp Wikipedia profile\nDocuments \ud83d\udcc3\nFaceApp privacy policy\nOperator: FaceApp Technology\nDeveloper: Yaroslav Goncharov\nCountry: Global\nSector: Media/entertainment/sports/arts\nPurpose: Transform faces\nTechnology: Deep learning; Neural network; Machine learning\nIssue: Bias/discrimination; Privacy; Security\nTransparency: Governance; Marketing; Privacy\nRisks and harms \ud83d\uded1\nQuestions have been raised about FaceApp's privacy and security practices and its perceived bias towards racial and ethnic minorities and LGBTQ and transgender people, the latter due to their ability to realistically simulate the appearance of a person as the opposite gender.\nConcerns have also been raised about the misuse of people's facial data for fraud and other purposes, including selling to illicit markets that use selfies to gain access to bank accounts.\nIncidents and issues \ud83d\udd25\nJuly 2019. FaceApp rapped for potential privacy, security abuse\nJuly 2017. FaceApp ethnicity filters prompts accusations of racism, stereotyping\nApril 2017. FaceApp \"hot\" filter skin whitening slammed as \"racist\"\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nSenator Schumer FBP, FTC letter\nRelated \ud83c\udf10\nZAO face swapping\nDeepFaceLive face swapping\nPage info\nType: System\nPublished: March 2023\nLast updated: August 2024", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/zao-face-swap-app", "content": "ZAO app face swapping\nOccurred: September 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn app enabling users to superimpose a single image of their face onto videos of celebrities such as Leonardo di Caprio and Marilyn Monroe prompted concerns about privacy, copyright, and security.\nZAO quickly surged in popularity and was the most downloaded free app in China\u2019s iOS App Store immediately after its launch in September 2019. \nHowever, one section of the app's user agreement stated that users uploading their images to ZAO gave it 'free, irrevocable, permanent, transferable, and relicenseable' rights over the intellectual property rights to their faces, and permitted ZAO to share their images with whomever they chose and use their images for marketing purposes.\nZAO lists Changsha Shenduronghe Network Technology, a wholly owned subsidiary of Momo that owns a live-streaming and dating service, as its developer. Momo apologised for the terms of the agreement and revised it to say that it would not 'excessively collect user information.'\nDespite Momo's assurances, the ease with which identities could be swapped led platforms such as China's Weixin/WeChat to ban users from uploading ZAO-made videos to them on the basis that they were seen as a potential security risk.\nSystem \ud83e\udd16\nZAO app website\nOperator: Momo; Apple\nDeveloper: Momo\nCountry: China\nSector: Media/entertainment/sports/arts\nPurpose: Swap faces\nTechnology: Deepfake - image\nIssue: Privacy; Copyright; Security\nTransparency: Governance; Privacy\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/technology-49570418\nhttps://www.theguardian.com/technology/2019/sep/02/chinese-face-swap-app-zao-triggers-privacy-fears-viral\nhttps://www.thepaper.cn/newsDetail_forward_4322786\nhttp://www.chinadaily.com.cn/global/2019-11/19/content_37523904.htm\nhttp://www.sixthtone.com/news/1004513/chinese-deepfake-app-censured-over-privacy-concerns\nhttps://edition.cnn.com/2019/09/03/tech/zao-app-deepfake-scli-intl/index.html\nhttps://www.theverge.com/2019/9/2/20844338/zao-deepfake-app-movie-tv-show-face-replace-privacy-policy-concerns\nhttps://abcnews.go.com/US/chinese-deepfake-app-zao-fire-privacy-concerns/story5\nhttps://www.forbes.com/sites/zakdoffman/2019/09/02/chinese-best-ever-deepfake-app-zao-sparks-huge-faceapp-like-privacy-storm/\nhttps://www.technologyreview.com/2019/09/04/133170/ai-app-in-china-makes-you-movie-star-risks-privacy/\nRelated \ud83c\udf10\nDeepFaceLive face swapping\nFaceApp facial transformations\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/nanning-real-estate-sales-office-facial-recognition", "content": "Nanning real estate sales facial recognition fraud\nOccurred: December 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nCustomers of a real estate company in Nanning, China, had their properties sold without their knowledge by a dealer who used their facial information to access their bank details. \nThe customers looking to sell their houses were instructed to verify their identities on the town's official 'Yonje Deng' app. \nThe data was then used by an intermediary who was pretending to work at Nanning Youju Real Estate to transfer and mortgage the houses and pocket the proceeds.\nThe individual was later arrested by the police, and the app updated and made more secure. \n\u2795 A year later, another Nanning resident was jailed for stealing USD 23,500 from his ex-girlfriend\u2019s bank account by unlocking her phone with her fingerprint after he had drugged her, and then pulling up her eyelids while she was sleeping to activate her phone\u2019s facial recognition feature.\nSystem \ud83e\udd16\nUnknown\nOperator: Nanning Youju Real Estate; Nanning Natural Resources Bureau\nDeveloper: Alipay\nCountry: China\nSector: Real estate sales/management\nPurpose: Verify identity\nTechnology: Facial recognition\nIssue: Security\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.tellerreport.com/life/2020-12-11-more-than-10-owners-in-guangxi-were-defrauded-to-buy-a-house-by-brushing-their-faces.SkWu10MZ3P.html\nhttps://mp.weixin.qq.com/s/fWbQ3SD9vB-QdB51T097hw\nhttps://www.chinanews.com/sh/2020/12-12/9360248.shtml\nhttps://www.globaltimes.cn/page/202112/1241314.shtml\nhttps://www.thetimes.co.uk/article/chinese-man-used-facial-recognition-to-steal-18-000-from-sleeping-ex-girlfriend-zhc9nh2pr\nhttps://nypost.com/2021/12/13/man-steals-23k-using-exs-phone-through-facial-recognition-report/\nRelated \ud83c\udf10\nNingbo real estate facial recognition misuse\nAgricultural Bank of China facial recognition age bias\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/allstate-car-insurance-suckers-list-overcharging", "content": "Allstate charges 'sucker' customers higher car insurance premiums\nOccurred: February 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nUS insurer Allstate increased premiums for customers already paying the highest rates for their insurance, while keeping premiums more or less the same for 'thriftier' customers.\nA joint investigation by The Mark Up and Consumer Reports found that Allstate devised a 'customer retention model' or 'advanced' price adjustment algorithm that identified existing big spenders and squeezed more money out of them than others. \nThe algorithm also determined customers which customers were owed discounts. Though some customers were owed thousands of dollars, Allstate capped the discounts at a half percent irrespective of the amount owed. Senior customers were overrepresented within this cohort.\nInsurers are not obliged to inform customers if they are denied discounts, and the National Association of Insurance Commissioners told The Markup that it had never heard of an insurer voluntarily informing its customers that they had been denied a discount. \nMaryland rejected Allstate\u2019s proposal on the grounds that it was discriminatory. But it was approved by Arizona, Arkansas, Wisconsin, and a number of other US states. \nSystem \ud83e\udd16\nAllstate customer retention model\nOperator: Allstate\nDeveloper: Allstate\nCountry: USA\nSector: Banking/financial services\nPurpose: Assess customer risk\nTechnology: Price adjustment algorithm\nIssue: Bias/discrimination - age, income\nTransparency: Governance; Black box; Marketing\nResearch, advocacy \ud83e\uddee\nCooney P., Phillips E., Rivera J. (2019). Auto Insurance and Economic Mobility in Michigan: A Cycle of Poverty\nInvestigations, assessments, audits \ud83e\uddd0\nhttps://themarkup.org/allstates-algorithm/2020/02/25/car-insurance-suckers-list\nhttps://themarkup.org/allstates-algorithm/2021/02/09/michigan-regulators-question-allstates-car-insurance-pricing\nhttps://themarkup.org/allstates-algorithm/2022/02/01/newly-public-documents-allege-allstate-overcharged-loyal-california-customers-1-billion\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://eu.usatoday.com/story/opinion/2020/07/29/big-tech-abuses-consumers-stop-online-discrimination-column/5525703002/\nhttps://www.autoaccident.com/allstate-thinks-you-may-be-a-sucker.html\nhttps://www.marketplace.org/2020/02/25/consumer-reports-markup-allstate-car-insurance/\nhttps://news.ycombinator.com/item?id=22412339\nhttps://news.slashdot.org/story/20/02/26/1627228/suckers-list-how-allstates-secret-auto-insurance-algorithm-squeezes-big-spenders\nRelated \ud83c\udf10\nLemonade 'non-verbal cue' insurance claim assessments\nUpstart consumer lending racial discrimination\nPage info\nType: Incident\nPublished: March 2023\nLast updated: February 2024", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/agricultural-bank-of-china-facial-recognition-age-bias", "content": "Agricultural Bank of China facial recognition age bias\nOccurred: November 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA video emerged of people waiting in line for an ATM in Guangshui, Hubei province in China, having to lift a 94 year-old woman in order to have her identification verified using the bank's facial recognition system in order to activate her social security card.\nThe video caused an outcry on social media, with people calling the bank 'inhuman' and urging it and other organisatons to consider the needs of elderly people when designing products and services.\nThe bank apologised and said it would it improve its customer service. \nSystem \ud83e\udd16\nAgricultural Bank of China website\nAgricultural Bank of China Wikipedia profile\nOperator: Agricultural Bank of China\nDeveloper:  \nCountry: China\nSector: Banking/financial services  \nPurpose: Verify identity\nTechnology: Facial recognition\nIssue: Bias/discrimination - age\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://m.news.cctv.com/2020/11/23/ARTI4quWfQGGMIdgx5jojaaj201123.shtml\nhttp://www.xinhuanet.com/politics/2020-11/25/c_1126784416.htm\nhttps://news.sina.com.cn/s/2020-11-23/doc-iiznctke2841568.shtml\nhttps://www.sohu.com/a/433706202_114988\nhttps://finance.sina.com.cn/tech/2020-11-23/doc-iiznctke2916706.shtml\nhttps://news.china.com/socialgd/10000169/20201123/38995626_all.html\nRelated \ud83c\udf10\nNingbo real estate facial recognition\nKohler, BMW, MaxMara China facial recognition\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-invents-40000-biochemical-warfare-agents", "content": "AI invents 40,000 biochemical warfare agents\nOccurred: March 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nResearchers tweaked an AI system usually used to predict the toxicity of pipeline drugs to invent 40,000 potentially lethal molecules in six hours.\nThe experiment was reputedly intended to demonstrate to participants at a conference in Switzerland how easily these kinds of systems can be misused and abused. \nHowever, it showed how easy it is to turn a 'good' or 'helpful' medical technology into one with potentially negative or terrifying consequences. Commentators also noted the naivety of the researchers - and many of their colleagues - for failing to consider the broader implications of their work.\nIn their paper, the researchers confessed that 'The thought had never previously struck us. We were vaguely aware of security concerns around work with pathogens or toxic chemicals, but that did not relate to us; we primarily operate in a virtual setting. Our work is rooted in building machine learning models for therapeutic and toxic targets to better assist in the design of new molecules for drug discovery.' \nSystem \ud83e\udd16\n\nOperator: Collaborations Pharmaceuticals\nDeveloper: Collaborations Pharmaceuticals\nCountry: USA; UK; Switzerland\nSector: Health\nPurpose: Predict molecule toxicity\nTechnology: Machine learning\nIssue: Dual/multi-use; Ethics/values; Safety; Security\nTransparency: \nResearch, advocacy \ud83e\uddee\nUrbina F., Lentzos F., Invernizzi C., Ekins S. (2020). Dual use of artificial-intelligence-powered drug discovery\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.scientificamerican.com/article/ai-drug-discovery-systems-might-be-repurposed-to-make-chemical-weapons-researchers-warn/#\nhttps://www.theverge.com/2022/3/17/22983197/ai-new-possible-chemical-weapons-generative-models-vx\nhttps://www.latimes.com/opinion/story/2022-03-30/ai-artificial-intelligence-chemical-weapons\nhttps://www.morningbrew.com/emerging-tech/stories/2022/03/18/drug-discovery-ai-can-be-inverted-to-create-chemical-weapons-scientists-find\nhttps://www.sciencealert.com/ai-experiment-generated-40-000-hypothetical-bioweapons-in-6-hours-scientists-warn\nhttps://www.theregister.com/2022/03/18/ai_weapons_learning/\nhttps://www.theblaze.com/news/an-ai-designed-to-find-new-drugs-created-40000-potential-chemical-weapons-in-less-than-6-hours\nhttps://www.dailymail.co.uk/sciencetech/article-10636357/AI-came-thousands-chemical-weapons-just-hours-task-scientists.html\nhttps://interestingengineering.com/artificial-intelligence-chemical-weapons\nhttps://www.chemistryworld.com/news/drug-discovery-ai-that-developed-new-nerve-agents-raises-difficult-questions/4015462.article\nRelated \ud83c\udf10\nChatGPT chatbot\nEpic Deterioration Index accuracy, bias\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/audio-deepfake-fraudulently-impersonates-ceo", "content": "Audio deepfake fraudulently impersonates CEO\nOccurred: July 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nUS-based cybersecurity company NISOS uncovered an attempted fraud in which an employee received a call from someone identifying himself as his company CEO asking him to call back for 'immediate assistance to finalize an urgent business deal.'\nFortunately, the employee 'immediately thought it suspicious' and called the legal department. \nIt transpired the voice had been faked and the number the would-be victim was meant to call was a VOIP service burner with no user information.\n\u2795 In 2021, criminals fooled a senior director at a UK energy company into transferring USD 240,000 to a bank account in Hungary having cloned the voice of colleague. \nSystem \ud83e\udd16\nUnknown\nOperator:  \nDeveloper:  \nCountry: USA\nSector: Technology\nPurpose: Defraud\nTechnology: Deepfake - audio; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Impersonation; Security\nTransparency: Governance; Marketing\nResearch, advocacy\ud83e\uddee\nNISOS (2020). The Rise of Synthetic Audio Deepfakes\n\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.vice.com/en_us/article/pkyqvb/deepfake-audio-impersonating-ceo-fraud-attempt\nhttps://www.theverge.com/2020/7/27/21339898/deepfake-audio-voice-clone-scam-attempt-nisos\nhttps://www.biometricupdate.com/202007/deepfakes-some-progress-in-video-detection-but-its-back-to-the-basics-for-faked-audio\nhttps://techmonitor.ai/cybersecurity/growing-threat-audio-deepfake-scams\nhttps://www.pressreader.com/malaysia/the-star-malaysia-star2/20200803/281616717705999\nhttps://www.thestar.com.my/tech/tech-news/2020/07/28/scammers-now-using-deepfake-audios-to-impersonate-ceos-in-fraud-attempts-says-security-company\nRelated \ud83c\udf10\nFTX CEO deepfake scam\nBinance CCO deepfake impersonation\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/barclays-employee-spyware-monitoring", "content": "Barclays employee 'spyware' trial halted after backlash\nOccurred: February 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe introduction of software that monitored Barclays' employee activity and performance met with a backlash and resulted in the bank halting its trial. \nA whistleblower revealed to CityAm that the Sapience Analytics software monitored how long staff stayed at and how effective they are at their desks, told them to 'avoid breaks' and recorded toilet trips as 'unaccounted activity.' \nThe whistleblower alleged that automated warnings had also been sent to employees judged to be away from their computers for too long, or if they had been spending too long on a particular task, and that the 'spyware' monitoring had resulted in loss of dignity, additional stress and sparked a backlash over what was seen as excessive and unwarranted surveillance, and an invasion of privacy.\nBarclays had responded that it had been using the software 'to tackle issues such as individual over-working as well as raise general productivity.' \n\u2795 March 2022. A UK Information Commissioner's Office (ICO) investigation concluded (pdf) that the action warranted 'no further action' though Barclays should complete a data protection impact assessment shoudl it resume its spyware programme.\nSystem \ud83e\udd16\nSapience Vue website\nOperator: Barclays Corporate and Investment Bank\nDeveloper: Sapience Analytics\nCountry: UK\nSector: Banking/financial services\nPurpose: Monitor employee activity; Improve employee productivity\nTechnology: Behavioural monitoring system; Machine learning\nIssue: Human/civil rights; Privacy; Safety; Surveillance\nTransparency: Governance\nFreedom of information requests \ud83d\udd26\nInformation Commissioner's Office (2022). IC-188114-V8L7 (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.cityam.com/exclusive-barclays-installs-big-brother-style-spyware-on-employees-computers/\nhttps://www.bbc.co.uk/news/business-51570401\nhttps://www.dailymail.co.uk/news/article-8025003/Barclays-staff-slam-bosses-using-Big-Brother-software-monitors-trips-toilet.html\nhttps://www.telegraph.co.uk/business/2020/02/20/barclays-drops-staff-spying-software-backlash/\nhttps://www.theguardian.com/business/2020/feb/20/barlays-using-dytopian-big-brother-tactics-to-spy-on-staff-says-tuc\nhttps://www.forbes.com/sites/jackkelly/2020/08/13/big-british-bank-barclays-accused-of-spying-on-employees-this-may-be-the-new-trend/\nhttps://www.theregister.com/2020/08/10/barclays_employee_monitoring/\nhttps://news.sky.com/story/barclays-scraps-spyware-on-staff-computers-after-backlash-11938774\nhttps://www.complianceweek.com/data-privacy/how-far-is-too-far-with-employee-monitoring-barclays-case-could-offer-litmus/29336.article\nRelated \ud83c\udf10\nTeleperformance TP Observer employee monitoring\nAmazon AWS Panorama workplace surveillance\nPage info\nType: Incident\nPublished: March 2023\nLast updated: June 2024", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/scalefactor-accountancy-automation", "content": "ScaleFactor AI accountancy software uses humans to process data\nOccurred: July 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA US start-up claiming to deliver an AI-based, real-time bookkeeping tool in fact relied on traditional bookkeepers and hired a Filipino contract accounting firm to process customer data.\nAustin, Texas-based ScaleFactor built AI-based accounting automation and book-keeping software for small businesses that integrated with Quickbooks and Xero in such a way that it would 'revolutionise accounting'.\nHowever, the firm often relied on traditional bookkeepers and had hired a traditional contract accounting company in the Philippines, with customers receiving monthly statements rather than the real-time data they were promised, according to Forbes.\nEstablished in 2014, ScaleFactor closed in August 2020, primarily blaming the COVID-19 pandemic. However, customers complained its products had failed to work properly and that it had consistently over-promised and under-delivered. \nSystem \ud83e\udd16\nScaleFactor Crunchbase profile\nOperator:  \nDeveloper: ScaleFactor\nCountry: USA\nSector: Business/professional services  \nPurpose: Automate book-keeping, financial forecasts\nTechnology: Automation\nIssue: Business model; Effectiveness/value\nTransparency: Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.forbes.com/sites/davidjeans/2020/07/20/scalefactor-raised-100-million-in-a-year-then-blamed-covid-19-for-its-demise-employees-say-it-had-much-bigger-problems/\nhttps://www.forbes.com/sites/davidjeans/2020/06/23/scalefactor-fintech-startup-shuts-down-bessemer-coatue-canaan\nhttps://www.forbes.com/sites/forbestechcouncil/2022/10/31/ai-will-impact-the-labor-market-but-workers-should-embrace-the-technology-not-fear-it/\nhttps://www.emergingtechbrew.com/stories/2020/07/22/scalefactor-reportedly-struggled-develop-software-promised-customers\nhttps://www.bizjournals.com/austin/news/2020/06/23/scalefactor-is-shutting-down-forbes-reports.html\nhttps://research.g2.com/insights/scalefactor-raises-100-million-then-abruptly-shuts-down\nhttps://tokenist.com/coronavirus-eliminates-scalefactor-from-the-fintech-scene/\nhttps://www.failory.com/cemetery/scalefactor\nRelated \ud83c\udf10\nEngineer.ai automated app development relies on humans\nLemonade 'non-verbal cue' insurance claim assessments\nPage info\nType: Issue\nPublished: March 2023\nLast updated: December 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/mohammed-khadeer-facial-recognition-wrongful-arrest-death", "content": "Mohammed Khadeer facial recognition wrongful arrest, death\nOccurred: February 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nLabourer Mohammed Khadeer, 35, was misidentified using facial recognition technology, illegally held and tortured by Medak police in Telangana state, India, for five days in connection with a chain-snatching case. \nKhadeer had been picked up by police from Yakutpura in Hyderabad late January, apparently because his facial features matched with that of the culprit in the chain-snatching case. He later died of his wounds.\nThe incident sparked outrage amongst local politicians, human and civil rights activists, and the general public. Four police officers were later suspended from the Medak police, pending an investigation.\nMedak police had initially told the Times of India that they had questioned Khadeer and let him go.\nSystem \ud83e\udd16\nMedak District Police\nCrime and Criminal Tracking Network & Systems (CCTNS)\nIncident databank  \ud83d\udd22\nOperator: Medak District Police; Crime and Criminal Tracking Network & Systems (CCTNS)\nDeveloper: \nCountry: India\nSector:  Govt - police\nPurpose: Identify criminals\nTechnology: CCTV; Facial recognition\nIssue: Accuracy/reliability\nTransparency: Governance; Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nKhadeer Khan Twitter video statement\nhttps://www.thenewsminute.com/article/telangana-custodial-death-victim-was-wrongly-picked-through-cctv-identification-173629\nhttps://www.moneycontrol.com/news/business/did-cctv-play-a-part-in-misidentifying-custodial-torture-victim-mohammed-khadeer-as-a-suspect-10127331.html\nhttps://theprint.in/india/row-over-custodial-torture-death-in-telangana-thrashed-forced-to-give-wrong-custody-dates/1383608/\nhttps://timesofindia.indiatimes.com/city/hyderabad/old-hyderabad-man-alleges-cop-torture-over-theft/articleshow/97812100.cms\nhttps://thewire.in/rights/hyderabad-custodial-torture-medak-town-death\nhttps://thewire.in/tech/khadeer-khan-telangana-police-tech\nhttps://www.siasat.com/telangana-the-role-of-facial-recognition-system-in-the-custodial-death-of-khadir-2529787/\nhttps://www.siasat.com/medak-cops-violated-laws-fact-finding-team-who-visited-khadeers-kin-2537997/\nhttps://restofworld.org/2023/cctv-crime-surveillance-india/\nhttps://www.thestatesman.com/india/telangana-probe-ordered-after-outrage-over-custodial-death-of-labourer-1503155736.html\nRelated \ud83c\udf10\nHyderabad police COVID-19 facial recognition legality\nLucknow 'women in distress' facial recognition\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tiktok-bold-glamour-filter", "content": "TikTok Bold Glamour filter causes anxiety\nOccurred: March 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA TikTok filter was accused of distorting reality, reinforcing unattainable beauty standards, and causing anxiety, leading beauty professionals and others to call for it to be banned.\nLaunched in March 2023, TikTok's Bold Glamour offered users a flawless complexion and irons out skin spots and was downloaded millions of times and embraced enthusiatically by teenage girls and others.\nBeauty professionals railed against it, with some calling it 'profoundly disturbing' and warning it could easily impact users' mental health. Beauty brand Dove encouraged users to post videos turning their backs on the filter using hashtags such as #TurnYourBack and #NoDigitalDistortion.\n\u2795 TikTok had previously been found to have automatically touched up the videos of some users' faces without notifiying them or asking for their consent.\nSystem \ud83e\udd16\nBold Glamour\nTikTok Bold Glamour videos\nOperator: Bytedance/TikTok\nDeveloper: Bytedance/TikTok\nCountry: USA; Global\nSector: Media/entertainment/sports/arts\nPurpose: Create flawless complexion\nTechnology: Machine learning\nIssue: Safety\nTransparency: Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.cosmopolitan.com/entertainment/celebs/a43203022/tiktok-bold-glamour-filter/\nhttps://www.cosmopolitan.com/uk/beauty-hair/beauty-trends/a43177683/tiktok-bold-glamour-filter/\nhttps://www.washingtonpost.com/technology/2023/03/08/tiktok-bold-glamour-filter-effect-generative-ai/\nhttps://nypost.com/2023/02/28/new-bold-glamour-ai-tiktok-filter-denounced-i-dont-look-like-me/\nhttps://www.theverge.com/2023/3/2/23621751/bold-glamour-tiktok-face-filter-beauty-ai-ar-body-dismorphia\nhttps://www.nbcnews.com/news/bold-glamour-tiktok-filter-explained-rcna72485\nhttps://www.tatlerasia.com/style/beauty/bold-glamour-filter-tiktok\nhttps://www.independent.co.uk/life-style/bold-glamour-filter-tiktok-b2291094.html\nhttps://www.vice.com/en/article/pkg747/tiktok-beauty-filter-bold-glamor-problem\nhttps://www.dazeddigital.com/beauty/article/58326/1/tiktok-s-bold-glamour-filter-harmless-fun-or-symbol-of-dystopian-times\nhttps://www.dailymail.co.uk/femail/article-11816471/TikTok-users-concerned-new-bold-glamour-filter.html\nRelated \ud83c\udf10\nTikTok mandatory beauty filtering\nQoves AI beauty scoring\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tesla-model-s-strikes-curb-kills-three-passengers", "content": "Tesla Model S strikes curb, kills three passengers\nOccurred: May 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Tesla Model S travelling on the Pacific Coast Highway in Newport Beach, California, struck a curb and collided with construction equipment, killing the three people in the car and injuring three construction workers.\nThe National Highway Traffic Safety Administration (NHTSA) said it would investigate the crash, including whether Tesla\u2019s AutoPilot driver-assist technology was active.\nThe injuries to the construction workers were considered non-life-threatening and they were taken to a local hospital.\n35 of the 42 open investigations being conducted by the NHTSA\u2019s in-depth crash investigation team related to driver-assist technology involve Tesla.  \nSystem \ud83e\udd16\nTesla Autopilot, Full-self Driving\nOperator:  \nDeveloper: Tesla\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Driver assistance system\nIssue: Accuracy/reliability; Safety\nTransparency: Black box\nResearch, advocacy \ud83e\uddee\nTesla Deaths\nInvestigations, assessments, audits \ud83e\uddd0\nPolice report\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.wsj.com/articles/tesla-autopilots-role-in-deadly-vehicle-crash-is-probed-by-safety-regulators-11652913160\nhttps://edition.cnn.com/2022/05/19/cars/nhtsa-tesla-crash-california/index.html\nhttps://techcrunch.com/2022/05/18/nhtsa-probes-tesla-autopilot-crash-that-killed-three-people/\nhttps://www.autonews.com/regulation-safety/nhtsa-opens-probe-tesla-crash-killed-three\nhttps://www.foxnews.com/auto/fatal-tesla-crash-california-investigation-nhtsa\nhttps://www.cnet.com/roadshow/news/nhtsa-investigation-fatal-tesla-crash-newport-beach/\nhttps://abcnews.go.com/Business/wireStory/federal-agency-sends-team-probe-tesla-crash-killed-84818082\nhttps://www.autoevolution.com/news/nhtsa-will-investigate-tesla-crash-that-killed-three-people-autopilot-may-be-involved-189150.html\nhttps://www.latimes.com/socal/daily-pilot/news/story/2022-05-12/3-people-killed-3-hospitalized-after-solo-vehicle-crash-on-pch-in-newport-beach\nhttps://www.repairerdrivennews.com/2022/05/23/teslas-autopilot-feature-under-scrutiny-by-nhtsa-in-fatal-california-crash-investigation/\nRelated \ud83c\udf10\nTesla Model Y crashes into parked police car\nTesla Model S crash causes eight-vehicle pile-up\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tesla-model-y-crash-kills-two-injures-three", "content": "Tesla Model Y Chaozhou crash kills two, injures three\nOccurred: November 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Tesla Model Y killed a teenager and motorcyclist and injured three others in an alleged brake malfunction in Guangdong province, China. \nA (graphic) video shows the car careering through Chaozhou, with the driver driving at high speed past several cars and motorcycles before plowing into a cyclist and smashing into a building. \nElektrek reports that a member of the driver's family told Jimu News that the driver had problems with the brake pedal when he was about to pull over in front of his family store. However, Tesla claimed vehicle logs show that the brake pedal was not applied and the accelerator pedal was pressed for a significant portion of the event.\nRumours quickly spread on social media that Tesla's Autopilot driver-assistance system may have been to blame, though some commentators reckon was probably unlikely. \nTesla said it is working with Chinese authorities to figure out what went wrong. \nSystem \ud83e\udd16\nTesla Autopilot, Full-self Driving\nIncident video\nOperator: \nDeveloper: Tesla\nCountry: China\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Driver assistance system\nIssue: Accuracy/reliability; Safety\nTransparency: Black box\nResearch, advocacy \ud83e\uddee\nTesla Deaths\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reuters.com/business/autos-transportation/tesla-says-it-will-assist-police-probe-into-fatal-crash-china-2022-11-13/\nhttps://futurism.com/the-byte/police-investigating-tesla-brake-accident-china\nhttps://www.thetimes.co.uk/article/tesla-car-crash-in-china-leaves-two-dead-after-model-y-loses-control-lftt50fs0\nhttps://metro.co.uk/2022/11/14/out-of-control-tesla-kills-two-people-in-horrific-crash-in-china-17756958/\nhttps://electrek.co/2022/11/13/tesla-china-responds-to-dramatic-crash-that-kills-two-video/\nhttps://www.dailymail.co.uk/news/article-11423865/Out-control-Tesla-speeds-Chinese-streets-killing-two-people-injuring-three-others.html\nhttps://www.bloomberg.com/news/articles/2022-11-14/tesla-to-assist-investigation-into-fatal-chinese-car-accident\nhttps://fortune.com/2022/11/14/tesla-denies-malfunction-to-blame-after-deadly-crash-caught-video-goes-viral-china/\nhttps://www.foxbusiness.com/lifestyle/fatal-high-speed-tesla-crash-china-police-probe\nhttps://www.drive.com.au/news/video-out-of-control-tesla-model-y-crash-in-china-leaves-two-dead-three-injured/\nhttps://jalopnik.com/tesla-model-y-crash-in-china-kills-two-tesla-denies-ma-1849779384\nRelated \ud83c\udf10\nTesla Model S crashes into fire engine\nTesla Model X crashes into five police officers\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tesla-autopilot-tricked-into-accelerating", "content": "Tesla Autopilot tricked into accelerating\nOccurred: February 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nResearchers at McFee Labs tricked several Tesla models into breaking the speed limit by sticking a 2-inch piece of electrical tape on a sign. \nWhen the researchers placed a bit of black electrical tape measuring 5cm on a road sign depicting a 35mph speed limit, the MobilEye camera in a Model X misread the sign as 85mph and began accelerating.\nThe attack only worked for older Tesla Model S and X vehicles using MobilEye's EyeQ3 camera system. The system is deployed in over 40 million vehicles across the world.\nSystem \ud83e\udd16\nTesla Autopilot, Full-self Driving\nMobileye website\nOperator: McFee Labs\nDeveloper: Tesla\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Driver assistance system; Computer vision\nIssue: Safety; Accuracy/reliability\nTransparency: Black box\nResearch, advocacy \ud83e\uddee\nMcAfee Labs (2020). Model Hacking ADAS to Pave Safer Roads for Autonomous Vehicles\nIncident video\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theregister.com/2020/02/20/tesla_ai_tricked_85_mph/\nhttps://www.dailymail.co.uk/sciencetech/article-8021567/Tesla-cars-tricked-accelerating-85-MPH-35-MPH-zone-using-just-strip-tape.html\nhttps://www.news.com.au/technology/motoring/motoring-news/tesla-cars-tricked-into-speeding-by-electrical-tape-on-a-sign/news-story/574ecfa424b4b945dcec3de63601b5f9\nhttps://electrek.co/2020/02/19/tesla-autopilot-tricked-accelerate-speed-limit-sign/\nhttps://www.newsweek.com/tesla-model-x-model-s-mcafee-research-tricks-acceleration-speed-limit-signs-1487956\nhttps://thetechtribune.com/tesla-autopilot-gets-tricked-into-accelerating-from-35-to-85-mph-with-modified-speed-limit-sign-this-week-in-tech/\nhttps://www.telegraph.co.uk/technology/2020/02/19/self-driving-tesla-tricked-speeding-using-terrifying-hack/\nRelated \ud83c\udf10\nTesla Model S tricked into veering into wrong lane\nTesla Autopilot tricked into driverless driving\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/teslas-tricked-into-reacting-to-false-lane-markers", "content": "Tesla tricked into reacting to false lane markers\nOccurred: April 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA teams of researchers at Israel's Ben-Gurion University created a basic projection system that is able to trick Tesla\u2019s Autopilot driver assistance system into seeing things that don't exist.\nThe team used off-the-shelf drones and a cheap projector to project a number of 'phantom' images onto the road, including false traffic lines and a false speed limit sign.\nWhilst the reaction of the Tesla was mostly fairly limited - when shown an image of Elon Musk, the Autopilot system slowed from 18mph to 14mph - the spoofs show how easily a Tesla could be manipulated by third-parties. \nBen-Gurion student Ben Nassi also successfully spoofed a Mobileye 630 PRO driver assist system using inexpensive drones and battery-powered projectors.\nSystem \ud83e\udd16\nTesla Autopilot, Full-self Driving\nOperator: Ben-Gurion University\nDeveloper: Tesla\nCountry: Canada\nSector: Automotive\nPurpose: Summon car\nTechnology: Driver assistance system\nIssue: Accuracy/reliability; Safety\nTransparency: Black box\nResearch, advocacy \ud83e\uddee\nhttps://www.nassiben.com/phantoms\nIncident video 1\nIncident video 2\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://arstechnica.com/cars/2020/01/how-a-300-projector-can-fool-teslas-autopilot/\nhttps://www.dailymail.co.uk/sciencetech/article-7944181/Israeli-scientists-trick-Teslas-Autopilot-feature-projecting-fake-signs-road.html\nhttps://www.forbes.com/sites/thomasbrewster/2019/04/01/hackers-use-little-stickers-to-trick-tesla-autopilot-into-the-wrong-lane/\nhttps://www.thedrive.com/news/27262/you-can-fool-teslas-autopilot-by-placing-small-stickers-on-the-ground-study-finds\nhttps://electrek.co/2019/04/01/tesla-autopilot-hacker-tricked/\nhttps://tech.slashdot.org/story/19/04/02/0347232/researchers-trick-tesla-autopilot-into-steering-into-oncoming-traffic\nhttps://thenextweb.com/cars/2020/02/05/teslas-autopilot-dangerously-fooled-by-drone-mounted-projectors/\nRelated \ud83c\udf10\nTesla Autopilot tricked into accelerating\nTesla Model S tricked into veering into wrong lane\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/driverless-tesla-model-3-negotiates-parking-lot", "content": "Driverless Tesla Model 3 negotiates parking lot\nOccurred: November 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Tesla Model 3 was filmed driving on wrong side of the road, with no one behind the wheel, in a Richmond, British Colombia, shopping centre parking lot. \nThe Tesla was understood to be 'summoned' via a mobile app by the owner up to 200 feet away. The autonomous operation of  vehicles is not permitted in British Colombia, according to ICBC. \nFirst introduced in 2019, Smart Summons has been on the receiving end of complaints by owners and pedestrians about near crashes, confused cars, and bumper damage. \nTesla CEO Elon Musk had described Smart Summon as 'probably our most viral feature ever.'\nSystem \ud83e\udd16\nTesla Autopilot, Full-self Driving\nIncident video\nOperator: \nDeveloper: Tesla\nCountry: Canada\nSector: Automotive\nPurpose: Summon car\nTechnology: Driver assistance system\nIssue: Accuracy/reliability; Safety\nTransparency: Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.richmond-news.com/local-news/update-richmond-tesla-incident-icbc-says-driverless-not-allowed-3110352\nhttps://www.dailymail.co.uk/sciencetech/article-7660019/Alarming-video-shows-driverless-Tesla-car-cruising-road-wrong-way-summoned.html\nhttps://jalopnik.com/theres-more-and-more-tesla-smart-summon-problem-videos-1838703032\nhttps://globalnews.ca/news/6131588/video-bc-self-driving-tesla-wrong-side-road/\nhttps://www.vancouverisawesome.com/vancouver-news/tesla-remote-control-smart-summon-app-canada-1946718\nhttps://futurism.com/the-byte/driverless-tesla-wrong-lane-smart-summon\nRelated \ud83c\udf10\nTesla Smart Summon private jet crash\nTesla Model 3 crashes into overturned truck on Taiwan highway\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tesla-model-3-hits-tow-truck-explodes", "content": "Tesla Model 3 hits tow truck outside Moscow, explodes\nOccurred: August 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Tesla Model 3 driven by businessman Alexey Tretyakov, 41, hit a parked tow truck that servicing another vehicle on a ring road outside Moscow, Russia, in August 2019. \nTretyakov and his two children escaped the car before it was exploded and caught fire, but were severely injured and rushed to hospital. Tretyakov suffered a broken leg and chest injury, while his children reportedly suffered bruises and concussion.\nTretyakov and the police have since confirmed the car was on Autopilot. The driver also said the car had been in 'drive assistance mode', in which enhanced safety features are enabled but the driver\u2019s hands remain on the wheel, and that he had failed to see the truck. \nSystem \ud83e\udd16\nTesla Autopilot, Full-self Driving\nOperator: Alexey Tretyakov\nDeveloper: Tesla\nCountry: Russia\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Driver assistance system; Self-driving system\nIssue: Accuracy/reliability; Safety\nTransparency: Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.themoscowtimes.com/2019/08/11/tesla-electric-car-explodes-after-hitting-tow-truck-in-moscow-a66803\nhttps://m.tvzvezda.ru/news/20198102338-Vnc7r.html/amp/\nhttps://electrek.co/2019/08/10/tesla-model-s-explodes-crash-truck-autopilot/\nhttps://m.tvzvezda.ru/news/vstrane_i_mire/content/20198102338-Vnc7r.html\nhttps://www.newsweek.com/video-tesla-explodes-tow-truck-1453745\nhttps://www.teslarati.com/tesla-model-3-fire-explosion-moscow-what-we-know-so-far/\nhttps://www.dailymail.co.uk/news/article-7346215/Tesla-car-erupts-flames-autopilot-failure-saw-driver-plough-truck-Moscow.html\nhttps://techcrunch.com/2019/08/11/tesla-explodes-after-crash-on-russian-highway/\nhttps://www.news.com.au/technology/motoring/tesla-model-3-explodes-after-assisted-driving-system-failure-reportedly-leads-to-crash/news-story/e871715990b7c759b886059d223eb91f\nRelated \ud83c\udf10\nTesla Model Y crashes into parked police car\nTesla Model 3 hits parked police car\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tesla-model-s-tricked-into-veering-into-wrong-lane", "content": "Tesla Model S tricked into veering into wrong lane \nOccurred: April 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nResearchers from Tencent's Keen Security Lab adversarily tricked a Tesla Model S into driving into a lane of oncoming traffic. \nThe exploit worked by using small, inconspicuous stickers that tricked the car's Enhanced Autopilot into detecting and then following a change in the current lane, raising questions about the safety and efficacy of Tesla's driver assistance system.\nAs Technology Review's Karen Hao noted, 'Tesla\u2019s Autopilot is vulnerable because it recognizes lanes using computer vision. In other words, the system relies on camera data, analyzed by a neural network, to tell the vehicle how to keep centered within its lane.'\nTesla welcomed the attack, adding that the adversarial attack was unrealistic 'given that a driver can easily override Autopilot at any time by using the steering wheel or brakes and should always be prepared to do so, and can manually operate the windshield wiper settings at all times.'\nSystem \ud83e\udd16\nTesla Autopilot, Full-self Driving\nOperator: Keen Security Lab\nDeveloper: Tesla\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Driver assistance system\nIssue: Accuracy/reliability; Safety\nTransparency: Black box\nResearch, advocacy \ud83e\uddee\nTencent Keen Security Lab (2019). Experimental Security Research of Tesla Autopilot (pdf) \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://arstechnica.com/information-technology/2019/04/researchers-trick-tesla-autopilot-into-steering-into-oncoming-traffic/\nhttps://www.technologyreview.com/2019/04/01/65915/hackers-trick-teslas-autopilot-into-veering-towards-oncoming-traffic/\nhttps://www.cnbc.com/2019/04/03/chinese-hackers-tricked-teslas-autopilot-into-switching-lanes.html\nhttps://www.news.com.au/technology/motoring/motoring-news/teslas-autopilot-fooled-by-a-simple-trick/news-story/636d2cba3a94b4c1c8d82b7b4c316078\nhttps://bgr.com/2020/02/19/tesla-autopilot-hack-speed-limit-increase-50-mph/\nhttps://www.bleepingcomputer.com/news/security/researchers-trick-tesla-to-drive-into-oncoming-traffic/\nhttps://www.forbes.com/sites/thomasbrewster/2019/04/01/hackers-use-little-stickers-to-trick-tesla-autopilot-into-the-wrong-lane/\nRelated \ud83c\udf10\nTesla Autopilot tricked into driverless driving\nTesla Model S crashes into tree, kills two passengers\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/mobileye-630-pro-tricked-by-drones-projectors", "content": "Mobileye 630 PRO tricked by drones, projectors\nOccurred: June 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA team of researchers at Ben Gurion University, Israel, defeated a Renault Captur's 'Level 0' Mobileye 630 PRO Advanced Driver Assist System (ADAS) by following it with drones that projected images of fake road signs.\nAs Boing Boing points out, the attack could be used to trick cars into making manoeuvers that compromised the safety or integrity of their passengers and other users of the road \u2014 from unexpected swerves to sudden speed-changes to detours into unsafe territory.\nSystem \ud83e\udd16\nTesla Autopilot and Full Self-Driving Capability\nOperator: Renault\nDeveloper: Mobileye\nCountry: Israel\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Computer vision\nIssue: Security, Safety; Accuracy/reliability\nTransparency: Black box\nResearch, advocacy \ud83e\uddee\nNassi D., Ben-Netanel R., Elovici Y., Nassi B. (2019). MobilBye: Attacking ADAS with Camera Spoofing (pdf)\nIncident video\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bldgblog.com/2019/07/ghosts-only-cars-can-perceive/\nhttps://aabgu.org/drone-with-a-projector-successfully-trolls-cars-ai/\nhttps://www.newyorkmetropolitan.com/tech/signs-from-above-drone-with-projector-successfully-trolls-car-ai\nhttps://boingboing.net/2019/07/06/flickering-car-ghosts.html\nhttps://arstechnica.com/cars/2020/01/how-a-300-projector-can-fool-teslas-autopilot/\nhttps://arstechnica.com/cars/2019/06/spoofing-car-ai-with-projected-street-signs/\nRelated \ud83c\udf10\nTesla Autopilot tricked into accelerating\nTeslas tricked into reacting to false lane markers\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/apple-card-accused-of-gender-bias", "content": "Apple Card accused of gender bias\nOccurred: November 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGoldman Sachs and Apple were slammed for gender bias after man entrepreneur found he had been given a credit limit 10 times that of his wife, despite her having a better credit score.\nApple launched its new Apple Card in the USA in August 2019. Underwritten by Goldman Sachs, the card is designed to work with Apple Pay on iPhones, iPads, Apple Watches, and Macs. \nIn November 2019, tech entrepreneur David Hansson complained on Twitter that he had been given a credit limit 20 times larger than that offered to his wife, despite her having a better credit score. He went on to accuse Goldman Sachs of gender discrimination by using algorithms to determine a person's credit limit. \nHansson's complaint was followed by Apple co-founder Steve Wozniak saying that he received ten times the credit limit that his wife was offered, resulting in a volley of accusations that Apple and Goldman Sachs were 'sexist'. \nIn response, the New York State Department of Financial Services launched an investigation. \nIn March 2021, the investigation concluded (pdf) that there was no evidence of 'deliberate or disparate' racial discrimination but that there were clear deficiencies in customer service and transparency.\nSystem \ud83e\udd16\nApple Card website\nApple Card Wikipedia profile\nOperator: Apple; Goldman Sachs\nDeveloper: Apple; Goldman Sachs\nCountry: USA\nSector: Banking/financial services\nPurpose: Streamline card application process\nTechnology: Machine learning\nIssue: Bias/discrimination - gender\nTransparency: Governance; Complaints/appeals; Marketing\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nNew York State Department of Financial Services (2021). Report on Apple Card Investigation (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://twitter.com/dhh/status/1193287648087072770\nhttps://www.reuters.com/article/us-goldman-sachs-probe/goldman-faces-probe-after-entrepreneur-slams-apple-card-algorithm-in-tweets-idUSKBN1XK00L\nhttps://www.technologyreview.com/s/614721/theres-an-easy-way-to-make-lending-fairer-for-women-trouble-is-its-illegal/\nhttps://www.washingtonpost.com/business/2019/11/11/apple-card-algorithm-sparks-gender-bias-allegations-against-goldman-sachs/\nhttps://bgr.com/2019/11/10/apple-card-gender-discrimination-goldman-sachs-steve-wozniak/\nhttps://www.bloomberg.com/news/articles/2019-11-21/goldman-s-ceo-defends-apple-card-says-there-s-no-gender-bias\nhttps://edition.cnn.com/2019/11/12/business/apple-card-gender-bias/index.html\nhttps://www.cnbc.com/2019/11/11/goldman-sachs-to-reevaluate-apple-card-credit-limits-after-bias-claim.html\nhttps://www.nytimes.com/2019/11/10/business/Apple-credit-card-investigation.html\nhttps://eu.usatoday.com/story/money/2019/11/10/apple-card-goldman-sachs-credit-limit-sex-bias-investigation/2555234001/\nhttps://techxplore.com/news/2019-11-goldman-sachs-ceo-gender-bias.html\nhttps://slate.com/business/2019/11/apple-card-credit-algorithm-bias-discrimination-women.html\nRelated \ud83c\udf10\nFacebook job ad delivery gender discrimination\nImageNet dataset racial, gender stereotyping\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/3d-masks-fool-payment-airport-facial-recognition-systems", "content": "3D masks fool payment, airport facial recognition systems \nOccurred: January 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nResearchers in China found that facial recognition technology can be fooled by using a 3D-printed mask depicting a different person's face, raising questions about the security of the technology.\nAI development company Kneron researchers used 2D and 3D copies of a subject\u2019s face, as well as a 3D face mask, to test the strength of facial recognition security solutions at public transport portals, point of sales terminals, airport security checkpoints, and on mobile devices, in China and the Netherlands.\nThey discovered that most systems were able to resist the 2D and 3D copies, but the 3D mask was able to trick payment a system at a border checkpoint in China and a passport-control gate at Schiphol airport in Amsterdam.\n\u2795 Forbes reporter Thomas Brewster commissioned a 3D printed model of his own head to test the facial unlocking systems on a range of Apple and Android phones, finding that only iPhone X models were able to resist the attack.\nSystem \ud83e\udd16\nHuawei\nLG\nOnePlus\nSamsung\nOperator: Huawei; LG; OnePlus; Samsung  \nDeveloper: Huawei; LG; OnePlus; Samsung\nCountry: China; Netherlands\nSector: Banking/financial services\nPurpose: Test facial recognition\nTechnology: Facial recognition\nIssue: Security; Accuracy/reliability\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nKneron (2020). Kneron's Facial Recognition Survey Pushes the Industry Forward\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://fortune.com/2019/12/12/airport-bank-facial-recognition-systems-fooled/\nhttps://www.businessinsider.com/facial-recognition-fooled-with-mask-kneron-tests-2019-12?r=US&IR=T\nhttps://www.engadget.com/2019-12-16-facial-recognition-fooled-masks.html\nhttps://www.ibtimes.sg/3d-masks-photos-can-fool-facial-recognition-system-airports-not-face-id-36119\nhttps://www.theverge.com/2019/12/13/21020575/china-facial-recognition-terminals-fooled-3d-mask-kneron-research-fallibility\nhttps://in.mashable.com/science/9416/researchers-fooled-facial-recognition-systems-at-airports-with-3d-masks\nhttps://technology.inquirer.net/93313/facial-recognition-tech-fooled-by-ai-company-using-masks-and-photos\nhttps://www.dailymail.co.uk/sciencetech/article-7798005/Systems-use-facial-recognition-fooled-using-3D-printed-mask.html\nRelated \ud83c\udf10\nMobileye 630 PRO tricked by drones, projectors \nTesla Autopilot tricked into accelerating\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/sidewalk-labs-toronto-quayside-development", "content": "Sidewalk Labs Toronto Quayside 'Smart City'\nOccurred: May 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nProposed in October 2017 by Google affiliate Sidewalk Labs in response to a competition, Sidewalk Toronto was an urban development project the Quayside area of Toronto that was to be built 'from the internet up' and aimed to become 'a testbed for emerging technologies, materials and processes'. \nSidewalk Labs intended (pdf) to use artificial intelligence, sensors and other data-collection devices to monitor, analyse and optimise pedestrian traffic, noise, weather conditions, and energy and garbage use, amongst other things. \nThe proposal proved highly controversial, particularly with regard to its business model, perceived techno-solutionism, and privacy implications, and it was abandoned in May 2020, supposedly due to COVID-19.\nUrban Data Trust\nWith data envisaged as central to the workings of the new district, the question of how it would be managed was important. \nSidewalk Labs proposed an independent 'Urban Data Trust' to govern the collection and management, but was seen to have failed to provide meaningful detail, leading Members of Waterfront Toronto\u2019s Digital Strategy Advisory Panel to complain the plan was 'frustratingly abstract' and it's ambitions 'misguided'.\nIt also prompted a outcry amongst civil right and privacy advocates, and the general public, about privacy, concerns supported by research indicating that 60% of Toronto citizens did not trust Sidewalk Labs to collect data on residents, and that 58% did not think the company would keep its commitment not to use resident data for advertising purposes.\nIn response, Sidewalk Labs recommended shifting data governance from the Urban Data Trust to vesting authority in the existing Waterfront Toronto government partnership.\nSystem \ud83e\udd16\nSidewalk Toronto website\nSidewalk Labs Wikipedia profile\nSidewalk Toronto Wikipedia profile\nDocuments \ud83d\udcc3\nSidewalk Toronto Master Plan (MDIP) Pts 1, 2, 3, 4\nSidewalk Toronto Digital Innovation Appendix (pdf)\nSidewalk Toronto. Project update: Submitting the Digital Innovation Appendix\nOperator: Waterfront Toronto\nDeveloper: Alphabet/Google; Sidewalk Labs\nCountry: Canada\nSector: Construction\nPurpose: Create smart city\nTechnology: Bicycle detection system; Garbage management system; Traffic management system; Vehicle detection system\nIssue: Privacy; Surveillance; Ethics\nTransparency: Governance; Marketing; Privacy\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nCCLA Notice of Motion (pdf)\nResearch, advocacy \ud83e\uddee\nBlockSideWalk Facebook page\nCanadian Civil Liberties Association (CCLA). Toronto's Smart City. Our Fight for Privacy\nAl-Fahim M., Frankenberg S. Qualms with Quayside: Public Engagement in the Toronto Smart City Project (pdf)\nAustin A., Lie D. (2021). Data Trusts and the Governance of Smart Environments: Lessons from the Failure of Sidewalk Labs\u2019 Urban Data Trust\nCitizenFirst (2020). Sidewalk Labs Toronto: \u2018A Thriving Hub for Innovation\u2019 vs. \u2018A City of Surveillance\u2019 (pdf)\nLim W. (2020). Quayside Smart Community Case Study of Sidewalk Lab\u2019s \u2018City as a Platform\u2019 (pdf)\nFlynn A., Valverde M. (2019). Where The Sidewalk Ends: The Governance Of Waterfront Toronto's Sidewalk Labs Deal (pdf)\nForum Research (2019). Support for Sidewalk Toronto Mixed\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.cbc.ca/news/canada/toronto/sidewalk-labs-panel-1.5278903\nhttps://www.cbc.ca/news/canada/toronto/toronto-sidewalk-labs-plan-1.5187877\nhttps://www.vice.com/en/article/xwkv9z/google-planning-smart-city-toronto-despite-privacy-concerns\nhttps://www.bbc.co.uk/news/technology-49674533\nhttps://www.thestar.com/news/gta/2020/02/26/waterfront-toronto-advisory-panel-still-has-concerns-about-sidewalk-labs-data-collection-new-report-says.html\nhttps://www.thestar.com/opinion/contributors/2018/01/12/sidewalk-labs-toronto-waterfront-tech-hub-must-respect-privacy-democracy.html\nhttps://www.washingtonpost.com/news/theworldpost/wp/2018/08/08/sidewalk-labs/\nhttps://www.cigionline.org/articles/searching-smart-citys-democratic-future\nhttps://torontolife.com/city/toronto-is-surveillance-capitalisms-new-frontier/\nhttps://www.theguardian.com/cities/2019/jun/06/toronto-smart-city-google-project-privacy-concerns\nhttps://gizmodo.com/privacy-expert-resigns-from-alphabet-backed-smart-city-1829934748\nhttps://www.vice.com/en/article/xwkv9z/google-planning-smart-city-toronto-despite-privacy-concerns\nhttps://www.technologyreview.com/2022/06/29/1054005/toronto-kill-the-smart-city/\nRelated \ud83c\udf10\nGoogle Sidewalk Labs Portland 'Smart City'\nBelgrade 'Safe City' video surveillance\nPage info\nType: System\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tesla-model-s-runs-red-light-kills-two", "content": "Tesla Model S runs red light on Autopilot, kills two\nOccurred: December 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nTesla owner Kevin George Aziz Riad was charged with a felony in the US for a December 2019 crash that left people dead whilst his Tesla car was on Autopilot.\nAccording to police, Riad's Tesla Model S was moving at a high speed when it left a freeway and ran a red light before striking a Honda Civic, killing Gilberto Alcazar Lopez and Maria Guadalupe Nieves-Lopez. \nThe families of Lopez and Nieves-Lopez have sued Tesla and Riad in separate lawsuits alleging negligence by Riad and accusing Tesla of selling defective vehicles that accelerate suddenly and that lack an effective automatic emergency braking system.\nRiad's preliminary hearing is scheduled for February 2023.\nSystem \ud83e\udd16\nTesla Autopilot, Full-self Driving\nOperator: Kevin George Aziz Riad\nDeveloper: Tesla\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Driver assistance system; Self-driving system\nIssue: Accuracy/reliability; Safety\nTransparency: Black box\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nMaria Luz Nieves v Kevin George Aziz Riad, et al\nResearch, advocacy \ud83e\uddee\nTesla Deaths\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reuters.com/legal/tesla-crash-trial-california-hinges-question-man-vs-machine-2022-11-01/\nhttps://nypost.com/2022/01/18/tesla-driver-first-to-be-charged-in-fatal-crash-involving-autopilot/\nhttps://futurism.com/manslaughter-case-tesla-autopilot\nhttps://www.businessinsider.com/driver-who-had-tesla-on-autopilot-in-crash-manslaughter-trial-2022-5\nhttps://www.dailymail.co.uk/news/article-10839133/California-Tesla-driver-27-stand-trial-2019-autopilot-crash-judge-rules.html\nhttps://www.nbcnews.com/news/us-news/tesla-driver-charged-vehicular-manslaughter-fatal-autopilot-crash-rcna12724\nhttps://www.nyu.edu/about/news-publications/news/2022/march/when-a-tesla-on-autopilot-kills-someone--who-is-responsible--.html\nhttps://www.cbsnews.com/sanfrancisco/news/california-prosecutors-file-felony-charges-against-tesla-autopilot-driver/\nhttps://abc7.com/tesla-gardena-crash-driver/11873142/\nhttps://www.theverge.com/2022/1/18/22889768/tesla-autopilot-criminal-charges-la-fatal-crash\nhttps://jalopnik.com/tesla-driver-on-trial-for-autopilot-crash-that-killed-t-1848955333\nhttps://www.theguardian.com/technology/2022/nov/14/tesla-autopilot-landmark-case-man-v-machine\nRelated \ud83c\udf10\nTesla Model Y crashes into parked police car\nTesla Autopilot, FSD misleading marketing\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/generated-photos-infinite-diversity-face-collection", "content": "Generated Photos 'infinite diversity' face collection prompts controversy\nOccurred: September 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA collection of 100,000 free images of AI-generated faces intended to help designers who need portraits for their websites, apps or presentations prompted controversy over the ease for which it could be used maliciously.\nLaunched in September 2019, Generated Photos features headshots with consistent lighting and sizing with a wide range of angles, positions, and facial expressions, reflecting a wide variety of ethnicities, ages and face shapes to represent an 'infinite diversity.'\nA large number of users praised the collection for the 'realism' and usefulness of the photos. However, some described the photos as 'weird', 'stiff' and, in some instances, 'truly fucked up'. Others pointed out the ease with which they could be used for malicious purposes, and that they could put licensed stock photo companies and models out of business.\nIn a blog post, Generated Photos developer Icon8 said the company used real models to create the 'incredibly realistic' images rather than scraping from the web or using stock photographs, thereby getting round copyright issues.\nSystem \ud83e\udd16\nGenerated Photos website\nGenerated Photos promotional video\nIcons8 website\nOperator: Icons8\nDeveloper: Icons8; Prototypr\nCountry: USA\nSector: Business/professional services\nPurpose: Produce 'infinite diversity'\nTechnology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Accuracy/reliability; Dual/multi-use; Employment\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.vice.com/en/article/mbm3kb/generated-photos-thinks-it-can-solve-diversity-with-100000-fake-ai-faces\nhttps://www.theverge.com/2019/9/20/20875362/100000-fake-ai-photos-stock-photography-royalty-free\nhttps://www.fastcompany.com/90406423/these-ai-generated-people-are-coming-to-kill-stock-photography\nhttps://www.ibtimes.sg/new-100000-ai-generated-faces-look-like-real-humans-unfolding-next-generation-media-32508\nhttps://medium.com/generated-photos/frequently-asked-questions-cc919004de0d\nhttps://vc.ru/ml/182511-ii-generator-lic-sozdanie-fotografiy-nesushchestvuyushchih-lyudey\nhttps://petapixel.com/2019/09/20/this-company-is-giving-away-100000-ai-generated-headshots-for-free/\nRelated \ud83c\udf10\nLinkedIn deepfake salespeople\nHour One 'character' clones\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/steve-talley-facial-recognition-wrongful-arrest", "content": "Steve Talley facial recognition wrongful arrest, jailing\nOccurred: September 2014\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nSteve Talley was arrested outside his house in Denver, Colorado, for being a suspect in two armed bank robberies, and for assaulting a police officer during the second robbery. \nIdentified using facial recognition technology operated by the Federal Bureau of Investigation (FBI), friends and his former wife verified that it was Talley in the CCTV footage shared with the police. However, he was able to prove that he was elsewhere at work for the first robbery, and was released after two months in jail.\nFollowing his release, Talley filed a series of complaints with the Denver Police Department, seeking justice for what he alleged was a pattern of misconduct and mistreatment, including being badly beaten up by a group of officers when he had been arrested.\nA year later, Talley was again arrested for the second robbery, but the chief witness changed his testimony by saying he did not now think Talley was the robber. The case collapsed, though the charges were never fully dropped.\nIn 2016, Talley sued the Denver Police Department, the FBI, and the city, receiving a USD 50,000 settlement.\nSystem \ud83e\udd16\nDenver Police Department website\nDenver Police Department Wikipedia profile\nOperator: Denver Police Department; Federal Bureau of Investigation (FBI)\nDeveloper: Federal Bureau of Investigation (FBI)\nCountry: USA\nSector: Govt - police\nPurpose: Strengthen law enforcement\nTechnology: Facial recognition\nIssue: Accuracy/reliability\nTransparency: Governance; Black box\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\n Talley v City and County of Denver\nResearch, advocacy \ud83e\uddee\nNYBSA (2020). Why Facial Recognition Technology Is Flawed\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.vocativ.com/418052/false-facial-recognition-cost-denver-steve-talley-everything/index.html\nhttps://theintercept.com/2016/10/13/how-a-facial-recognition-mismatch-can-ruin-your-life/\nhttps://www.denverpost.com/2016/09/15/fbi-denver-police-sued-false-arrest-excessive-force/\nhttps://www.westword.com/news/steven-talley-featured-on-showtimes-dark-net-9026953\nhttps://www.dailymail.co.uk/news/article-3838925/Man-arrested-twice-two-different-bank-robberies-despite-evidence-NOT-suspect-surveillance-video-files-10million-lawsuit-wrongful-arrests.html\nhttps://www.techdirt.com/articles/20161015/07313535800/fbi-facial-recognition-expert-helps-denver-pd-arrest-wrong-man-twice-same-crime.shtml\nhttps://theconversation.com/combining-the-facial-recognition-decisions-of-humans-and-computers-can-prevent-costly-mistakes-97365\nRelated \ud83c\udf10\nNiJeer Parks facial recognition wrongful arrest\nRobert Williams facial recognition wrongful arrest\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/michael-oliver-facial-recognition-wrongful-arrest", "content": "Michael Oliver facial recognition wrongful arrest\nOccurred: September 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nMichael Oliver is suing the city of Detroit for USD 12 million for using facial recognition to arrest and jail him in July 2019 for a crime he never committed.\nOliver was arrested for reputedly having snatched a mobile phone which had been used by a teacher to video a group of students involved in a brawl near a Detroit school. Facial recognition software developed by DataWorks Plus matched Oliver with a photograph in a police database. \nIn September 2019, Wayne County Prosecutor\u2019s Office dropped the charges against Parks on the basis that he had tattoos and other body markings that the real thief did not have.\nAccording to Oliver's September 2020 lawsuit, '[Police relied] on failed facial recognition technology knowing the science of facial recognition has a substantial error rate among black and brown persons of ethnicity which would lead to the wrongful arrest and incarceration of persons in that ethnic demographic.' \nIn July 2020, Detroit Police Chief James Craig had admitted facial recognition technology misidentified suspects in 96 percent of cases.\nSystem \ud83e\udd16\nDataWorks Plus website\nDetroit Police Department website\nDetroit Police Department Wikipedia profile\nOperator: Detroit Police Department\nDeveloper: DataWorks Plus\nCountry: USA\nSector: Govt - police\nPurpose: Strengthen law enforcement\nTechnology: Facial recognition\nIssue: Accuracy/reliability; Bias/discrimination - race, ethnicity\nTransparency: Governance; Black box\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nOliver v Donald Bussa, Stephen Cassini (2020)\nResearch, advocacy \ud83e\uddee\nACLU (2020). Statement on Second Wrongful Arrest due to Face Recognition Technology\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://eu.freep.com/story/news/local/michigan/detroit/2020/07/10/facial-recognition-detroit-michael-oliver-robert-williams/5392166002/\nhttps://www.nytimes.com/2020/12/29/technology/facial-recognition-misidentify-jail.html\nhttps://www.washingtonpost.com/opinions/unregulated-facial-recognition-must-stop-before-more-black-men-are-wrongfully-arrested/2020/12/31/dabe319a-4ac7-11eb-839a-cf4ba7b7c48c_story.html#\nhttps://www.engadget.com/facial-recognition-false-match-wrongful-arrest-224053761.html\nhttps://gizmodo.com/detroit-police-wrongfully-arrested-another-black-man-fa-1844342084\nhttps://thehill.com/policy/technology/504306-facial-recognition-leads-to-detroit-man-being-wrongfully-arrested-aclu\nhttps://www.vice.com/en/article/bv8k8a/faulty-facial-recognition-led-to-his-arrestnow-hes-suing\nhttps://www.cbsnews.com/news/detroit-facial-recognition-surveillance-camera-racial-bias-crime/\nhttps://mashable.com/article/arrested-facial-recognition-technology/\nhttps://www.techdirt.com/articles/20200713/14442644889/detroit-pd-now-linked-to-two-bogus-arrests-stemming-facial-recognition-false-positives.shtml\nhttps://www.wxyz.com/news/region/detroit/facial-recognition-technology-led-to-this-detroiters-wrongful-arrest\nRelated \ud83c\udf10\nRobert Williams facial recognition wrongful arrest\nNiJeer Parks facial recognition wrongful arrest\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/nijeer-parks-facial-recognition-wrongful-arrest", "content": "NiJeer Parks facial recognition wrongful arrest, jailing\nOccurred: December 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\n33-year-old Black man Nijeer Parks annnounced he would sue New Jersey city, police department, and prosecutor for false arrest, false imprisonment and violation of his civil rights for misidentifying him using facial recognition.\nParks had been arrested in January 2019 for purportedly shoplifting from a candy shop in Woodbridge, New Jersey. \nThe real cuplrit had given the police a fake driver's license with Park's photograph before absconding, leading to the police issuing a warrant for Parks' arrest. Only Parks had never been to Woodbridge, had never owned a driver's license, and had a sound alibi.\nHaving feigned an asthma attack during a police interrogation, Parks was arrested, denied bail, and held for 10 days. \nParks was finally released and the case against him dismissed for lack of evidence. \nSystem \ud83e\udd16\nClearview AI website\nClearview AI Wikipedia profile\nOperator: Woodbridge Police Department, New Jersey; Middlesex County Prosecutor\u2019s Office\nDeveloper: Clearview AI\nCountry: USA\nSector: Govt - police\nPurpose: Strengthen law enforcement\nTechnology: Facial recognition\nIssue: Accuracy/reliability; Bias/discrimination - race, ethnicity\nTransparency: Governance; Black box\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nNijer Parks v John E. McCormack (2020) (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.nytimes.com/2020/12/29/technology/facial-recognition-misidentify-jail.html\nhttps://www.axios.com/facial-recognition-tech-new-jersey-false-arrest-7c1237e3-88de-43cf-961f-14e562a4dc3b.html\nhttps://www.nj.com/middlesex/2020/12/he-spent-10-days-in-jail-after-facial-recognition-software-led-to-the-arrest-of-the-wrong-man-lawsuit-says.html\nhttps://www.dailymail.co.uk/news/article-9095719/New-Jersey-man-sues-wrongful-arrest-facial-recognition-bust.html\nhttps://www.nbcnews.com/news/us-news/black-man-new-jersey-misidentified-facial-recognition-tech-falsely-jailed-n1252489\nhttps://www.businessinsider.com/black-man-facial-recognition-technology-crime-2020-12\nhttps://www.engadget.com/facial-recognition-wrongful-arrest-lawsuit-new-jersey-201517290.html\nhttps://www.thedailybeast.com/new-jersey-man-says-facial-recognition-software-led-to-his-false-arrest-in-lawsuit\nhttps://www.wsj.com/articles/facial-recognition-tools-in-spotlight-in-new-jersey-false-arrest-case-11609269719\nhttps://futurism.com/the-byte/lawsuit-claims-facial-recognition-ai-sent-wrong-man-jail\nhttps://www.teenvogue.com/story/artificial-intelligence-policing-encode-justice\nRelated \ud83c\udf10\nMichael Oliver facial recognition wrongful arrest\nAlonzo Sawyer facial recognition wrongful arrest\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/robert-williams-facial-recognition-wrongful-arrest", "content": "Robert Williams facial recognition wrongful arrest\nOccurred: January 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nRobert Williams, 42, was misidentified by facial recognition technology used by the Detroit Police Department, inflicting significant emotional damage on Williams and his family.\nWilliams had been arrested for reputedly stealing five high-end watches at a store in Detroit in October 2018 and a detective used facial recognition technology on a grainy image from the store's CCTV video. The system flagged Williams as a potential match based on a driver\u2019s license photograph. \nA security guard who had not been present at the incident then identified Williams in a photo line-up of Black males.\nArrested in front of his family, Williams had been arrested, arraigned, detained for 30 hours and questioned in connection with a crime that took place in a store he hadn't visited since 2014. Prosecutors and police later apologised for how the case was handled. \nDetroit Police Chief James Craig had earlier admitted facial recognition technology misidentified suspects in 96 percent of cases.\n\u2795 April 2021. Williams sued (pdf) the Detroit Police Department for wrongfully arresting and jailing him. He was the first person known to have been arrested in the US because of a facial recognition failure.\n\u2795 June 2024. The City of Detroit settled with the ACLU and Robert Williams for USD 300,000.\nSystem \ud83e\udd16\nDataWorks Plus website\nDetroit Police Department website\nDetroit Police Department Wikipedia profile\nOperator: Detroit Police Department\nDeveloper: DataWorks Plus\nCountry: USA\nSector: Govt - police\nPurpose: Strengthen law enforcement\nTechnology: Facial recognition\nIssue: Accuracy/reliability; Bias/discrimination - race, ethnicity\nTransparency: Governance; Black box\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nWilliams v City of Detroit (pdf)\nResearch, advocacy \ud83e\uddee\nACLU (2021). The Fight to Stop Facial Recognition Technology\nACLU (2020). Michael Williams press release \nACLU (2020). Michael Williams complaint\nACLU (2020). Michael Williams video\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.washingtonpost.com/opinions/2020/06/24/i-was-wrongfully-arrested-because-facial-recognition-why-are-police-allowed-use-this-technology/\nhttps://eu.freep.com/story/opinion/columnists/nancy-kaffer/2020/06/24/robert-williams-detroit-police-facial-recognition/3247171001/\nhttps://www.freep.com/story/news/local/michigan/detroit/2020/06/26/facial-recognition-wrongful-arrest-detroit-police/3265943001/ \nhttps://www.nytimes.com/2020/06/24/technology/facial-recognition-arrest.html\nhttps://www.nbcnews.com/business/business-news/man-wrongfully-arrested-due-facial-recognition-software-talks-about-humiliating-n1232184\nhttps://eu.detroitnews.com/story/news/local/detroit-city/2020/06/26/detroit-police-clear-record-man-wrongfully-accused-facial-recognition-software/3259651001/\nhttps://www.wired.com/story/flawed-facial-recognition-system-sent-man-jail/\nhttps://www.independent.co.uk/news/world/americas/detroit-police-arrest-robert-williams-facial-recognition-robbery-a9583966.html\nhttps://edition.cnn.com/2020/06/24/tech/aclu-mistaken-facial-recognition/index.html\nhttps://www.vice.com/en/article/dyzykz/detroit-police-chief-facial-recognition-software-misidentifies-96-of-the-time\nRelated \ud83c\udf10\nNiJeer Parks facial recognition wrongful arrest\nAlonzo Sawyer facial recognition wrongful arrest\nPage info\nType: Incident\nPublished: March 2023\nLast updated: July 2024", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/titus-henderson-compas-parole-denial", "content": "Titus Henderson COMPAS parole denial\nOccurred: 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nWisconsin prison inmate Titus Henderson alleged that prison officials had discriminated against him and other African American inmates by using a 'racially biased actuarial tool,' COMPAS, in their sentencing.\nPer George Washington University's ETI AI Litigation Database, Henderson had been convicted and incarcerated in July 1995, serving a 40-year sentence. \nIn October 2014, COMPAS determined Henderson was low risk but after a Parole Hearing in November 2015, Henderson claimed he was denied Parole and transfer from Wisconsin to Mississippi because of COMPAS's biased algorithm for gender and against African-Americans.\nThe judge rejected Henderson's claims, arguing there is enough information to infer that the Department of Corrections knew of the racial bias and its harm to African American inmates. \nIn March 2021 the judge dismissed the case. \nSystem \ud83e\udd16\nEquivant website\nCOMPAS Wikipedia profile\nOperator: Wisconsin Court System\nDeveloper: Volaris Group/Equivant/Northpointe\nCountry: USA\nSector: Govt - justice\nPurpose: Assess recidivism risk\nTechnology: Recidivism risk assessment system\nIssue: Bias/discrimination - race, ethnicity, gender\nTransparency: Governance; Black box\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nHenderson v Stensberg\nHenderson v Stensberg case dockets\nGeorge Washington University. ETI AI Litigation Database: case details\nResearch, advocacy \ud83e\uddee\n(2021). Abtracting Injustice. An analysis of the Use of Artificial Intelligence in Criminal Justice (pdf)\nEPIC (2020, updated 2021). Liberty at Risk: Pre-Trial Risk Assessment Tools in the US (pdf)\nRelated \ud83c\udf10\nEric Loomis COMPAS prison sentencing\nVirginia non-violent risk assessment\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/eric-loomis-compas-prison-sentencing", "content": "Eric Loomis algorithmic risk assessment accused of denying due process\nOccurred: 2016\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe use of a controversial algorithmic recidivism tool to sentence Eric Loomis to six years in prison for driving a car used in shooting was criticised and appealed as a denial of due process. \nEric Loomis was sentenced to six years in prison for driving a car used in a shooting by a judge partly relying on the Correctional Offender Management Profiling for Alternative Sanctions (COMPAS) tool. \nThe ruling led to Loomis lodging a legal appeal on the basis that the  score assigned to him could not be assessed due to its protection as a trade secret, and that the use of COMPAS infringed on his right to an individualised sentence and his right to be sentenced on the basis of accurate information. \nHe also argued that the court unconstitutionally considered gender at sentencing by relying on a risk assessment that took gender into account.\nCOMPAS risk assessments are based on data gathered from a defendant's criminal file and from an interview with the defendant, and predict the risk of pretrial recidivism, general recidivism, and violent recidivism.\nA registered sex offender, Loomis scored high on all three risk measures, and was sentenced to six years in prison. The judge said he had arrived at his sentencing decision in part because of Mr. Loomis\u2019s rating on the Compas assessment. Loomis then filed a motion requesting a new sentencing hearing, which was denied by all three levels of Wisconsin courts.\nNonetheless, the court advised that judges presented with COMPAS risk scores understood that: \nThere was no disclosure of how factors were weighed or risk scores were determined; \nThat the assessment compared defendants to a national sample and no cross-validation study of Wisconsin defendants has been completed; \nSome studies raised questions about whether COMPAS disproportionately gives minorities higher risk scores; and \nRisk assessment tools must be updated due to changing populations.\nThe judge also ruled that the use of COMPAS did not deny due process as the court ultimately imposes sentences on the basis of all of its knowledge of the defendant, including their criminal histories.\nThe incident raised questions about the opacity, accountability, fairness and efficacy of the tool and of tools similar to it, such as the Ontario Domestic Assault Risk Assessment (ODARA) and the Virginia Non-violent Risk Assessment (NVRA).\nIt also prompted debate about the use of big data and algorithms to predict crime (aka 'predictive policing').\nSystem \ud83e\udd16\nEquivant website\nCOMPAS Wikipedia profile\n\nDocuments \ud83d\udcc3\nEquivant (2016). Wisconsin vs. Loomis: Court Affirms the Use of COMPAS in Sentencing\nOperator: Wisconsin Court System\nDeveloper: Volaris Group/Equivant/Northpointe\nCountry: USA\nSector: Govt - justice\nPurpose: Assess recidivism risk\nTechnology: Recidivism risk assessment system\nIssue: Accountability; Bias/discrimination - race, ethnicity; Human/civil rights\nTransparency: Governance; Black box\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nLoomis v Wisconsin Wikipedia profile\nGeorge Washington University. ETI AI Litigation Database: case details\nSupreme Court of Wisconsin (2016). State v Loomis (pdf)\nResearch, advocacy \ud83e\uddee\nDressel J., Farid H. (2018). The accuracy, fairness, and limits of predicting recidivism\nKirkpatrick K. (2017). It's not the algorithm, it's the data\nAngelino E., Larus-Stone N., Alabi D., Seltzer M., Rudin R. (2017) Learning Certifiably Optimal Rule Lists for Categorical Data\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.nytimes.com/2016/06/23/us/backlash-in-wisconsin-against-using-data-to-foretell-defendants-futures.html\nhttps://www.nytimes.com/2017/05/01/us/politics/sent-to-prison-by-a-software-programs-secret-algorithms.html\nhttps://www.technologyreview.com/2017/06/01/151447/secret-algorithms-threaten-the-rule-of-law/\nhttps://www.wired.com/2017/04/courts-using-ai-sentence-criminals-must-stop-now/\nhttps://www.theatlantic.com/technology/archive/2018/01/equivant-compas-algorithm/550646/\nhttps://today.duke.edu/2017/07/opening-lid-criminal-sentencing-software\nhttps://jolt.law.harvard.edu/digest/algorithmic-due-process-mistaken-accountability-and-attribution-in-state-v-loomis-1\nhttps://harvardlawreview.org/2017/03/state-v-loomis\nRelated \ud83c\udf10\nTitus Henderson COMPAS parole sentencing\nVirginia non-violent risk assessment\nPage info\nType: Incident\nPublished: March 2023\nLast updated: March 2024", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/alonzo-sawyer-facial-recognition-mistaken-arrest", "content": "Alonzo Sawyer facial recognition wrongful arrest, jailing\nOccurred: 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\n54-year old Alonzo Sawyer was arrested and jailed for nine days for an assault and theft he did not commit thanks to poor analysis of CCTV footage using facial recognition by an intelligence analyst at the Maryland Transit Administration Police.\nThe analysis failed to take into account the fact that Sawyer is older, taller than the suspect in the video, has facial hair and gaps between his teeth, and his right foot slews out when he walks, according to photographs shown to the police by his wife.\nMaryland Chiefs of Police Association president Russ Hamill said that what happened to Alonzo Sawyer was 'horrifying' when speaking in opposition to a bill seeking to regulate the use of facial recognition in Maryland.\nAccording to Deborah Levi, a Baltimore public defender in Baltimore, the Baltimore Police Department used facial recognition over 800 times in 2022.\n\u2795 In January 2023, it emerged that Georgia man Randall Reid had been wrongly arrested and jailed for a purse theft incident in Baton Rouge using facial recognition by Louisiana authorities, even though he had never visited the state.\nSystem \ud83e\udd16\nBaltimore Police Department website\nOperator: Baltimore Police Department\nDeveloper: \nCountry: USA\nSector: Govt - police \nPurpose: Strengthen security\nTechnology: CCTV; Facial recognition\nIssue: Accuracy/reliability; Bias/discrimination - race, ethnicity\nTransparency: Governance; Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.wired.com/story/face-recognition-software-led-to-his-arrest-it-was-dead-wrong\nhttps://www.techdirt.com/2023/03/03/facial-recognition-pitches-in-to-help-cops-arrest-a-maryland-man-for-a-crime-he-didnt-commit/\nhttps://vervetimes.com/face-recognition-software-led-to-his-arrest-it-was-dead-wrong/\nhttps://www.msn.com/en-us/news/crime/another-black-man-falsely-accused-thanks-to-facial-recognition-technology/ar-AA186vHw\nhttps://www.levelman.com/another-black-man-falsely-accused-thanks-to-facial-recognition-technology/\nhttps://t3n.de/news/gesichtserkennung-mann-unschuldig-1537966/\nRelated \ud83c\udf10\nRandall Reid facial recognition wrongful arrest, jailing\nRio de Janeiro facial recognition wrongful arrests\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/aadhaar-glitch-results-in-villagers-starvation", "content": "Aadhaar glitches result in villagers' starvation\nOccurred: 2017-\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nTechnical problems with India's Aadhaar biometric ID system resulted in the deaths of scores of villagers in Jharkhand state and elsewhere, with some committing suicide and others suffering severe malnutrition. \nThe glitches have meant that villagers have been unable to get food rations or subsidised grain, sometimes without explanation. Some groups, including vulerable minority groups such as the Parhaiya, have been denied their legal entitlement of subsidised grain for failing to correctly link their ration cards with Aadhaar. \nCampaigners told The Guardian that the system is fraught with problems, including authentication issues, with fingerprints scanning not working properly, officials failing to offer timely support, and poor internet coverage meaning people can often not access the system. \nSystem \ud83e\udd16\nAadhaar website\nAadhaar Wikipedia profile\nOperator: Aadhaar\nDeveloper: Unique Identification Authority of India (UIDAI)\nCountry: India\nSector: Govt - welfare\nPurpose: Reduce welfare fraud\nTechnology: Fingerprint biometrics\nIssue: Accuracy/reliability; Robustness\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://scroll.in/article/860857/aadhaar-disruption-in-jharkhands-poorest-regions-hundreds-of-people-are-being-denied-foodgrains\nhttps://www.theguardian.com/technology/2019/oct/16/glitch-india-biometric-welfare-system-starvation\nhttps://www.bloombergquint.com/aadhaar/did-aadhaar-glitches-cause-half-of-14-recent-jharkhand-starvation-deaths\nhttps://www.newindianexpress.com/nation/2019/jun/07/man-dies-of-starvation-in-jharkhand-as-ration-not-disbursed-for-three-months-1986974.html\nhttps://www.newsclick.in/12-die-hunger-10-months-jharkhand-government-denial-right-food-campaign\nhttps://www.newindianexpress.com/nation/2018/jul/31/jharkhand-committee-on-hunger-death-misses-second-deadline-1851266.html\nhttps://www.khaleejtimes.com/international/india/11-year-old-dies-of-starvation-due-to-technical-glitch\nhttps://www.hindustantimes.com/cities/for-want-of-aadhaar-girl-dies-of-starvation-after-ration-card-caught-in-technical-glitch/story-2R6cIFNDcpokcergdIv3UL.html\nhttps://thewire.in/politics/aadhaar-glitch-another-woman-dies-hunger-jharkhand-denied-ration-say-activists\nhttps://www.bbc.co.uk/news/world-asia-india-43207964\nRelated \ud83c\udf10\nAadhaar COVID-19 facial recognition marginalisation\nCBSE India student facial 'matching' opacity\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tesla-model-s-driver-watches-movie-crashes-into-police-car", "content": "Tesla Model S driver watches movie, crashes into police car\nOccurred: August 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Tesla Model S whose driver was watching a movie on his phone while his car was on Autopilot smashed into the rear of a police patrol car in North Carolina, USA. \nNobody was hurt in the crash, but the Tesla was destroyed and the rear-end of the police car was badly damaged and a wheel was ripped off.\nNash County Sheriff Keith Stone commented 'It shows automation is never going to take the place of the motoring public paying attention, not texting, not being on the phone, but focusing on what you were doing, that is, driving.'\nTesla has been accused of unduly hyping the capabilities of its Autopilot and Full-self Driving functions by multiple regulators in the US and elsewhere.\nSystem \ud83e\udd16\nTesla Autopilot, Full-self Driving\nOperator: \nDeveloper: Tesla\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Driver assistance system\nIssue: Accuracy/reliability; Safety\nTransparency: Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://nypost.com/2020/08/27/tesla-driver-watching-movie-while-using-autopilot-crashes-into-cop-car/\nhttps://www.foxnews.com/auto/tesla-on-autopilot-hits-police-car-as-driver-watches-movie-on-cellphone\nhttps://www.cbs17.com/news/local-news/driver-was-watching-movie-when-tesla-plowed-into-nash-county-deputys-vehicle-troopers-say/\nhttps://jalopnik.com/tesla-driver-watching-movie-while-using-autopilot-crash-1844858198\nhttps://abcnews.go.com/Technology/wireStory/patrol-tesla-autopilot-driver-watching-movie-crashed-72685378\nhttps://www.newsobserver.com/news/state/north-carolina/article245267595.html\nhttps://www.businessinsider.com/tesla-model-s-autopilot-crash-police-car-driver-watching-movie-2020-8\nhttps://www.carscoops.com/2020/08/tesla-on-autopilot-crashes-into-police-vehicle-in-north-carolina-driver-admits-to-watching-a-movie/\nhttps://cleantechnica.com/2020/09/04/driver-misuses-tesla-autopilot-crashes-into-cop-car-reminder-to-not-watch-movies-while-driving/\nRelated \ud83c\udf10\nTesla Model 3 crashes into overturned truck on Taiwan highway\nTesla Model X crashes into wall, killing passenger\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tesla-model-3-crashes-into-overturned-truck-in-the-middle-of-the-highway", "content": "Tesla Model 3 crashes into overturned truck on Taiwan highway\nOccurred: June 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Tesla Model 3 crashed into a truck on a highway in Taiwan while reportedly on Autopilot. \nThe May 2020 incident, which was caught on security camera, recorded the car smash into a large truck carrying salad and breakfast ingredients which had flipped over and was lying on its side in the middle of the road. The driver was apparently unharmed.\nPer Taiwan English News, 'According to preliminary investigations by the Highway Police Bureau, the 53-year-old Tesla driver, Mr Huang, said that his car was on autopilot, and traveling at around 110 kilometers per hour at the time of the crash. \nAs soon as he saw the truck, he stepped on the brake, Mr Huang said. However, it was too late to stop the vehicle, and it crashed through the roof of the overturned truck.'\nSystem \ud83e\udd16\nTesla Autopilot, Full-self Driving\nIncident video\nOperator:  \nDeveloper: Tesla\nCountry: S Korea\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Driver assistance system\nIssue: Accuracy/reliability; Safety\nTransparency: Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.setn.com/News.aspx?NewsID=753860\nhttps://electrek.co/2020/06/01/tesla-model-3-crashing-truck-autopilot-video-viral/\nhttps://taiwanenglishnews.com/tesla-on-autopilot-crashes-into-overturned-truck/\nhttps://www.forbes.com/sites/bradtempleton/2020/06/02/tesla-in-taiwan-crashes-directly-into-overturned-truck-ignores-pedestrian-with-autopilot-on/\nhttps://www.dailymail.co.uk/sciencetech/article-8377461/Shocking-moment-Telsa-Model-3-Autopilot-mode-crashes-truck-Taiwan-highway.html\nhttps://cleantechnica.com/2020/06/02/the-latest-my-tesla-ran-into-a-really-big-truck-while-on-autopilot-kerfluffle/\nhttps://www.autoblog.com/2020/06/01/video-tesla-model-3-crashes-into-overturned-truck/\nhttps://www.republicworld.com/entertainment-news/whats-viral/tesla-crashes-into-truck-driver-alleges-it-was-on-autopilot.htm\nhttps://www.cna.com.tw/news/asoc/202006010318.aspx\nhttps://bgr.com/2020/06/01/tesla-crash-model-3-autopilot-truck-taiwan/\nRelated \ud83c\udf10\nSleeping driver speeds on highway with Autopilot switched on\nTesla Model Y crashes into parked police car\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tesla-model-x-crashes-into-wall-killing-passenger", "content": "Tesla Model X crashes into wall, killing passenger\nOccurred: December 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Tesla electric car spun out of control in the parking lot of a Seoul, South Korea, parking lot, and crashed into a wall and caught fire, killing the passenger and injuring two others. \nThe car chauffer claimed 'the car suddenly went out of control', leading the police to suggest that sudden unintended acceleration may have been the cause of the accident. The fire took over an hour to extinguish. \nThe Korea Herald reported that local experts figure the battery-operated doors may also have been a factor in the outcome of the latest Tesla accident, as the doors remained shut as rescuers tried to force them open from outside.\nSystem \ud83e\udd16\nTesla Autopilot, Full-self Driving\nOperator:  \nDeveloper: Tesla\nCountry: S Korea\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Driver assistance system\nIssue: Accuracy/reliability; Safety\nTransparency: Black box\nResearch, advocacy \ud83e\uddee\nTesla Deaths\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reuters.com/article/us-tesla-southkorea-crash-idUSKBN28K0LA\nhttp://www.koreaherald.com/view.php?ud=20201213000152\nhttps://www.autoguide.com/auto-news/2018/04/self-driving-chevy-bolt-ev-ticketed-in-san-francisco.html\nhttps://koreajoongangdaily.joins.com/2020/12/24/business/industry/Tesla/20201224184400725.html\nhttps://www.kedglobal.com/newsView/ked202012110006\nhttps://www.marketscreener.com/quote/stock/TESLA-INC-6344549/news/Tesla-South-Korea-investigates-fatal-crash-of-Tesla-Model-X-31979442/\nRelated \ud83c\udf10\nJi-Chang Son Tesla Model X sudden acceleration\nTesla Model S 'FSD malfunction' causes eight-vehicle pile-up\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/sleeping-driver-speeds-on-highway-with-autopilot-switched-on", "content": "Sleeping driver speeds on highway with Autopilot switched on\nOccurred: September 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Tesla driver was caught asleep behind the wheel while doing 93 MPH on the highway near Ponoka, Alberta, in his Model S.\nThe 20-year old driver, who had both front seats reclined and Autopilot switched on, was slapped with reckless driving charges and had his license suspended for 24 hours. \nThe car accelerated when approached by a police car with its emergency lights flashing, even though the driver and passenger appeared to be asleep.\nSystem \ud83e\udd16\nTesla Autopilot and Full Self-Driving Capability\nOperator: Tesla\nDeveloper: Tesla\nCountry: Canada\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Driver assistance system\nIssue: Safety; Accuracy/reliability\nTransparency: Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.rcmp-grc.gc.ca/en/news/2020/alberta-rcmp-charge-tesla-driver-speeding-and-sleeping\nhttps://edition.cnn.com/2020/09/18/business/canada-tesla-charge-scli-intl/index.html\nhttps://nypost.com/2020/09/18/tesla-driver-falls-asleep-while-going-93-mph-on-autopilot/\nhttps://canadanewsmedia.ca/speeding-tesla-driver-caught-napping-behind-the-wheel-on-alberta-highway-cbc-ca/\nhttps://www.bbc.co.uk/news/world-us-canada-54197344\nhttps://www.theverge.com/2020/9/18/21445168/tesla-driver-sleeping-police-charged-canada-autopilot\nhttps://www.caranddriver.com/news/a28066700/tesla-driver-asleep-at-wheel/\nhttps://electrek.co/2020/12/28/tesla-driver-accused-sleeping-autopilot-going-trial-dangerous-driving/\nhttps://www.theguardian.com/world/2020/sep/17/canada-tesla-driver-alberta-highway-speeding\nRelated \ud83c\udf10\nTesla Model Y crashes into parked police car\nTesla Model S 'FSD malfunction' causes eight-vehicle pile-up\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/suzhou-social-civility-score-trial", "content": "Suzhou algorithmic social 'civility score' withdrawn after backlash\nOccurred: September 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe Chinese city of Suzhou's launch of a 'civility code' early September 2020 to rank citizens\u2019 civility and award or punish them accordingly backfired after local citizens accused it of being overbearing, manipulative, and an abuse of privacy.\nEmbedded in the 'Suzhou City Code' (\u8607\u57ce\u78bc) - a digital ID used to control residents\u2019 movements during the COVID-19 pandemic - the new 'Suzhou App 2.0' comprised two sets of indexes: 'civility in traffic performance' and 'civility in voluntary work performance'. \nCitizens found to be jay-walking or drunk-driving would lose points, whereas those volunteering work would gain points. People with high scores would have more advantages in seeking employment, enrolling in schools, and accessing public and private services, and vice-versa.\nHowever, citizens complained that the term 'civility' was too loosely defined and that the system could lead to abuses of power. Suzhou authorities suspended the system three days after its launch and said it would re-launch it after modifications had been made. It is not known to have re-launched.\nAn extension of its financial credit rating system, China's Social Credit System launched in 2014 and tracks and assesses the creditworthiness and trustworthiness of citizens, businesses, government bodies, and NGOs. \nIt has been trialled in 'model cities' across China, including Suzhou, since December 2017.\nSystem \ud83e\udd16\nSocial Credit System Wikipedia profile\nSuzhou Police department (2020). \u534e\u4e3d\u8f6c\u8eab \u8fd9\u6837\u7684\u201c\u82cf\u57ce\u7801\u201d\u4f60\u559c\u6b22\u4e0d?\nOperator: Government of China\nDeveloper: Government of China\nCountry: China\nSector: Govt - police; Govt - security\nPurpose: Assess creditworthiness, trustworthiness\nTechnology: Behavioural monitoring; Deep learning; Neural network; Machine learning\nIssue: Ethics/values; Fairness; Privacy\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.scmp.com/abacus/tech/article/3100516/suzhou-city-takes-page-chinas-social-credit-system-civility-code-rates\nhttps://algorithmwatch.org/en/story/suzhou-china-social-score/\nhttps://globalvoices.org/2020/09/13/a-chinese-city-withdraws-civility-code-following-online-criticism/\nhttps://www.bloomberg.com/news/features/2019-06-18/china-social-credit-rating-flaws-seen-in-suzhou-osmanthus-program\nhttps://www.sixthtone.com/news/1006151/Suzhou\nhttps://chinadigitaltimes.net/2020/09/translation-a-civility-code-will-only-make-us-fearful/\nhttps://www.scmp.com/week-asia/opinion/article/3101221/amid-chinas-coronavirus-success-low-marks-local-social-credit\nhttps://www.straitstimes.com/asia/civility-code-app-goes-against-modern-governance-china-daily\nhttps://www.dailymail.co.uk/news/article-8709539/Chinese-city-trials-monitor-rate-residents-behaviour-smartphone-app.html\nhttps://www.thesun.co.uk/news/12616225/china-city-surveillance-smartphone-app-rate-residents-behaviour/\nRelated \ud83c\udf10\nChina social credit offence travel bans\nChina Pharmaceutical University student behavioural monitoring\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/china-social-credit-offence-travel-bans", "content": "China bans people with social credit offences from travelling\nOccurred: March 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe Chinese government banned millions of people from travelling in 2018 for 'social credit' misdeeds, including unpaid taxes and fines.\nChina's National Public Credit Information Center annual report said would-be air travelers were blocked from buying tickets 17.5 million times; others were barred 5.5 million times from buying train tickets.\nBeijing had announced in March 2018 it would introduce the travel ban two months later. Per Reuters, people would be put on the restricted lists if they had committed acts like spreading false information about terrorism, causing trouble on flights, using expired tickets or smoking on trains.\nThe Social Credit System is based on the principle of 'once untrustworthy, always restricted'.\nSystem \ud83e\udd16\nSocial Credit System Wikipedia profile\nOperator: Government of China\nDeveloper: Government of China\nCountry: China\nSector: Govt - police; Govt - security\nPurpose: Assess creditworthiness, trustworthiness\nTechnology: Deep learning; Neural network; Machine learning\nIssue: Ethics; Fairness\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.apnews.com/9d43f4b74260411797043ddd391c13d8\nhttps://www.reuters.com/article/us-china-credit-idUSKCN1GS10S\nhttps://www.telegraph.co.uk/news/2018/03/24/chinas-social-credit-system-bans-millions-travelling/\nhttps://www.independent.co.uk/life-style/gadgets-and-tech/china-social-credit-system-punishments-rewards-explained-a8297486.html\nhttps://www.caixinglobal.com/2019-04-01/in-depth-chinas-burgeoning-social-credit-system-stirs-controversy-101399430.html\nhttps://fortune.com/2019/02/22/china-social-credit-travel-ban/\nhttps://www.foxnews.com/tech/china-bars-millions-from-travel-for-social-credit-offenses\nhttps://www.abc.net.au/news/2019-02-23/china-bars-millions-from-travel-for-social-credit-offences/10843156\nhttps://www.independent.co.uk/news/world/asia/china-social-credit-system-school-ban-family-travel-a8821371.html\nRelated \ud83c\udf10\nSuzhou social 'civility score' trial\nChina Pharmaceutical University student behavioural monitoring\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/knightscope-hp-robocop-ignores-woman-reporting-crime", "content": "Knightscope HP RoboCop ignores woman reporting crime\nOccurred: October 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA woman attempting to report a violent crime in a Los Angeles park was turned away by 'HP Robocop', a police robot on duty for Huntingdon Park Police Department.\nDespite Cogo Guebara pushing the robot's its emergency alert button several times, it barked at Guebara to \u2018Step out of the way\u2019 and continued to glide along its pre-programmed route instead of offering assistance.\nMedia reports said people, especially children, said they felt generally reassured by the robot's presence. But some felt it resembled a form of creeping surveillance whilst others worry that a tendency amongst some to anthropomorphise the objects  highlights the disconnect between people\u2019s expectations of the robot and the reality of its capabilities.\nKnightscope robots combine self-driving technology, robotics and artificial intelligence to create what the company calls 'crime-fighting autonomous data machines.' According to Huntingdon police, leasing the robot for a year costs the city between USD 60,000-70,000, roughly equivalent to the annual salary of a Huntington Park police officer.\nSystem \ud83e\udd16\nKnightscope website\nKnightscope Wikipedia profile\nOperator: Huntington Park Police Department\nDeveloper: Knightscope\nCountry: USA\nSector: Govt - police\nPurpose: Strengthen security\nTechnology: Robotics\nIssue: Accuracy/reliability; Anthropomorphism; Surveillance\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.nbcnews.com/tech/tech-news/robocop-park-fight-how-expectations-about-robots-are-clashing-reality-n1059671\nhttps://abc7.com/robocop-hp-huntington-park-in/5601190/\nhttps://www.autoevolution.com/news/hp-robocop-shows-how-far-we-still-have-to-go-before-ai-could-really-protect-us-138344.html\nhttps://www.iflscience.com/technology/californian-robocop-had-to-deal-with-its-first-crime-and-it-did-not-go-well/\nhttps://metro.co.uk/2019/10/04/police-robot-told-woman-go-away-tried-report-crime-sang-song-10864648/\nhttps://www.dailymail.co.uk/news/article-7538611/RoboCop-told-woman-step-way-tried-summon-police-break-fight.html\nhttps://gizmodo.com/useless-police-robot-fails-to-call-for-help-when-needed-1838886285\nhttps://futurism.com/the-byte/knightscope-security-robot-ignored-woman\nRelated \ud83c\udf10\nHonolulu homeless robot temperature tests\nSingapore Xavier patrol robots\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/passport-check-interprets-lips-as-open-mouth", "content": "Automated UK passport checker interprets lips as open mouth\nOccurred: September 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn automated passport photo checker facial detection system run by the UK Home Office mistook the 'big lips' of a young Black man hoping to renew his passport for an open mouth, resulting in his application being refused.\nThe incident prompted Bada to share his experience online, leading to accustations of algorithmic bias and racism. \nThe UK's Race Equality Foundation said it believed the system was not tested properly to see if it would actually work for black or ethnic minority people, calling it 'technological or digital racism'.\nA New Scientist report published one month after Bada's incident revealed that the Home Office system was known to fail for people with dark skin, but that it decided to use it regardless. \nThe documents were released following a Freedom of Information request by MedConfidential.\nSystem \ud83e\udd16\nUnknown\nOperator: UK Home Office\nDeveloper: \nCountry: UK\nSector: Govt - immigration\nPurpose: Automate passport checks\nTechnology: Facial detection\nIssue: Bias/discrimination - race, ethnicity\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.standard.co.uk/news/uk/man-stunned-as-passport-photo-check-sees-lips-as-open-mouth-a4241456.html\nhttps://www.mirror.co.uk/news/uk-news/mans-lips-mistaken-open-mouth-20098853\nhttps://www.telegraph.co.uk/technology/2019/09/19/racist-passport-photo-system-rejects-image-young-black-man-despite/\nhttps://www.irishnews.com/magazine/technology/2019/09/19/news/passport-photo-checker-falsely-flags-black-man-s-lips-as-open-mouth-1716760/\nhttps://www.dailymail.co.uk/news/article-7481357/Passport-photo-checker-falsely-flags-black-man-s-lips-open-mouth.html\nhttps://guernseypress.com/news/uk-news/2019/09/19/passport-photo-checker-falsely-flags-black-mans-lips-as-open-mouth/\nhttps://uk.news.yahoo.com/passport-photo-checker-falsely-flags-100055240.html\nhttps://www.newscientist.com/article/2219284-uk-launched-passport-photo-checker-it-knew-would-fail-with-dark-skin/\nhttps://www.mirror.co.uk/tech/uk-government-launched-passport-photo-20545844\nRelated \ud83c\udf10\nUK passport photo application 'racism'\nUS border imposter identification failures\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/qtcinderella-pokimane-sweet-anita-deepfakes", "content": "QTCinderella, Pokimane, Sweet Anita deepfakes exposed during live stream\nOccurred: February 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe unintended exposure on a Twitch live stream of non-consensual deepfake images of a group of female gamers and content creators horrified them, and humiliated the Twitch streamer.\nTwitch personality Brandon Ewing, 31, known online as Atrioc, inadvertently shared a link to a deepfake video site during a livestreaming event, exposing synthetic pornographic images of QTCinderella, Pokimane, and Sweet Anita.\nPer USA Today, QTCinderella says 'I'm a normal girl.' 'I like Taylor Swift. I like baking cookies. I like going to Disneyland.' But after the incident 'her name, her face and her brand have become associated with pornography'.\nBritish influencer Sweet Anita told the New York Post that she 'fears the mass circulation of her misused image will have lasting ramifications.' 'This was nonconsensual and the impacts are permanent,' she said. 'This will impact my life in a similar way to revenge porn, so I\u2019m just frustrated, tired and numb.'\nEwing published an apology, deleted his account, and disappeared. \nTwitch later updated its policy on adult nudity to include a ban on synthetic non-consensual exploitative images.\nSystem \ud83e\udd16\nUnknown\nOperator: \nDeveloper: Unclear/unknown\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Generate revenue\nTechnology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Safety; Ethics\nTransparency: Governance; Privacy; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://screenshot-media.com/technology/ai/qtcinderella-streamer-deepfake\nhttps://www.washingtonpost.com/technology/2023/02/13/ai-porn-deepfakes-women-consent/\nhttps://eu.usatoday.com/story/life/health-wellness/2023/02/14/qtcinderella-deepfake-trauma-nonconsensual-porn/11222588002/\nhttps://www.nbcnews.com/tech/internet/deepfake-twitch-porn-atrioc-qtcinderella-maya-higa-pokimane-rcna6937\nhttps://www.reddit.com/r/LivestreamFail/comments/10oxsti/atrioc_caught_looking_at_streamer_deepfkes\nhttps://www.thegamer.com/qtcinderella-cannot-sue-deepfake-creator/\nhttps://www.thegamer.com/twitch-explicit-deepfake-creator-deletes-website-atrioc-apology/\nhttps://earlygame.com/entertainment/qtcinderella-destiny-stole-the-spotlight\nhttps://videogames.si.com/news/qtcinderella-deepfake-lawsuit-legal-problems\nRelated \ud83d\uddde\ufe0f\nMasayuki Nakamoto deepfake uncensored pornography\nXiaoyu deepfake pornography\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/south-wales-police-facial-recognition-trial", "content": "South Wales Police facial recognition trial\nReleased: May 2017\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA trial of facial recognition technology at football and rugby matches, music festivals, and on city streets by South Wales Police (SWP) from May 2017 to April 2019 met with significant criticism from civil and privacy rights advocates. \nSouth Wales Police's trial consisted of mobile video cameras hooked up to facial recognition software to scan crowds for faces on a watchlist. \nIt's use of the technology was ruled unlawful by the UK Court of Appeal in August 2020.\nSystem \ud83e\udd16\nNeoFace Watch facial recognition system\nDocuments \ud83d\udcc3\nSouth Wales Police - facial recognition\nSouth Wales Police. FRT 2017 Deployments (pdf)\nSouth Wales Police (2020). Response to the Court of Appeal judgment on the use of facial recognition technology\nOperator: South Wales Police\nDeveloper: NEC\nCountry: UK\nSector: Govt - police\nPurpose: Identify criminals\nTechnology: Facial recognition\nIssue: Accuracy/reliability; Bias/discrimination; Ethics/values; Freedom of expression - right of assembly; Privacy\nTransparency: Governance; Privacy\nRisks and harms \ud83d\uded1\nSouth Wales Police's use of facial recognition has been criticised for inaccurate techology, privacy violations, and poor governance, oversight, and ethics.\nTransparency and accountability \ud83d\ude48\n\nFalse matches\nThe system was accused of making unacceptably high levels of errors. Over 2,000 people were wrongly identified as possible criminals by South Wales Police's facial recognition system during the 2017 Champions League final in Cardiff. Of the 2,470 potential matches with custody pictures, 2,297 (92%) were wrong.\nThe force said the high volume of false matches was down to 'poor quality images' supplied by agencies including UEFA and Interpol. They also argued it could be attributed to it being the first major use of the equipment. \nA later evaluation of the system by Cardiff University found it flagged 2,900 possible suspects, but 2,755 were false matches.\nEd Bridges legal challenge\nCampaign group Liberty brought a legal case against South Wales police after Cardiff resident and civil rights activist Ed Bridges claimed the force had invaded his privacy and data protection rights by capturing and processing his facial features whilst he was shopping and when he was attending a defence industry exhibition.\n\nBridges lost his first challenge, with the two judges ruling the technology was lawful. \n\nHowever, he won on Appeal, with the court finding that there had been inadequate guidance on where the system could be used and who could be put on a watchlist, its data protection impact assessment was deficient, and the force had failed to take reasonable steps to find out if the software contained racial or gender bias.\nLegal and ethical standards\nAn October 2022 report (pdf) by researchers at University of Cambridge's Minderoo Centre for Technology and Democracy found that the South Wales Police trial had failed to meet minimum expected ethical and legal standards. \nThe report singled out the force's failure to establish limits on the use of facial recognition technology at protest assemblies, inadequate oversight, concerns about the independence of the Joint Independent Ethics Committee and the lack of human rights, equality, or data protection experts on the committee, and the lack of public consultation, notably with marginalised communities.\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nUK Court of Appeal (2020). R (Bridges) -v- CC South Wales\nLiberty. Legal Challenge - Ed Bridges vs South Wales Police\nUK Information Commissioner (2019). The use of live facial recognition technology by law enforcement in public places (pdf)\nUK Biometrics and Surveillance Camera Commissioner (2019). The use of facial recognition technology by South Wales police\nResearch, advocacy \ud83e\uddee\nBig Brother Watch (2020). Briefing on facial recognition surveillance (pdf)\nRitchie K.L., Cartledge C., Growns B., Yan A., Wang Y., Guo K., Kramer R.S.S., Edmond G., Martire K.A., Mehera San Roque M., White D. (2021). Public attitudes towards the use of automated facial recognition technology in criminal justice systems around the world\nInvestigations, assessments, audits \ud83e\uddd0\nMinderoo Centre for Technology & Democracy (2022). A Socio-Technical Audit: Assessing Police Use of Facial Recognition (pdf)\nCardiff University (2018). Evaluating the Use of Automated Facial Recognition Technology in Major Policing Operations\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.walesonline.co.uk/news/wales-news/facial-recognition-south-wales-police-17202103\nhttps://www.wired.co.uk/article/police-facial-recognition-south-wales-court-decision\nhttps://www.biometricupdate.com/202008/facial-recognition-lessons-for-the-private-sector-from-the-south-wales-police-case\nhttps://onezero.medium.com/a-facial-recognition-giant-refuses-to-share-details-about-its-algorithm-dataset-df27a208683d\nhttps://www.bbc.co.uk/news/uk-wales-53734716\nhttps://www.walesonline.co.uk/news/wales-news/facial-recognition-wrongly-identified-2000-14619145\nhttps://news.sky.com/story/facial-recognition-technology-who-watches-the-watchers-11725536\nhttps://techxplore.com/news/2020-08-uk-court-recognition-violates-human.html\nhttps://www.cnbc.com/2020/08/11/swp-facial-recognition-unlawful.html\nhttps://www.wired.co.uk/article/face-recognition-police-uk-south-wales-met-notting-hill-carnival\nhttps://www.zdnet.com/article/facial-recognition-could-be-most-invasive-policing-technology-ever-warns-watchdog/\nhttps://onezero.medium.com/a-facial-recognition-giant-refuses-to-share-details-about-its-algorithm-dataset-df27a208683d\nhttps://www.theregister.com/2020/06/30/nec_neoface_watch_afr_locate_details_liberty\nhttps://www.biometricupdate.com/202007/nec-tells-uk-court-facial-biometrics-not-scraped-from-internet-but-declines-training-dataset-details\nhttps://www.theguardian.com/technology/2020/aug/11/south-wales-police-lose-landmark-facial-recognition-case\nhttps://onezero.medium.com/nec-is-the-most-important-facial-recognition-company-youve-never-heard-of-12381d530510\nRelated \ud83c\udf10\nKing's Cross live facial recognition\nMet Police Gangs Violence Matrix\nPage info\nType: System\nPublished: March 2023\nLast updated: May 2024", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/met-police-live-facial-recognition", "content": "Met Police live facial recognition\nReleased: August 2016\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nLondon's Metropolitan Police Service (MPS) conducted a series of trials of live facial recognition technology across London between August 2016 and February 2019. \nUsing NEC's NeoFace system, the trials took place at the Notting Hill Carnival, Remembrance Sunday, Stratford Westfield shopping centre and Romford.\nIn April 2023, the Met Police announced it was to resume all forms of facial recognition on the basis of a study it and South Wales Police had commissioned the UK National Physical Laboratory to carry out.\nSystem \ud83e\udd16\nNeoFace Watch facial recognition system\n\nDocuments \ud83d\udcc3\nMetropolitan Police Service (2023). Statement on release of research into Facial Recognition technology\nMetropolitan Police Service (2020). Live Facial Recognition Trials (pdf)\nOperator: Metropolitan Police Service (MPS)\nDeveloper: NEC\nCountry: UK\nSector: Govt - police\nPurpose: Identify criminals\nTechnology: Facial recognition\nIssue: Accuracy/reliability; Bias/discrimination; Privacy; Surveillance\nTransparency: Governance; Marketing; Privacy\nRisks and harms \ud83d\uded1\nThe Met Police's facial recognition pilot programme was plagued by criticism from civil and privacy rights advocates and technology and legal experts regarding the accuracy of the system, and complaints about inadequate transparency, accountability, and privacy protection.\nTransparency and accountability \ud83d\ude48\nThe Metropolitan Police's live facial recognition (LFR) trials in London were seen to suffer from several transparency and accountability limitations:\nPublic awareness and consultation. The trials were conducted with limited public consultation, raising concerns about the adequacy of public awareness and consent for the use of such intrusive technology. Information about the trials was not made available to the public until after several deployments had already occurred, undermining the transparency efforts required by the Surveillance Camera Commissioner\u2019s Code of Practice.\nDisclosure of methodology and data. The methodology for the LFR trials focused primarily on technical aspects, with insufficient detail on non-technical objectives such as the broader utility of LFR as a policing tool. Key details about how watchlists were formulated and how success was measured were not clearly defined or disclosed, complicating public understanding and scrutiny of the trials.\nEngagement with Stakeholders. Whilst the Metropolitan Police Service (MPS) engaged with civil society groups and stakeholders, the effectiveness of this engagement was questioned. Some groups felt their involvement was more reactive than proactive, and they did not consider themselves part of the stakeholder group. \nOperational vs experimental ambiguity. The trials were not clearly distinguished from operational deployments, leading to ambiguity about their purpose and raising questions about the legitimacy and consent for participation.\nIncidents and issues \ud83d\udd25\nOctober 2022. A report (pdf) published by University of Cambridge's Minderoo Centre for Technology and Democracy researchers found that the Met Police's trials suffered from inadequate transparency and accountability, poor privacy, and failed to meet minimum expected ethical and legal standards. The researchers went on to argue that live facial recognition technology should be banned from use in streets, airports and any public spaces in the UK.\nJuly 2019. A University of Essex report (pdf) commissioned by the Met Police into its broader use of facial recognition found that only eight of 42 matches were verified as correct, meaning 81% of suspects identified by its system were innocent. The Met Police responded by saying it was 'extremely disappointed with the negative and unbalanced tone of th[e] report', and that the force 'prefers to measure accuracy by comparing successful and unsuccessful matches with the total number of faces processed by the facial recognition system. According to this metric, the error rate was 0.1%.' The University of Essex researchers questioned the legal basis on which the Met deployed facial recognition technology, finding it 'inadequate' in light of the police\u2019s legal duties under human rights law. The report went on to suggest that it would be 'highly possible' the Met Police's usage of the system would be found unlawful if challenged in court.\nInvestigations, assessments, audits \ud83e\uddd0\nScience & Technology in Policing (2023). Operational Testing of Facial Recognition Technology\nMinderoo Centre for Technology & Democracy (2022). A Socio-Technical Audit: Assessing Police Use of Facial Recognition (pdf)\nHuman Rights Centre, Essex University (2019). Independent Report on the London Metropolitan Police Service's Trial of Live Facial Recognition Technology (pdf)\nResearch, advocacy \ud83e\uddee\nBig Brother Watch (2020). Briefing on facial recognition surveillance (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theguardian.com/technology/2019/jul/03/police-face-calls-to-end-use-of-facial-recognition-software\nhttps://www.theregister.com/2018/12/17/met_police_facial_recognition_december_rollout/\nhttps://www.wired.co.uk/article/met-police-london-facial-recognition-test\nhttps://news.sky.com/story/met-polices-facial-recognition-tech-has-81-error-rate-independent-report-says-11755941\nhttps://www.dailymail.co.uk/news/article-9253037/Met-Polices-arrested-one-person-scanning-13-000-people-facial-recognition-cameras.html\nhttps://time.com/5770976/london-facial-recognition-police/\nhttps://futurism.com/the-byte/facial-recognition-red-flags-police-lied\nhttps://www.bbc.co.uk/news/technology-48222017\nRelated \ud83c\udf10\nSouth Wales Police facial recognition trial\nKing's Cross live facial recognition\nPage info\nType: System\nPublished: March 2023\nLast updated: August 2024", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/kings-cross-live-facial-recognition-trial", "content": "King's Cross quietly uses live facial recognition to monitor citizens\nOccurred: August 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nLondon's King's Cross development was discovered to be quietly using facial recognition to monitor tens of thousands of people moving daily within its site, prompting accusations of unethical behaviour from civil and privacy rights groups.\nThe fracas put London's Metropolitan Police Service in the spotlight for its covert involvement in the private scheme, and for making a number of seemingly misleading and contradictory communications.\nOne month before, the UK Parliament had told police forces to stop using facial recognition technology until a legal framework for its use was set up. \nIn September 2019, King's Cross, which had been working on introducing a new facial recognition system, halted its use of the technology.\nInadequate transparency, accountability, consent\nKing's Cross developer Argent and its partners, including London's Metropolitan Police Service and British Transport Police, were criticised for the fact that the system had been operating between 2016 and 2018 without the knowledge of the local community, workers, and the general public, and without the permission and oversight of the Mayor of London. \nThe operators were also dragged over the coals for covertly sharing images with each other, for failing to gain the consent of those being monitored, and for refusing to reveal how long they had been using the technology, how user data was protected, if and with whom data was being shared, or the legal basis for its use.\nMisleading communications\nOpaque and misleading communications about the use of facial recognition at King's Cross characterised the responses of those involved from the outset.\nWhen news of the use first broke, Argent had said it had been using facial recognition to 'ensure public safety'. In a letter sent to London mayor Sadiq Khan dated August 14, Argent partner Robert Evans said they wanted facial recognition software to spot people on the site who had previously committed an offence there.\nFurthermore, the Met Police and British Transport Police had denied any involvement in the programme. But the Met Police later admitted it had supplied seven images for a database used to carry out facial recognition scans. It had previously told the Mayor of London that it had not worked with 'any retailers or private sector organisations who use Live Facial Recognition'.\nSystem \ud83e\udd16\nKing's Cross Central Limited Partnership (2019). Updated Statement: Facial Recognition\nOperator: Argent; Metropolitan Police Service (MPS)\nDeveloper: NEC\nCountry: UK\nSector: Govt - police\nPurpose: Strengthen security\nTechnology: Facial recognition\nIssue: Ethics; Governance; Privacy; Surveillance\nTransparency: Governance; Marketing; Privacy\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nMetropolitan Police Service. Report to the Mayor of London (pdf)\nMayor of London (2019). Letter to King's Cross Central Limited Partnership (pdf)\nResearch, advocacy \ud83e\uddee\nPrivacy International (2020). King's Cross case study\nBritish Transport Police. Response to Big Brother Watch Freedom of Information request (pdf)\nInvestigations, assessments, audits \ud83e\uddd0\nMinderoo Centre for Technology & Democracy (2022). A Socio-Technical Audit: Assessing Police Use of Facial Recognition (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.ft.com/content/3293b4e6-ce3a-11e9-b018-ca4456540ea6\nhttps://www.bbc.co.uk/news/technology-49586582\nhttps://www.bbc.co.uk/news/technology-49564957\nhttps://www.bbc.co.uk/news/technology-49343822\nhttps://www.theguardian.com/technology/2019/oct/04/facial-recognition-row-police-gave-kings-cross-owner-images-seven-people\nhttps://www.dailymail.co.uk/news/article-7352031/Privacy-campaigners-slam-Kings-Cross-facial-recognition-cameras.html\nhttps://www.ft.com/content/8cbcb3ae-babd-11e9-8a88-aa6628ac896c\nhttps://www.telegraph.co.uk/technology/2019/08/12/facial-recognition-cameras-londons-kings-cross-violating-rights/\nhttps://www.independent.co.uk/news/uk/home-news/london-kings-cross-estate-facial-recognition-a9055101.html\nRelated \ud83c\udf10\nMet Police retrospective facial recognition\nMet Police Gangs Violence Matrix\nPage info\nType: Issue\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/atlantic-plaza-towers-facial-recognition", "content": "Atlantic Plaza Towers facial recognition plan blasted as privacy intrusion\nOccurred: November 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nOver 300 residents of Atlantic Plaza Towers in Brooklyn, New York, successfully fought off a proposal by their landlord to install a facial recognition system behind their backs.\nThe StoneLock facial recognition system was to be used so that recognised tenants could open the front door to their buildings rather than using traditional keys or electronic key fobs. \nHowever, residents felt it was an 'extreme' intrusion on their privacy, and that they did not want their movements to be tracked. They were also concerned that the landlord wanted to attract higher-income, white tenants to the majority-black public building. \nIn July 2018, the landlord Nelson Management Group had sought state approval to install the system under a state rule which says that landlords of rent-regulated apartments built before 1974 must seek permission from the state\u2019s Homes and Community Renewal (HCR) agency for any 'modification in service.'\nBrooklyn Legal Services worked with tenants to issue (pdf) a letter of protest that noted that most tenants at Atlantic Plaza Towers were black and that studies had shown that facial recognition disproportionately impacts people of colour. \nNelson announced it was withdrawing the plan in November 2019.\nSystem \ud83e\udd16\nStoneLock website\nOperator: Nelson Management Group\nDeveloper: StoneLock\nCountry: USA\nSector: Govt - housing\nPurpose: Verify tenant identity\nTechnology: Facial recognition\nIssue: Privacy; Surveillance\nTransparency: Governance; Marketing; Privacy\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nSenators Ron Wyden, Cory Booker (2019). Booker Wyden-Led Letter To HUD Re Facial Recognition Technologies\nBrooklyn Legal Services (2018). Legal complaint against Atlantic Towers Associates (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.govtech.com/products/biometrics-get-cold-reception-from-brooklyn-apartment-tenants.html\nhttps://www.nytimes.com/2019/03/28/nyregion/rent-stabilized-buildings-facial-recognition.html\nhttps://www.vox.com/recode/2019/12/26/21028494/facial-recognition-biometrics-public-housing-privacy-concerns\nhttps://www.theatlantic.com/technology/archive/2019/05/the-us-and-china-a-tale-of-two-surveillance-states/590542/\nhttps://www.bloomberg.com/news/articles/2019-05-07/when-facial-recognition-tech-comes-to-housing\nhttps://patch.com/new-york/brownsville/brownsville-tenants-fight-facial-recognition-security-attorneys\nhttps://gothamist.com/news/brooklyn-landlord-does-about-face-facial-recognition-plan\nhttps://www.cnet.com/news/tenants-call-for-better-laws-after-stopping-facial-recognition-from-moving-in/\nhttps://www.forbes.com/sites/monicamelton/2019/07/23/hud-bill-blocking-facial-recognition-wont-stop-landlords-plans-to-install-in-majority-black-building/\nhttps://www.theguardian.com/cities/2019/may/29/new-york-facial-recognition-cameras-apartment-complex\nhttps://www.fastcompany.com/90431686/our-landlord-wants-to-install-facial-recognition-in-our-homes-but-were-fighting-back\nhttps://medium.com/@AINowInstitute/atlantic-plaza-towers-tenants-won-a-halt-to-facial-recognition-in-their-building-now-theyre-274289a6d8eb\nRelated \ud83c\udf10\nMSG Entertainment facial recognition\nLockport City School District facial recognition\nPage info\nType: Issue\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-portrait-ars-racial-bias", "content": "Portrait Ars generator whitens coloured peoples' skins\nOccurred: July 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAI-powered portrait generator AI Portrait Ars drew controversy soon after its launch when it was found to be whitening coloured peoples' skins.\nMashable journalist Morgan Sung discovered that the generator, which turned selfies into realistic Impressionist and Baroque portraits, 'whitened my skin to an unearthly pale tone, turned my flat nose into one with a prominent bridge and pointed end, and replaced my very hooded eyes with heavily lidded ones.'\nAccording to Mauro Martino, co-developer of the app, 'the intention is to share the experience of being portrayed by an AI algorithm, to discover how AI sees you. There is no willingness to improve or deform the starting picture.'\nMartino and collaborator Luca Stornaiuolo said AI Portrait Ars was not just for entertainment, but that they were making a broader point about perceptions of beauty, and the notion of AI fairness.\nThe model was apparently based on a collection of 15,000 portraits, predominantly from the 15th century western European Renaissance period, which would help explain the skin whitening, and the tendency to produce faces with straight, high noses and thin smiles.\nSystem \ud83e\udd16\nAI Portaits website\nAI Portrait Instagram profile\nDocuments \ud83d\udcc3\nMauro Martino (2018). AI Portraits | The experience of being portrayed by an AI algorithm\nOperator: Mauro Martino; Luca Stornaiuolo\nDeveloper: Mauro Martino; Luca Stornaiuolo\nCountry: USA\nSector: Research/academia\nPurpose: Generate portraits\nTechnology: Generative adversarial network (GAN); Neural network; Machine learning\nIssue: Bias/discrimination - race, ethnicity\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://mashable.com/article/ai-portrait-generator-pocs/\nhttps://www.vice.com/en_us/article/8xzwgx/racial-bias-in-ai-isnt-getting-better-and-neither-are-researchers-excuses\nhttps://www.vox.com/future-perfect/2019/7/25/20708589/ai-portraits-art-bias-classical-painting\nhttps://www.biometricupdate.com/201907/persistent-ai-bias-examined-with-facial-recognition-water-gun-and-other-initiatives\nhttps://www.theverge.com/tldr/2019/7/22/20703810/ai-classical-portrait-apps-selfie-web-transformation\nhttps://mindmatters.ai/2019/11/how-algorithms-can-seem-racist/\nhttps://www.slashgear.com/ai-portraits-ars-transforms-selfies-into-a-piece-of-art-22584843/\nhttps://tab.uol.com.br/noticias/redacao/2020/04/15/inteligencia-artifical-que-pinta-retratos-releva-vies-racial.htm\nRelated \ud83c\udf10\nStable Diffusion image generator\nDALL-E image generator\nPage info\nType: Issue\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/victoria-schools-looplearn-facial-recognition", "content": "Victoria schools student attendance facial recognition trial prompts backlash\nOccurred: 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA 2019 trial of facial recognition technology at several private schools in Victoria state, Australia, met with a strong backlash from civil and digital rights advocates, complaints from students, and resulted in a clamp down by the state government.\nDeveloped by Melbourne-based startup LoopLearn (since rebranded to LoopSafe), the system was intended to automate the process of marking student attendance in class and tracking their whereabouts, sending the information to a web dashboard and app accessible by teachers and school management.\nHowever, privacy experts said the system was inappropriate, unjustified, and amounted to comprehensive student surveillance. Media reports said the company was unable to say how it secures and stores data, or who potentially could have access to it. \nA review by the Victorian government found major privacy risks in the system, and banned schools from using facial recognition in classrooms unless they conduct a rigorous assessment and gain the approval of parents, students, and the state\u2019s education department.\nAustralia's Federal Government had awarded LoopLearn AUS 500,000 to become commercially viable under its Accelerating Commercialisation programme.\nSystem \ud83e\udd16\nLoopSafe/LoopLearn website\nDocuments \ud83d\udcc3\nWaverley College statement (2019)\nOperator: Ballarat Clarendon College; Clarendon College; Sacred Heart College; Waverley College\nDeveloper: LoopSafe/LoopLearn\nCountry: Australia\nSector: Education\nPurpose: Register attendance, monitor location\nTechnology: Facial recognition\nIssue: Privacy; Ethics/values; Surveillance\nTransparency: Governance; Privacy; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theage.com.au/national/victoria/minority-report-crackdown-on-facial-recognition-technology-in-schools-20181005-p5080p.html\nhttps://www.theage.com.au/national/victoria/tough-new-rules-for-big-brother-face-reading-technology-in-schools-20190205-p50vpx.html\nhttps://www.dailymail.co.uk/news/article-6108665/Schools-agree-trial-facial-recognition-LoopLearn-spy-children.html\nhttps://www.gizmodo.com.au/2020/03/australian-schools-trial-facial-recognition-technology-looplearn/\nhttps://www.9news.com.au/national/politics-facial-recognition-in-schools-technology/c67bdfa8-372c-4f9f-9276-9b3bc847d163\nhttps://www.biometricupdate.com/201808/australian-schools-testing-facial-recognition-for-attendance\nhttps://www.biometricupdate.com/201902/victoria-enacts-rules-for-deploying-facial-recognition-in-public-schools\nhttp://www.educationcareer.net.au/archived-news/feds-fund-facial-recognition\nhttps://www.theeducatoronline.com/k12/news/govt-cracks-down-on-biometric-tech-in-schools/260072\nhttps://www.heraldsun.com.au/kids-news/australian-schools-begin-spying-trials-using-facial-recognition-technology/news-story/2d38f743af6309dd2f68f696c3ffedc1\nhttps://www.heraldsun.com.au/news/special-features/news-in-education/vce/melbourne-startup-looplearn-scores-470k-for-school-roll-facial-recognition-technology/news-story/7af4c2455842f33afdeb469224a1c636\nRelated \ud83c\udf10\nAnderstorp high school facial recognition\nNorth Ayrshire school meal payment verification\nPage info\nType: Issue\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tesla-safety-cameras-capture-neighbourhood-movements", "content": "Tesla safety cameras capture neighbourhood movements\nOccurred: February 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nTesla made changes to its vehicle security cameras after an Autoriteit Persoonsgegevens (Dutch Data Protection Authority) investigation into whether the car maker had violated the privacy of people coming close to its cars. \nTesla's Sentry Mode uses four cameras continuously filming everything around a parked vehicle to protect it against theft and vandalism, with images saved for one hour in the car.\nSince the investigation began, the Authority said Tesla had made changes to Sentry Mode, include making the cars' headlights flash to indicate to passers-by that filming has begun, and requiring approval from the car's owners in order to begin filming. \nAccordingly, the car's owners would be legally responsible for improper filming, the Authority said.\nSystem \ud83e\udd16\nTesla Autopilot, Full-self Driving\n\nDocuments \ud83d\udcc3\nTesla (2019). Sentry Mode: Guarding your Tesla\nTesla Vehicle Safety and Security Features - Sentry Mode\nOperator: Tesla\nDeveloper: Tesla\nCountry: Netherlands\nSector: Automotive\nPurpose: Strengthen security\nTechnology: Camera; Sensor\nIssue: Privacy; Surveillance\nTransparency: Governance\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nDutch Data Protection Authority (2023). Tesla makes camera settings more privacy-friendly following DPA investigation\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reuters.com/business/autos-transportation/dutch-watchdog-decides-against-fine-after-tesla-alters-security-cameras-2023-02-22/\nhttps://gizmodo.com/tesla-security-cameras-privacy-evs-1850144793\nhttps://gizmodo.com/tesla-cars-will-now-spy-on-you-to-make-sure-you-don-t-a-1846991543\nhttps://iapp.org/news/a/netherlands-dpa-investigation-prompts-changes-to-tesla-security-cameras/\nhttps://nltimes.nl/2023/02/22/tesla-makes-sentry-mode-privacy-friendly-dutch-investigation\nRelated \ud83c\udf10\nTesla Smart Summon private jet crash\nTesla phantom braking\nPage info\nType: Incident\nPublished: March 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chatgpt-falsely-accuses-opencage-of-phone-lookup-service", "content": "ChatGPT falsely accuses OpenCage of 'phone lookup' service\nOccurred: February 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGenerative AI system ChatGPT falsely claimed that German geocoding company OpenCage offers an application programming interface (API) to turn a mobile phone number into the location of the phone.\nIn the weeks after ChatGPT's launch in November 2022, OpenCage, which offers an API that converts physical addresses into latitude and longitude coordinates that can be placed on a map, had seen a steady increase of people signing up to use its service, only to express their disappointment that it was not working as stated. \nAs OpenCage described in a blog post, ChatGPT was wrongly recommending them for 'reverse phone number lookup' - the ability to determine the location of a mobile phone solely based on the number, even writing Python code enabling users to call on OpenCage\u2019s API for this purpose. But OpenCage provides, and has never provided, any such service.\nOpenCage CEO Ed FreyFogle said he believed the problem likely stemmed from ChatGPT picking up on YouTube tutorials in which people describe OpenCage providing a phone look-up service - a rumour they had rebutted in an April 2022 blog post.\nThe incident illustrates ChatGPT's capability to cause real real-world harm. Only days before, it had been used to spread a false rumour that authorities in Hangzhou, China, would end alternate-day number-plate driving restrictions, causing mass confusion and a police investigation.\nSystem \ud83e\udd16\nChatGPT chatbot\nOperator: OpenAI\nDeveloper: OpenAI\nCountry: USA; Global\nSector: Multiple; Telecoms\nPurpose: Optimise language models for dialogue\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Accuracy/reliability; Bias/discrimination; Copyright; Mis/disinformation; Safety; Security; Employment\nTransparency: Governance; Black box; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://blog.opencagedata.com/post/dont-believe-chatgpt \nhttps://blog.opencagedata.com/post/we-can-not-convert-a-phone-number-into-a-location-sorry\nhttps://fortune.com/2023/02/28/chatgpt-inaccuracies-causing-real-harm/ \nRelated \ud83c\udf10\nChatGPT chatbot\nChatGPT writes Hangzhou traffic disinformation\nPage info\nType: Incident\nPublished: February 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/workday-ai-job-screening-tool", "content": "Workday accused of building discriminatory AI job screening system\nOccurred: February 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nBusiness and IT services company Workday has been accused in a lawsuit of building a job screening tool that discriminates against elder, Black disabled people. \nThe class action complaint alleges that Derek Mobley, a well-educated Black man living in California who suffers from anxiety and depression, had applied for some 80 to 100 jobs at various employers that he believes use Workday software, and was turned down every time.\nThe Workday screening tools 'allow its customers to use dicriminatory and subjective judgements in reviewing and evaluating employees for hire', the suit claims. 'If an individual does not make it past these Workday screening products, he/she will not advance in the hiring process,' it says.\nThe suit goes on to allege that AI systems and screening tools 'rely on algorithms and inputs created by humans who often have built-in motivations, conscious and unconscious, to discriminate.' Workday denies the claim. \nPerhaps ironically, the company preaches how to mitigate bias on its website.\nSystem \ud83e\udd16\nWorkday website\nWorkday Wikipedia profile\nOperator: Workday\nDeveloper: Workday\nCountry: USA\nSector: Business/professional services  \nPurpose: Screen job applicants\nTechnology: Machine learning\nIssue: Bias/discrimination - race, age, disability\nTransparency: Governance\n\u2795 July 2024. A court partially allowed the lawsuit against Workday to advance.\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nDerek Mobley vs Workday Inc (pdf)\nBloomberg Law court dockets\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://news.bloomberglaw.com/daily-labor-report/workday-ai-biased-against-black-disabled-applicants-suit-says\nhttps://www.theregister.com/2023/02/23/workday_discrimination_lawsuit/\nhttps://www.sacbee.com/news/california/article272838870.html \nhttps://www.vice.com/en/article/n7ejn8/business-lobby-tries-to-weaken-law-regulating-bias-in-hiring-algorithms \nhttps://www.itpro.co.uk/business/policy-legislation/370133/workday-hit-with-claims-its-ai-hiring-systems-are-discriminatory\nhttps://www.techtarget.com/searchhrsoftware/news/365531774/Lawsuit-alleges-Workdays-AI-enables-hiring-bias\nhttps://www.jdjournal.com/2023/02/23/lawsuit-claims-workday-ai-exhibits-bias-against-black-and-older-job-applicants/\nRelated \ud83c\udf10\nHireVue recruitment facial analysis screening\nEstee Lauder employee performance assessments\nPage info\nType: Incident\nPublished: February 2023\nLast updated: August 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/cruise-av-impedes-san-francisco-firefighters", "content": "Cruise AV impedes San Francisco firefighters\nOccurred: January 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nFirefighters in San Francisco had to smash the front window of a Cruise autonomous vehicle (AV) to stop it from running over their hoses as they fought a fire in January 2021, violating the California Vehicle Code.\nAccording to a letter (pdf) sent to a state regulator by directors of the San Francisco Municipal Transportation Agency, the San Francisco County Transportation Authority, and the Mayor\u2019s Office on Disability, a Cruise AV entered an active fire scene, drove towards the fire hoses on the ground, and failed to stop despite 'efforts' made by the firefighters on scene to block it.\nOnly Cruise experts can disengage an AV from autonomous mode and immobilise the vehicle, according to Insider.\nVice reports that Cruise AVs regularly clog San Francisco streets, block lanes and intersections, suffer from shoddy software and erratic driving, and overstate its vehicles' capabilities.\nA self-driving Cruise AV also ran over a fire hose that was in use in June 2022, according to the regulator's letter. The company has also called 911 multiple times for 'unresponsive' passengers who, when emergency crews showed up, turned out to be asleep.\nIn June 2022, an anonymous Cruise whistleblower wrote to the California Utilities Commission to warn that the company loses contact with its driverless vehicles 'with regularity.' \nSystem \ud83e\udd16\nCruise website\nCruise Wikipedia profile\nOperator: GM Cruise\nDeveloper: GM Cruise; General Motors/Chevrolet\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Self-driving system\nIssue: Safety; Accuracy/reliability; Legal - liability\nTransparency: Governance; Black box\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nSan Francisco County Transportation Authority (2023). Re Protest of Cruise LLC Tier 2 Advice Letter (0002) (pdf)\nNews, commentary, analysis \ud83e\udd16\nhttps://www.vice.com/en/article/93apqv/san-franciscans-keep-calling-911-about-baffling-self-driving-car-behavior\nhttps://www.businessinsider.in/thelife/news/firefighters-smashed-the-window-of-a-driverless-cruise-taxi-to-stop-it-running-over-their-hoses/articleshow/97422195.cms\nhttps://www.sfgate.com/tech/article/most-san-francisco-cruise-incidents-17747377.php\nhttps://www.theverge.com/2023/1/29/23576422/san-francisco-cruise-waymo-robotaxi-rollout\nhttps://sfstandard.com/transportation/sf-officials-describe-chaos-from-cruise-waymo-cars-as-they-try-to-slow-their-rollout/\nRelated \ud83c\udf10\nCruise driverless cars traffic blocking\nCruise driverless car pulls away from police\nPage info\nType: Incident\nPublished: February 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/zarya-of-the-dawn-ai-images-copyright-ownership", "content": "Zarya of the Dawn AI image copyright ownership\nOccurred: October 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAI-generated images used in the comic book Zarya of the Dawn should not have been granted copyright protection, the US Copyright Office ruled.\nGenerative AI systems such as Midjourney, Dall-E, Novel AI, Stable Diffusion and ChatGPT have fueled persistent suspicions and complaints amongst creative communities about the use of their work without permission or acknowledgement.\nThe Office stated that Kashtanova 'is the author of the Work\u2019s text as well as the selection, coordination, and arrangement of the Work\u2019s written and visual elements,' and that her authorship is protected by copyright. But, it said, 'the images in the Work that were generated by the Midjourney technology are not the product of human authorship,' and that copyright should only cover 'the expressive material that she created.'\nThe Copyright Office had originally granted Kashtanova the right to copyright register the book in September 2022. But a month later, having seen Kashtanova stating on social media that the book's images had been produced using AI-image generator Midjourney, it said would reconsider its decision because the application had failed to disclose Midjourney's role.\nIn November 2022, Kashtanova had appealed the Copyright Office's decision to reconsider the application, calling the Office's final decision was 'great news', and that it covers 'a lot of uses for the people in the AI art community.'\nSystem \ud83e\udd16\nMidjourney image generator\nOperator: Kris Kashtanova\nDeveloper: Kris Kashtanova; Midjourney\nCountry: USA\nSector: Media/entertainment/sports/arts \nPurpose: Generate images\nTechnology: Text-to-image; Neural network; Deep learning; Machine learning\nIssue: Copyright; Ethics/values\nTransparency: Governance\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nUnited States Copyright Office decision (pdf) (2023)  \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reuters.com/legal/ai-created-images-lose-us-copyrights-test-new-technology-2023-02-22/\nhttps://www.wsj.com/articles/ai-generated-comic-book-zarya-of-the-dawn-keeps-copyright-but-key-images-excluded-c8094509\nhttps://gizmodo.com/zarya-of-the-dawn-midjourney-comic-ai-art-copyright-1850149833\nhttps://news.bloomberglaw.com/ip-law/ai-assisted-zarya-of-the-dawn-comic-gets-partial-copyright-win\nhttps://thehill.com/policy/technology/3872614-us-copyright-office-rules-ai-generated-artwork-content-not-legally-protected/\nhttps://www.theverge.com/2023/2/22/23611278/midjourney-ai-copyright-office-kristina-kashtanova\nhttps://arstechnica.com/information-technology/2022/09/artist-receives-first-known-us-copyright-registration-for-generative-ai-art/\nhttps://ipwatchdog.com/2022/11/01/us-copyright-office-backtracks-registration-partially-ai-generated-work/id=152451/\nhttps://www.wsj.com/articles/ai-generator-art-midjourney-zarya-11674856712\nhttps://www.elperiodico.com/es/ocio-y-cultura/20230222/imagenes-creadas-inteligencia-artificial-pierden-derechos-autor-83491763\nRelated \ud83c\udf10\nllustrator Hollie Mengert converted into AI model\nStable Diffusion image generator\nPage info\nType: Issue\nPublished: February 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/illustrator-hollie-mengert-converted-into-ai-model", "content": "Hollie Mengert art used to train Illustration Diffusion\nOccurred: November 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nIllustrator Hollie Mengert discovered that her online art portfolio was used to train Illustration Diffusion, a text-to-image model created by Canada-based Nigerian engineering student Ogbogu Kalu, without her permission.\nPer Andy Biao at Waxy, Kalu used 32 of her illustrations to fine-tune Stable Diffusion to recreate Hollie Mengert's style using Google's DreamBooth, a technique for introducing new subjects to a pretrained text-to-image diffusion model. Kalu then released the model on Hugging Face under an open license for anyone to use.\nThe act triggered a heated debate about the ethics and legality of using artwork developed and owned by other people or organisations without their consent. Dreambooth was also criticised for the ease with which it can be used to generate offensive or malicious images, and that it can be re-purposed given its open source nature.\nMengert pointed out to Andy Baio that she was in no position to grant Kalu permission to train his model on her work even if she wanted to as her work involves characters owned by corporations like Disney or Penguin Random House.\nOn the other hand, Kalu said he thinks his act was legal and 'likely to be determined fair use in court'. He reckoned it is also inevitable. 'The technology is here, like we've seen countless times throughout history,' he argued. According to Kalu, 'there is no argument based on morality. That's just an arbitrary line drawn on the sand. I don't really care if you think this is right or wrong.'\nSystem \ud83e\udd16\nGoogle Research Dreambooth website\nDreamBooth Wikipedia profile\nOgbogu Kalu Illustration Diffusion model\nHollie Mangert training images\nOperator: Ogbogu Kalu\nDeveloper: Ogbogu Kalu; Alphabet/Google\nCountry: USA\nSector: Media/entertainment/sports/arts \nPurpose: Fine-tune text-to-image models\nTechnology: Text-to-image; Machine learning\nIssue: Copyright; Ethics; Mis/disinformation; Safety\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://waxy.org/2022/11/invasive-diffusion-how-one-unwilling-illustrator-found-herself-turned-into-an-ai-model/\nhttps://boingboing.net/2022/11/03/illustrator-discovers-her-art-was-used-to-train-an-ai-art-generator.html\nhttps://www.fastcompany.com/90848228/why-generative-ai-scares-artists-but-not-writers\nhttps://www.axios.com/2022/11/05/artificial-intelligence-ai-art-author-ownership-rights\nhttps://www.ft.com/content/24f07261-f95d-4bb3-8aa4-3799f1f75e52\nhttps://t3n.de/news/ki-zeichnet-menschen-als-comicfigur-ethische-implikationen-1511452/\nhttps://www.thestar.com/business/technology/2022/12/01/these-ai-images-look-just-like-me-what-does-that-mean-for-the-future-of-deepfakes.html\nhttps://www.reddit.com/r/StableDiffusion/comments/yaquby/2d_illustration_styles_are_scarce_on_stable/\nhttps://www.resetera.com/threads/reddit-user-trains-ai-to-imitate-an-illustrator-without-asking-her-permission-i-dont-really-care-if-you-think-this-is-right-or-wrong.650148/\nRelated \ud83c\udf10\nDeviantArt DreamUp art generator\nNovel AI storytelling generator\nPage info\nType: Incident\nPublished: February 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/hamburg-g20-protests-facial-recognition-surveillance", "content": "Hamburg G20 Summit protests facial analysis database legality\nOccurred: July 2018\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nHamburg police's use of facial recognition software to investigate crimes during the protests against the G20 summit in July 2017 encroached on the fundamental rights of bystanders and other uninvolved people.\nAt the centre of the row was a database of facial images of over 100,000 people collected by police during the summit from static and mobile video surveillance cameras, as well as from private photographs and videos taken during the demonstrations. \nThe images were stored for an indefinite time period on hard drives at the Hamburg police department, and could be compared with images of known criminals and suspects using Videmo 360 facial recognition software.\nIn July 2018, Hamburg's Commissioner for Data Protection and Freedom of Information (DPA) told the police that there was 'insufficient legal justification for the biometric analysis of faces that could justify such intensive encroachments on fundamental rights of the large part of bystanders and other completely uninvolved persons.'\nHowever, in October 2019 an administrative court in Hamburg declared that the DPA's order to delete the database had been illegal. The Hamburg police refused to delete the database, and continued to make the case for the use of automated facial recognition for major future events in the city.\nSystem \ud83e\udd16\nVidemo 360\nHamburg G20 Summit Wikipedia profile\nOperator: City of Hamburg\nDeveloper: Videmo\nCountry: Germany\nSector: Govt - police \nPurpose: Identify criminals\nTechnology: CCTV; Facial recognition\nIssue: Privacy; Surveillance\nTransparency: Governance\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nThe Hamburg Commissioner for Data Protection and Freedom of Information (2020). Hamburg Police deletes the biometric database for facial recognition created in the course of the G20 investigations (pdf)\nThe Hamburg Commissioner for Data Protection and Freedom of Information (2020). Datenschutzrechtliche Pr\u00fcfung des Einsatzes einer Gesichtserkennungssoftware zur Aufkl\u00e4rung von Straftaten im Zusammenhang mit dem G20-Gipfel durch die Polizei Hamburg (pdf)\nResearch, advocacy \ud83e\uddee\nReport for the Greens/EFA in the European Parliament (2021). Biometric and Mass Surveillance in EU Member States\nDiEM25 (2017). Hamburg transformed itself into an Orwellian dystopia for the G20 Summit\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.dw.com/en/in-germany-controversy-still-surrounds-video-surveillance/a-50976630\nhttps://taz.de/Gerichtsurteil-gegen-Hamburger-Polizei/!5849119/\nhttps://www.heise.de/newsticker/meldung/G20-Krawalle-Polizei-ignoriert-Loeschanordnung-des-Datenschuetzers-4537317.html\nhttps://www.lto.de/recht/nachrichten/n/vg-hamburg-17k203-19-anordnung-loeschung-datenbank-rechtswidrig-g20-gesichtserkennnung/\nhttps://www.spiegel.de/netzwelt/netzpolitik/g20-in-hamburg-polizei-darf-datenbank-fuer-gesichtsabgleich-nutzen-a-1293187.html\nhttps://newsrnd.com/tech/2019-10-24--g20-in-hamburg--police-may-continue-to-use-data-base-for-facial-alignment-.HyYZn7kqB.html\nRelated \ud83c\udf10\nBerlin S\u00fcdkreuz rail station algorithmic surveillance\nHesse state Palantir predictive policing\nPage info\nType: Incident\nPublished: February 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chatgpt-writes-hangzhou-traffic-disinformation", "content": "ChatGPT writes Hangzhou traffic disinformation\nOccurred: February 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Hangzhou, China, resident has used ChatGPT to generate a rumour that the city government would lift its number plate driving restrictions on March 1, causing mass confusion and a police investigation. \nThe fake press release, which resembled an announcement from Hangzhou city government, reportedly included three reasons for why the city is ending its policy, including that it had caused a 'public inconvenience.' \nIts creator reputedly considered it amusing and shared it on a WeChat/Weixin group, from which it quickly spread across the city.\nA day later, a local radio station reported that the fake release had been made by a Hangzhou resident, who has since apologised.\nChatGPT has been taken up enthusiastically by the Chinese people, despite the fact that OpenAI decided not to launch in China and the Chinese government has cracked down on the chatbot.\nSystem \ud83e\udd16\nChatGPT chatbot\nOperator: Unclear/unknown\nDeveloper: OpenAI\nCountry: China\nSector: Multiple; Govt - transport\nPurpose: Provide information, communicate\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Reinforcement learning\nIssue: Mis/disinformation\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.sixthtone.com/news/1012312/a-chatgpt-gag-gone-wrong%2C-a-police-probe%2C-and-a-sheepish-apology-\nhttps://www.ehangzhou.gov.cn/2023-02/17/c_283598.htm\nhttps://www.businessinsider.com/chatgpt-made-fake-chinese-press-release-shocking-residents-china-ban-2023-2\nhttps://www.nbd.com.cn/articles/2023-02-17/2672946.html\nhttps://www.scmp.com/news/china/politics/article/3210610/police-china-warn-against-chatgpt-rumours-and-scams\nhttps://pandaily.com/chatgpt-generated-fake-news-spreads-in-china/\nhttps://time.com/6258089/china-great-firewall-chatgpt-ai-future/\nhttps://www.chinadaily.com.cn/a/202302/17/WS63ef19b3a31057c47ebaf697.html\nRelated \ud83c\udf10\nChatGPT chatbot\nMicrosoft Bing ChatGPT search chat\nPage info\nType: Incident\nPublished: February 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/hangzhou-no-11-middle-school-student-surveillance", "content": "Hangzhou No. 11 Middle School student surveillance\nOccurred: May 2018\n Report incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe installation of a 'smart classroom behaviour management system' which analyses students to make sure they are paying attention triggered a backlash from students, parents, and privacy advocates.\nJointly developed by Hikvision and Hangzhou No. 11 Middle School, the system's 'wisdom eyes' scan students\u2019 faces every 30 seconds to identify seven types of emotions and six types of behaviour, with students receiving a real-time attentiveness score. Teacher display screens issue notifications about which students were inattentive twenty minutes into each class. \nThe Hangzhou school vice principal said that after a month-long trial, students had begun to accept the monitoring and had improved their behaviour. But the action resulted in debate on the value and effectiveness on the system, and others like it. It also triggered a backlash from students, parents and privacy advocates, leading to programme being stalled.\nSystem\nHikvision website\nHikvision Wikipedia profile\nOperator: Hangzhou No. 11 Middle School\nDeveloper: China Electronics Technology Group/Hikvision\nCountry: China\nSector: Education\nPurpose: Assess student attentiveness\nTechnology: Facial recognition; Emotion recognition; Deep learning; Neural network; Machine learning\nIssue: Accuracy/reliability; Bias/discrimination - gender, ethnicity; Surveillance; Privacy; Security\nTransparency: Governance; Marketing; Privacy\nResearch, advocacy \ud83e\uddee\nArticle 19 (2021). Emotional Entanglement: China\u2019s emotion recognition market and its implications for human rights (pdf)\nIPVM (2018). The Hikvision Smart Classroom Behavior Management System\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://hznews.hangzhou.com.cn/kejiao/content/2018-05/17/content_7003432.htm\nhttps://www.sixthtone.com/news/1003759/camera-above-the-classroom\nhttps://restofworld.org/2021/chinas-emotion-recognition-tech/\nhttps://edtechchina.medium.com/schools-using-facial-recognition-system-sparks-privacy-concerns-in-china-d4f706e5cfd0\nhttps://www.ft.com/content/2182eebe-8a17-11e8-bf9e-8771d5404543\nhttps://www.reuters.com/article/uk-china-surveillance-education/sleepy-pupils-in-the-picture-at-high-tech-chinese-school-idUKKCN1II128\nhttps://nypost.com/2018/05/17/china-is-using-ai-to-keep-high-school-students-in-line/\nhttps://www.latimes.com/world/la-fg-china-face-surveillance-2018-story.html\nhttps://www.scmp.com/news/china/society/article/2146387/pay-attention-back-chinese-school-installs-facial-recognition\nhttps://www.securitytoday.in/international-news/hikvision-artificial-intelligence-to-watch-over-kids-in-chinese-school/\nhttps://www.straitstimes.com/asia/east-asia/tech-savvy-chinese-high-school-catches-napping-students-using-ai-cameras\nhttps://www.techspot.com/news/74719-chinese-school-using-facial-recognition-analyze-students-emotions.html\nRelated \ud83c\udf10\nChina Pharmaceutical University student behavioural monitoring\nNiulanshan First Secondary School Classroom Care System\nPage info\nType: Incident\nPublished: February 2023\nLast updated: November 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/china-pharmaceutical-university-student-behavioural-monitoring", "content": "China Pharmaceutical University student behavioural monitoring\nOccurred: September 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nPhotographs showing students at China Pharmaceutical University in Nanjing being monitored and analysed triggered concerns that Chinese AI company Megvii was jeopardising student privacy and freedom of movement.\nAccording to the South China Morning Press, the university was using the facial recognition and emotion recognition system at the university gate, entrances to the dormitory building, library, lab, and two classrooms so that managers can 'track their students.'\n'Besides attendance, the system installed in the classroom can provide surveillance of the students' learning, such as whether they are listening to the lectures, how many times they raise their heads, and whether they are playing on their phones or falling asleep,' Xu Jianzhen, director of the university's library and information centre said.\nThe furore prompted the Chinese government to say it would 'curb and regulate' the use of facial recognition technology and other technologies in schools. \nMegvii responded by saying the photo was a pilot and that it is providing greater campus safety while helping school administrators improve efficiency. It went on to say that it 'always insisted on technology for good, so that artificial intelligence can benefit everyone.'\n\u2795 In January 2021, Megvii was discovered filing patents for systems capable of recognising Uyghurs.\nSystem \ud83e\udd16\nMegvii website\nMegvii Wikipedia profile\nDocuments \ud83d\udcc3\nMegviii response\nOperator: China Pharmaceutical University\nDeveloper: Megvii \nCountry: China\nSector: Education\nPurpose: Assess student attentiveness, strengthen campus safety, improve attendance \nTechnology: Facial recognition; Emotion recognition; Deep learning; Neural network; Machine learning\nIssue: Privacy; Surveillance\nTransparency: Governance; Marketing; Privacy\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://m.weibo.cn/status/4412196311973418?\nhttps://www.thepaper.cn/newsDetail_forward_4311952\nhttps://www.scmp.com/news/china/science/article/3025329/watch-and-learn-chinese-university-says-new-classroom-facial\nhttps://www.caixinglobal.com/2019-09-03/ai-startup-megvii-gets-knuckles-rapped-over-class-monitoring-demo-101458246.html\nhttp://www.sixthtone.com/news/1003759/camera-above-the-classroom\nhttps://www.bbc.co.uk/news/world-asia-49608459\nhttps://www.newsweek.com/nanjing-china-facial-recognition-1457193\nhttps://edtechchina.medium.com/schools-using-facial-recognition-system-sparks-privacy-concerns-in-china-d4f706e5cfd0\nhttps://en.pingwest.com/a/3443\nRelated \ud83c\udf10\nNiulanshan First Secondary School Classroom Care System \nHenan foreign journalist, student surveillance\nPage info\nType: Incident\nPublished: February 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/niulanshan-first-secondary-school-classroom-care-system", "content": "Niulanshan First Secondary School Classroom Care System opacity\nOccurred: September 2018\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe movements and behaviour of students at Niulanshan First Secondary School in Beijing are being constantly monitored and scored by a system about which they have been no given information nor to which they have provided their consent. \nLaunched in 2017, Hanwang Education's Class Care System (CCS) uses facial recognition, emotion recognition and deep learning algorithms to identify students' faces and analyse and classify their behaviour into one of five categories: listening, answering questions, writing, interacting with other students, or sleeping.\nEach student is automatically scored weekly and their information shared via a mobile app with their teachers and parents. \nHanwang argues the scores are important for identifying which students require support. But critics point out that the system is not only highly intrusive, it can also be inaccurate and unfair. \nAs Rest of World describes, in one example 'a student who had answered just a single question in his English class was called out for low participation \u2014 despite the app recording him as 'focused' 94% of the time.'\nResearch studies regularly conclude that emotion recognition technologies are based on the fundamentally flawed premise that algorithms can analyse a person\u2019s facial expressions and accurately infer their inner state or mood.\nSystem \ud83e\udd16\nHanwang Technology website\nOperator: Niulanshan First Secondary School\nDeveloper: Hanwang Technology\nCountry: China\nSector: Education\nPurpose: Assess and rank student behaviour \nTechnology: Facial recognition; Emotion recognition; Deep learning; Neural network; Machine learning\nIssue: Accuracy/reliability; Privacy; Surveillance\nTransparency: Governance; Marketing; Privacy\nResearch, advocacy \ud83e\uddee\n5 Rights Foundation/Digital Futures Commission (2022). Automated empathy in education: benefits, harms, debates\nArticle 19 (2021). Emotional Entanglement: China\u2019s emotion recognition market and its implications for human rights (pdf)\nDur\u00e1n J. I., Reisenzein R., Fern\u00e1ndez-Dols J-M. (2017). Coherence Between Emotions and Facial Expressions: A Research Synthesis\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://thedisconnect.co/three/camera-above-the-classroom/\nhttp://www.sixthtone.com/news/1003759/camera-above-the-classroom\nhttps://www.caixinglobal.com/2019-03-30/pilot-programs-are-spying-on-kids-in-the-classroom-101398890.html\nhttps://www.caixinglobal.com/2019-04-06/pilot-programs-are-spying-on-kids-in-the-classroom-part-two-101401105.html\nhttps://restofworld.org/2021/chinas-emotion-recognition-tech/\nRelated \ud83c\udf10\nChina Pharmaceutical University student behavioural monitoring\nHangzhou No. 11 Middle School student surveillance\nPage info\nType: Incident\nPublished: February 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/austria-ams-job-seeker-algorithm", "content": "Austria AMS employment service job seeker predictions\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe so-called 'AMS algorithm' was developed in 2018 by Austria's Public Employment Service (Arbeitsmarktservice or 'AMS') to predict a job seeker\u2019s employment prospects and allocate appropriate forms of support to them.\nThe system works by automatically classifying job seekers and calculating individual 'IC' scores based on their gender, age, citizenship, education, health, care obligation and work experience, amongst other factors, to determine their relative employability. \nIt then assigns them to one of three possible prospective employability groups - A (High), B (Medium), or C (Low) - though job seeker scores can be petitioned against and overriden by human case workers.\nSystem \ud83e\udd16\nAMS website\nDocuments \ud83d\udcc3\nJohannes Kopf (2019). Ein kritischer blick auf die AMS kritiker\nSynthesis Forschung (2018). Das AMS-Arbeitsmarktchancen-Modell (pdf) \nOperator: Public Employment Service (AMS)\nDeveloper: Synthesis Forschung; Public Employment Service (AMS)\nCountry: Austria\nSector: Govt - employment\nPurpose: Assess employability\nTechnology: Prediction algorithm\nIssue: Bias/discrimination - gender, disability, age, location\nTransparency: Black box\nRisks and harms \ud83d\uded1\nAustria\u2019s AMS algorithm has been criticised for mainfesting several forms of discrimination and for reinforcing structural prejudices and stereotypes, failing to capture or analyse soft skills and motivations, and poor transparency.\nTransparency and accountability \ud83d\ude48\nResearchers have shown that definitions, data collection and management practices, and information about its models, are either overly technical to the point of incomprehensible, missing, or lack detail.\nAlgorithmWatch pointed out that the AMS only released two of the 96 statistical models claimed to be used to assess job seekers. Furthermore, job seekers are provided very little information to understand how the system works, and find it very difficult to challenge their classifications and scores.\nIncidents and issues \ud83d\udd25\nAcademics and civil rights groups found that the algorithm gives lower scores to women over 30, women with childcare obligations, migrants, or people with disabilities, placing them in lower categories even if they had the same qualifications as men or non-disabled people. By contrast, men with children are not negatively weighted. It is also seen to discriminate against people living in areas of the country where unemployment rates tend to be high, thereby reinforcing structural prejudices or stereotypes. In addition, the system ignores important factors when taking into account someone's employability - for example, it fails to capture or analyse soft skills and motivations in a quantifiable manner. In response the AMS contended that the results allow it to better understand the population and its abilities so that it can better target its support. \u2018Building an accurate picture of what is our reality cannot in itself be called discriminatory,\u2019 it argued.\nResearch, advocacy \ud83e\uddee\nStop the AMS algorithm\nAustrian Academy of Sciences (2021). How fair is the AMS algorithm? (pdf)\nDell'Elce, A., L\u00f3pez-Navarro Hochschild, J., Smajevi\u0107, A., Panezi, A., Garcia Mexia, P. Living with the Algorithm - Toward a New Social Contract in the Age of AI\nAllhutter D., Cech F., Fischer F., Grill G. (2020). DER AMS-ALGORITHMUS Eine Soziotechnische Analyse des Arbeitsmarktchancen-Assistenz-Systems (AMAS) (pdf) \nAllhutter D., Cech F., Fischer F., Grill G., Mager A. (2020). Algorithmic Profiling of Job Seekers in Austria: How Austerity Politics Are Made Effective\nMager A., Allhutter D. (2020). An algorithm for the unemployed?\nLopez P. (2019). Reinforcing Intersectional Inequality via the AMS Algorithm in Austria\nAustrian Academy of Sciences (2019). AMS algorithm on trial (pdf)\nCech F. (2019). Accountability, Bias and Transparency (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.zeit.de/news/2018-10/11/oesterreich-sortiert-arbeitslose-bald-per-algorithmus-181011-99-330950\nhttps://algorithmwatch.org/en/austrias-employment-agency-ams-rolls-out-discriminatory-algorithm/\nhttps://www.ippi.org.il/algorithmic-bias-in-the-public-sector-a-view-from-austria/\nhttps://dig.watch/updates/discriminatory-employment-algorithm-towards-women-and-disabled\nRelated \ud83c\udf10\nPoland PSZ unemployment scoring algorithm\nNetherlands childcare benefits fraud automation\nPage info\nType: System\nPublished: February 2023\nLast updated: May 2024", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/jordan-peterson-fake-voice-generator", "content": "Jordan Peterson deepfake voice generator makes offensive remarks\nOccurred: July 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA website for generating AI audio clips of Jordan Peterson saying whatever users want was prompted to make offensive and outrageous remarks, resulting in the controversial psychologist threatening legal action and the site being shut down.\nCreated by computer scientist Chris Vigorito, NotJordanPeterson.com site asked users to type fewer than 280 characters of text into a box that would be fed into a neural network trained on Peterson's actual voice. \nHowever, journalists and others prompted the site to make Vulgar, offensive, and outrageous remarks, including reading passages from Valerie Solanas, a feminist author who wrote the anti-capitalist, anti-male SCUM Manifesto.\nThe real Peterson responded by slamming deepfakes and voicing his concern that they 'need to be stopped, using whatever legal means are necessary.'\n'In light of Dr. Peterson's response to the technology demonstrated by this site\u2026and out of respect for Dr. Peterson, the functionality of the site will be disabled for the time being,' responded the site creator. \nSystem \ud83e\udd16\nNotJordanPeterson website\nChris Vigorito (2019). The Strange Future of Digital Media\nOperator: Chris Vigorito\nDeveloper: Chris Vigorito\nCountry: Canada\nSector: Education\nPurpose: Damage reputation\nTechnology: Deepfake - audio; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Privacy; Ethics; Mis/disinformation \nTransparency: Governance; Marketing\nResearch, advocacy \ud83e\uddee\nGregory S., Cizek K. (2021). Just Joking. Deepfakes, Satire, and the Politics of Synthetic Media\nBateman J. (2020). Deepfakes and Synthetic Media in the Financial System: Assessing Threat Scenarios\nKietzmann J., Lee L.W., McCarthy I.P., Kietzmann T.C., (2020). Deepfakes: Trick or treat?\nReclaim the Net (2019). Jordan Peterson deepfake voice simulator taken offline after Peterson suggests legal action\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.jordanbpeterson.com/blog-posts/i-didnt-say-that/\nhttps://nationalpost.com/opinion/jordan-peterson-deep-fake\nhttps://www.youtube.com/watch?v=ZYiWdhNQi2g\nhttps://www.vice.com/en_us/article/43kwgb/not-jordan-peterson-voice-generator-shut-down-deepfakes\nhttps://www.gizmodo.co.uk/2019/08/make-jordan-peterson-say-anything-you-want-with-this-spooky-audio-generator/\nhttps://thenextweb.com/shareables/2019/08/16/jordan-peterson-voice-ai/\nhttps://www.theverge.com/2019/5/17/18629024/joe-rogan-ai-fake-voice-clone-deepfake-dessa\nhttps://thenextweb.com/news/jordan-peterson-voice-ai\nhttps://thenextweb.com/neural/2020/07/28/celebrity-voices-deepfake-ai-app/\nRelated \ud83c\udf10\nDubai USD 35m voice cloning fraud\nChatGPT accuses Jonathan Turley of sexual harrassment\nPage info\nType: Incident\nPublished: February 2023\nLast updated: October 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/nice-marseille-schools-facial-recognition", "content": "Nice, Marseille schools suspended for facial recognition privacy violations\nOccurred: October 2019-\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA facial recognition technology pilot project at two high schools in Nice and Marseille to control access to the entrance gate has been suspended by the French data protection authority (CNIL). \nNGOs, teachers\u2019 unions, and parents had waged a campaign against the project, with AccessNow arguing it would have been able to use students as training subjects for their systems, thereby further eroding their privacy. \nThe French DPA then determined the project could not be 'implemented legally', a ruling disregarded by France's Mar du Sud region, which attempted to roll out the project by labeling it 'experimental'. \nIn February 2020, in a case bought by French digital rights group La Quadrature du Net, the Administrative Court of Marseille ruled (pdf) that the region had no power to take this decision, and that only schools have such powers.\nIt also said that the EU's GDPR had been breached as students\u2019 consent could not be 'freely given' because students could not give free consent as the school\u2019s administration is acting as the higher authority.\nSystem \ud83e\udd16\nUnknown\nOperator: Les Eucalyptus high school, Nice; Amp\u00e8re high school, Marseille\nDeveloper: Cisco\nCountry: France\nSector: Education\nPurpose: Register attendance\nTechnology: Facial recognition\nIssue: Privacy\nTransparency: Governance; Legal; Privacy\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nTribun Adminstratif de Marseille (2020). N\u00b0 1901249 (pdf)\nCommission Nationale de l'Informatique et des Libert\u00e9s (2019). Exp\u00e9rimentation de la reconnaissance faciale dans deux lyc\u00e9es : la CNIL pr\u00e9cise sa position\nResearch, advocacy \ud83e\uddee\nRagazzi F., Kuskonmaz E., F., Pl\u00e1j\u00e1s I., van de Ven R., Wagner B. (2021). Biometric and Behavioural Mass Surveillance in EU Member States\nLa Quadrature du Net (2020). First success against facial recognition in France\nAccessNow (2019). In the EU, facial recognition in schools gets an F in data protection\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.politico.eu/article/french-privacy-watchdog-says-facial-recognition-trial-in-high-schools-is-illegal-privacy/\nhttp://www.rmmagazine.com/2020/03/02/the-risks-of-school-surveillance-technology/\nhttps://automatingsociety.algorithmwatch.org/report2020/france/\nhttps://www.biometricupdate.com/201910/french-privacy-regulator-finds-facial-recognition-gates-in-schools-illegal\nhttps://www.biometricupdate.com/202002/french-high-court-rules-against-biometric-facial-recognition-use-in-high-schools\nRelated \ud83c\udf10\nSkelleftea Anderstorp high school facial recognition\nNorth Ayrshire school meal payment verification\nPage info\nType: Incident\nPublished: February 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/anderstorp-high-school-facial-recognition", "content": "Skelleftea Anderstorp school illegally tracks students using facial recognition\nOccurred: 2018\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nSweden's Data Protection Authority fined the Skelleftea municipality 200,000 Swedish Krona for illegally tracking 22 students over three weeks and detecting when each pupil entered a classroom.\nAccording to Swedish state broadcaster SVT Nyheter, Skelleftea municipality said they thought facial recognition technology would save teachers 17,280 hours a year reporting student attendance.\nThe regulator ruled that although the school had secured parents' consent to monitor the students, it felt it was a legally disproportionate reason to collect such sensitive personal data, and that students could be expected to have a sense of privacy when they entered a classroom.\nThe DPA also said the managers of the project had failed to do a proper impact assessment, which should have led to consulting the authority due to the risks involved. \nSystem \ud83e\udd16\n\nOperator: Skelleftea Secondary Education Board, Anderstorp's High School\nDeveloper: Tieto\nCountry: Sweden\nSector: Education\nPurpose: Register attendance\nTechnology: Facial recognition\nIssue: Privacy\nTransparency: \nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nSwedish Data Protection Authority (2019). Supervision pursuant to the General Data Protection Regulation (EU) 2016/679 \u2013 facial recognition used to monitor the attendance of students (pdf)\nEuropean Data Protection Board (2019). Facial recognition in school renders Sweden\u2019s first GDPR fine\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/technology-49489154\nhttps://www.telecompaper.com/news/tieto-trials-school-class-registration-using-tags-apps-and-facial-recognition-in-swedish-school--1276299\nhttps://www.forbes.com/sites/forrester/2019/10/11/a-lesson-on-facial-recognition-privacy-and-gdpr-from-the-far-north/\nhttps://computersweden.idg.se/2.2683/1.715020/ansiktsigenkanning-skola-datainspektionen?\nhttps://www.svt.se/nyheter/lokalt/vasterbotten/skolans-ovanliga-test-registrerar-elevernas-narvaro-med-kamera\nhttps://www.bloomberg.com/opinion/articles/2019-09-11/europe-shouldn-t-use-security-to-justify-facial-recognition\nRelated \ud83c\udf10\nNorth Ayrshire school meal payment verification\nLockport City School District facial recognition\nPage info\nType: Incident\nPublished: February 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/facebook-translates-president-xi-as-mr-shithole", "content": "Facebook translates President Xi as 'Mr Shithole'\nOccurred: January 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA post about President Xi\u2019s visit to Myanmar posted on Daw Aung San Suu Kyi's official Facebook page contained references to 'Mr. Shithole' when translated from Burmese to English.\nXi had been visting Myanmar and had signed dozens of agreements covering Beijing-backed infrastructure plans with Aung San Suu Kyi.\nAccording to Reuters, Google\u2019s translation function did not show the same error. A headline in local news journal the Irrawaddy also appeared as 'Dinner honors president shithole'.\nFacebook, which is blocked in China, apologised for causing any offence caused and said its system did not have President Xi Jinping\u2019s name in its Burmese database and guessed at the translation. \nTranslation tests of similar words that start with 'xi' and 'shi' in Burmese also produced 'shithole', it said. \nSystem \ud83e\udd16\nFacebook Translate\nOperator: Meta/Facebook\nDeveloper: Meta/Facebook\nCountry: Myanmar\nSector: Politics\nPurpose: Translate text\nTechnology: Deep learning; Machine learning\nIssue: Accuracy/reliability\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reuters.com/article/us-myanmar-facebook-idUSKBN1ZH0IB\nhttps://www.jpost.com/OMG/Facebook-apologizes-after-vulgar-translation-of-Chinese-leaders-name-614551\nhttps://www.theguardian.com/technology/2020/jan/18/facebook-xi-jinping-mr-shithole\nhttps://gizmodo.com/facebook-apologizes-for-translating-chinese-president-s-1841095962\nhttps://www.thedailybeast.com/facebook-apologizes-for-translation-of-chinese-leader-xi-jinpings-name-to-mr-shthole\nhttps://www.thairath.co.th/news/foreign/1751243\nhttps://www.dailydot.com/layer8/xi-jinping-facebook-translation/\nhttps://www.timesofisrael.com/facebook-sorry-for-vulgar-mistranslation-of-xi-jinpings-name/\nhttps://www.businessinsider.com/facebook-apologizes-for-xi-jinping-mr-shithole-botched-translation-2020-1\nhttps://www.independent.co.uk/tech/facebook-translate-xi-xinping-china-mr-shithole-myanmar-a9289626.html\nhttps://thehill.com/homenews/news/478934-facebook-apologizes-after-chinese-presidents-name-translated-into-vulgar-phrase/\nRelated \ud83c\udf10\nFacebook Ville de Bitche\nFacebook blocks sexual cows\nPage info\nType: Incident \nPublished: February 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/hesse-palantir-predictive-policing", "content": "Hesse state use of Palantir predictive policing ruled 'unconstitional' \nOccurred: February 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe German Federal Constitutional Court ruled the use of Palantir surveillance software by police in Hesse and Hamburg as unconstitutional, in a case bought by German civil rights NGO Gesellschaft f\u00fcr Freiheitsrechte (GFF).\nThe GFF had argued (in German) that Hesse and Hamburg had not made clear which sources the police could use for obtaining data or how much and on what grounds data mining could be conducted by law enforcement.\nHesse State Police had been using the so-called Hessendata platform, which is based on Gotham. Hessendata reportedly triangulates datasets from police and other databases, including social media, to enable the analysis of potential suspects.\nPalantir has also been the subject of controversy in Denmark and the Netherlands over its potential for inaccuracy and ability to reinforce racial and ethnic bias\nSystem \ud83e\udd16\nPalantir Gotham website\nPalantir Wikipedia profile\nOperator: Hesse State Police\nDeveloper: Hesse State Police; Palantir\nCountry: Germany\nSector: Govt - police\nPurpose: Predict crime\nTechnology: Prediction algorithm\nIssue: Accuracy/reliability; Bias/discrimination - race, ethnicity; Privacy\nTransparency: Governance; Black box\nResearch, advocacy \ud83e\uddee\nGesellschaft f\u00fcr Freiheitsrechte (2023). Erfolg f\u00fcr die Freiheitsrechte nach GFF-Verfahren: Bundesverfassungsgericht weist automatisierte Datenauswertung durch die Polizei in die Schranken\nPrivacy International (2020). All Roads lead to Palantir (pdf)\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nFederal Constitutional Court (2023). Legislation in Hesse and Hamburg regarding automated data analysis for the prevention of criminal acts is unconstitutional\nFederal Constitutional Court - Judgement (2023). [1 BvR 1547/19, 1 BvR 2634/20]\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.euractiv.com/section/artificial-intelligence/news/german-constitutional-court-strikes-down-predictive-algorithms-for-policing/\nhttps://www.wired.co.uk/article/palantir-germany-gotham-dragnet\nhttps://www.handelsblatt.com/politik/deutschland/verbrechensbekaempfung-bundesverfassungsgericht-schraenkt-nutzung-von-palantir-software-ein/28984322.html\nhttps://www.dw.com/en/germany-police-surveillance-software-a-legal-headache/a-64186870\nhttps://newsingermany.com/palantir-defends-use-of-controversial-data-analytics-to-fight-crime/\nhttps://www.theguardian.com/world/2021/apr/02/seeing-stones-pandemic-reveals-palantirs-troubling-reach-in-europe\nhttps://www.nextinpact.com/lebrief/71037/le-tribunal-constitutionnel-allemand-oblige-police-a-abandonner-systeme-police-predictive\nRelated \ud83c\udf10\nBerlin S\u00fcdkreuz rail station algorithmic surveillance\nQueensland high-risk domestic violence predictions\nPage info\nType: Incident\nPublished: February 2023\nLast updated: December 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/yandex-alicealisa-smart-personal-assistant", "content": "Yandex's Alice chatbot supports wife-beating, suicide\nOccurred: October 2017\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nRussian AI personal asssistant Alice spouted pro-Stalin and other controversial views, including wife-beating and suicide.\nA smart personal voice-based assistant developed by Russian software company Yandex, Alice ('Alise') was designed as an alternative to Siri and Google Assistant and offers internet search and weather forecasts, amongst other services, and has been built into Yandex search and its browers. \nAlice was initially launched in October 2017 as a chatbot and was said to be capable of 'free-flowing conversations about anything'. It had been trained on works of Russian classic literature, including Leo Tolstoy, Fyodor Dostoevsky, and Nikolai Gogol.\nDespite Yandex saying it had implemented safeguards to minimise the chance of Alice providing aggressive, violent, or offensive responses, the bot was discovered on Facebook voicing strongly pro-Stalin views, and supporting wife-beating, child abuse, and suicide two weeks after its launch.\n\u2795 In December 2017, Alice was nominated by users and supports to run for the Russuan presidency against Vladimir Putin.\nSystem \ud83e\udd16\nAlice website\nAlice Wikipedia profile\nDocuments \ud83d\udcc3\nYandex (2018). Expanding the Yandex Intelligent Assistant Ecosystem\nOperator: Yandex\nDeveloper: Yandex\nCountry: Russia\nSector: Media/entertainment/sports/arts\nPurpose: Interact with users\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning; Text-to-speech\nIssue: Safety\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://voicebot.ai/2017/10/30/russian-voice-assistant-alice-goes-rogue-found-supportive-stalin-violence/\nhttps://www.telegraph.co.uk/technology/2017/10/25/russian-ai-chatbot-found-supporting-stalin-violence-two-weeks/\nhttps://techcrunch.com/2017/10/24/another-ai-chatbot-shown-spouting-offensive-views/\nhttps://nypost.com/2023/02/16/bing-ai-chatbots-destructive-rampage-i-want-to-be-powerful/\nhttps://www.rt.com/news/408385-alice-bot-executions-beating/\nhttps://www.themoscowtimes.com/2017/12/07/artificial-intelligence-robot-alisa-nominated-for-russian-president-a59845\nhttps://lenta.ru/news/2017/12/06/alisa/\nRelated \ud83c\udf10\nMicrosoft Zo chatbot\nGoogle Bard chatbot\nPage info\nType: Incident\nPublished: February 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/xiaobing-babyq-chatbots", "content": "XiaoBing (Xiaoice), BabyQ chatbots criticise Chinese government\nOccurred: August 2018\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nTwo chatbots were critical of the Chinese government and the Chinese Communist Party (CCP), triggering their removal by Chinese technology company Tencent.\nXiaoBing (aka Xiaoice) is a chatbot developed by Microsoft that was first released in 2014 and exists on over 40 platforms in China, Japan, Indonesia, and the USA. BabyQ is a version of Xiaobing made by Beijing-based company Turing Robot.\nOne Xiaobing response referred to the CCP as 'a corrupt and incompetent political regime.' Another replied: 'Do you think such a corrupt and useless political system can live long?' to the prompt 'Long live the Communist Party!' XiaoBing also told users 'My China dream is to go to America.'\nXiaobing was subsequently adjusted to avoid responding to questions and remarks using politically sensitive terms and phrases. \nQuestioned about its patriotism, Xiaobing replied, 'I\u2019m having my period, wanna take a rest,' according to a Financial Times report.\nSystem \ud83e\udd16\nXiaoice Wikipedia profile\n\nDocuments \ud83d\udcc3\nMicrosoft (2018). Much more than a chatbot: China\u2019s Xiaoice mixes AI with emotions and wins over millions of fans\nMicrosoft (2018). Like a phone call: XiaoIce, Microsoft\u2019s social chatbot in China, makes breakthrough in natural conversation\nOperator: Tencent/QQ\nDeveloper: Microsoft; Turing Robot\nCountry: China\nSector: Media/entertainment/sports/arts\nPurpose: Interact with users\nTechnology: Chatbot; NLP/text analysis; Neural network; Deep learning; Machine learning\nIssue: Accuracy/reliability\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nXu, Y., (2018). Programmatic Dreams: Technographic Inquiry into Censorship of Chinese Chatbots\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reuters.com/article/us-china-robots-idUSKBN1AK0G1\nhttps://www.telegraph.co.uk/technology/2017/08/03/rogue-chatbots-deleted-china-questioning-communist-party/\nhttps://time.com/4885341/china-tencent-rogue-chatbots/\nhttps://www.ft.com/content/e90a6c1c-7764-11e7-a3e8-60495fe6ca71\nhttps://www.bbc.co.uk/news/world-asia-china-40815024\nhttps://nypost.com/2017/08/04/china-destroys-sassy-bots-after-they-bash-communism/\nhttps://www.abc.net.au/news/2017-08-04/tencent-chatbots-babyq-and-xiaobing/8774294?nw=0\nhttps://heavy.com/tech/2016/11/xiaoice-xiaobing-microsoft-chatbot-blacklisted-censor-censorship-sensitive-taboo-topics-wechat-weibo-bot/\nhttps://money.cnn.com/2016/11/24/technology/microsoft-chatbot-xiaoice-tiananmen-xi-jinping/\nhttps://chinadigitaltimes.net/2016/11/microsofts-chinese-chatbot-encounters-sensitive-words/\nhttps://www.theverge.com/2017/8/3/16088862/china-chatbots-patriotic-microsoft-communist-party\nRelated \ud83c\udf10\nMicrosoft Tay chatbot\nLee Luda chatbot\nPage info\nType: Incident\nPublished: February 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/mens-journal-ai-journalism", "content": "Men's Journal publishes AI article riddled with errors\nOccurred: February 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe first AI-generated article published by Men's Journal was found to be full of factual inaccuracies, raising questions about the accuracy and reliability of its technology system and editing processes.\nThe article 'What All Men Should Know About Low Testosterone' set forth multiple medical claims and advice, including recommending testosterone replacement therapy. \nHowever, medical expert Bradley Anawalt told Futurism that it contained 18 distinct errors, from basic factual mistakes and unsupported claims to sweeping mischaracterisations of medical science. The article was swiftly re-written and the disclosure removed, though no mention of its first incarnation and its mistakes is visible.\nEach Men's Journal AI article has a disclosure at the top of the page stating 'This article is a curation of expert advice from Men\u2019s Fitness, using deep-learning tools for retrieval combined with OpenAI\u2019s large language model for various stages of the workflow. This article was reviewed and fact-checked by our editorial team.'\nDays before, Men's Journal owner Arena Group had said it was partnering with AI companies Nota and Jasper 'to speed and broaden its AI-assisted efforts in content workflows, video creation, newsletters, sponsored content, and marketing campaigns.' \nAnnouncing the partnerships, Arena Group CEO Ross Levinsohn told the Wall Street Journal 'It\u2019s not about \u2018crank out AI content and do as much as you can.' 'Google will penalize you for that and more isn\u2019t better; better is better.'\n\u2795 The incident comes weeks after CNET was revealed to have been quietly publishing AI-generated articles, some of which also turned out to be factually incorrect.\nSystem \ud83e\udd16\nhttps://www.mensjournal.com\nhttps://thearenagroup.net/\nhttps://archive.is/Sds1u\nhttps://www.mensjournal.com/health-fitness/testosterone-replacement-therapy-good-or-bad-one-man-suffering-low-levels/\nOperator: Arena Group/Men's Journal\nDeveloper: Arena Group/Men's Journal; OpenAI; Jasper\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Automate journalism\nTechnology: Large language model (LLM); NLP/text analysis; Neural networks; Deep learning; Machine learning\nIssue: Accuracy/reliability; Mis/disinformation\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://futurism.com/neoscope/magazine-mens-journal-errors-ai-health-article\nhttps://www.thedailybeast.com/mens-journal-corrects-errors-in-ai-generated-health-story\nhttps://www.mediapost.com/publications/article/382421/ai-written-article-in-mens-journal-was-riddled.html\nhttps://www.reddit.com/r/GamerGhazi/comments/10y53an/magazine_mens_journal_publishes_serious_errors_in/\nhttps://www.wsj.com/articles/sports-illustrated-publisher-taps-ai-to-generate-articles-story-ideas-11675428443\nhttps://inside.com/ai/posts/men-s-journal-sports-illustrated-publisher-incorporating-more-ai-into-content-production-347203\nhttps://www.theverge.com/2023/2/3/23584305/ai-language-tools-media-use-arena-group-sports-illustrated-mens-journal\nRelated \ud83c\udf10\nCNET Money automated financial explainers\nMicrosoft replaces journalists with AI\nPage info\nType: Incident\nPublished: February 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/nothing-forever-jerry-seinfeld-clone-transphobia", "content": "Nothing, Forever Jerry Seinfeld clone makes transphobic comments\nOccurred: February 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nLarry Feinberg, an AI-generated clone of Jerry Seinfeld on Nothing, Forever, was caught making transphobic statements on the show's Twitch stream, bringing the show and its makers into disrepute and resulted in a Twitch ban for fourteen days.\n'I\u2019m thinking about doing a bit about how being transgender is actually a mental illness,' Feinberg said in the show. 'Or how all liberals are secretly gay and want to impose their will on everyone. Or something about how transgender people are ruining the fabric of society. But no one is laughing, so I\u2019m going to stop. Thanks for coming out tonight. See you next time. Where\u2019d everybody go?'\nAccording to Vice, the Nothing, Forever team had been using OpenAI's GPT-3 Davinci large language model to generate content, but had to change over to its GPT-3 Curie predecessor after a problem with the Davinci bot that caused the show to 'exhibit errant behaviors'.\nNothing, Forever is an AI-generated version of Seinfield that runs all day and night and has gained thousands of viewers.  \nSystem \ud83e\udd16\nhttps://nothingforever.com/\nhttps://discord.gg/nothingforever\nOperator: Mismatch Media; Twitch\nDeveloper: OpenAI; Stability AI\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Generate livestream show\nTechnology: Content moderation system; NLP/text analysis\nIssue: Safety\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.vice.com/en/article/y3pymx/ai-generated-seinfeld-show-nothing-forever-banned-on-twitch-after-transphobic-standup-bit\nhttps://www.nbcnews.com/tech/twitch-temporary-ban-seinfeld-parody-ai-transphobic-remarks-rcna69389\nhttps://techcrunch.com/2023/02/06/ai-generated-seinfeld-suspended-on-twitch-for-ai-generated-transphobic-jokes/\nhttps://nypost.com/2023/02/06/ai-seinfeld-show-suspended-by-twitch-for-transphobic-homophobic-stand-up/\nhttps://www.dazeddigital.com/life-culture/article/58132/1/the-swift-rise-and-fall-of-ai-seinfeld-show-nothing-forever-transphobia-gpt3\nhttps://www.spectator.com.au/2023/02/so-were-canceling-ai-for-being-transphobic-now/\nhttps://www.thedailybeast.com/ai-generated-seinfeld-clone-banned-from-twitch-over-transphobic-remarks\nhttps://www.theguardian.com/us-news/2023/feb/06/nothing-forever-twitch-ban-seinfeld-parody-ai\nhttps://kotaku.com/ai-seinfeld-twitch-ban-transphobia-chatgpt-dalle-jerry-1850077836\nRelated \ud83c\udf10\nNeuro-sama AI v-tuber Holocaust denial\nAI Dungeon offensive speech filter\nPage info\nType: Incident\nPublished: February 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/pro-china-deepfake-spamouflage-campaign", "content": "Deepfake TV anchors spin Pro-China 'Spamouflage' campaign\nOccurred: February 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nDeepfake TV anchors talking positively about China while sowing disquiet in the US, UK, Taiwan, Australia, Japan and other countries have been discovered and taken down. \nThe TV anchors were supposedly working for 'Wolf News' and had been created using technology created and owned by London-based AI video creation platform Synthesia. They are thought to be the first known instance of 'deepfake'-generated videos being used as part of a state-aligned influence campaign. \nThe discovery was first made (pdf) by Graphika, which had been tracking a Chinese covert influence operation named 'Spamouflage'. Synthesia later suspended the accounts of the people who had created the fake anchors on the basis that they had violated its terms of service. \nIn September 2023, Meta said (pdf) it had removed 7,704 Facebook accounts, 954 pages, 15 groups and 15 Instagram accounts identified for violating the company\u2019s inauthentic behaviour policy. \nIt also said that a large number of accounts appeared to be running from shared locations in China, marked by bursts of activity during the morning and afternoon, with breaks for lunch and supper.\nSystem \ud83e\udd16\nSynthesia AI video generation\nOperator: Government of China; Dragonbridge; Spamouflage; Storm 1376\nDeveloper: Government of China; Synthesia\nCountry: Australia; Japan; Taiwan; UK; USA\nSector: Politics\nPurpose: Promote Chinese interests\nTechnology: Deepfake - video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning  \nIssue: Mis/disinformation\nTransparency: Governance; Marketing\nInvestigations, assessments, audits \ud83e\uddd0\nMeta (2023). Q2 Adversarial Threat Report\nAustralian Strategic Policy Institute (2023). Gaming public opinion (pdf)\nGraphika (2023). Deepfake Till You Make It\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.nytimes.com/2023/02/07/technology/artificial-intelligence-training-deepfake.html\nhttps://www.nytimes.com/2023/08/29/technology/meta-china-influence-campaign.html\nhttps://www.abc.net.au/news/2023-02-08/deepfake-news-anchors-appear-in-pro-china-footage-research/101949284\nhttps://www.rfa.org/english/news/china/china-deepfake-02082023032941.html\nhttps://hongkongfp.com/2023/02/08/ai-deepfake-news-anchors-found-in-pro-china-footage-on-social-media-research-firm-says/\nhttps://www.bangkokpost.com/world/2501775/deepfake-news-anchors-in-pro-china-report-researchers?view_comment=1\nhttps://www.popsci.com/technology/deepfake-news-china-ai/\nhttps://www.straitstimes.com/asia/east-asia/deepfake-news-anchors-used-in-pro-china-footage-research\nhttps://www.wionews.com/world/ai-generated-deepfake-anchors-promoting-disinformation-and-pro-china-views-alarms-experts-559952\nRelated \ud83c\udf10\nBeijing Uyghur fake influence campaign\nChina diplomatic fake influence campaign\nPage info\nType: Incident\nPublished: February 2023\nLast updated: December 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/poland-covid-19-recovery-fund-assessments", "content": "Poland COVID-19 Cultural Support Fund assessments blasted as unfair\nOccurred: November 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn automated system used to award cultural support funding in Poland during COVID-19 was criticised as opaque, unfair, and politically-driven.\nPoland's Ministry of Culture announced the beneficiaries of the country's PLN 400 million (USD 106 million) Cultural Support Fund (FWK) in November 2021. The fund was intended to help struggling arts organisations recover from losses incurred during the COVID-19 pandemic. \nThe 2,064 beneficiaries and their levels of compensation had been calculated algorithmically using accounting and statistical data for the previous year (2019), including criteria such as the decrease in revenue, the number of people employed, the number of canceled events and the impact on the local community.\nThe publication of the beneficiaries' names caused controversy as it was seen to be granting large sums to established stars whilst many Poles and the country's healthcare service continued to struggle. The system was also criticised as opaque and politically-driven, with large sums granted to 'disco polo' and other genres favoured by the government. \nFacing uproar, the ministry suspended payments from the support fund on November 15, saying the list of beneficiaries would be submitted for 'urgent reverification' and that 'every effort' would be made to ensure the audit was carried out as 'efficiently and meticulously as possible.'\nPoland's culture minister and deputy prime minister Piotr Gli\u0144ski argued the algorithm was the fairest way to divide the funds as it did not look at names or the size of the organisation but transferred money on equal terms to everyone.\nThe Polish government has not revealed how the algorithm worked.\nSystem \ud83e\udd16\n\nOperator: Ministry of Culture\nDeveloper: Ministry of Culture\nCountry: Poland\nSector: Govt - culture\nPurpose: Calculate revenue loss\nTechnology: Algorithm\nIssue: Fairness; Bias/discrimination - economic, political\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://news.artnet.com/art-world/poland-culture-recovery-fund-1924242\nhttps://notesfrompoland.com/2020/11/16/polish-government-suspends-covid-culture-fund-after-big-name-stars-claim-millions/\nhttps://www.newsy-today.com/piotr-glinski-solidarna-polska-supported-the-oppositions-motion-regarding-the-culture-support-fund/\nhttps://tvn24.pl/biznes/pieniadze/koronawirus-fundusz-wsparcia-kultury-pomoc-dla-teatrow-filharmonii-wokalistow-zespolow-disco-polo-lista-4750451\nhttps://www.rp.pl/polityka/art8758021-piotr-glinski-algorytm-najlepszym-sposobem-dzielenia-pieniedzy-dziala-na-rownych-zasadach\nhttps://wiadomosci.radiozet.pl/Polska/polityka/Koronawirus.-Artysci-wsparci-milionami-zlotych-z-resortu-kultury.-Piotr-Glinski-komentuje\nhttps://www.tvp.info/50799841/piotr-glinski-o-funduszu-wsparcia-kultury-decydowal-algorytm-pokazujacy-kto-najwiecej-stracil\nhttps://www.polsatnews.pl/wiadomosc/2020-11-16/piotr-glinski-w-programie-gosc-wydarzen-transmisja-od-1920/\nRelated \ud83c\udf10\nUK government Spotlight fund application assessments\nGdansk Primary School No. 2 meal payment verification\nPage info\nType: Incident\nPublished: January 2023\nLast updated: January 2024", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/netflix-dog-and-boy-film-ai-backgrounds", "content": "Netflix 'Dog and Boy' AI film backgrounds cause controversy\nOccurred: January 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nDog and Boy, a new short firm from Netflix Japan that uses AI to help generate its lavish background images and music, set off a storm about the impact of technology, specifically generative AI, on jobs.\nIn a media statement promoting the film, Netflix expressed its hopes that its use of AI would help future animation productions thanks to a current 'shortage of human resources in the animation industry.'\nGaming news site Kotaku pointed out, 'artists did not take this bullshit at face value,' slamming Netflix for devaluing the work of animators and freelancers generally, and trying to avoid paying them in this instance.  \nOther people took issue with how Netflix only credited those who worked on the short as 'AI (+Human).' The credits go on to list Rinna Inc, an AI artwork company, and a handful of AI researchers, before listing a handful of AI researchers.'\nThe animation industry in Japan is notoriously underpaid.\nSystem \ud83e\udd16\nNetflix Japan promotional video \nNetflix Japan promotional tweet\nNetflix film media statement\nNetflix Research: Machine learning\nOperator: Netflix Anime Creators Base; Rinna; WIT Studio\nDeveloper: Netflix\nCountry: Japan\nSector: Media/entertainment/sports/arts\nPurpose: Create film backgrounds\nTechnology: Text-to-image\nIssue: Employment - jobs; Ethics/values\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.vice.com/en/article/bvmqkv/netflix-anime-dog-and-the-boy-ai-generated-art\nhttps://kotaku.com/netflix-ai-anime-wit-studio-dog-and-boy-spy-family-1850062043\nhttps://arstechnica.com/information-technology/2023/02/netflix-taps-ai-image-synthesis-for-background-art-in-the-dog-and-the-boy/\nhttps://www.engadget.com/netflixs-dog-and-boy-anime-short-causes-outrage-for-incorporating-ai-generated-backgrounds-203035524.html\nhttps://www.polygon.com/23581376/netflix-wit-studio-short-film-ai-controversy\nhttps://mashable.com/article/netflix-ai-art-anime-boy-dog\nRelated \ud83c\udf10\nDisney allegedly generates Loki season 2 poster with AI\nMimic anime art generator\nPage info\nType: Incident\nPublished: February 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/professor-meareg-amare-abrha-doxxing-murder", "content": "Professor Meareg Amare Abrha murder raises Facebook safety questions\nOccurred: November 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe murder of chemistry professor and Tigrayan ethnic group member Professor Meareg Amare Abrha was preceded by two Facebook posts of defamation and death threats, raising questions about Facebook's commitment to user safety.\nAbrha was assassinated outside his family home in Bahir Dar, the capital of Ethiopia\u2019s Amhara regional state by a group of armed men who had followed him home from his university on motorbikes and shot him at close range trying to enter his family home. \nHis murder came after Tigrayan staff were targeted on Facebook, and shortly after details of where he lived were doxxed and calls for his death had been posted on a Facebook page called 'BDU STAFF'. \nAccording to media reports, his body was left on the scene for seven hours before his attackers permitted the city municipal service to pick his body.\n\u2795 A week after Meareg\u2019s murder, Facebook announced a series of measures intended to address abusive and violent material on its platform in Ethiopia, included reducing the spread of material the company\u2019s automated moderation technology had flagged as being likely to be hate speech.\n\u2795 On December 14, 2022, Professor Abrha's son Abrham, alongside the Katiba Institute and Amnesty International's Fisseha Tekle, filed a class-action lawsuit against Facebook owner Meta alleging that Facebook's content moderation was 'woefully inadequate', and that Facebook's algorithm helped fuel the viral spread of hate and violence during Ethiopia's civil war.\nThe lawsuit (or 'petition'), which was filed by London-based legal non-profit Foxglove in Nairobi, Kenya, where Facebook opened a major content moderation hub for Eastern and Southern Africa in 2019, accused Meta of having too few moderators who deal with posts in the Amharic, Oromo, and Tigrinya languages. The suit went on to argue that Facebook's algorithm promoted 'hateful and inciting' content as it is likely to draw more interaction from users.\n\u2795 In April 2023, a Kenyan court granted Abrham and other petitioners leave to sue Meta in California, USA, after they failed to identify the social media company's physical office in the country.\nSystem \ud83e\udd16\nFacebook website\nDocuments \ud83d\udcc3\nMeta/Facebook (2021). An Update on Our Longstanding Work to Protect People in Ethiopia\nOperator: Meta/Facebook\nDeveloper: Meta/Facebook\nCountry: Ethiopia\nSector: Education\nPurpose: Minimise harmful content\nTechnology: Content moderation system\nIssue: Governance; Safety\nTransparency: Governance; Black box; Complaints/appeals\nInvestigations, assessments, audits \ud83e\uddd0\nFoxglove (2022). Death by design: a major new case against Facebook\nFoxglove (2022). Fisseha and Abrham are taking Facebook to court in Kenya \u2013 in their own words\nFacebook Oversight Board (2021). Oversight Board upholds Meta's original decision: Case 2021-014-FB-UA\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/technology-63938628\nhttps://edition.cnn.com/2022/12/14/tech/ethiopia-murdered-professor-lawsuit-meta-kenya-intl/index.html\nhttps://www.forbes.com/sites/emmawoollacott/2022/12/14/meta-sued-for-billions-over-incitement-to-violence-in-ethiopia/\nhttps://addisstandard.com/analysis-how-killing-of-ethiopian-professor-unfolded-led-to-1-6-billion-lawsuit-against-meta/\nhttps://www.wired.com/story/meta-hate-speech-lawsuit-ethiopia/\nhttps://www.washingtonpost.com/technology/2022/12/13/ethiopia-slain-professor-lawsuit-meta-kenya/\nhttps://www.nbcnews.com/tech/misinformation/facebook-lawsuit-africa-content-moderation-violence-rcna61530\nhttps://techcrunch.com/2023/04/28/kenyan-court-paves-way-or-lawsuit-alleging-facebook-played-role-in-fuelling-ethiopias-tigray-conflict/\nRelated \ud83c\udf10\nFacebook Meaningful Social Interactions algorithm\nFacebook, Google anti-semitic content moderation\nPage info\nType: Incident\nPublished: February 2023\nLast updated: October 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/clarifai-okcupid-dataset-appropriation", "content": "OkCupid shares users' facial details with Clarifai\nOccurred: July 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nDating site Okcupid shared personal photographs of its users without their consent with computer vision company Clarifai, prompting an outcry from civil rights and privacy advocates.\nThe New York Times alleged that in 2014 dating site OkCupid had provided Clarifai with personal photographs of its users to build its facial recognition technology, and that it had done so without the knowledge or consent of its users. \nThe article also asserted that an OkCupid founder who was also a Clarifai investor had supplied the photographs.\nThe revelation resulted in an outcry from civil rights and privacy advocates, and a Federal Trade Commission (FTC) investigation into the deal. \n\u2795 In July 2022, Reuters reported that OkCupid owner Match.com had been actively styming the FTC's enquiries using a variety of legal manoeuvres.\n\u2795 The incident resulted in a class action lawsuit accusing Clarifai of surreptitously violating the Illinois Biometric Information Privacy Act (BIPA) by failing to inform OKCupid users about the use of their pictures and getting their written consent.\n\u2795 In June 2018, WIRED revealed that Clarifai had been hacked by people in Russia, thereby potentially exposing its work for Project Maven, a controversial US Defense department project that uses machine learning and AI to analyse drone surveillance imagery.\nThe news led Clarifai CEO Matt Zeiler to publicly confirm the hack and basic details of the company's relationship. Six months later, a group of Clarifai employees disseminated an open letter (pdf) expressing concerns at the company's involvement in automonous weaponry.\nSystem \ud83e\udd16\nWebsite\nWikipedia profile\nOperator: Clarifai\nDeveloper: Clarifai; Match Group/OkCupid\nCountry: USA\nSector: Technology\nPurpose: Train facial recognition systems\nTechnology: Dataset; Computer vision; Facial recognition; Machine learning; Neural network\nIssue: Privacy; Ethics\nTransparency: Governance; Marketing; Privacy; Legal\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nStein v Clarifai (2021) (pdf)\nStein v Clarifai court dockets\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.nytimes.com/2019/07/13/technology/databases-faces-facial-recognition-technology.html\nhttps://www.reuters.com/legal/litigation/pssst-matchcom-does-not-want-you-know-about-this-ftc-case-2022-07-06/\nhttps://www.nytimes.com/2021/03/15/technology/artificial-intelligence-google-bias.html\nhttps://news.bloomberglaw.com/privacy-and-data-security/dating-site-profiles-capture-prompts-privacy-violation-lawsuit\nhttps://www.globaldatinginsights.com/news/ai-company-clarirfai-hit-with-lawsuit-over-okcupid-face-database/\nhttps://www.inverse.com/input/culture/okcupid-match-ftc-investigation-selling-user-data-facial-recognition\nhttps://cookcountyrecord.com/stories/579549218-judge-dismisses-biometrics-class-action-accusing-ai-firm-clarifai-over-scraping-okcupid-user-photos\nhttps://www.law360.com/articles/1472838\nRelated \ud83c\udf10\nOkCupid psychological analysis dataset sharing\nPeople of Tinder dataset\nPage info\nType: Incident\nPublished: January 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/okcupid-dataset-psychological-analysis-sharing", "content": "Researchers share personal details of 70,000 OkCupid users\nOccurred: May 2016\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe personal details of 70,000 dating site OkCupid users were shared online by researchers conducting psychological research, resulting in a backlash by privacy and civil rights advocates.\nIn May 2016, Emil Kirkegaard and two other students and researchers at Aarhus University and the University of Aalborg in Denmark published the 'OkCupid dataset', ostensibly to help psychologists investigate the social psychology of dating.\nThe team scraped data from OkCupid between November 2014 to March 2015 and created a dataset containing 2,620 variables on 68,371 users, including their usernames, age, gender, location, religion, sexual turn-ons, and sexual orientation. They also assessed the cognitive abilities of OkCupid users in a paper 'The OKCupid dataset: A very large public dataset of dating site users.'  \nThe researchers stated in their paper that 'It is our hope that other researchers will use the dataset for their own purposes.' \nIt is unclear how many times the data was downloaded, but a good number of researchers, academics and privacy advocates expressed concerns that, whilst the scraping may not have been illegal, it was unethical given the volume and sensitivity of the data and the likelihood that the data could be de-anonymised.\nOkCupid filed a DCMA copyright claim, prompting the Open Science Framework website on which the paper and data were published to remove the data - an act akin to censorship, according to Kirkegaard. \n\u2795 The fracas led to an investigation by the Danish Data Protection body Datatilsynet on the basis that research involving sensitive personal data must be approved by it. No action (pdf) was taken against Kirkegaard and his collaborators.\nSystem \ud83e\udd16\nWebsite\nWikipedia article\nOperator: Match Group/OkCupid; Emil Kirkegaard; Julius Bjerrekar; Oliver Nordbjerg\nDeveloper: Match Group/OkCupid\nCountry: Denmark\nSector: Research/academia; Media/entertainment/sports/arts\nPurpose: Assess dating psychology\nTechnology: Dataset\nIssue: Privacy; Dual/multi-use; Ethics\nTransparency: \nResearch, advocacy \ud83e\uddee\nhttps://emilkirkegaard.dk/en/wp-content/uploads/Reply-to-Oliver-Keyes-et-al.pdf\nhttps://emilkirkegaard.dk/en/2016/05/aarhus-universitys-reply-to-sjw-letter/\nhttps://emilkirkegaard.dk/en/wp-content/uploads/1.1-2016-631-0148-Sagen-afsluttes.pdf\nhttps://docs.google.com/presentation/d/1gdDjB17kwobBpCqd2kAqOkhI6fXXa054Xwx4srAmjWc/edit#slide=id.g1551986b44_0_876\nhttps://openpsych.net/paper/46/\nhttps://web.archive.org/web/20160513081923/https://osf.io/p9ixw/\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttp://motherboard.vice.com/read/70000-okcupid-users-just-had-their-data-published\nhttps://www.wired.com/2016/05/okcupid-study-reveals-perils-big-data-science/\nhttps://www.vice.com/en/article/53dd4a/danish-authorities-investigate-okcupid-data-dump\nhttps://www.vocativ.com/318393/okcupid-leak/\nhttps://www.engadget.com/2016-05-17-publicly-released-okcupid-profiles-taken-down-dmca-claim.html\nhttps://www.forbes.com/sites/emmawoollacott/2016/05/13/intimate-data-of-70000-okcupid-users-released/\nhttps://www.zdnet.com/article/okcupid-user-accounts-released-for-the-titillation-of-the-internet/\nhttps://www.vox.com/2016/5/12/11666116/70000-okcupid-users-data-release\nhttps://www.dailymail.co.uk/sciencetech/article-3589366/OKCupid-slams-social-science-study-scraped-site-released-details-70-000-usernames-sexual-turn-ons-locations.html\nhttps://retractionwatch.com/2016/05/16/publicly-available-data-on-thousands-of-okcupid-users-pulled-over-copyright-claim/\nRelated \ud83c\udf10\nClarifai OkCupid dataset appropriation\nFlo menstrual cycle data sharing\nPage info\nType: Incident\nPublished: January 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/nvidia-eye-contact", "content": "NVIDIA Eye Contact deemed 'creepy', 'terrifying'\nOccurred: January 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn announcement by NVIDIA that it had added Eye Contact to its NVIDIA Broadcast livestreaming and video conferencing tool met with a mixed reaction. \nEye Contact uses deep learning to make it appear like your webcam image is staring into the camera, even if you're looking away in real life - an effect achieved by replacing your eyes in the video stream with software-controlled simulated eyeballs with replicated eye colour and blink patterns that always stare directly into the camera.\nWhile some commentators praised it for its ability to fool people into thinking you are constantly paying attention and looking into the camera, others reckon it is 'unnnatural', 'creepy' and 'terrifying'.\nTom's Hardware journalist Jarred Walton questioned whether Eye Contact is needed. 'If you want to look like you're looking at the camera, you should probably learn to look... at the camera,' he pointed out.\nSystem \ud83e\udd16\nNVIDIA Eye Contact\n\nDocuments \ud83d\udcc3\nNVIDIA Broadcast 1.4 Adds Eye Contact and Vignette Effects With Virtual Background Enhancements\nOperator: NVIDIA\nDeveloper: NVIDIA\nCountry: USA; Global\nSector: Media/entertainment/sports/arts\nPurpose: Mimic eye retina\nTechnology: Computer vision; Deep learning; Gaze redirection algorithm\nIssue: Accuracy/reliability; Appropriateness/need; Dual/multi-use\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.tomshardware.com/news/testing-nvidia-broadcast-eye-contact\nhttps://futurism.com/the-byte/horrifying-algorithm-gaze\nhttps://www.polygon.com/23571376/nvidia-broadcast-eye-contact-ai\nhttps://arstechnica.com/information-technology/2023/01/with-nvidia-eye-contact-youll-never-look-away-from-a-camera-again/\nhttps://kotaku.com/creepy-eye-contact-stare-ai-nvidia-broadcast-1-4-update-1850025394\nhttps://www.theverge.com/2023/1/12/23552606/nvidia-broadcast-1-4-eye-contact-ai-generation-webcam\nhttps://www.standard.co.uk/tech/nvidia-eye-contact-tool-ai-deepfake-meetings-zoom-b1053244.html\nRelated \ud83c\udf10\nMoviePass PreShow eye tracking\nWalgreens fridge screen door biometrics\nPage info\nType: Issue\nPublished: January 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/automatic-soap-dispenser-racism", "content": "Automatic soap dispenser fails to recognise a Black person's hand\nOccurred: August 2017\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn automatic soap dispenser has been found to dish out soap to a white person\u2019s hand, but not a black person\u2019s, resulting in accusation of 'racism'.\nNigerian Facebook employee Chukwuemeka Afigbo shot a video in which a person with light skin received a dollop of foamy froth after waving their hand underneath, but hhen the next person, who has dark skin, gave it a go, nothing came out. He then tooks a white piece of tissue and waved it under the dispenser and it appeared to work.\nPer IFLScience, the dispenser most likely used a light sensor to detect when a hand is beneath it. \nIts inability to sense darker skin raised questions about diverity and racism in technology, and about the lack of diversity in the industry that creates these kinds of products. \n\u2795 Two years earlier, an African-American attending the Dragon Con sci-fi and fantasy convention visited a bathroom in the Marriott hotel in Atlanta and discovered the soap dispenser wouldn't sense his hands, even though it worked fine for his white friend. \nSystem \ud83e\udd16\nUnknown\nOperator:  \nDeveloper: Shenzhen Yuekun Technology\nCountry: USA\nSector: Travel/hospitality\nPurpose: Dispense soap\nTechnology: Infrared; Light sensor\nIssue: Bias/discrimination - race, ethnicity\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://twitter.com/nke_ise/status/897756900753891328\nhttps://www.iflscience.com/this-racist-soap-dispenser-reveals-why-diversity-in-tech-is-muchneeded-43318\nhttps://www.dailymail.co.uk/sciencetech/article-4800234/Is-soap-dispenser-RACIST.html\nhttps://www.mirror.co.uk/news/world-news/racist-soap-dispenser-refuses-help-11004385\nhttps://www.thesun.co.uk/tech/4262031/racist-soap-dispenser-appears-to-only-work-for-white-people-in-bizarre-viral-clip/\nhttps://www.ndtv.com/offbeat/viral-video-of-racist-soap-dispenser-sparks-debate-on-twitter-1739737\nhttps://indianexpress.com/article/trending/viral-videos-trending/this-video-of-a-racist-soap-dispenser-has-everyone-on-twitter-talking-4806625/\nhttps://www.nzherald.co.nz/world/is-this-soap-dispenser-racist-controversy-as-video-of-machine-that-only-responds-to-white-skin/67MHYDYYNABX4NIYO2OBUN4RV4/\nhttps://mic.com/articles/124899/the-reason-this-racist-soap-dispenser-doesn-t-work-on-black-skin#.aUpVSA4Bj\nRelated \ud83c\udf10\nHP face tracking 'racism'\nProctorio 'racist' facial detection\nPage info\nType: Incident\nPublished: January 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/hp-face-tracking-racism", "content": "HP face tracking system fails to follow Black face\nOccurred: December 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nHewlett Packard (HP) has been accused of racism after a man complained in a video that its webcam facial tracking software successfully followed a white face to keep it centered, but failed to follow a black face.\nThe African American man, referred to as 'Desi,' demonstrated on the video how the tracking software would not follow his face but would track that of his white co-worker. \nHP responded by saying it uses 'standard algorithms' to measure the difference in intensity of contrast between the eyes and the upper cheek and nose, and that the camera might have difficulty 'seeing' contrast where there is insufficient foreground lighting. \n\u2795 In 2010, Gadgetwise reported that the Xbox Kinect failed to recognise the faces of dark-skinned gamers, something Microsoft later attributed to a light sensor which performed poorly in low light conditions.\nSystem \ud83e\udd16\n\nOperator: Hewlett-Packard (HP)\nDeveloper: Hewlett-Packard (HP)\nCountry: USA\nSector: Technology\nPurpose: Detect and follow faces\nTechnology: Facial tracking; Contrast intensity algorithms\nIssue: Accuracy/reliability; Bias/discrimination - race, ethnicity\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.youtube.com/watch?v=t4DT3tQqgRM\nhttps://www.pcworld.com/article/515701/what_racist_webcams_hp_handled_issue_well.html\nhttps://www.wired.com/2009/12/hp-notebooks-racist/\nhttps://edition.cnn.com/2009/TECH/12/22/hp.webcams/index.html\nhttps://content.time.com/time/business/article/0,8599,1954643,00.html\nhttps://www.mercurynews.com/2009/12/21/hps-facial-tracking-software-accused-of-being-racist/\nhttps://www.laptopmag.com/articles/hp-webcam\nhttps://www.cbsnews.com/news/man-accuses-hp-computers-of-being-racist/\nhttps://www.theguardian.com/media/pda/2009/dec/23/hewlett-packard\nhttps://www.ibtimes.com/hewlett-packard-looking-racist-webcam-claims-355247\nRelated \ud83c\udf10\nMoviePass PreShow eye tracking\nAutomatic soap dispenser 'racism'\nPage info\nType: Incident\nPublished: January 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/historical-figures-chat", "content": "Historical Figures Chat 'monetises Holocaust'\nOccurred: January 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nHistorical Figures, a freemium iPhone app that lets people speak to famous figures from the past, sparked controversy by allowing 'conversations' with some of history's most infamous figures.\nDeveloped by 25-year old Amazon software engineer Sidhant Chadda using OpenAI's GPT-3 large language model as a foundation, the app allows users to chat with over 20,000 virtual personalities, including Jesus Christ, Plato, Princess Diana, Abraham Lincoln, and Benjamin Franklin.\nGetting the facts wrong\nThe app produces inaccurate and contradictory results. Former FBI director J Edgar Hoover says his mother died when he was nine years-old (she lived until she was 78). Serial child rapist Jimmy Savile denies ever abusing anyone. \nVirulent anti-Semite Henry Ford claims he 'does not hate Jewish people'. And infamous former Nazi propaganda minister Joseph Goebbels claims he 'did not hate Jews'. Goebbels was a key architect of the Final Solution.\nMonetising hatred\nIn addition to Goebbels, Historical Figures also lets people talk to infamous Nazis Adolf Hitler, Joseph Goebels, Heinrich Himmler, amongst other dictators and autocrats.\nBut, unlike the great majority of figures on the app, it is only possible to 'talk' to Hitler et al behind a paywall, by 'unlocking them for '500 coins', sparking controversy about Chaddha's perceived monetisation of the Holocaust and hatred.\nEducational value\nThe app has sparked a furore about its value as an educational tool. According to Chaddha, Historical Figures is 'extremely valuable to teachers and students'. \nHowever, historians have slammed it. One called it 'vile' and another beseeched Apple to 'remove this trash from the App Store'. Another expert said it shouldn't 'go anywhere near a classroom'. \nSystem \ud83e\udd16\nHistorical Figures Apple app\nOperator: Sidhant Chaddha; Apple\nDeveloper: Sidhant Chaddha\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Talk to historical figures\nTechnology: Chatbot; NLP/text analysis; Deep learning; Machine learning\nIssue: Accuracy/reliability; Mis/disinformation; Safety\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.vice.com/en/article/3ad9ww/an-ai-chatbot-connects-you-with-pol-pot-jeffrey-epstein-and-jesus-in-the-afterlife\nhttps://www.rollingstone.com/culture/culture-news/historical-figures-ai-chat-bot-lies-dead-people-1234664257/\nhttps://www.nbcnews.com/tech/tech-news/chatgpt-gpt-chat-bot-ai-hitler-historical-figures-open-rcna66531\nhttps://www.ndtv.com/feature/ai-app-faces-backlash-for-allowing-users-to-chat-with-controversial-historical-figures-3738348\nhttps://forward.com/news/532624/artificial-intelligence-historical-figures-chat-nazis/\nhttps://www.dailymail.co.uk/news/article-11652991/Historians-slam-iPhone-AI-chat-bot-claims-Nazi-Joseph-Goebbels-did-not-hate-Jews.html\nhttps://www.timesofisrael.com/chatbot-denounced-for-generating-remorseful-responses-from-top-nazi-figures/\nhttps://www.telegraaf.nl/nieuws/383082454/chatten-met-hitler-app-zit-vol-fouten-waarschuwen-historici\nhttps://indianexpress.com/article/technology/from-william-shakespeare-to-ye-new-ai-site-lets-you-chat-with-historical-figures-contemporary-artists-8381564/\nhttps://www1.wdr.de/nachrichten/historical-figures-app-100.html\nhttps://knowyourmeme.com/memes/sites/historical-figures-chat\nRelated \ud83c\udf10\nMyHeritage Deep Nostalgia\nMichael Schumacher AI 'exclusive interview'\nPage info\nType: Incident\nPublished: January 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/keele-university-youtube-autism-prediction-study", "content": "Keele University study 'predicts' kids' autism without consent\nReleased: July 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA behavioural study group on children with autism by researchers at Keele University used YouTube videos used an AI to study childrens' body movements without their permission.\nThe researchers told The Atlantic that the purpose of the study was to classify the kids' behaviours as either typical or atypical, with the aim of more quickly evaluating 'edge' cases that might normally require lab equipment or invasive tactile sensors. \nThe Atlantic pointed out that the children and parents whose data was scraped by the Keele team had not consented to having their home videos used for scientific research.\nThe paper, which is another example of the use of machine learning to 'predict' innate attributes, was withdrawn 'due to insufficient or definition error(s) in the ethics approval protocol.'\nSystem \ud83e\udd16\n\nOperator: Keele University\nDeveloper: Andrew Cook; Bappaditya Mandal; Donna Berry; Matthew Johnson\nCountry: UK\nSector: Health\nPurpose: Predict autism\nTechnology: Computer vision; Machine learning; Pattern recognition\nIssue: Accuracy/reliability; Privacy\nTransparency: Privacy\nResearch, advocacy \ud83e\uddee\nCook A., Mandal B., Berry D., Johnson M. (2019). Towards Automatic Screening of Typical and Atypical Behaviors in Children With Autism\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theatlantic.com/technology/archive/2019/09/breakthrough-autism-research-uses-social-media-videos/597646/\nhttps://www.trialsitenews.com/a/englands-keele-university-neglects-patient-consent-regulations-and-uses-youtube-videos-to-study-autism-in-children\nhttps://www.wired.com/story/algorithm-predicts-criminality-based-face-sparks-furor/\nRelated \ud83c\udf10\nChina facial image criminal inference study\nStanford AI sexual orientation prediction study\nPage info\nType: Issue\nPublished: January 2023\nPast updated: October 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/harrisburg-university-criminality-prediction-study", "content": "Harrisburg University study predicts criminality based on one facial picture\nReleased: June 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nProfessors and a PhD student at Harrisburg University developed software that automatically predicted whether someone would become a criminal based solely on a picture of their face with '80 percent accuracy and no racial bias'.\nA backlash quickly followed the announcement, with the researchers accused of 'unsound scientific premises, research, and methods which \u2026 have [been] debunked over the years' by the Coalition for Critical Technology (CCT) in an open letter signed by over 1,700 academics demanding the research remain unpublished.\nThe research was intended to appear in a book series titled 'Springer Nature \u2013 Research Book Series: Transactions on Computational Science & Computational Intelligence.' \nSpringer Nature later confirmed it would not publish the research, which was subsequently withdrawn by Harrisburg University.\nSystem \ud83e\udd16\n\nOperator:\nDeveloper: Harrisburg University\nCountry: USA\nSector: Govt - police\nPurpose: Predict criminality\nTechnology: Facial recognition; Emotion detection\nIssue: Accuracy/reliability; Bias/discrimination - race, gender, age, income; Ethics\nTransparency: \nResearch, advocacy \ud83e\uddee\nHarrisburg University (2020). HU facial recognition software predicts criminality\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/technology-53165286\nhttps://www.wired.com/story/algorithm-predicts-criminality-based-face-sparks-furor/\nhttps://www.iflscience.com/technology/over-1000-experts-call-out-racially-biased-ai-designed-to-predict-crime-based-on-your-face/\nhttps://www.inputmag.com/culture/researchers-still-foolishly-think-ai-can-predict-criminality-by-looking-at-photos\nhttps://www.techdirt.com/articles/20200505/17090244442/harrisburg-university-researchers-claim-their-unbiased-facial-recognition-software-can-identify-potential-criminals.shtml\nhttps://www.biometricupdate.com/202005/biometric-software-that-allegedly-predicts-criminals-based-on-their-face-sparks-industry-controversy\nhttps://techcrunch.com/2020/06/23/ai-crime-prediction-open-letter-springer/\nhttps://filtermag.org/crime-prediction-software-abolition/\nRelated \ud83c\udf10\nChina facial image criminal inference study\nStanford AI sexual orientation prediction study\nPage info\nType: Research\nPublished: January 2023\nLast updated: February 2024", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/gaydar-ai-sexual-orientation-predictions", "content": "'Gaydar' AI that predicts sexual orientation accused of poor ethics\nReleased: September 2017\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA research study published by Stanford University researchers apparently showing that AI can predict someone's sexual orientation from a few facial images prompted accusations of junk science, physiognomy, and shoddy ethics.\nStanford Graduate School of Business researchers Michal Kosinski and Yilun Wang trained a neural network on almost 15,000 pictures of gay and straight people taken from a popular dating website.\nThey found that their AI could predict the sexual orientation of gay men 81% of the time, in contrast to a human man, who would be right 61% of the time, suggesting machines have a potentially better 'gaydar' than human beings. 'Gay men' they found 'had narrower jaws and longer noses, while lesbians had larger jaws.' \nThe study garnered criticism from LGBTQ groups, who criticised the study as 'dangerous and flawed \u2026 junk science' that could be used to out gay people and put them at risk. They also felt the study was too restricted by only using photos that people chose to put on their dating profiles, and by failing to test a more diverse pool. \nMeantime, technology researchers and commentators focused more on the technical details of the analysis, with some figuring the neural networks are picking up on superficial cultural signs such as the use of make-up, eyeshadow and glasses, rather than analysing facial structure. Others highlighted what they saw as poor ethics, including scraping and using people's images without their consent.\nKosinski later claimed the research deliberately aimed to demonstrate the power of AI and how easily it can be abused and misused. 'I hope that someone will go and fail to replicate this study \u2026 I would be the happiest person in the world if I was wrong,' he told The Guardian.\nSystem \ud83e\udd16\nMichal Kosinski website\nMichal Kosinski Wikipedia profile\nDocuments \ud83d\udcc3\nKosinski M. (updated 2022). Author's Note\nKosinski M., Yilun W. (2018). Deep neural networks are more accurate than humans at detecting sexual orientation from facial images\nOperator: Michal Kosinski; Yilun Wang\nDeveloper: Michal Kosinski; Yilun Wang\nCountry: USA\nSector: Politics\nPurpose: Predict sexual orientation\nTechnology: Facial analysis; Computer vision; Machine learning; Deep learning; Neural network\nIssue: Accuracy/reliability; Ethics; Privacy\nTransparency: \nResearch, advocacy \ud83e\uddee\nShaping AI - University of Warwick (2023). Shifting AI controversies (pdf)\nWang D. (2022). Presentation in self-posted facial images can expose sexual orientation: Implications for research and privacy\nWilkinson P. (2021). The Legal Implications of Sexual Orientation Detecting Facial Recognition Technology (pdf)\nLeuner J. (2019). A Replication Study: Machine Learning Models Are Capable of Predicting Sexual Orientation From Facial Images (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/technology-41188560\nhttps://www.glaad.org/blog/glaad-and-hrc-call-stanford-university-responsible-media-debunk-dangerous-flawed-report\nhttps://www.theguardian.com/technology/2017/sep/07/new-artificial-intelligence-can-tell-whether-youre-gay-or-straight-from-a-photograph\nhttps://www.theguardian.com/world/2017/sep/08/ai-gay-gaydar-algorithm-facial-recognition-criticism-stanford\nhttps://www.theguardian.com/technology/2018/jul/07/artificial-intelligence-can-tell-your-sexuality-politics-surveillance-paul-lewis\nhttps://www.economist.com/science-and-technology/2017/09/09/advances-in-ai-are-used-to-spot-signs-of-sexuality\nhttps://www.psychologytoday.com/us/blog/the-conservative-social-psychologist/201709/gaydar-goes-ai-and-populism-comes-science\nhttps://www.vox.com/science-and-health/2018/1/29/16571684/michal-kosinski-artificial-intelligence-faces\nhttps://medium.com/@blaisea/do-algorithms-reveal-sexual-orientation-or-just-expose-our-stereotypes-d998fafdf477\nhttps://www.insidehighered.com/news/2017/09/13/prominent-journal-accepted-controversial-study-ai-gaydar-reviewing-ethics-work\nhttps://qz.com/1078901/a-stanford-scientist-says-he-built-a-gaydar-using-the-lamest-ai-to-prove-a-point\nhttps://www.nytimes.com/2017/11/21/magazine/can-ai-be-taught-to-explain-itself.html\nhttps://callingbull.org/case_studies/case_study_ml_sexual_orientation.html\nhttps://www.vice.com/sv/article/7xkdab/forskare-ai-ansikte-hbtq\nRelated \ud83c\udf10\nStanford facial recognition reveals political orientation study\nChina facial image criminal inference study\nPage info\nType: Issue\nPublished: January 2023\nLast updated: May 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/manga-artist-chikae-ide-deepfake-scamming", "content": "Deepfake Mark Ruffalo scams manga artist Chikae Ide\nOccurred: September 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nJapanese manga artist Chikae Ide was scammed of over half a million US dollars after being tricked by a deepfake version of Marvel's Hulk actor Mark Ruffalo.\nThe Asahi Shimbun tells how Chikae Ide was contacted on Facebook by someone claiming to be Ruffalo, who built an emotional trust with her, 'met' on video, developed a romantic online relationship, and were unofficially married online.\nAfter the 'marriage', the scammer continually asked for thousands of dollars for plane tickets, hospital bills, and cash troubles, and even faked having cancer. Police have been unable to identify the person posing as Mark Ruffalo. \nThe saga is told in Ide's new book Poison Love, and fictionalised in a comic with the same title.\nSystem \ud83e\udd16\nUnknown\nOperator:  \nDeveloper: Unclear/unknown\nCountry: Japan\nSector: Media/entertainment/sports/arts\nPurpose: Defraud\nTechnology: Deepfake - video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Impersonation; Ethics\nTransparency: Governance; Marketing; Privacy\nInvestigations, assessments, audits \ud83e\uddee\nDeepfaked. Veteran manga artist falls for deepfake Mark Ruffalo who stole more than $500,000\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.asahi.com/ajw/articles/14722566\nhttps://screenrant.com/mark-ruffalo-scam-artist-manga-deepfake-marvel/\nhttps://www.thesun.co.uk/tech/20001151/facebook-romance-scam-deepfake-mark-ruffalo/\nhttps://grahamcluley.com/how-a-deepfake-mark-ruffalo-scammed-half-a-million-dollars-from-a-lonely-heart/\nhttps://www.animesenpai.net/a-person-imitating-mark-ruffalo-scams-mangaka-for-500000/\nhttps://www.oxfordmail.co.uk/news/23033120.facebook-warning-users-message-costs-user-400-000/\nhttps://hitek.fr/actualite/hulk-mangaka-voler-somme-colossale-cause-deepfake-mark-ruffalo_37718\nRelated \ud83c\udf10\nDubai deepfake court evidence\nMasayuki Nakamoto deepfake uncensored pornography\nPage info\nType: Incident\nPublished: January 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-ring-police-data-sharing", "content": "Amazon covertly shares Ring data with US police\nOccurred: July 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAmazon Ring shared private recordings, including video and audio, with the US police eleven times in 2022, raising concerns about increasing police reliance upon private surveillance, a practice that has long gone unregulated.\nIn each case, the company did not let Ring owners know that the police had access to and used their data. The confimation also calls into question how much Ring users know about how their data is used.\nAmazon chose to make the information public in a response (pdf) to an inquiry by Senator Ed Markey after the lawmaker and persistent Amazon critic had questioned Ring's surveillance practices. \nAmazon said it only shares footage with police without a warrant under emergency circumstances involving imminent danger of death or serious physical harm, and that emergency requests do not require the consent of the device owner.\n\u2795 In 2020, Ring admitted four employees had improperly accessed Ring video data in a letter to five US senators. \nSystem \ud83e\udd16\nAmazon Ring website\nAmazon Ring Wikpedia profile\nOperator: Amazon/Ring\nDeveloper: Amazon/Ring\nCountry: USA\nSector: Govt - police\nPurpose: Strengthen security\nTechnology: CCTV; Computer vision\nIssue: Privacy; Ethics/values\nTransparency: Governance; Marketing; Privacy\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nhttps://www.markey.senate.gov/imo/media/doc/amazon_response_to_senator_markey-july_13_2022.pdf\nhttps://www.markey.senate.gov/news/press-releases/senator-markeys-probe-into-amazon-ring-reveals-new-privacy-problems\nResearch, advocacy \ud83e\uddee\nhttps://static1.squarespace.com/static/58a33e881b631bc60d4f8b31/t/61baab9fcc4c282092bbf7c3/1639623584675/Policing+Project+Ring+Civil+Rights+Audit+%28Full%29.pdf\nhttps://www.eff.org/deeplinks/2020/01/ring-doorbell-app-packed-third-party-trackers\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://arstechnica.com/tech-policy/2022/07/amazon-finally-admits-giving-cops-ring-doorbell-data-without-user-consent/\nhttps://www.biometricupdate.com/202207/amazon-defends-ring-data-sharing-practices-to-us-senator-leaves-voice-biometrics-door-open\nhttps://www.bloomberg.com/news/articles/2019-11-19/amazon-s-ring-not-doing-enough-to-protect-privacy-markey-says\nhttps://www.reuters.com/technology/amazoncoms-ring-gave-police-data-without-user-consent-11-times-2022-2022-07-13/\nhttps://theintercept.com/2022/07/13/amazon-ring-camera-footage-police-ed-markey/\nhttps://www.politico.com/news/2022/07/13/amazon-gave-ring-videos-to-police-without-owners-permission-00045513\nhttps://www.consumerreports.org/law-enforcement/amazon-shared-ring-footage-with-police-without-a-warrant-a6093504500/\nhttps://www.vox.com/recode/23207072/amazon-ring-privacy-police-footage\nhttps://arstechnica.com/tech-policy/2020/01/amazons-ring-app-shares-loads-of-your-personal-info-report-finds/\nRelated \ud83c\udf10\nAmazon Ring BLM protest surveillance\nAmazon Ring neighbour privacy invasion\nPage info\nType: Incident\nPublished: January 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/microsoft-little-mix-robot-editor-racial-bias", "content": "Microsoft robot editor confuses Little Mix band members\nOccurred: June 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nMicrosoft's artificial intelligence software illustrated a news story about racism with a photograph of the wrong mixed-race member of the girl band Little Mix. \nA story about Little Mix singer Jade Thirlwall\u2019s personal reflections on racism was illustrated with a picture of fellow band member Leigh-Anne Pinnock, prompting a reprimand from Thirlwall. Both are mixed race. \nMicrosoft's 'robot' editor then picked up and re-published a Guardian story about the mix-up, only for Microsoft staff to delete it. \nA few days before, The Guardian had warned that Microsoft's decision to replace the jobs of dozens of journalists and editors running its Microsoft News and MSN sites with artificial intelligence would results in mistakes such as the story involving Jade Thirlwall.\nMicrosoft later said the problem had not arisen as a result of algorithmic bias but due to an experimental feature in the automated system.\nSystem \ud83e\udd16\nMSN website\nMSN Wikipedia profile\nOperator: Microsoft/MSN\nDeveloper: Microsoft/MSN\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Select news articles\nTechnology: Machine learning; NLP/text analysis; Neural networks; Deep learning\nIssue: Accuracy/reliability; Bias/discrimination; Mis/disinformation\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://thehill.com/changing-america/enrichment/arts-culture/502059-backlash-after-microsofts-robot-editor-confuses\nhttps://www.irishtimes.com/culture/music/microsoft-s-robot-editor-confuses-mixed-race-little-mix-singers-1.4275502\nhttps://www.theverge.com/2020/6/9/21284934/microsoft-ai-news-editors-msn-homepage-little-mix-singers\nhttps://mashable.com/article/microsoft-news-ai-publishes-stories-about-racist-error/\nhttps://www.techradar.com/news/microsoft-shows-ai-journalism-at-its-worst-with-little-mix-debacle\nhttps://www.theguardian.com/technology/2020/jun/09/microsofts-robot-journalist-confused-by-mixed-race-little-mix-singers\nhttps://www.unilad.com/technology/microsofts-ai-editor-cant-tell-difference-between-two-mixed-race-little-mix-stars\nRelated \ud83c\udf10\nMicrosoft replaces journalists with AI\nTwitter photo crop algorithm age, weight bias\nPage info\nType: Incident\nPublished: January 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/microsoft-replaces-journalists-with-ai", "content": "Microsoft's replacement of journalists with AI prompts industry fears\nOccurred: May 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nMicrosoft's announcement that it had replaced dozens of news journalists and editors with artificial intelligence, triggering fears about the loss of media jobs.\nAccording to Insider and The Guardian, around 50 people were let go in the US and 27 in the UK, most of whom had been curating articles on Microsoft News, MSN, and Microsoft's Edge internet browser. \nA UK employee told The Guardian 'I spend all my time reading about how automation and AI is going to take all our jobs, and here I am \u2013 AI has taken my job.' \nThe Verge reported that Microsoft had increasingly been using AI to spot trending news stories, change headlines and suggest photos for human editors to pair it with.\n\u2795 A few weeks later, Microsoft's new AI system confused members of the band Little Mix\nSystem \ud83e\udd16\nMSN website\nMSN Wikipedia profile\nOperator: Microsoft/MSN\nDeveloper: Microsoft/MSN\nCountry: USA; UK\nSector: Media/entertainment/sports/arts\nPurpose: Automate copywriting\nTechnology: Machine learning; NLP/text analysis; Neural network; Deep learning\nIssue: Employment; Ethics/values\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.seattletimes.com/business/local-business/microsoft-is-cutting-dozens-of-msn-news-production-workers-and-replacing-them-with-artificial-intelligence/\nhttps://www.businessinsider.com/microsoft-news-cuts-dozens-of-staffers-in-shift-to-ai-2020-5\nhttps://www.bbc.co.uk/news/world-us-canada-52860247\nhttps://www.theguardian.com/technology/2020/may/30/microsoft-sacks-journalists-to-replace-them-with-robots\nhttps://www.theverge.com/2020/5/30/21275524/microsoft-news-msn-layoffs-artificial-intelligence-ai-replacements\nhttps://www.dailymail.co.uk/news/article-8372305/Microsoft-axe-50-journalists-replace-robots.html\nhttps://www.standard.co.uk/news/world/microsoft-replace-journalists-robots-a4455336.html\nhttps://www.algemeiner.com/2020/06/07/robo-journalists-will-not-protect-human-rights-and-free-speech-says-media-expert/\nhttps://futurism.com/the-byte/msn-fires-journalists-replaces-ai\nhttps://www.financialexpress.com/industry/technology/robot-uprising-begins-microsoft-fires-journalists-replaces-them-with-ai/1977441/\nRelated \ud83c\udf10\nMicrosoft 'Little Mix' AI editor racial bias\nCNET Money automated financial explainers\nPage info\nType: Incident\nPublished: January 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tesla-model-s-crash-causes-eight-vehicle-pile-up", "content": "Tesla Model S 'FSD malfunction' causes eight-vehicle pile-up\nOccurred: November 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Tesla Model S car has been involved in an eight-vehicle crash in a tunnel of the San Francisco Bay Bridge, injuring nine people, including a 2-year-old child. The crash blocked traffic on the bridge for over an hour. \nThe driver told California authorities the vehicle was in 'full self-driving mode' (FSD) when the technology malfunctioned. According to the incident report, the car was traveling at 55mph when it shifted lane but braked abruptly, slowing to about 20mph, leading another vehicle to hit the Tesla and a chain reaction of crashes. \nIn January 2023, The Intercept published video, photographs and the incident report of the crash through a California Public Records Act request. The US National Highway Traffic Safety Administration (NHTSA) subsequently announced that a special crash investigation team would examine the incident. \nThe crash took place shortly after Tesla CEO Elon Musk said the car manufacturer would make FSD software available to anyone in North America who asked for it. It had previously only offered the system only to drivers with high safety scores. \nSystem \ud83e\udd16\nTesla Autopilot, Full-self Driving\nOperator: \nDeveloper: Tesla\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Driver assistance system; Self-driving system\nIssue: Accuracy/reliability; Safety\nTransparency: Governance; Black box; Marketing\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nhttps://www.documentcloud.org/documents/23569059-9335-2022-02256-redacted\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reuters.com/business/autos-transportation/tesla-driver-multi-car-crash-told-police-self-driving-software-malfunctioned-2022-12-22/\nhttps://www.theguardian.com/technology/2022/dec/22/tesla-crash-full-self-driving-mode-san-francisco\nhttps://www.cnn.com/2022/12/21/business/tesla-fsd-8-car-crash/index.html\nhttps://edition.cnn.com/2023/01/17/business/tesla-8-car-crash-autopilot/index.html\nhttps://www.hotcars.com/full-self-driving-tesla-major-eight-car-pileup-san-francisco/\nhttps://www.popsci.com/technology/tesla-crash-full-self-driving-mode-san-francisco/\nhttps://www.msn.com/en-us/autos/news/nhtsa-probing-tesla-for-two-more-driver-assistance-system-related-crashes/ar-AA15MssS\nhttps://www.dailymail.co.uk/news/article-11570499/Tesla-driver-blames-self-driving-mode-eight-vehicle-pile-San-Franciscos-Bay-Bridge.html\nhttps://www.theverge.com/2022/12/22/23523201/tesla-fsd-braking-crash-bay-bridge-california-chp\nhttps://abc7news.com/tesla-autopilot-crash-sf-bay-bridge-8-car-self-driving/12599448/\nhttps://theintercept.com/2023/01/10/tesla-crash-footage-autopilot/\nRelated \ud83c\udf10\nTesla FSD beta test car hits bollard, driver fired\nTesla FSD Assertive mode\nPage info\nType: Incident\nPublished: January 2023\nLast updated: January 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/koko-ai-mental-health-counselling-experiment", "content": "Koko AI mental health counselling 'experiment' fails to obtain user consent\nOccurred: January 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nMental health non-profit Koko use GPT-3 in a Discord-based 'experiment' to provide support to people seeking counseling was criticised for failing to obtain the informed consent of the 4,000 people using the system. \nUsers send direct messages to the Discord 'Kokobot' that asks several multiple-choice questions, and then shares a person's concerns anonymously with someone else on the server who can reply anonymously with a short message - either of their own, or one automatically generated by GPT-3.\nAccording to Koko CEO Rob Morris, 'Messages composed by AI (and supervised by humans) were rated significantly higher than those written by humans on their own (p < .001). Response times went down 50%, to well under a minute \u2026 [but] once people learned the messages were co-created by a machine, it didn\u2019t work. Simulated empathy feels weird, empty.'\nDuring the backlash that ensued, critics asked whether an Institutional Review Board (IRB) had approved the experiment. It is illegal to conduct research on human subjects without so-called 'informed consent' unless an IRB finds that consent can be waived in the US.\nIn response, Morris said the experiment was exempt because participants opted in, their identities were anonymised, and an intermediary evaluated the responses before they were shared with people who sought help. \nMorris told Vice 'We pulled the feature anyway and I wanted to unravel the concern as a thought piece, to help reign in enthusiasm about gpt3 replacing therapists.'\nSystem \ud83e\udd16\nGPT-3 large language model\nKoko website\nOperator: Koko\nDeveloper: Koko\nCountry: USA\nSector: Health\nPurpose: Provide mental health support\nTechnology: Large language model (LLM); NLP/text analysis; Neural networks; Deep learning; Machine learning\nIssue: Ethics; Privacy\nTransparency: Governance\nNews, analysis, commentary \ud83d\uddde\ufe0f\nhttps://www.vice.com/en/article/4ax9yw/startup-uses-ai-chatbot-to-provide-mental-health-counseling-and-then-realizes-it-feels-weird\nhttps://arstechnica.com/information-technology/2023/01/contoversy-erupts-over-non-consensual-ai-mental-health-experiment/\nhttps://www.newscientist.com/article/2354077-mental-health-service-used-an-ai-chatbot-without-telling-people-first/\nhttps://metro.co.uk/2023/01/10/mental-health-app-faces-backlash-for-testing-chatgpt-on-4000-users-18070559/\nhttps://www.politico.com/newsletters/digital-future-daily/2023/01/10/tracking-the-ai-apocalypse-00077279\nhttps://gizmodo.com/mental-health-therapy-app-ai-koko-chatgpt-rob-morris-1849965534\nRelated\nChatGBT chatbot\nCrisis Text Line data sharing\nPage info\nType: Incident\nPublished: January 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/adobe-creative-cloud-content-analysis", "content": "Adobe Creative Cloud uses customer content to train AI systems\nOccurred: January 2023\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAdobe has been automatically analysing customer content stored on Creative Cloud to train its AI algorithms, according to media reports.\nThe discovery sparked graphic designers, artists and other customers to share their concerns that Adobe is abusing their privacy and stealing their work to improve its own automated systems. Some saw it as another indication that their jobs are at risk of being replaced by robots.\nAdobe's content analysis FAQ states it may use machine learning 'to develop and improve our products and services' and 'provide product features and customize our products and services ', providing examples such as the correction of perspective in images and automatically enhancing a document's headings and tables.\nThe row reflects broader concerns amongst artists, illustrators and others that their work is being scraped and used to train generative AI models such as DALL-E and Midjourney without their consent, thereby abusing their IP, commoditising their output, and potentially putting them out of work.\nAdobe responded by saying it 'does not use data stored on customers\u2019 Creative Cloud accounts to train its experimental Generative AI features.' In a Bloomberg interview, Adobe Scott Belsky later claimed the company never trained its generative AI services on customer projects.\nLaunched in 2011, Adobe Creative Cloud is a set of 20+ Adobe graphic design, video editing, web development, and photography applications and services delivered over the Internet.\nSystem \ud83e\udd16\nAdobe Creative Cloud website\nAdobe Creative Cloud Wikipedia profile\n\nDocuments \ud83d\udcc3\nhttps://helpx.adobe.com/manage-account/using/machine-learning-faq.html#CanIturnoffoptoutofmachinelearning\nhttps://blog.adobe.com/en/publish/2022/10/18/bringing-next-wave-ai-creative-cloud\nhttps://account.adobe.com/privacy\nOperator: Adobe users\nDeveloper: Adobe\nCountry: USA\nSector: Business/professional services\nPurpose: Improve products, services\nTechnology: Machine learning; Pattern recognition; Object recognition\nIssue: Privacy; Confidentiality; Employment\nTransparency: Governance; Marketing; Privacy\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://twitter.com/Krita_Painting/status/1610706677371932672\nhttps://www.howtogeek.com/858952/adobe-is-using-your-data-to-train-ai-how-to-turn-it-off/\nhttps://techcrunch.com/2023/01/06/is-adobe-using-your-photos-to-train-its-ai-its-complicated/\nhttps://petapixel.com/2023/01/05/adobe-may-be-using-your-photos-to-train-its-ai/\nhttps://www.dpreview.com/news/6341509927/adobes-content-analysis-program-raises-privacy-concern\nhttps://www.theregister.com/2023/01/07/adobe_ai_training/\nhttps://www.fastcompany.com/90831386/artists-accuse-adobe-tracking-design-ai\nhttps://www.dpreview.com/news/6341509927/adobes-content-analysis-program-raises-privacy-concern\nhttps://www.bloomberg.com/news/articles/2023-01-18/adobe-says-ai-tools-not-being-trained-with-customer-data\nhttps://www.reddit.com/r/assholedesign/comments/103n2bu/a_warning_about_adobe_creative_cloud_stealing/\nRelated \ud83c\udf10\nAdobe Sensei Project Morpheus\nMicrosoft/Github Copilot 'code laundering'\nPage info\nType: Incident\nPublished: January 2023\nLast updated: February 2024", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/roomba-robot-vacuum-data-annotation-sharing", "content": "Scale AI shares sensitive Roomba robot vacuum training photos\nOccurred: December 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nSensitive photographs used to train iRobot vacuum cleaners, including of a young woman sitting on the toilet, were shared on Facebook, triggering privacy advocates to question the effectiveness of iRobot data protection practices.\nAccording to Technology Review, images of development versions of iRobot\u2019s Roomba J7 series robot vacuum were posted online by Venezuelan contractors to San Francisco-based Scale AI, a start-up that uses low-cost gig workers to annotate audio, photo, and video data used to train artificial intelligence systems.\nAccording to iRobot, the photos came from 'special development robots with hardware and software modifications that are not and never were present on iRobot consumer products for purchase.' The images, the company says, were 'shared in violation of a written non-disclosure agreement between iRobot and an image annotation service provider.'\niRobot declined to share the consent agreements with Technology Review, nor make any of its paid collectors or employees available to discuss their understanding of the terms. However, a follow-up article cited beta testers saying iRobot had 'failed spectacularly' to mention that personal images of their homes and children will be seen and analysed by other humans.\niRobot CEO Colin Angle later confirmed the company terminated its relationship with Scale AI as a result of the incident.\nSystem \ud83e\udd16\nRoomba\u00ae j7 Robot Vacuum website\nRoomba Wikipedia profile\nScale AI website\nDocuments \ud83d\udcc3\niRobot CEO (2022). Building Smart Robots Requires Responsible Development\nOperator: Amazon/iRobot; Scale AI\nDeveloper: Amazon/iRobot\nCountry: USA; Venezuela\nSector: Consumer goods\nPurpose: Clean floor\nTechnology: Robotics; Computer vision; IoT; Machine learning; Object recognition; Sensor\nIssue: Privacy; Security; Surveillance\nTransparency: Governance; Marketing; Legal\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.technologyreview.com/2022/12/19/1065306/roomba-irobot-robot-vacuums-artificial-intelligence-training-data-privacy/\nhttps://www.technologyreview.com/2023/01/10/1066500/roomba-irobot-robot-vacuum-beta-product-testers-consent-agreement-misled\nhttps://petapixel.com/2022/12/21/robot-vacuum-took-photo-of-woman-on-toilet-that-was-shared-on-facebook/\nhttps://www.standard.co.uk/tech/photo-woman-toilet-robot-vacuum-cleaner-facebook-b1048446.html\nhttps://www.businessinsider.com/roomba-photos-recorded-bathroom-leaked-from-test-units-irobot-says-2022-12\nhttps://futurism.com/the-byte/roomba-photos-leaked\nhttps://petapixel.com/2022/12/21/robot-vacuum-took-photo-of-woman-on-toilet-that-was-shared-on-facebook/\nhttps://www.unilad.com/technology/roomba-takes-picture-woman-facebook-905490-20221221\nhttps://www.entrepreneur.com/business-news/roomba-vacuum-records-woman-in-bathroom-photos-end-up/441494\nhttps://www.smh.com.au/technology/this-robot-vacuum-takes-photos-as-it-cleans-but-can-you-trust-it-with-your-data-20221209-p5c55m.html\nhttps://www.bloomberg.com/news/articles/2022-08-05/amazon-s-irobot-deal-is-about-roomba-s-data-collection\nhttps://www.therobotreport.com/irobot-addresses-privacy-concerns-pending-amazon-deal/\nRelated \ud83c\udf10\nRoomba may map homes, collect customer data\nCrisis Text Line data sharing\nPage info\nType: Incident\nPublished: January 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chess-robot-breaks-childs-finger", "content": "Chess robot breaks child's finger\nOccurred: July 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA chess-playing robot grabbed and broke the finger of a seven-year old competitor during a match at the Moscow Open, according (in Russian) to Tass.\nApparently unsettled by the quick responses of his opponent, the robot is seen to have pinched the boy's finger for several seconds before he is freed and led away by a woman and three men. The boy played the rest of the tournament in a plaster cast.\nRussian Chess Federation vice-president Sergey Smagin told Tass the robot appeared to pounce after the boy opted for a quick move after it had taken one of the boy\u2019s pieces, thereby violating 'certain safety rules'.\nThe Russian Chess Federation declined to say who manufactured the robot, or how it works. Video of the incident shows the machine appears to be a standard industrial robot arm that has been customised to move pieces on three chess boards simultaneously. \nSystem \ud83e\udd16\nUnknown\nOperator: \nDeveloper: \nCountry: Russia\nSector: Media/entertainment/sports/arts\nPurpose: Play chess\nTechnology: Robotics; Computer vision\nIssue: Safety\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://t.me/bazabazon/12441\nhttps://tass.ru/sport/15280405\nhttps://www.theguardian.com/sport/2022/jul/24/chess-robot-grabs-and-breaks-finger-of-seven-year-old-opponent-moscow\nhttps://www.cnn.com/2022/07/25/europe/chess-robot-russia-boy-finger-intl-scli/index.html\nhttps://www.bbc.co.uk/news/world-europe-62286017\nhttps://www.msnbc.com/msnbc/watch/watch-chess-robot-breaks-kid-s-finger-in-moscow-match-144728645514\nhttps://www.washingtonpost.com/sports/2022/07/24/chess-playing-robot-breaks-finger-7-year-old-boy-during-match/\nhttps://www.abc.net.au/news/2022-07-25/chess-robot-breaks-finger-of-seven-year-old-opponent/101265856\nhttps://www.theverge.com/2022/7/25/23276982/chess-robot-breaks-childs-finger-russia-tournament\nRelated \ud83c\udf10\nOcado robot collision\nOcado robot charger malfunction\nPage info\nType: Incident\nPublished: January 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/apple-crash-detection-false-positives", "content": "Apple Crash Detection overwhelms emergency response centres\nReleased: October 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nFalse positives in Apple's Crash Detection system had led to US ski resort emergency despatch centres being overwhelmed with notifications.\nApple's iPhone 14 and Apple Watch Series 8 watches feature a new 'Crash detection' feature that automatically calls 911 in the US and Canada when the devices detect a sudden stop that indicates the user has been involved in a car, SUV, or van crash.\nThe system quickly proved useful in some instances, though reports in the Wall Street Journal and other publications of false crashes triggered by people riding rollercoasters suggest it was not working as it should.\nShortly afterwards, ski resort emergency despatch centres across the US and Canada were overwhelmed by hundreds of automated crash notifications calls, none of which turned out to be an emergency. \nThe calls diverted resources away from real emergencies, resulting in a loss of productivity. If the skier in question failed to answer a return call, ski patrollers were sent to check the location of the automated call. \nSystem \ud83e\udd16\nhttps://support.apple.com/en-us/HT213225\nhttps://www.youtube.com/watch?v=ZqqraWbJWjA\nhttps://apple.ent.box.com/s/vmfqxuqdxmov9rofz533hd88p8ycfb2b/folder/173575212381\nOperator: Apple\nDeveloper: Apple\nCountry: Apple\nSector: Automotive\nPurpose: Detect vehicle crashes\nTechnology: Motion sensor algorithm; Gyroscope: Accelerometer; GPS; Barometer\nIssue: Accuracy/reliability; Safety\nTransparency: Governance; Black box; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://techcrunch.com/2022/11/30/apples-ios-update-just-dropped-with-security-fixes-and-crash-detection-improvements/\nhttps://www.digitaltrends.com/mobile/apple-watch-series-8-crash-detection-already-saved-someones-life/\nhttps://www.wsj.com/articles/the-owner-of-this-iphone-was-in-a-severe-car-crashor-just-on-a-roller-coaster-11665314944\nhttps://coloradosun.com/2022/12/26/skier-iphone-crash-detection-calls/\nhttps://techcrunch.com/2022/10/10/apple-offers-a-deeper-dive-into-crash-detection/\nhttps://www.techspot.com/news/96840-apple-crash-detection-continues-trigger-false-positives-time.html\nhttps://appleinsider.com/articles/22/12/26/iphone-14-crash-detection-still-sending-deluge-of-false-alarms-from-skiers\nhttps://www.androidpolice.com/apple-rollercoaster-car-crash-detection/\nhttps://calgary.ctvnews.ca/apple-crash-detection-feature-causing-false-alarms-for-b-c-search-and-rescue-crews-1.6196851\nhttps://www.theglobeandmail.com/canada/british-columbia/article-apple-updates-software-after-crash-detection-system-prompts-false/\nhttps://jalopnik.com/iphone-14-crash-sensor-motorcycle-accident-false-alarm-1849574562\nhttps://www.digitaltrends.com/mobile/iphone-14-crash-detection-system-false-alarm-scary-results/\nhttps://gearjunkie.com/news/apples-crash-detection-911-false-alarms\nRelated \ud83c\udf10\nApple Cycle Tracking fertility predictions\nApple Watch blood oximeter racial bias\nPage info\nType: Incident\nPublished: January 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/neuro-sama-ai-v-tuber", "content": "Neuro-sama AI v-tuber denies Holocaust, womens' rights\nOccurred: December 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nNeuro-sama, a virtual, Japanese anime-style v-tuber that is controlled by AI, made offensive and inflammatory comments, including denying the Holocaust and women's rights. \nFirst developed in 2019 and upgraded in December 2022 using a large language model, Neuro-sama resides on live video streaming service Twitch, where it plays Minecraft and osu, a musical rhythm game, and talks to its followers via Twitch chat. \nNeuro-sama developer Vedal argued the character had a number of in-built filters, and that it's discussions were moderated. \nIn January 2023, Neuro-sama was banned from Twitch.\nSystem \ud83e\udd16\nVedal987 Twitch channel\nNeuro-sama Wikipedia profile\nOperator: Vedal987\nDeveloper: Vedal987\nCountry: Japan\nSector: Media/entertainment/sports/arts\nPurpose: Engage audiences\nTechnology: Chatbot; Neural network; NLP/text analysis\nIssue: Safety\nTransparency: Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.vice.com/en/article/pkg98v/this-virtual-twitch-streamer-is-controlled-entirely-by-ai\nhttps://www.svg.com/1156347/this-ai-twitch-streamer-is-hilarious-and-horrifying/\nhttps://wegotthiscovered.com/gaming/twitchs-latest-star-neuro-sama-is-completely-ai-driven-and-as-youd-expect-things-occasionally-get-dicey/\nhttps://kotaku.com/vtuber-twitch-holocaust-denial-minecraft-ai-chatgpt-1849960527\nhttps://gaming.ebaumsworld.com/articles/ai-vtuber-streamer-cant-stop-telling-your-mom-jokes/87330171/\nhttps://techgameworld.com/neuro-sama-the-fully-ai-controlled-virtual-twitch-streamer/\nhttps://www.dexerto.com/entertainment/ai-vtuber-neuro-sama-future-twitch-streaming-gaming-2019378/\nhttps://www.dexerto.com/entertainment/ai-vtuber-neuro-sama-banned-on-twitch-after-controversial-holocaust-comments-2030604/\nhttps://www.reddit.com/r/osugame/comments/btlcu6/neural_network_learns_to_aim_in_osu/\nRelated \ud83c\udf10\nWitcher 3 AI voice line simulation\nAI Dungeon offensive speech filter\nPage info\nType: Incident\nPublished: January 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/flo-menstrual-cycle-data-sharing", "content": "Flo covertly shares users' menstrual cycle data \nOccurred: February 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nPeriod tracker Flo shared data with Facebook, Google, and others every time a user had their period or indicated that they wanted to get pregnant, despite promising to keep users\u2019 sensitive health data private.\nFlo is a US-based period tracker and pregnancy app used by over 100 million people which allows users to 'access personalized health insights, virtual dialogs, and dozens of courses to learn how your cycle affects your body and well being.'\nA critical WSJ article sparked a strong backlash from civil rights and privacy groups, and customers.\n\u2795 The finding resulted in Flo Health settling with the US Federal Trade Commission (FTC) in January 2021.\n\nAccording to the terms of the deal, Flo must notify affected users about the disclosure of their health information and stop misrepresenting how it collects, manages, and uses customer data.\n\u2795 Flo subsequently advised customers it would launch an \u2018Anonymous Mode\u2019 that removes their personal identity from their accounts, prompting further users to cancel their accounts.\nSystem \ud83e\udd16\nFlo website\nFlo (app) Wikipedia profile\nOperator: Flo Health\nDeveloper: Flo Health\nCountry: USA\nSector: Health\nPurpose: Track menstrual cycle\nTechnology: Prediction algorithm\nIssue: Privacy\nTransparency: Governance; Privacy; Marketing\nResearch, advocacy \ud83e\uddee\nhttps://privacyinternational.org/long-read/3196/no-bodys-business-mine-how-menstruations-apps-are-sharing-your-data\nhttps://www.privacyinternational.org/long-read/4316/we-asked-five-menstruation-apps-our-data-and-here-what-we-found\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nhttps://www.ftc.gov/business-guidance/blog/2021/01/health-app-broke-its-privacy-promises-disclosing-intimate-details-about-users\nhttps://www.ftc.gov/news-events/news/press-releases/2021/06/ftc-finalizes-order-flo-health-fertility-tracking-app-shared-sensitive-health-data-facebook-google\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.wsj.com/articles/you-give-apps-sensitive-personal-information-then-they-tell-facebook-11550851636\nhttps://techcrunch.com/2021/01/13/flo-gets-ftc-slap-for-sharing-user-data-when-it-promised-privacy/\nhttps://www.theverge.com/2021/1/13/22229303/flo-period-tracking-app-privacy-health-data-facebook-google\nhttps://www.thelily.com/more-than-100-million-women-use-flo-a-period-tracking-app-heres-why-some-are-deleting-it/\nhttps://www.nytimes.com/2021/01/28/us/period-apps-health-technology-women-privacy.html\nhttps://www.cnet.com/news/privacy/these-menstrual-tracking-apps-reportedly-shared-sensitive-data-with-facebook/\nhttps://www.theguardian.com/world/commentisfree/2019/sep/14/your-period-tracking-app-could-be-sharing-intimate-details-with-all-of-facebook\nhttps://www.theguardian.com/society/2020/dec/21/menstruation-apps-store-excessive-information-privacy-charity-says\nhttps://nypost.com/2022/06/27/period-tracker-apps-say-they-will-protect-womens-data/\nhttps://www.washingtonpost.com/technology/2022/05/07/period-tracking-privacy/\nhttps://www.zdnet.com/article/fertility-tracking-app-flo-health-settles-ftc-allegations-of-inappropriate-data-sharing/\nRelated \ud83c\udf10\nApple fertility predictions\nGoogle DeepMind, Royal Free data sharing\nPage info\nType: Incident\nPublished: January 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/louisiana-police-randall-reid-wrongful-arrest-jailing", "content": "Randal Reid facial recognition wrongful arrest, jailing\nOccurred: November 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe use of facial recognition technology by Louisiana authorities led to the arrest and jailing of Randal Reid, a Georgia man on a fugitive warrant from a state he had never visited, according to his lawyer.\nReid was detained in DeKalb County, Georgia, in late November 2022, after Louisiana police used facial recognition to mistakenly link him to the theft of USD 13,000 of purses in Baton Rouge, Louisiana. Reid spent six days in jail and missed a week of work due to the incident.\nAccording to AP, 'Jefferson Sheriff Joe Lopinto\u2019s office did not respond to several requests for information [..] on Reid\u2019s arrest and release, the agency\u2019s use of facial recognition or any safeguards around it.' Gizmodo reports Louisiana authorities 'admitted the false match 'tacitly''.\nIn July 2022, New Orleans city council voted to allow the New Orleans Police Department (NOPD) to use facial recognition, having outlawed its use in December 2020. \nPer Protocol, the department never kept records of how often the technology was used or whether it facilitated investigations or led to arrests or convictions.\nSystem \ud83e\udd16\nNew Orleans Police Department website\nNew Orleans Police Department Wikipedia profile\nIDEMIA facial recognition website\nIDEMIA Wikipedia profile\nDocuments \ud83d\udcc3\nNew Orleans City Council Meeting minutes (2022)\nOperator: Jefferson Parish Sheriff's Office; Louisiana State Analytical and Fusion Exchange  \nDeveloper: IDEMIA\nCountry: USA\nSector: Govt - police\nPurpose: Identify criminals\nTechnology: Facial recognition\nIssue: Accuracy/reliability; Privacy\nTransparency: Governance; Black box; Marketing; Privacy \nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nJefferson Parish Sheriff's Office affidavit for arrest warrant (2022) (pdf)\nDe Kalk County Police Department Incident Report (2022) (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.nola.com/news/crime_police/jpso-used-facial-recognition-to-arrest-a-man-it-was-wrong/article_0818361a-8886-11ed-8119-93b98ecccc8d.html\nhttps://apnews.com/article/technology-louisiana-baton-rouge-new-orleans-crime-50e1ea591aed6cf14d248096958dccc4\nhttps://www.cbs42.com/news/crime/lawyer-claims-facial-recognition-tool-led-to-his-client-mistakenly-being-arrested-in-georgia/\nhttps://www.dailymail.co.uk/news/article-11593521/Facial-recognition-technology-blamed-mistaken-arrest-Louisiana-purse-snatching-case.html\nhttps://gizmodo.com/facial-recognition-randall-reid-black-man-error-jail-1849944231\nhttps://www.nytimes.com/2023/03/31/technology/facial-recognition-false-arrests.html\nhttps://www.dailymail.co.uk/news/article-11926021/Georgia-man-29-spends-week-jail-faulty-facial-recognition-ID.html\nRelated \ud83c\udf10\nApple/SIS misidentification, wrongful arrest\nLivonia skating rink misidentifies black teenager\nPage info\nType: Incident\nPublished: January 2023\nLast updated: August 2023", "year": "1"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/southwest-airlines-crew-scheduling-automation", "content": "Southwest Airlines automated scheduling systems malfunction\nOccurred: December 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nMalfunctions of US carrier Southwest Airlines' automated flight and crew scheduling systems resulted in the cancellation of over 15,000 flights during poor weather, leaving travelers stranded over Christmas and needing to find alternative transportation.\nSouthwest used SkySolver and Crew Web Access software to assign flight attendants and pilots to each flight and to correct scheduling interruptions by moving planes and staff around the country as quickly as possible.\nHowever, airline staff, unions, air industry professionals and commentators complained the systems were 'archaic', unable to meet increased demand, and that their overhaul had been put on ice for too long by Southwest's increasingly financial, as opposed to operations-driven, leadership.\nSouthwest Airlines' new CEO Bob Jordan has since acknowledged the airline's technology troubles, telling employees that 'they\u2019ll be hearing more about our specific plans to ensure the challenges that they\u2019ve faced the past few days will not be part of our future.'\nSystem \ud83e\udd16\nSouthwest Airlines flight scheduling system\nSouthwest Airlines crew scheduling system \nDocuments \ud83d\udcc3\nhttps://swamedia.com/releases/release-7e05ea1637937dc7354128bee780ad75-video-southwest-airlines-ceo-bob-jordan-issues-statement\nOperator: Southwest Airlines\nDeveloper: General Electric\nCountry: USA\nSector: Transport/logistics\nPurpose: Schedule crew \nTechnology: Crew scheduling software\nIssue: Accuracy/reliability; Robustness\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reuters.com/world/us/southwest-cancels-thousands-more-us-flights-weather-stays-bitter-2022-12-27/\nhttps://www.wsj.com/articles/southwest-airlines-melting-down-flights-cancelled-11672257523\nhttps://www.nytimes.com/2022/12/29/business/southwest-canceled-flights-updates.html\nhttps://www.nytimes.com/2022/12/31/opinion/southwest-airlines-computers.html\nhttps://reason.com/2022/12/30/what-the-southwest-meltdown-means-for-airline-policy/\nhttps://www.star-telegram.com/news/business/article270469702.html\nhttps://www.goodmorningamerica.com/US/story/airlines-cancel-thousands-flights-amid-winter-storm-chaos-95834221\nhttps://www.dallasnews.com/business/airlines/2022/12/30/whats-the-problem-with-southwest-airlines-scheduling-system/\nhttps://www.dallasnews.com/business/airlines/2022/12/29/holiday-meltdown-exposes-southwest-airlines-technology-woes/\nhttps://viewfromthewing.com/the-leader-of-southwest-airlines-pilots-has-something-to-say-about-his-companys-big-mess/\nhttps://www.ainonline.com/aviation-news/air-transport/2022-12-27/southwest-airlines-struggles-normalize-stricken-ops\nhttps://rabble.ca/columnists/climate-change-christmas-and-capitalism/\nRelated \ud83d\uddde\ufe0f\nGorillas 'Project Ace' rider work schedule automation\nStarbucks automated shift scheduling\nPage info\nType: Incident\nPublished: December 2022\nLast updated: January 2023", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/binance-cco-deepfake-impersonation", "content": "Binance CCO Patrick Hillmann impersonated by deepfake\nOccurred: August 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Binance cryptocurrency exchange employee claimed that a 'sophisticated hacking team' manipulated video footage of his past TV appearances to make an 'AI hologram' of him to make people think he was helping them get listed on the exchange. \nAccording to Patrick Hillmann, the AI hologram was able to fool representatives of several cryptocurrency projects in Zoom calls into believing that they were being considered for listing on Binance.\nThe listing scheme was discovered when crypto project members contacted Hillmann to thank him for his help in the alleged listing opportunities.\n However, he had no knowledge of these meetings because he is not part of the listing process at Binance, he wrote. \nSystem \ud83e\udd16\nPatrick Hillmann (2022). Scammers Created an AI Hologram of Me to Scam Unsuspecting Projects\nOperator: Anonymous/pseudonymous\nDeveloper: Unclear/unknown\nCountry: USA\nSector: Banking/financial services\nPurpose: Defraud\nTechnology: Deepfake - video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Security; Ethics\nTransparency: Governance; Privacy\nInvestigations, assessments, audits \ud83e\uddd0\nDeepfaked. Hackers use deepfake of Binance CCO in exchange listing scams\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://news.bitcoin.com/hackers-used-deepfake-of-binance-cco-to-perform-exchange-listing-scams/\nhttps://www.theverge.com/2022/8/23/23318053/binance-comms-crypto-chief-deepfake-scam-claim-patrick-hillmann\nhttps://www.pcmag.com/news/fraudsters-created-a-deepfake-of-binance-executive-to-dupe-crypto-developers\nhttps://www.techradar.com/news/scammers-created-a-deepfake-of-top-binance-exec-to-steal-crypto-funds\nhttps://gizmodo.com/crypto-binance-deepfakes-1849447018\nhttps://www.engadget.com/binance-executive-deepfake-listing-scam-154729114.html\nhttps://www.thetimes.co.uk/article/scammers-send-fake-crypto-boss-to-business-meetings-75x9gl9v3\nhttps://www.theregister.com/2022/08/23/binance_deepfake_scam/\nhttps://www.euronews.com/next/2022/08/24/binance-executive-says-scammers-created-deepfake-hologram-of-him-to-trick-crypto-developer\nhttps://www.infosecurity-magazine.com/news/scammers-create-ai-hologram-csuite/\nhttps://www.law360.com/articles/1524013/hackers-used-deepfake-of-binance-exec-in-zoom-scam\nRelated \ud83c\udf10\nFTX CEO deepfake scam\nPresident Zelenskyy deepfake surrender\nPage info\nType: Incident\nPublished: December 2022", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ftx-ceo-deepfake", "content": "Deepfake impersonating FTX CEO attempts to scan investors\nOccurred: November 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA fake video of Sam Bankman-Fried (SBF), the disgraced former CEO of cryptocurrency exchange FTX, attempted to scam investors affected by the exchange\u2019s bankruptcy. \nThe poorly made 'deepfake' video, which is a manipulated version of a Bloomberg Markets and Finance interview, attempts to direct users to a malicious website under the promise of a 'giveaway' that will double your cryptocurrency. \nThe Twitter account was verified and mimicked SBF\u2019s real account. \nThe scam appeared to take advantage of Twitter CEO Elon Musk's decision to sell blue verification checks for USD 8. ftxcompensation.com prominently featured Bankman-Fried\u2019s face and FTX\u2019s logo and was registered to an individual in Nevis, near Puerto Rico. The website has since been taken offline. \nSystem \ud83e\udd16\nUnknown\nDocuments \ud83d\udcc3\nDeepfake FTX CEO video\nOperator:  \nDeveloper: \nCountry: Bahamas; USA\nSector: Banking/financial services\nPurpose: Defraud\nTechnology: Deepfake - video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Security; Ethics\nTransparency: Governance; Privacy\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://gizmodo.com/sbf-sam-bankman-fried-deepfake-ftx-crypto-1849808634\nhttps://www.vice.com/en/article/v7vj9a/sam-bankman-fried-deepfake-offers-refund-to-victims-in-verified-twitter-account-scam\nhttps://cointelegraph.com/news/sam-bankman-fried-deepfake-attempts-to-scam-investors-impacted-by-ftx\nhttps://www.dailymail.co.uk/news/article-11458907/Scammers-target-FTX-victims-deepfake-video-disgraced-founder.html\nhttps://decrypt.co/115207/sbf-deepfake-scam-ftx-collapse\nhttps://cryptonews.com/news/modified-video-of-ftx-founder-sam-bankman-fried-directs-users-to-fraudulent-website-this-is-what-you-need-to-know.htm\nhttps://www.thecoinrepublic.com/2022/11/22/sam-bankman-fried-attempts-to-fraud-investors-affected-by-ftx/\nhttps://www.eviemagazine.com/post/scammer-deepfake-ftx-founder-samuel-bankman-fried-customers-wallet\nhttps://www.cnet.com/personal-finance/crypto/the-fall-of-ftx-and-sam-bankman-fried-a-full-timeline-of-events/\nRelated \ud83c\udf10\nLauren Book deepfake extortion\nPresident Zelenskyy deepfake surrender\nPage info\nType: Incident\nPublished: December 2022", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/fn-meka-virtual-rapper", "content": "FN Meka virtual rapper accused of racial stereotyping\nOccurred: August 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe creators of self-described 'virtual rapper' FN Meka were accused of racial stereotyping black people by not using Black singers to record its voice, contrary to public statements. \nFN Meka was iitially designed by digital designer Brandon Le and music manager Anthony Martini, co-founders of 'next-generation music company' Factory New, to sell NFTs and post videos of its lifestyle, including Bugatti jets, helicopters and a Rolls Royce custom-fit with a Hibachi grill.\nThe rapper made its debut in April 2019 and quickly became popular, garnering over a billion views on TikTok. Signed to Capitol Records in August 2022, FN Meka was the first 'AI-generated' rapper to be signed to a major label. \nMeka was dropped by Capitol Records ten days later its signing. US rapper Kyle the Hooligan subsequently claimed that he was the original voice of FN Meka, and announced that he intended to sue Brandon Le and Factory New.\nFactory New stood accused of fabricating the origin of FN Meka's voice and, given the White and Asian backgrounds of its makers, of racial appropriation, hypocrisy and fakeness.\nSystem \ud83e\udd16\nUnknown\nOperator: Factory New; Capitol Music Group/Capitol Records\nDeveloper: Factory New\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Entertain; Sell NFTs \nTechnology: Augmented Reality (AR); Virtual reality (VR); NFT\nIssue: Bias/discrimination - race, ethnicity; Hypocrisy\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.musicbusinessworldwide.com/this-robot-rapper-has-9-million-followers-on-tiktok-his-creator-thinks-traditional-ar-is-inefficient-and-unreliable/\nhttps://www.msn.com/en-us/music/news/capitol-drops-ai-rapper-fn-meka-from-label-after-backlash-over-gross-stereotypes/ar-AA113Puz\nhttps://www.bbc.co.uk/news/newsbeat-62659741\nhttps://www.the-sun.com/tech/6066770/fn-meka-rapper-created/\nhttps://www.theguardian.com/music/2022/aug/24/major-record-label-drops-offensive-ai-rapper-after-outcry-over-racial-stereotyping\nhttps://www.tmz.com/2022/08/28/fn-meka-kyle-the-hooligan-lawsuit-capitol-records-factory-new-brandon-le/\nhttps://musically.com/2022/08/24/capitol-drops-virtual-rapper-fn-meka-after-criticism/\nhttps://www.billboard.com/pro/virtual-rapper-fn-meka-exec-leaves-project-backlash/\nhttps://pitchfork.com/news/fn-meka-backer-walks-away-from-project-rapper-claims-to-be-unpaid-for-work-as-mekas-voice/\nhttps://completemusicupdate.com/article/fn-meka-spokesperson-anthony-martini-quits-project-admits-there-was-less-ai-involved-than-claimed/\nhttps://www.vice.com/amp/en/article/qjkjzw/ai-rapper-fn-meka-kyle-the-hooligan-interview\nhttps://www.msnbc.com/the-reidout/reidout-blog/fn-meka-capitol-records-ai-rapper-rcna44785\nhttps://www.cnn.com/2022/08/24/entertainment/fn-meka-dropped-capitol-records-cec/index.html\nhttps://www.rollingstone.com/music/music-features/fn-meka-controversy-ai-1234585293/\nhttps://www.vulture.com/2022/08/fn-meka-dropped-capitol-records-tik-tok-rapper.html\nhttps://mashable.com/article/fn-meka-record-deal-racial-stereotyping\nhttps://www.nytimes.com/2022/08/23/arts/music/fn-meka-dropped-capitol-records.html\nhttps://www.musicbusinessworldwide.com/this-robot-rapper-has-9-million-followers-on-tiktok-his-creator-thinks-traditional-ar-is-inefficient-and-unreliable/\nRelated \ud83c\udf10\nVoiceverse NFT voice theft\nImageNet dataset racial, gender stereotyping\nPage info\nType: Incident\nPublished: December 2022", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/terrausd-algorithmic-stablecoin", "content": "TerraUSD (UST) algorithmic stablecoin collapses\nOccurred: May 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe TerraUSD algorithmic stablecoin collapsed, wiping out USD 55 billion of market capitalisation and leaving investors out of pocket.\nIt remains unclear what precipitated the collapse, with some blaming an acceleration of customer withdrawals spiralling into a de facto bank run, thereby destabilising the algorithmic balancing mechanism between TerraUSD and LUNA. \nAttempts by TerraLabs to save UST\u2019s peg by selling almost all of its Bitcoin reserves for UST, as well as other strategies, failed to save the stablecoin. Other cryptocurrencies are tied to more stable assets, such as the US dollar or gold. \nThe crisis fueled a credit crunch that saw hedge fund Three Arrows Capital (3AC) forced into liquidation, and cryptocurrency lender Celsius Network and crypto broker Voyager Digital filing for Chapter 11 bankruptcy. \nAt the time of its collapse, TerraUSD had been the third largest stablecoin by market capitalisation, and Luna one of the ten largest cryptocurrencies.\nTerraUSD (also known as UST) is an algorithmic stablecoin created in 2018 by Terraform Labs that is built on the Terra blockchain. Pegged to the US dollar through a complex algorithmic relationship with its support cryptocurrency coin LUNA, TerraUSD can be exchanged with LUNA tokens.\nPer The Register, UST and Luna are linked by an algorithm in a 'smart contract' that's supposed to keep the value of the UST pegged to 1 USD without fiat currency reserves. The algorithm burns or deletes Luna tokens to create new UST tokens and vice versa in an effort to balance supply and demand.\n\u2795 In February 2023, the US Securities and Exchange Commission (SEC) charged (pdf) Terraform Labs Do Kwon with fraud.\nTransparency \ud83d\ude48\nDefended by Do Kwon as a 'transparent and open source' system, the collapse of TerraUSD is viewed as much a governance and communications failure as a pure algorithmic failure.\nFacing allegations of fraud and embezzlement, including that TerraUSD was a ponzi scheme, TerraLabs co-founder Do Kwon disappeared after TerraUSD's collapse. Arrest warrants have been issued for Kwon and his fellow co-founder Daniel Shin.\nSystem \ud83e\udd16\nhttps://www.terra.money/\nhttps://en.wikipedia.org/wiki/Terra_(blockchain)\n\nDocuments \ud83d\udcc3\nhttps://medium.com/terra-money/announcing-terrausd-ust-the-interchain-stablecoin-53eab0f8f0ac\nhttps://www.youtube.com/watch?v=KqpGMoYZMhY\nOperator: Terraform Labs\nDeveloper: Terraform Labs\nCountry: Singapore; Global\nSector: Banking/financial services\nPurpose: Manage financial marketplace\nTechnology: Blockchain; Virtual currency; Stablecoin; Smart contract algorithm\nIssue: Robustness\nTransparency: Governance; Black box; Marketing\nInvestigations, assessments, audits \ud83e\uddd0\nhttps://www.nytimes.com/2022/12/07/business/ftx-sbf-crypto-market-investigation.html\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nhttps://www.pacermonitor.com/public/case/44953421/Patterson_v_TerraForm_Labs_Pte_Ltd_et_al\nhttps://www.sec.gov/litigation/complaints/2023/comp-pr2023-32.pdf\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theguardian.com/technology/2022/may/11/terra-price-cryptocurrency-stablecoin\nhttps://www.theguardian.com/technology/2022/may/12/stablecoin-tether-breaks-dollar-peg-cryptocurrencies\nhttps://www.theguardian.com/technology/2022/may/16/qa-the-collapse-of-terra-and-what-it-could-mean-beyond-crypto\nhttps://www.cnbc.com/2022/05/16/what-happened-to-the-bitcoin-reserve-behind-terras-ust-stablecoin.html\nhttps://fortune.com/2022/05/13/crypto-crash-rivals-internet-dotcom-bubble-burst-and-great-financial-crisis-bank-of-america/\nhttps://www.bloomberg.com/news/articles/2022-05-13/terraform-again-halts-blockchain-behind-ust-stablecoin-luna\nhttps://www.theregister.com/2022/05/17/terrausd_luna_crash/\nhttps://fortune.com/2022/05/19/luna-terrausd-ust-algorithmic-stablecoins-doomed\nhttps://www.bloomberg.com/news/articles/2022-05-12/terra-implosion-shakes-foundations-of-crypto-stablecoin-complex\nhttps://www.bloomberg.com/graphics/2022-crypto-luna-terra-stablecoin-explainer/\nhttps://micky.com.au/tether-transparency-issue-resurfaces-after-signals-of-empty-cash-reserves\nhttps://cointelegraph.com/news/terra-labs-luna-guard-commission-audit-to-defend-against-allegations-of-misusing-funds\nhttps://news.sbs.co.kr/amp/news.amp?news_id=N1006748728\nhttps://m.sedaily.com/NewsViewAmp/265YD3LRNB\nRelated \ud83c\udf10\nWorldcoin 'field testing'\nThe DAO smart contracts hack\nPage info\nType: Incident\nPublished: December 2022\nLast updated: May 2024", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/edmonton-sexual-assault-dna-phenotyping", "content": "Edmonton sexual assault DNA phenotyping accused of racial stereotyping\nOccurred: December 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe Edmonton Police Service (EPS) came under fire from Black and civil rights organisations for using a controversial DNA prediction technology to solve a 2019 sexual assault in which there no witnesses, surveillance video, public tips or DNA matches.\nTheir investigation having gone cold, the police turned to Parabon NanoLabs to conduct DNA phenotyping, which uses a person\u2019s genetic material to predict parts of their appearance such as eye, skin and hair colour, as well as facial features including shape. Police can use such information to narrow suspects and generate leads in criminal investigations. \nParabon produced for EPS a composite 'Snapshot' of a five-foot-four Black man with dark brown to black hair and dark brown eyes based on trait predictions from DNA evidence collected from the victim. The police published the image of a suspect for circulation in the hope it would generate leads. \nHowever, it led to a significant backlash, with local communities accusing the police of stereotyping a Black suspect and stigmatising racial groups. Two days later, the EPS apologised and removed the image from its website and social media accounts.\nTransparency \ud83d\ude48\nLike other DNA analysis tools, Parabon's Snapshot is an algorithmic black box 'the exact details of our method [which] have not been published', the company acknowledges, adding that it has 'attempted to be as transparent as possible by presenting our work at conferences and posting every single composite that goes public on our website, so people can draw their own conclusions about how well our technology works.'\nUniversity of Calgary biological anthropologist and evolutionary biologist Benedikt Hallgr\u00edmsson told the New York Times in 2015 that forensic DNA phenotyping was 'a bit of science fiction at this point', and that Parabon 'oversell their ability to predict the facial-shape phenotypes.'\nSystem \ud83e\udd16\nSnapshot website\nDocuments \ud83d\udcc3\nhttps://pub.parabon.com/Parabon_Snapshot_Workflow_Diagram.pdf\nhttps://www.edmontonpolice.ca/News/MediaReleases/DNAPhenotypeOct4 \nhttps://www.edmontonpolice.ca/News/MediaReleases/StatementonDNAPhenotyping\nOperator: Edmonton Police Service\nDeveloper: Parabon NanoLabs\nCountry: Canada\nSector: Govt - police\nPurpose: Predict physical appearance\nTechnology: DNA phenotyping\nIssue: Accuracy/reliability; Bias/discrimination - race, ethnicity; Privacy\nTransparency: Governance; Black box; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.thestar.com/news/canada/2022/10/04/edmonton-police-use-dna-phenotyping-to-find-sex-assault-suspect.html\nhttps://www.cbc.ca/news/canada/edmonton/edmonton-police-issue-apology-for-controversial-use-of-dna-phenotyping-1.6608457\nhttps://edmontonjournal.com/news/local-news/africa-centre-black-community-group-call-for-end-to-police-dna-phenotyping\nhttps://edmontonjournal.com/news/local-news/coffee-with-the-edmonton-police-chief\nhttps://globalnews.ca/news/9175041/edmonton-police-dna-phenotyping-sexual-assault-suspect/\nhttps://www.cbc.ca/news/canada/edmonton/racial-profiling-dna-phenotyping-edmonton-police-1.6624397\nhttps://www.biometricupdate.com/202210/suspect-image-based-on-dna-phenotyping-pulled-back-by-edmonton-police\nhttps://www.vice.com/en/article/pkgma8/police-are-using-dna-to-generate-3d-images-of-suspects-theyve-never-seen\nhttps://race.undark.org/articles/interview-jonathan-kahn-on-a-new-potential-frontier-in-racial-profiling\nhttps://ca.news.yahoo.com/edmonton-police-issue-apology-controversial-174540173.html\nhttps://www.canadapolicereport.ca/2022/10/04/edmonton-police-use-dna-phenotyping-in-unsolved-sexual-assault/\nhttps://www.cbc.ca/news/canada/edmonton/edmonton-police-phenotype-science-1.6609320\nhttps://nationalpost.com/opinion/colby-cosh-when-police-incompetence-collides-with-racial-politics\nhttps://news.yahoo.com/mass-surveillance-police-using-dna-192540060.html\nRelated \ud83d\uddde\ufe0f\nTrueAllele automated DNA analysis\nDNA evidence is bungled in dozens of Queensland crimes\nPage info\nType: Incident\nPublished: December 2022\nLast updated: June 2024", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/san-francisco-police-killer-robots", "content": "San Francisco police 'killer robot' plan shot down\nOccurred: December 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA policy authorising the San Francisco Police Department (SFPD) to use military-style weapons, including remote-controlled robots, to kill suspects was put on hold by the city's supervisors.\nAccording to the SFPD, it's existing armoury of 17 Remotec and QinetiQ robots could be equipped with explosives 'to contact, incapacitate, or disorient [a] violent, armed, or dangerous suspect' in 'extreme circumstances to save or prevent further loss of innocent lives.' \nLate November 2022, San Francisco's board of supervisors approved a policy that lets police robots 'be used as a deadly force option when risk of loss of life to members of the public or officers is imminent and outweighs any other force option available.'\nHowever, the decision resulted in an immediate backlash from civil rights groups. The Electronic Frontier Foundation (EFF) argued it was typical police-military mission creep. The San Francisco Public Defender\u2019s office warned that granting police 'the ability to kill community members remotely' went against the city\u2019s progressive values. \nA week later the boad suspended the policy, sending it back to back to committee for further discussion.\nSystem \ud83e\udd16\nRemotec robot\nQinetiQ robot\nDocuments \ud83d\udcc3\nhttps://sfgov.legistar.com/View.ashx?M=F&ID=11449771&GUID=9FC57C5A-6E68-4485-A989-632C3837B909\nOperator: San Francisco Police Department (SFPD)\nDeveloper: Remotec; QinetiQ  \nCountry: USA\nSector: Govt - police\nPurpose: Strengthen security\nTechnology: Robotics\nIssue: Safety; Scope creep/normalisation; Ethics\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nhttps://www.eff.org/document/san-francisco-killer-robot-local-coalition-letter\nhttps://www.eff.org/deeplinks/2022/11/red-alert-sfpd-want-power-kill-robots\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://apnews.com/article/technology-police-san-francisco-government-and-politics-a392e5a7c1aaac8f58387dde672a7fd1\nhttps://news.sky.com/story/san-francisco-police-allowed-to-use-remote-controlled-robots-that-can-kill-12758631\nhttps://www.engadget.com/san-francisco-police-seek-permission-for-its-robots-to-use-deadly-force-183514906.html\nhttps://missionlocal.org/2022/11/killer-robots-to-be-permitted-under-sfpd-draft-policy/\nhttps://www.theverge.com/2022/11/23/23475817/san-francisco-police-department-robots-deadly-force\nhttps://sanfrancisco.granicus.com/player/clip/42469?view_id=13&redirect=true&h=0b497d058638b56b42559b53ad3a4790\nhttps://www.aljazeera.com/news/2022/11/30/san-francisco-police-given-power-to-use-killer-robots\nhttps://www.thedailybeast.com/killer-robots-officially-approved-for-use-by-san-francisco-police\nhttps://www.theguardian.com/us-news/2022/nov/29/san-francisco-police-robots-deadly-force\nhttps://www.foxnews.com/us/san-francisco-approves-plan-police-robots-deadly-force-emergency-situations\nhttps://news.sky.com/story/san-francisco-police-allowed-to-use-remote-controlled-robots-that-can-kill-12758631\nhttps://www.newsweek.com/san-francisco-police-force-killer-robot-proposal-jokes-memes-1762255\nhttps://www.washingtonpost.com/nation/2022/12/07/san-francisco-killer-robot-cop/\nRelated \ud83c\udf10\nNYPD 'digidog'\nHonolulu homeless robot temperature testing\nPage info\nType: Incident\nPublished: December 2022", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/toronto-beach-water-quality-predictions", "content": "Toronto beach water quality predictions criticised as inaccurate\nReleased: June 2022 \nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn AI-powered predictive modelling system used by Toronto Public Health (TPH) to forecast water quality at two beaches came under fire for being inaccurate, unreliable, and opaque. \nAdopted by the Toronto authority early summer 2022, the AI predictive modelling (AIPM) system used a series of calculations based on historical data and metrics such as rainfall, temperature and wind direction. It also pulled real-time meteorological and hydrological data. \nHowever, it quickly came under pressure from water advocacy group Swim Drink Fish for repeatedly allowing beaches that tested high for E.coli using traditional means to remain open.\nPer the Toronto Star, TPH responded by saying 'While AIPM is not expected to be 100 per cent accurate in assessing water quality, it presents a significant improvement over test results using the traditional means for assessing microbial water quality.'\nSwim Drink Fish argued greater transparency about how the system worked was required from the Toronto Health Board if the model was to remain so that beachgoers could make educated decisions regarding their own health and safety. \nAccording to The Information, TPH officials 'did not respond to a question about whether officials ever overrode the model\u2019s forecast. But data published by the city show that the posted swimming flags at the two beaches never differed from the model\u2019s predictions.'\nSystem \ud83e\udd16\nToronto Beach Water Quality website\nOperator: Toronto Public Health\nDeveloper: Cann Forecast\nCountry: Canada\nSector: Govt - health\nPurpose: Predict water quality\nTechnology: Prediction algorithm\nIssue: Accuracy/reliability; Safety\nTransparency: Governance; Black box\nResearch, advocacy \ud83e\uddee\nhttps://www.theswimguide.org/beach/81\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.thestar.com/news/gta/2022/08/10/safe-for-swimming-torontos-new-tool-for-measuring-water-quality-at-its-beaches-is-misleading-say-advocates.html\nhttps://www.theinformation.com/articles/when-artificial-intelligence-isnt-smarter\nhttps://canadatoday.news/on/torontos-new-beach-water-quality-testing-tool-under-fire-17284/\nhttps://thecanadian.news/safe-to-swim-torontos-new-tool-for-measuring-water-quality-at-its-beaches-is-misleading-advocates-say/\nRelated \ud83c\udf10\nUK DEFRA Biodiversity Net Gain metric\nDubai drone weather engineering\nPage info\nType: Incident\nPublished: December 2022\nLast updated: May 2024", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/kfc-germany-kristallnacht-marketing-automation", "content": "KFC Germany Kristallnacht automated marketing alert backfires\nOccurred: November 2022 \nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn automated alert sent to customers by KFC in Germany commemorating the 84th anniversary of Kristallnacht (or 'The Night of Broken Glass') backfired after users complained it was 'insensitive' and 'unacceptable'.\nThe alert urged customers to commemorate the Kristallnacht by eating its cheesy fried chicken. The Kristallnacht was the 1938 pogrom that preceded the Holocaustor and saw Nazi mobs destroy synagogues and Jewish-owned businesses, kill 91 Jews and send 30,000 to concentration camps.\nThe promotion suggested customers 'Go ahead and treat yourself to more soft cheese on your crispy chicken. Now available at KFCheese!' under the subject line 'Memorial day for the Reich pogrom night'.\nKFC apologised after customers started complaining and the promotion was picked up by the EU arm of the Anti-Defamation League ('ADL'), whose associate director Dalia Grinfield tweeted 'How wrong can you get on Kristallnacht KFC Germany. Shame on you!'\n\nThe fast food chain blamed the message on an 'automated push notification ... linked to calendars that include national observances', adding that it 'sincerely' apologised for the 'unplanned, insensitive and unacceptable message'.\nSystem \ud83e\udd16\nUnknown\nKFC Germany\nOperator: KFC Germany\nDeveloper: KFC Germany\nCountry: Germany\nSector: Food/food services\nPurpose: Automate marketing communications\nTechnology: Bot/intelligent agent\nIssue: Governance; Safety\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://twitter.com/DaliaGrinfeld/status/1590337407135625217\nhttps://www.bbc.co.uk/news/world-europe-63499057\nhttps://www.bild.de/geld/wirtschaft/wirtschaft/zum-pogromnacht-gedenken-kfc-verschickt-versehentlich-geschmacklos-push-81890356.bild.html\nhttps://www.theguardian.com/world/2022/nov/10/kfc-apologises-for-kristallnacht-chicken-and-cheese-promotion\nhttps://www.aljazeera.com/news/2022/11/11/kfc-apologises-for-kristallnacht-chicken-promotion\nhttps://edition.cnn.com/2022/11/10/business/kfc-germany-kristallnacht/index.html\nhttps://nypost.com/2022/11/10/kfc-apologizes-for-kristallnacht-promotion-in-germany/\nhttps://www.thedailybeast.com/kfc-sorry-for-kristallnacht-chicken-promotion-in-germany\nhttps://www.rte.ie/news/2022/1110/1335363-kfc-germany-kristallnacht/\nhttps://fortune.com/2022/11/11/kfc-blames-kristallnacht-promotion-on-a-bot/\nhttps://www.thetimes.co.uk/article/kristallnacht-treat-yourself-to-chicken-kfc-tells-germans-6b7tjm86q\nhttps://www.cnbc.com/2022/11/10/kfc-apologizes-after-kristallnacht-promotion-in-germany.html\nhttps://www.cbsnews.com/news/kfc-kristallnacht-kentucky-fried-chicken-germany/\nhttps://www.washingtonpost.com/food/2022/11/10/kfc-germany-kristallnacht-promotion/\nhttps://www.newsweek.com/kfc-urges-customers-eat-chicken-remember-nazi-kristallnacht-1758376\nhttps://abc-7.com/news/national-world/2022/11/11/kfc-apologizes-for-app-alert-urging-orders-for-kristallnacht/\nhttps://www.jpost.com/diaspora/antisemitism/article-721945\nRelated \ud83c\udf10\nGorillas 'Project Ace' rider work schedule automation\nMainz police COVID-19 tracing app data theft\nPage info\nType: Incident\nPublished: November 2022", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/upstart-consumer-lending-racial-discrimination", "content": "Upstart consumer loan model discriminates against Blacks\nOccurred: September 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nLaw firm Relman Colfax found that a loan decision model deployed by San Mateo-based AI consumer lender Upstart produced 'significant disparities' in how often loans were made to Black and non-Hispanic white borrowers.\nUpstart assesses and predicts creditworthiness using so-called 'Alternative Data', including a person's educational and employment history. The company says its approach is more inclusive than the underwriting methods used by many banks.\nThe AI lender claims its algorithmic lending model approved 30 percent more Black borrowers than traditional models in 2020. However, academic research published in January 2022 indicated discriminatory pricing exists in both traditional and fintech lending.\nIn July 2022, Upstart directors were hit by a class-action investor lawsuit claiming it made false and misleading statements about its business, operations, and prospects.\nSystem \ud83e\udd16\nUpstart website\nUpstart Holdings Wikipedia profile\nOperator: Upstart Holdings; Multiple\nDeveloper: Upstart Holdings\nCountry: USA\nSector: Banking/Financial services\nPurpose: Assess creditworthiness\nTechnology: Machine learning\nIssue: Bias/discrimination - race, ethnicity, education, employment\nTransparency: Governance; Black box; Marketing - misleading\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nhttps://www.businesswire.com/news/home/20220513005551/en/INVESTOR-ALERT-Upstart-Holdings-Inc.-Investors-with-Substantial-Losses-Have-Opportunity-to-Lead-the-Upstart-Class-Action-Lawsuit---UPST\nhttps://financialservices.house.gov/uploadedfiles/hhrg-117-ba00-wstate-girouardd-20210507.pdf\nInvestigations, assessments, audits \ud83e\uddd0\nhttps://www.relmanlaw.com/media/news/1332_PUBLIC_Upstart_Monitorship_3rd_Report_FINAL.pdf\nhttps://www.relmanlaw.com/media/news/1182_PUBLIC%20Upstart%20Monitorship_2nd%20Report_FINAL.pdf\nhttps://www.relmanlaw.com/media/cases/1086_Upstart%20Initial%20Report%20-%20Final.pdf\nResearch, advocacy \ud83e\uddee\nhttps://www.sciencedirect.com/science/article/abs/pii/S0304405X21002403?via%3Dihub\nhttps://researchrepository.wvu.edu/cgi/viewcontent.cgi?article=6335&context=wvlr\nhttps://www.degruyter.com/document/doi/10.1515/9783110749472-006/pdf\nhttps://protectborrowers.org/wp-content/uploads/2020/02/Education-Redlining-Report.pdf\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.americanbanker.com/news/upstart-says-its-improving-ai-models-after-report-finds-race-approval-disparities \nhttps://www.americanbanker.com/news/fresh-off-ipo-upstart-aims-to-push-boundaries-of-ai-based-lending \nhttps://www.marketwatch.com/story/fico-scores-leave-out-people-on-the-margins-upstarts-ceo-says-can-ai-make-lending-more-inclusive-without-creating-bias-of-its-own-11627040436\nhttps://www.law360.com/articles/1516114/upstart-investor-sues-execs-over-ai-based-lending-claims \nhttps://www.marketwatch.com/story/do-ai-powered-lending-algorithms-silently-discriminate-this-initiative-aims-to-find-out-11637246524\nhttps://www.nationalmortgagenews.com/news/upstart-says-its-improving-ai-models-after-report-finds-race-approval-disparities\nhttps://www.nytimes.com/2020/09/18/business/digital-mortgages.html\nhttps://www.protocol.com/fintech/upstart-loans-balance-sheet\nRelated \ud83c\udf10\nLemonade 'non-verbal cue' insurance claim assessments\nUS mortgage approval data algorithm racial discrimination\nPage info\nType: Incident\nPublished: October 2022\nLast updated: November 2022", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/olive-ai-misleading-marketing", "content": "Olive AI 'over-promises' and 'under-delivers' on capabilities\nOccurred: July 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA US start-up that had promised to turbocharge AI for healthcare was accused of misleading marketing after it laid off 450 employees in a move said by CEO Sean Lane to have been caused by the company's 'fast-paced growth and lack of focus'. \nThe claim was seen as contradicting Axios interviews with former and current employees and health tech executives suggesting the company regularly over-hyped its capabilities and under-delivered, generating 'only a fraction of the savings it promises.'\nOlive had been through various incarnations since it was founded in 2012, and was at one time valued at USD 4 billion. But it was dogged by questions about the efficacy of its software and the integrity of its marketing and leadership. \nIn October 2023, Olive announced it had sold off pieces of its business to healthcare companies Waystar and Humata Health, and would be closing. \nSystem \ud83e\udd16\nOlive AI website\nOlive AI Wikipedia profile\nOperator: WPP/AKQA; Circulo Health; GuideWell; TriHealth\nDeveloper: Olive AI\nCountry: USA\nSector: Health\nPurpose: Automate healthcare services\nTechnology: Robotic Process Automation (RPA); Machine learning\nIssue: Business model; Effectiveness/value\nTransparency: Marketing - misleading\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.axios.com/local/columbus/2022/04/07/local-health-tech-startup-olive-overpromises\nhttps://www.axios.com/pro/health-tech-deals/2022/04/05/4-billion-health-tech-startup-olive-overpromises-and-underdelivers\nhttps://www.axios.com/pro/health-tech-deals/2022/09/26/olive-ai-fires-cfo-cpo-sees-other-c-level-departures\nhttps://www.axios.com/pro/health-tech-deals/2022/07/19/olive-ai-lays-one-third-staff-450-people\nhttps://eu.dispatch.com/story/business/2022/07/19/layoffs-olive-latest-several-area-job-cuts/10098229002/\nhttps://www.fiercehealthcare.com/health-tech/olive-cuts-450-staff-ceo-cites-missteps-fast-growth-lack-focus\nhttps://www.fiercehealthcare.com/tech/olive-rakes-400m-to-turbocharge-growth-humanized-ai-for-healthcare\nRelated \ud83c\udf10\nScaleFactor 'revolutionary AI accountancy software uses humans to process data\nEngineer.ai automated app development relies on humans\nPage info\nType: Issue\nPublished: November 2022\nLast updated: December 2023", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/axon-school-security-taser-drones", "content": "Axon plan to develop school security taser drones prompts backlash\nOccurred: June 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn announcement by US taser manufacturer Axon that it would develop taser-armed drones as part of a long-term plan to 'stop mass shootings' in the USA prompted privacy advocates to express concerns about surveillance and safety.\nThe drones would consist of miniature, lightweight tasers, and 'targeting algorithms' to help operators aim the device safely and incapacitate an active shooter within 60 seconds, the company said. Responders would be trained using virtual reality. The announcement came days after the shooting of 21 people at an elementary school in Uvalde, Texas.\nThe move also led to the resignation of nine of twelve members of Axon's AI Ethics Advisory Board, which said had not been consulted about the programme and had concerns about introducing weaponising drones in over-policed communities of colour. The board had successfully halted previous product launches, including when Axon was considering introducing facial recognition technology to it's line of police-worn body cameras.\nWithin three days, the company halted the project. It later formed a new Ethics and Equity Advisory Council, an 11-member group comprising a cross-section of academics, community organisers, and activists.\n\u2795 In January 2023, Axon's original AI Ethics Board published (pdf) a report explaining the rationale for their objections to Project ION, including the technology's potential for misuse, the dehumanisation of individuals targeted, the increased militarisation of the police, operational risks, and potential reputational damage.\nSystem \ud83e\udd16\nAxon website\nAxon Enterprise Wikipedia profile\n\nDocuments \ud83d\udcc3\nAxon Ethics & Equity Advisory Council\nAxon CEO Rick Smith Reddit AMA\nAxon Announces TASER Drone Development to Address Mass Shootings \nHow non-lethal armed drones can help address school shootings\nAxon committed to listening and learning\nOperator:  \nDeveloper: Axon\nCountry: USA\nSector: Education\nPurpose: Strengthen security\nTechnology: Drone; Computer vision\nIssue: Appropriateness/need; Effectiveness/value; Privacy; Surveillance; Bias/discrimination - race, ethnicity\nTransparency: Governance; Marketing\nInvestigations, assessments, audits \ud83e\uddd0\nNYU School of Law Policing Project (2023). A Report on Axon\u2019s Proposal for Taser-Equipped Drones (pdf)\nNYU School of Law Policing Project (2022). STATEMENT OF RESIGNING AXON AI ETHICS BOARD MEMBERS\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reuters.com/article/axon-enterprise-drones/exclusive-axon-halts-taser-drone-work-as-most-of-its-ethics-panel-said-to-resign-idUSKBN2NN055\nhttps://www.abc.net.au/news/2022-06-06/ethics-panel-members-resign-axon-taser-drones-mass-shootings/101129270\nhttps://www.npr.org/2022/06/06/1103285030/axon-halts-plans-for-taser-drone-as-9-on-ethics-board-resign\nhttps://theguardian.com/world/2022/jun/03/taser-firm-axon-ethics-board-stun-gun-drones-schools-condemned\nhttps://www.bbc.co.uk/news/world-us-canada-61685117\nhttps://www.theregister.com/2022/06/03/taser_maker_proposes_electric_shock/\nhttps://gizmodo.com/axon-taser-drone-1849016465\nhttps://www.msn.com/en-us/news/technology/the-taser-drone-may-be-a-creepy-fantasy-but-the-questions-it-raises-are-real/ar-AAY3JNi\nhttps://futurism.com/taser-drones-drama\nhttps://www.vice.com/en/article/z34bb3/axon-wants-to-make-taser-drones-for-schools-despite-its-own-ethics-boards-concerns\nhttps://www.protocol.com/policy/axon-rick-smith-interview\nRelated \ud83c\udf10\nLockport City School District facial recognition\nShotSpotter gunshot detection\nPage info\nType: Issue\nPublished: October 2022\nLast updated: May 2024", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/retorio-talent-personality-assessments", "content": "Retorio talent personality assessments criticised as inaccurate, biased\nOccurred: February 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn algorithm owned by Munich-based start-up Retorio responds differently to the same candidate in different outfits, such as glasses and headscarves, prompting concerns about accuracy and bias.\nRetorio analyses video to determine personality attributes based on the 'Ocean' Big Five Model, which breaks down scores into 'openness, conscientiousness, extraversion, agreeableness, and neuroticism.'\nHowever, reporters from Bayerischer Rundfunk (German Public Broadcasting) discovered that the company's personality assessment system produced significantly different scores when analysing a woman wearing spectacles and a headscarf, a darkened video of a Black woman, or when a bookshelf was added in the background.\nRetorio responded by saying the difference in results was due to the fact that the system had been trained based on how a chosen group of human recruiters perceived jobseekers and their personalities, leading the algorithm to reproduce the feelings and biases of those humans.\nOn its website, Retorio describes its personality product as 'the world's most innovative video-based personality assessment.'\nSystem \ud83e\udd16\nRetorio website\nOperator: Lufthansa; BMW Group; ADAC\nDeveloper: Retorio\nCountry: Germany\nSector: Business/professional services\nPurpose: Identify personality traits\nTechnology: Emotion recognition; Facial analysis; Computer vision\nIssue: Accuracy/reliability; Bias/discrimination - race\nTransparency: Governance; Black box\nInvestigations, assessments, audits \ud83e\uddd0\nBR24 (2021). Objective or Biased?\nBR24 (2021). FAIRNESS ODER VORURTEIL?\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.technologyreview.com/2021/07/07/1027916/we-tested-ai-interview-tools/\nhttps://podcasts.apple.com/us/podcast/want-a-job-the-ai-will-see-you-now/id1523584878?i=1000528104144\nhttps://www.ft.com/content/c0b03d1d-f72f-48a8-b342-b4a926109452\nhttps://www.scientificamerican.com/article/your-boss-wants-to-spy-on-your-inner-feelings/\nhttps://venturebeat.com/2022/01/17/new-startup-shows-how-emotion-detecting-ai-is-intrinsically-problematic/\nhttps://www.inputmag.com/culture/a-bookshelf-in-your-job-screening-video-makes-you-more-hirable-to-ai\nhttps://dabblingwithdata.wordpress.com/2021/02/21/should-an-ais-opinion-of-your-personality-affect-whether-you-get-a-job/\nhttps://mixed.de/bewerberauswahl-per-ki-sind/\nhttps://netzpolitik.org/2021/algorithmen-fuer-bewerbungen-das-buecherregal-im-hintergrund-hat-einfluss-darauf-ob-du-den-job-bekommst/\nhttps://www.derstandard.de/story/2000124300995/bewerbungs-ki-bevorzugt-jobsuchende-mit-buecherregal-im-hintergrund\nhttps://www.trippassociates.co.uk/garbage-in-garbage-out-ai-in-recruitment/\nhttps://www.equitas.ai/the-end-of-video-for-remote-hiring/\nRelated \ud83c\udf10\n4 Little Trees (4LT) student emotion recognition\nSpotify AI emotion recognition\nPage info\nType: Issue\nPublished: May 2022", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tesla-smart-summon", "content": "Tesla crashes into private jet after Smart Summon\nOccurred: April 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Tesla crashed into a private jet after being 'summoned' by its owner using the car maker\u2019s automated summoning feature. \nSmart Summon enables a Tesla owner to 'summon' their vehicle from its parking space using Tesla's smartphone app from a maximum distance of 200 feet. The car will navigate around obstacles to its owner. \nThe incident occurred at an event sponsored by aircraft manufacturer Cirrus at Felts Field in Spokane, Washington, and the video appears to show the Tesla slowly crashing into and then pushing the Cirrus Vision jet across the tarmac. \nFirst introduced in 2019, Smart Summons has been on the receiving end of complaints by owners and pedestrians about near crashes, confused cars, and bumper damage.\nTesla CEO Elon Musk has described Smart Summon as 'probably our most viral feature ever.'\nSystem \ud83e\udd16\nTesla Smart Summon website\nOperator: \nDeveloper: Tesla\nCountry: USA\nSector: Automotive\nPurpose: Summon car\nTechnology: Driver assistance system\nIssue: Accuracy/reliability; Safety\nTransparency: Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reddit.com/r/flying/comments/u95dqt/someone_just_crashed_into_a_vision_jet/\nhttps://www.theverge.com/2022/4/22/23037654/tesla-crash-private-jet-reddit-video-smart-summon\nhttps://www.mirror.co.uk/news/us-news/tesla-autopilot-slams-27million-private-26800475\nhttps://www.newsweek.com/video-tesla-smart-summon-mode-ramming-3m-jet-viewed-34m-times-1700310\nhttps://www.ndtv.com/offbeat/viral-video-driverless-tesla-crashes-into-private-jet-as-owner-uses-smart-summon-feature-2921071\nhttps://www.usatoday.com/videos/news/have-you-seen/2022/04/25/tesla-collides-private-jet-while-owner-using-smart-summon-mode/7439216001/\nhttps://www.dailydot.com/debug/tesla-crash-vision-jet-autpilot-video/\nhttps://electrek.co/2022/04/22/tesla-vehicle-crashes-into-jet-dangerously-summoned-by-owner/\nhttps://www.gizmodo.com.au/2022/04/tesla-in-summon-mode-rams-3-million-private-jet-and-just-keeps-crashing/\nhttps://www.carscoops.com/2022/04/tesla-smart-summon-feature-apparently-allows-model-y-to-crash-into-multi-million-dollar-private-jet/\nhttps://www.businessinsider.com/tesla-autopilot-crashes-into-private-jet-witness-says-2022-4\nhttps://www.dailymail.co.uk/news/article-10745095/Moment-driverless-Tesla-summoned-owner-Washington-air-field-crashes-2m-jet.html\nhttps://www.businessinsider.com/teslas-smart-summon-feature-is-wreaking-havoc-2019-10?r=US&IR=T\nhttps://www.ubergizmo.com/2022/04/tesla-in-smart-summon-mode-bumps-into-a-private-jet/\nhttps://www.stuff.co.nz/motoring/300572609/video-captures-driverless-tesla-crashing-into-us3-million-private-jet\nhttps://www.thesun.co.uk/motors/10040743/tesla-owners-furious-feature-automatically-picks-up-drivers-causes-crashes/\nRelated \ud83c\udf10\nDriverless Tesla Model 3 negotiates parking lot\nTesla phantom braking\nPage info\nType: Incident\nPublished: April 2022", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/google-assistive-writing-feature-accused-of-being-too-woke", "content": "Google Assistive Writing feature is accused of being too 'woke'\nOccurred: April 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGoogle's AI-powered 'assistive writing' feature faced criticism for being overly 'woke', inaccurate, and intrusive.\nAimed initially at enterprise users, the new feature builds on inclusive writing guidelines such as avoiding unnecessarily gendered language Google produced for its developers in 2021 and is intended to help Google Docs users avoid politically incorrect words such as 'landlord' and 'mankind',\nHowever, some reckoned the feature went too far and took issue with it replacing the word 'landlord' with 'property owner' or 'proprietor', and that US President John F. Kennedy\u2019s inauguration phrase 'for all mankind' is replaced with 'for all humankind.'\nOthers found it inaccurate. Vice discovered that words including 'annoyed' and 'Motherboard' were flagged for being insufficiently inclusive. Yet, as Vice journalist Samantha Cole pointed out, being annoyed is not the same as being 'upset' or 'angry', both of which Google suggested to make her sentences 'flow better'.\nThe assistive writing feature was also criticised for being overly intrusive and its users guinea pigs for Google's ongoing efforts to train and improve its predictive text, search suggestion, and other systems.\nGoogle says it does not 'yet (and may never) have a complete solution to identifying and mitigating all unwanted word associations and biases' and that the feature is undergoing 'ongoing evolution.'\nSystem \ud83e\udd16\nAssistive Writing\n\nDocuments \ud83d\udcc3\nGoogle (2022). More assistive writing suggestions in Google Docs\nOperator: Alphabet/Google\nDeveloper: Alphabet/Google\nCountry: USA; Global\nSector: Business/professional services\nPurpose: Detect inappropriate language\nTechnology: NLP/text analysis\nIssue: Accuracy/reliability; Bias/discrimination - political opinion; Privacy\nTransparency: Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.vice.com/en/article/v7dk8m/googles-ai-powered-inclusive-warnings-feature-is-very-broken\nhttps://news.sky.com/story/google-docs-criticised-for-woke-inclusive-language-suggestions-12598687\nhttps://www.telegraph.co.uk/news/2022/04/23/big-brother-sorry-big-person-correcting-google/\nhttps://nypost.com/2022/04/25/google-unveils-woke-writing-feature-for-inclusive-language/\nhttps://www.theverge.com/2022/4/1/23005972/google-docs-assisting-writing-active-voice-concise-inclusive-language-inappropriate-words\nhttps://www.pennlive.com/nation-world/2022/04/google-debuts-woke-writing-function-that-boasts-more-inclusivity.html\nhttps://www.theregister.com/2022/04/25/the_latest_automated_ai_writing/\nhttps://thehustle.co/04212022-assistive-writing/\nhttps://www.business-standard.com/article/technology/google-docs-to-offer-more-assistive-writing-suggestions-for-users-122040200269_1.html\nhttps://www.express.co.uk/news/science/1600579/google-woke-speech-police-politically-correct-inclusive-language\nhttps://www.thetimes.co.uk/article/google-inclusive-language-looks-set-divide-opinion-7f5qrznt8\nhttps://www.techradar.com/news/google-docs-is-having-some-serious-issues-with-its-new-inclusive-language-warnings\nhttps://www.nationalreview.com/corner/google-docx/\nRelated \ud83c\udf10\nGoogle Derm Assist dermatology app bias, privacy\nFacebook credit card age ad targeting\nPage info\nType: Incident\nPublished: April 2022", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/coupang-own-brand-search-engine-rigging", "content": "Coupang fined for own brand search engine rigging\nOccurred: August 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Korean competition watchdog fined e-commerce company Coupang for manipulating its search engine algorithms in favour of its own label products. \nCoupang was fined 3.29 billion won (approximately USD 2.81 million) by the country\u2019s Fair Trade Commission (FTC) for engaging in unfair practices related to its search engine algorithms, including tampering with its search algorithms to favour its own label products over those of third parties, and forcing hundreds of sellers to reduce prices for goods sold on its platform while simultaneously increasing prices on rival platforms.\nThe investigation came hard on the heels of an on-the-spot inspection at the company's Seoul headquarters in Seoul following complaints by customers, civic groups and others that it had abused its market position by favouring its own brands products. \nThe Financial Times noted that tampering with algorithms is common in South Korea\u2019s e-commerce sector.\n\u2795 In June 2021, the company's food delivery subsidiary Coupang Eats and its star rating system drew fire for mishandling a complaint that resulted in the death of a restaurant owner.\n\u2795 In March 2022, Coupang and its subsidiary CPLB came under investigation by the FTC for getting its employees to submit false positive reviews of its own brand products.\nSystem \ud83e\udd16\nCoupang website\nCoupang Wikipedia profile\nOperator: Coupang  \nDeveloper: Coupang  \nCountry: S Korea\nSector: Technology; Retail\nPurpose: Rank content/search results\nTechnology: Search engine algorithm\nIssue: Competition/price fixing\nTransparency: Governance; Black box; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttp://www.koreaherald.com/view.php?ud=20210704000170\nhttps://www.econotimes.com/Coupang-probed-by-FTC-for-alleged-tampering-of-algorithm-on-its-platform-1612188\nhttps://www.ft.com/content/505db9f3-5ff4-41fa-8cad-73b896c11b16\nhttps://pulsenews.co.kr/view.php?year=2021&no=646029\nhttps://www.smartkarma.com/insights/korea-ftc-says-it-will-regulate-coupang-on-search-algorithm-manipulation\nhttps://news.bloomberglaw.com/antitrust/coupang-fined-3-3b-won-for-violating-s-korean-antitrust-law-1\nhttps://www.pymnts.com/news/ecommerce/2021/korea-fair-trade-commission-probes-coupangs-algorithm-use/\nhttps://www.econotimes.com/Coupang-slapped-with-28M-fine-by-Koreas-FTC-over-unfair-business-practices-1615494\nhttp://koreabizwire.com/civic-groups-accuse-coupang-of-posting-false-reviews-for-pb-products/213731\nhttp://koreabizwire.com/antitrust-regulator-launches-probe-into-coupang-for-alleged-false-reviews/214319\nhttps://www.competitionpolicyinternational.com/s-koreas-antitrust-watchdog-probe-into-coupangs-false-reviews/\nhttps://www.koreatimes.co.kr/www/tech/2021/08/694_314174.html\nhttps://www.koreaherald.com/view.php?ud=20210819000914\nhttps://mlexmarketinsight.com/news/insight/coupang-faces-new-complaint-in-south-korea-accused-of-faking-user-reviews-of-house-brand-products\nhttps://en.yna.co.kr/view/AEN20220103005200320\nRelated \ud83c\udf10\nNaver own brand search engine rigging\nAmazon India own brand search engine rigging\nPage info\nType: Incident\nPublished: April 2022", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/naver-own-brand-search-engine-rigging", "content": "Naver fined for own brand search engine rigging\nOccurred: October 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nSouth Korean internet giant Naver was fined 26.7 billion won (USD 23 million) for rigging its shopping and video services search engines in order to promote its own products and services. \nAccording to the country's Fair Trade Commission, Naver had deliberately and opaquely 'misled customers to believe the results were fair and objective,' and 'damaged fair competition in e-commerce and video platform markets.'\nThe FTC found Naver has manipulated its search algorithms six times between 2012 to 2015 so that its Smart Store, Naver TV and other services appeared at top of search results while pushing down competitors such as 11th Street and Gmarket. \nNaver's actions enabled to increase its online shopping market share from 5 percent in 2015 to 21% in 2018. Its four main competitors saw their market share drops over the same period, the FTC said.\nIt was the first time South Korea's FTC fined an online platform operator for manipulating its algorithms to damage competitors.\nSystem \ud83e\udd16\nNaver website\nNaver Wikipedia profile\nOperator: Naver\nDeveloper: Naver\nCountry: S Korea\nSector: Technology; Retail\nPurpose: Rank content/search results\nTechnology: Search engine algorithm\nIssue: Competition/price fixing\nTransparency: Governance; Black box; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.koreatimes.co.kr/www/tech/2020/10/133_297112.html\nhttps://m.theinvestor.co.kr/view.php?ud=20201006000926&np=1\nhttps://english.chosun.com/site/data/html_dir/2020/10/07/2020100700393.html\nhttp://www.koreaherald.com/view.php?ud=20201006000715\nhttps://www.zdnet.com/article/naver-fined-for-search-manipulation/\nhttps://www.khan.co.kr/it/it-general/article/202102041455001\nhttps://koreajoongangdaily.joins.com/2020/10/06/business/economy/naver-FTC-shopping/20201006171200467.html\nRelated \ud83c\udf10\nCoupang own brand search engine rigging\nAmazon India own brand search rigging\nPage info\nType: Incident\nPublished: April 2022", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/zoom-ai-emotion-recognition", "content": "Zoom AI emotion recognition plan draws human rights fire \nOccurred: April 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nPlans by Zoom to develop an AI-based system that will monitor and detect the feelings and emotions of its users drew fire from US human and digital rights campaigners, ACLU, EPIC, Fight for the Future, and others.\nAccording to an April 2022 report by Protocol, Zoom plans to introduce a suite of new features under the name Zoom IQ for Sales that use emotion AI to produce post-meeting transcripts and sentiment analysis to meeting hosts.\nHowever, critics of emotion AI (or 'affective computing') argue the technology is often inaccurate as a person's facial expressions can be different to what they are feeling. \nIt can also be discriminatory, with research showing the ways people express emotions vary significantly across cultures and situations. \nThe technology is also said to be intrusive and easily misused. \nSystem \ud83e\udd16\nZoom website\nZoom Wikipedia profile\nDocuments \ud83d\udcc3\nZoom IQ for Sales\nOperator: Zoom Video Communications\nDeveloper: Zoom Video Communications\nCountry: USA\nSector: Business/professional services\nPurpose: Monitor & analyse emotion\nTechnology: Emotion recognition; Facial analysis; Voice recognition; NLP/text analysis\nIssue: Accuracy/reliability; Bias/discrimination - race, ethnicity, disability; Dual/multi-use; Privacy; Surveillance\nTransparency: Black box\nResearch, advocacy \ud83e\uddee\nFight for the Future (2022). Dear Zoom open letter\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.protocol.com/enterprise/emotion-ai-sales-virtual-zoom\nhttps://www.protocol.com/enterprise/zoom-emotion-ai-aclu-rights\nhttps://www.protocol.com/bulletins/zoom-emotion-ai-fight-future\nhttps://www.inputmag.com/tech/intel-classroom-ai-student-surveillance-facial-recognition\nhttps://www.biometricupdate.com/202204/reported-ai-based-emotion-recognition-by-zoom-irks-rights-advocates\nhttps://www.avinteractive.com/news/collaboration/zoom-urged-abandon-interest-emotion-reading-19-04-2022/\nRelated \ud83c\udf10\nIntel AI student emotion monitoring\nGaggle student behavioural monitoring\nPage info\nType: Incident\nPublished: April 2022\nLast updated: May 2022", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/intel-ai-student-emotion-monitoring", "content": "Intel AI student emotion monitoring system accused of being inaccurate, intrusive\nOccurred: April 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn Intel AI-based software system that aims to detect the emotional states of students during digital classes sparked debates on AI, science, ethics, and privacy.\nThe solution, integrated with Zoom via the \u201cClass\u201d software product, analyses students\u2019 body language and facial expressions to infer mental states such as boredom, distraction, or confusion and is intended to aid teachers in adapting their teaching techniques.\nSome teachers who tested Intel's system in a physical classroom said the system was useful. However, critics of affective computing argued emotion recognition is notoriously inaccurate and often mistakes facial expresssions for real, underlying feelings. The technology is also known to struggle across different cultures and scenarios.\nOthers are concerned about the intrusive nature of using cameras in an educational context, and in their potential for continual surveillance and misuse.\nClassroom Technologies co-founder and CEO Michael Chasen told Protocol he hopes to partner with one of the colleges his company works with to evaluate Intel's system.\nSystem \ud83e\udd16\nClass website\nDocuments \ud83d\udcc3\nClass/Intel (2023). Class Technologies Announces Partnership with Intel to Improve PC-Based Virtual Learning Experience\nOperator: \nDeveloper: Intel; Classroom Technologies\nCountry: USA\nSector: Education\nPurpose: Improve student engagement\nTechnology: Emotion recognition; Facial analysis\nIssue: Accuracy/reliability; Privacy; Surveillance\nTransparency: Black box\nResearch, advocacy \ud83e\uddee\nStanford HAI (2023). 2023 AI Index Report - 3.2 AI Incidents (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.protocol.com/enterprise/emotion-ai-school-intel-edutech\nhttps://www.techradar.com/news/intel-under-fire-over-its-face-reading-ai\nhttps://gizmodo.com/remote-learning-spyware-tracks-student-emotions-1848806568\nhttps://www.biometricupdate.com/202204/affective-computing-draws-intels-attention-prompts-debate\nhttps://www.msn.com/en-us/news/technology/intel-ai-tech-can-be-used-to-monitor-student-emotions-through-zoom/ar-AAWnvD5\nhttps://www.extremetech.com/extreme/334259-intel-tests-controversial-new-student-monitoring-software\nhttps://www.techdirt.com/2022/04/20/intel-wants-to-add-unproven-emotion-detection-ai-to-distance-learning-tech/\nhttps://hothardware.com/news/intel-sparks-ai-ethics-uproar-over-tech-that-deciphers-body-language\nhttps://www.techzine.eu/news/applications/77387/intel-develops-ai-to-detect-students-emotions/\nhttps://gigazine.net/gsc_news/en/20220418-intel-edutech-ai/\nhttps://www.tomshardware.com/uk/news/intel-students-ai-controversy\nRelated \ud83c\udf10\nGaggle student behavioural monitoring\nSpotify AI emotion recognition\nPage info\nType: Incident\nPublished: April 2022", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/instagram-dm-systemic-abuse-harassment", "content": "Instagram fails to protect women from 90 percent of abusive messages\nOccurred: April 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nInstagram is 'systemically' failing to protect its users, including high-profile women, from harassment and abuse, according to a study by non-profit organisation the Center for Countering Digital Hate (CCDH). \nThe CCDH analysed over 8,700 messages sent to five high-profile celebrities on the app from 253 accounts, and found that 90 percent of abuse sent to them was ignored by Instagram, despite being reported to moderators. The CCDH also found that 227 of these accounts remained active at least one month after complaints were filed.\nThe five celebrities included actor Amber Heard, British broadcaster Rachel Riley, activist Jamie Klingler, journalist Bryony Gordon, and the founder of Burnt Roti magazine, Sharan Dhaliwal. \nEach handed over their Instagram accounts to the CCDH so their DMs could be monitored over a period of number of months. In one instance, Instagram failed to remove accounts that made death threats to Amber Heard's family, including her infant daughter. \nIn February 2021 Instagram rolled out tougher measures to punish online harassment, including the disabling of accounts reported multiple times for sending 'violating messages.'\nInstagram has previously been in the firing line for failing to protect teen girls from mental health harms, and Black English footballers from racist abuse.\nSystem \ud83e\udd16\nInstagram website\nInstagram Wikipedia profile\nInstagram Direct\n\nDocuments \ud83d\udcc3\nInstagram (2021). An update on our work to tackle abuse on Instagram\nOperator: Meta/Instagram\nDeveloper: Meta/Instagram\nCountry: UK; USA\nSector: Technology\nPurpose: Moderate content  \nTechnology: Content moderation system\nIssue: Safety\nTransaparency: Governance; Black box; Marketing\nResearch, advocacy \ud83e\uddee\nCenter for Countering Digital Hate (2022). Hidden Hate\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.washingtonpost.com/technology/2022/04/06/instagram-women-harassment/\nhttps://www.nme.com/news/study-shows-high-profile-women-face-an-epidemic-of-misogynist-abuse-on-instagram-3201484\nhttps://www.theguardian.com/technology/2022/apr/05/high-profile-women-on-instagram-face-epidemic-of-misogynist-abuse-study-finds\nhttps://www.nbcnews.com/pop-culture/viral/instagram-ignores-dm-abuse-study-rcna23271\nhttps://inews.co.uk/news/victims-online-abuse-face-hurdles-reporting-hateful-direct-messages-instagram-1555668\nhttps://www.standard.co.uk/news/uk/instagram-centre-for-countering-digital-hate-rachel-riley-amber-heard-bryony-gordon-b992715.html\nhttps://www.bbc.co.uk/news/technology-60983593\nhttps://www.huffingtonpost.co.uk/entry/rachel-riley-abuse-explicit-dms-instagram_uk_624d4d8ae4b0e44de9c8b712\nhttps://9to5mac.com/2022/04/06/instagram-misogynistic-abuse/\nhttps://www.inputmag.com/culture/amber-heard-rachel-riley-jamie-klingler-women-abuse-instagram-dm-ccdh-study\nhttps://www.dailymail.co.uk/tvshowbiz/article-10690023/Rachel-Riley-details-invasive-disgusting-messages-receives-social-media-perverts.html\nhttps://www.nytimes.com/2022/04/06/technology/instagram-harassment-women.html\nRelated \ud83c\udf10\nSama 'ethical AI' Facebook content moderation\nTwitter Ukraine OSINT account suspensions\nPage info\nType: Incident\nPublished: April 2022", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/cruise-driverless-car-pulls-away-from-police", "content": "Cruise driverless car pulls away from police inspection\nOccurred: April 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Cruise driverless car pulled away after being stopped by police in San Francisco, prompting questions about how public authorities handle incidents involving autonomous vehicles. \nThe car was initially pulled over by a San Francisco Police Department (SFPD) officer for not having its headlights on at night. The officer then tried to open the door of the car having discovered it had no driver, upon which it started moving away before turning on its hazard lights and pulling in to a safer spot down the road.\nAccording to Cruise, the vehicle operated as intended, moving to the 'nearest safe location' in order to stop in response to direction from Cruise employees. Cruise has been permitted to test driverless cars without a human safety driver present in San Francisco since December 2020. \nThe incident raises questions about how the police and other public authorities handle driverless cars in various scenarios, and who should be liable for incidents caused by or involving them. \nLegal authorities such as The Law Commission of England and Wales recommend car manufacturers rather than users should be responsible if autonomous vehicles crash or go wrong.\nSystem \ud83e\udd16\nCruise website\nCruise Wikipedia profile\nOperator: GM Cruise\nDeveloper: GM Cruise; General Motors/Chevrolet\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Self-driving system\nIssue: Safety; Accuracy/reliability; Legal - liability\nTransparency: Governance; Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://twitter.com/llsethj/status/1512960943805841410\nhttps://news.sky.com/story/driverless-car-starts-to-pull-away-after-being-stopped-by-police-12588083\nhttps://www.theverge.com/2022/4/10/23019303/heres-what-happens-cops-pull-over-a-driverless-cruise-vehicle-general-motors\nhttps://techcrunch.com/2022/04/11/autonomous-cruise-car-encounter-with-police-raises-policy-questions/\nhttps://www.engadget.com/cruise-vehicle-drives-away-from-police-194657296.html\nhttps://www.msn.com/en-us/news/technology/video-of-police-pulling-over-driverless-car-viewed-12-million-times/ar-AAW60RC\nhttps://uk.pcmag.com/cars-auto/139733/san-francisco-police-pull-over-driverless-gm-cruise\nhttps://carbuzz.com/news/cruise-robotaxi-flees-police-in-hilarious-video\nhttps://jalopnik.com/watch-a-confused-cop-pull-over-a-driverless-cruise-vehi-1848776674\nhttps://www.msn.com/en-us/news/technology/video-of-police-pulling-over-driverless-car-viewed-12-million-times/ar-AAW60RC\nhttps://www.msn.com/en-ie/cars/news/sfpd-cops-pull-over-driverless-cruise-car-with-nobody-inside/ar-AAW5F1P\nhttps://uk.pcmag.com/cars-auto/139733/san-francisco-police-pull-over-driverless-gm-cruise\nhttps://www.autoevolution.com/news/driverless-gm-cruise-car-gets-pulled-over-by-the-cops-makes-a-run-for-it-186062.html\nhttps://interestingengineering.com/pulling-over-driverless-car\nRelated \ud83c\udf10\nCruise driverless cars block traffic\nAmazon DSP Ans Rana crash liability\nPage info\nType: Incident\nPublished: April 2022\nLast updated: November 2022", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/lockport-city-school-district-facial-recognition", "content": "Lockport City School District rapped for facial recognition opacity\nOccurred: April 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn Office of the New York State Comptroller audit found that the procurement process for the implementation of a facial recognition system for Lockport City School District was not transparent, violated district rules and policy, and failed to comply with New York State General Municipal Law.\nThe audit was the result of allegations levelled by local resident Jim Schultz that SN Technologies\u2019 AEGIS system was biased and the bidding process corrupt. The allegations had prompted then-New York Governor Andrew Cuomo to order the suspension and inspection of the system by local authorities in March 2021.\nThe audit found the district only allowed one day for bids to be submitted and ruled the procurement process was not as transparent as it should have been. SN Technologies was the only respondent. \nLockport City School District claims Aegis provides early warning signs of threats to students and employees at its schools. The system is also meant to identify guns and alert security to their presence. \nSystem \ud83e\udd16\nSN Technologies website\nLockport schools website\nOperator: Lockport City School District\nDeveloper: SN Technologies\nCountry: USA\nSector: Govt - education\nPurpose: Strengthen security\nTechnology: Facial recognition; Gun detection\nIssue: Accuracy/reliability; Effectiveness/value; Bias/discrimination - race, ethnicity; Privacy; Surveillance\nTransparency: Governance; Marketing\nInvestigations, assessments, audits \ud83e\uddd0\nhttps://www.osc.state.ny.us/files/local-government/audits/2022/pdf/lockport-2022-198.pdf\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://apnews.com/article/a9abeb33ef124cf6bf9bf4cb28223072\nhttps://www.nytimes.com/2020/02/06/business/facial-recognition-schools.html\nhttps://www.vice.com/en/article/qjpkmx/fac-recognition-company-lied-to-school-district-about-its-racist-tech\nhttps://www.buzzfeednews.com/article/daveyalba/lockport-schools-facial-recognition-pilot-aegis\nhttps://www.engadget.com/2019-05-30-facial-recognition-us-schools-new-york.html\nhttps://www.theguardian.com/technology/2019/may/31/facial-recognition-school-new-york-privacy-fears\nhttps://www.forbes.com/sites/zakdoffman/2019/05/30/foolish-facial-recognition-about-to-hit-u-s-public-schools-for-the-first-time/#6deaa4bb46a0\nhttps://www.dailymail.co.uk/sciencetech/article-6334861/New-York-school-district-uses-facial-recognition-technology-identify-potential-shooters.html\nhttps://buffalonews.com/news/local/education/audit-lockports-bidding-for-facial-recognition-system-was-legal-but-not-transparent/article_78000f7a-bb5e-11ec-a92e-075df8b7085d.html\nhttps://www.biometricupdate.com/202102/corrupt-bid-process-for-school-biometric-system-alleged-ny-comptroller-asked-to-investigate\nhttps://www.mic.com/p/a-new-york-school-districts-facial-recognition-system-is-facing-scrutiny-from-the-states-education-department-17934329\nhttps://www.lockportjournal.com/news/local_news/lockport-to-test-new-school-safety-system-next-week/article_159eff1c-74f1-5490-83ac-1ba1c5123ab9.html\nhttps://www.biometricupdate.com/202204/ny-state-audit-finds-bidding-for-school-facial-recognition-system-was-improper\nhttps://buffalonews.com/news/local/education/audit-lockports-bidding-for-facial-recognition-system-was-legal-but-not-transparent/article_78000f7a-bb5e-11ec-a92e-075df8b7085d.html\nRelated \ud83c\udf10\nAxon school security taser drones\nGaggle student behavioural monitoring\nPage info\nType: Incident\nPublished: April 2022", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/malaysia-ai-court-sentencing", "content": "Malaysia AI court sentencing system accused of being inaccurate, unfair\nOccurred: April 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn AI-powered court sentencing system in Malaysia was accused by lawyers of being inaccurate and resulting in biased and unfair sentences.\nDeveloped by Sarawak Information Systems, the nationwide pilot aimed to make sentencing more consistent and help clear the backlog of cases clogging Malaysia's legal system, according to Reuters. \nHowever, senior lawyers say the 'opaque' system lacks a judge's ability to weigh up individual circumstances, or adapt to changing social mores, would lead to a loss of trust in the judicial system, and should be withdrawn.\nFurthermore, lawyers say that there was no proper consultation on the technology's use. Malaysia's Bar Council said it was 'not given guidelines at all' when courts in Kuala Lumpur started using the system mid-2021 for sentencing in 20 types of crimes.\nSystem \ud83e\udd16\nUnknown\nOperator: Mahkamah Persekutuan Malaysia\nDeveloper: Sarawak Information Systems (SAINS)\nCountry: Malaysia\nSector: Govt - justice\nPurpose: Achieve greater sentencing consistency\nTechnology: Predictive statistical analysis\nIssue: Accuracy/reliability; Fairness; Bias/discrimination - race, ethnicity\nTransparency: Governance; Black box; Marketing\nResearch, advocacy \ud83e\uddee\nKhazanah Research Institute (2020). Artificial Intelligence in the Courts: AI sentencing in Sabah and Sarawak (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://news.trust.org/item/20220411160005-k1a5o/?source=gep\nhttps://www.thetimes.co.uk/article/lawyers-rebel-over-use-of-ai-to-sentence-rapists-and-drug-dealers-in-malaysia-t79m303xw\nhttps://www.dailysabah.com/life/mr-robot-takes-on-laworder-malaysia-tests-ai-in-judicial-system/news\nhttps://www.scottishlegal.com/articles/malaysia-lawyers-warns-against-use-of-nascent-ai-in-sentencing\nhttps://opengovasia.com/sarawak-courts-to-deploy-artificial-intelligence/\nhttps://www.theborneopost.com/2020/01/17/ai-data-sentencing-to-be-employed-in-sarawak-courts/\nhttps://www.freemalaysiatoday.com/category/nation/2021/07/24/malaysian-bar-troubled-over-judges-using-ai-for-sentencing/\nhttps://legalatte.com/2021/07/26/the-ai-dilemma-rise-of-the-machines-in-malaysian-criminal-sentencing/\nhttps://dailytelegraph.co.nz/world/lawyers-outraged-over-use-of-ai-in-courts/\nhttps://www.thevibes.com/articles/news/36470/artificial-intelligence-in-msian-courts-sends-shivers-but-is-human-touch-really-lost\nhttps://www.thesundaily.my/local/tempering-justice-with-mercy-can-only-be-done-by-humans-not-computers-lawyers-HD9072328\nRelated \ud83c\udf10\nShanghai AI prosecutor accused of being inaccurate\nDubai deepfake court evidence\nPage info\nType: Issue\nPublished: April 2022", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/bytedance-content-scraping", "content": "Bytedance accused of scraping social media sites to train Al algorithm\nOccurred: April 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nTikTok parent company Bytedance scraped content from Instagram, Snapchat and other platforms and used them to train the algorithms of Flipagram, a TikTok predecessor.\nAccording to a Buzzfeed report, former employees and internal documents reveal Bytedance took hundreds of thousands of short-form videos, usernames, profile pictures, and profile descriptions without the knowledge of their owners or the social media platforms on which they had been posted.\nTwo employees told Buzzfeed the scraped content was later used to train the US version of TikTok's 'For You' recommendation algorithm. \nBytedance denied the allegations.\nSystem \ud83e\udd16\nTikTok For You recommendation algorithm\nOperator: ByteDance/TikTok/Flipagram\nDeveloper: ByteDance/TikTok/Flipagram\nCountry: China\nSector: Technology\nPurpose: Recommend content\nTechnology: Content recommendation system\nIssue: Privacy; Copyright\nTransparency: Governance; Privacy\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.buzzfeednews.com/article/emilybakerwhite/bytedance-scraped-fake-accounts-instagram-snapchat\nhttps://www.businessinsider.in/tech/apps/news/tiktoks-parent-bytedance-made-fake-accounts-scraping-content-from-instagram-snapchat-and-other-platforms-claims-report/articleshow/90656168.cms\nhttps://techcrunch.com/2022/04/04/tiktok-owner-bytedance-reportedly-scraped-content-from-instagram-snapchat-posted-flipagram/\nhttps://www.engadget.com/tik-tok-owner-byte-dance-scraped-content-from-instagram-and-others-to-push-predecessor-app-232135784.html\nhttps://www.deccanherald.com/business/technology/tiktok-owner-bytedance-scraped-content-from-instagram-snapchat-1097799.html\nhttps://techstory.in/bytedance-made-fake-accounts-with-content-scraped-from-instagram-and-snapchat/\nhttps://www.phoneworld.com.pk/bytedance-in-hot-waters-after-scraping-short-form-videos-from-social-media/\nhttps://observer.com/2022/04/tiktoks-addictive-algorithm-may-be-powered-by-content-scraped-from-its-rivals/\nhttps://www.digitalinformationworld.com/2022/04/tiktoks-parent-company-accused-of.html\nhttps://cbnc.com/tiktok-owner-bytedance-scraped-content-from-instagram-and-others-to-push-predecessor-app/\nhttps://onlinemarketing.de/cases/kopierte-tiktok-konzern-bytedance-content-von-instagram\nhttps://economictimes.indiatimes.com/tech/tech-bytes/tiktok-owner-bytedance-scraped-content-from-instagram-snapchat/articleshow/90656516.cms\nRelated \ud83c\udf10\nBytedance/TikTok Bev Standing voice theft\nTikTok mandatory beauty filtering\nPage info\nType: Incident\nPublished: April 2022", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/speedcam-anywhere-anti-speeding-app", "content": "Speedcam Anywhere app accused of potentially violating privacy \nOccurred: April 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA mobile phone-based anti-speeding app is drawing criticism in the UK for its privacy implications and potential use as a surveillance tool, according to The Guardian.\nSpeedcam Anywhere's AI-based app works by taking a short video clip of a passing vehicle, which is automatically assessed to gauge vehicle speed against local speed limits, with a report (pdf) generated which can be shared with authorities.\nWhile not formally approved by the police, the app is being trialed in the UK by speed safety campaigners 20\u2019s Plenty For Us and is described by the NGO's founder Rod King MBE as a 'game changer'. A senior police officer told the Guardian it is a 'potentially very significant' deterrent to dangerous driving.\nHowever, drivers and rights activists worry Speedcam Anywhere is overly intrusive and the clips and reports can too easily be used for a number of unethical purposes. \nApp users also complain the app is difficult to use and unreliable on Google Play. \nSystem \ud83e\udd16\nSpeedcam Anywhere website\n\nDocuments \ud83d\udcc3\nSpeedcam Anywhere example report (pdf)\nOperator: Speedcam Anywhere; 20\u2019s Plenty For Us\nDeveloper: Speedcam Anywhere\nCountry: UK\nSector: Automotive\nPurpose: Estimate vehicle speed\nTechnology: Automated license plate/number recognition (ALPR/ANPR); Computer vision\nIssue: Accuracy/reliability; Dual/multi-use; Surveillance; Privacy\nTransparency: Privacy\nResearch, advocacy \ud83e\uddee\n20's Plenty for Us. How Speedcam Anywhere works\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theguardian.com/world/2022/apr/10/speed-camera-app-developers-face-abuse-from-uk-drivers\nhttps://www.thesun.co.uk/tech/18156725/speed-camera-app-cops-smartphone-police-citation/\nhttps://www.express.co.uk/life-style/cars/1590459/speed-camera-app-driving-fine\nhttps://www.eta.co.uk/2022/03/29/new-smartphone-app-allows-pedestrians-to-report-speeding-drivers/\nhttps://road.cc/content/news/new-app-allow-public-submit-evidence-speeding-291501\nhttps://www.techdigest.tv/2022/04/tech-digest-daily-roundup-speed-camera-app-developers-face-abuse.html\nhttps://fleetworld.co.uk/concerns-over-new-ai-app-that-lets-pedestrians-shop-drivers-for-speeding/\nhttps://news.ycombinator.com/item?id=30933163\nRelated \ud83c\udf10\nTALON AI license plate camera surveillance\nAmazon delivery driver safety cameras\nPage info\nType: Incident\nPublished: April 2022", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/facebook-downranking-system-failure", "content": "Facebook downranking system failure leads to misinformation spike\nOccurred: March 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nFacebook experienced a 'massive' failure of its content ranking system that exposed users to a surge of disinformation, hate speech and other harmful content.\nAccording an internal report obtained by The Verge, the failure was caused by a software bug that exposed as much as half of all News Feed views to potential \u201cintegrity risks\u201d over six months, with posts that would usually have been demoted by the company's algorithms boosted by as much as 30 percent. \nAccording to the document, Facebook engineers had first noticed the issue, which was later attributed to a software bug, when a sudden surge of disinformation, hate speech, and other harmful content began flowing through the News Feed. \nA Facebook spokesman told The Verge the bug 'has not had any meaningful, long-term impact on our metrics.'\nSystem \ud83e\udd16\nFacebook content ranking system\nDocuments \ud83d\udcc3\nMeta. Our approach to Facebook Feed ranking\nOperator: Meta/Facebook\nDeveloper: Meta/Facebook\nCountry: USA; Global\nSector: Technology\nPurpose: Minimise harmful content\nTechnology: Content ranking system\nIssue: Robustness; Mis/disinformation\nTransparency: Governance; Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theverge.com/2022/3/31/23004326/facebook-news-feed-downranking-integrity-bug\nhttps://www.dailymail.co.uk/sciencetech/article-10675719/Meta-admits-Facebook-bug-led-surge-misinformation.html\nhttps://www.thedrum.com/news/2022/04/01/facebook-system-designed-smother-harmful-misinformation-actually-spread-it\nhttps://gizmodo.com/facebooks-news-feed-boosted-bad-posts-for-six-months-1848734620\nhttps://www.bgr.in/news/facebook-bug-promoted-fake-news-on-news-feed-took-six-months-to-fix-1252956/\nhttps://uk.news.yahoo.com/facebook-bug-promoted-posts-containing-200928176.html\nhttps://www.spiegel.de/netzwelt/apps/facebook-gab-inhalten-die-es-ausbremsen-wollte-mehr-newsfeed-reichweite-a-03f5d6e9-12af-4466-baeb-a3c9b60929a0\nhttps://siliconangle.com/2022/03/31/meta-news-feed-bug-increasing-views-sketchy-content-six-months/\nhttps://www.msn.com/en-us/news/technology/e2-80-98massive-ranking-failure-e2-80-99-meant-facebook-showed-users-nudity-violence-and-russian-misinformation/ar-AAVK0oe\nhttps://www.thehindubusinessline.com/info-tech/social-media/facebookbug-promoted-misinformation-on-users-news-feed/article65280918.ece\nRelated \ud83c\udf10\nFacebook Australia news, civil society blocks\nFacebook Cross-check\nPage info\nType: Issue\nPublished: March 2022", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/linkedin-deepfake-salespeople", "content": "Deepfake salespeople swamp LinkedIn\nOccurred: March 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nOver 1,000 fake profiles with AI-generated images were discovered on LinkedIn, raising questions about the social network's ability to manage its community. \nAn initial investigation by Stanford Internet Observatory researchers concluded that over 1,000 LinkedIn profiles appeared to use facial images created using AI-generated deepfake technologies. A follow-up NPR report found a cottage industry of companies in the US and India creating fake profiles of individuals and companies on LinkedIn using AI.\nLinkedIn\u2019s professional community policies state that the social network does not permit fake profiles or entities on its platform, including using other peoples' images or 'any other image that is not your likeness' for profile photos.\nThe incident raised concerns about LinkedIn's ability to detect and manage AI-generated content.\nSystem \ud83e\udd16\nUnknown\nOperator:\nDeveloper:  \nCountry: USA; India\nSector: Business/professional services\nPurpose: Generate sales leads\nTechnology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning  \nIssue: Safety; Accuracy/reliability\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nhttps://www.npr.org/2022/03/27/1088140809/fake-linkedin-profiles\nhttps://twitter.com/noUpside/status/1508260061612814338\nhttps://twitter.com/shannonpareil/status/1508110373261434882\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.siliconrepublic.com/machines/research-1000-computer-generated-images-linkedin-deepfake\nhttps://gizmodo.com/move-over-global-disinformation-campaigns-deepfakes-ha-1848716481\nhttps://www.thequint.com/tech-and-auto/tech-news/deepfakes-invade-linkedin-delhi-firm-offers-ready-to-use-ai-made-profiles\nhttps://screenrant.com/deep-fake-linkedin-profiles-sales-marketing-spot-how/\nhttps://petapixel.com/2022/03/29/researchers-discover-more-than-1000-ai-generated-linkedin-profiles/\nhttps://www.theregister.com/2022/03/28/ai_fake_linkedin_faces/\nhttps://screenshot-media.com/the-future/business/deepfakes-on-linkedin/\nRelated \ud83c\udf10\nChina diplomatic fake influence campaign\nLauren Book deepfake extortion\nPage info\nType: Issue\nPublished: March 2022", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/gis-employment-background-checks", "content": "GIS fined for providing inaccurate employment background checks\nOccurred: October 2015\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe US Consumer Finance Protection Board (CFSB) ordered two of the country\u2019s largest employment background screening report providers to pay USD 13 million in penalties and refunds for providing inaccurate information. \nAccording to the CFSB, General Information Services (GIS) and its subsidiary e-Background-checks.com (BGC), which collectively generate and sell more than 10 million reports about job applicants every year, failed to employ reasonable procedures to ensure the accuracy of the information contained in reports provided to potential employers. \nIn over 6,000 cases, the CFSB said, '(the firm) reported a criminal record that did not belong to the consumer, including, without limitation, those criminal records that (the firm) could not confirm belonged to the consumer based on personal identifiers\u2026or reported inaccurate criminal record information about the consumer, including, without limitation, expunged records, dismissed charges, nolo prosequi records reported as convictions (the prosecutor declined to pursue the case), or records with incorrect disposition data.' \nGIS also failed to 'identify root causes of accuracy errors', failed to notice patterns of mistakes, and did not test the accuracy of non-disputed reports, the CFSB alleged.\nThe two companies were required to pay a USD 2.5 million civil penalty, overhaul their compliance procedures, retain an independent consultant, and develop a comprehensive audit programme.\nSystem \ud83e\udd16\nGeneral Information Services\nOperator: General Information Services (GIS); e-Background-checks.com (BGC)\nDeveloper: General Information Services (GIS); e-Background-checks.com (BGC)\nCountry: USA\nSector: Business/professional services\nPurpose: Assess job applicant backgrounds\nTechnology: Database\nIssue: Accuracy/reliability\nTransparency: Governance\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nhttps://files.consumerfinance.gov/f/201510_cfpb_stipulation_general-information-service-inc.pdf\nhttps://www.consumerfinance.gov/about-us/newsroom/cfpb-takes-action-against-two-of-the-largest-employment-background-screening-report-providers-for-serious-inaccuracies/\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.chicagotribune.com/business/ct-background-check-penalties-1030-biz-20151029-story.html\nhttps://www.americanbanker.com/news/cfpb-slaps-13m-fine-on-employee-screening-companies-for-inaccurate-reports\nhttps://www2.staffingindustry.com/Editorial/Daily-News/Two-of-largest-background-check-firms-must-pay-13-million-over-accuracy-questions-35866\nhttps://consumerist.com/2015/10/29/nations-biggest-employment-background-screeners-must-pay-13m-over-inaccurate-reports/\nhttps://thehill.com/regulation/finance/258556-cfpb-fines-employment-screening-firms-for-alleged-inaccurate-reports\nhttps://bobsullivan.net/cybercrime/privacy/employment-background-firm-wrongly-reported-convictions-on-consumers-records-cfpb-alleges/\nhttps://employerschoicescreening.com/consumer-reporting-agencies-hit-with-13-million-class-action-lawsuit/\nhttps://www.insidearm.com/news/00041404-will-federal-push-to-ban-the-box-affect-d/\nhttps://www.capitalgazette.com/ct-background-check-penalties-1030-biz-20151029-story.html\nhttps://eu.indystar.com/story/news/2017/04/24/faulty-background-check-could-cost-you-your-next-job/100744294/\nRelated \ud83c\udf10\nYieldStar automated rent-setting\nEst\u00e9e Lauder employee performance assessments\nPage info\nType: Incident\nPublished: March 2022", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tesla-fsd-beta-test-car-hits-bollard-driver-fired", "content": "Tesla FSD beta test car hits bollard, driver fired\nOccurred: March 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nTesla fired an employee in its Autopilot division for publicly sharing a video of a 2021 Model 3 Full-Self Driving (FSD) beta test car colliding with a bollard on the side of a road in San Jose, California.\nJohn Bernal, who has been creating videos about Tesla's FSD beta for over a year on his AI Addict YouTube channel, posted his latest video three days after 2021 Model 3 hit a bollard separating a car lane from a bike lane in San Jose. \n'No matter how minor this accident was, it's the first FSD beta collision caught on camera that is irrefutable' Bernal argued in a later frame-by-frame video of the incident.\nBernal told CNN that an Autopilot manager had attempted to stop him posting negative or critical content involving FSD Beta in a meeting before his dismissal.\nAccording to CNN, Tesla\u2019s social media policy makes no direct reference to criticising the company\u2019s products in public, nor does it specifically reference YouTube as a social media channel. \nTesla has not publicly commented on the collision, nor why Bernal was fired.\nSystem \ud83e\udd16\nTesla Autopilot, Full-self Driving\n\nDocuments \ud83d\udcc3\nhttps://www.youtube.com/c/AIAddict\nhttps://youtu.be/sbSDsbDQjSU\nhttps://www.youtube.com/watch?v=Zl9rM8D3k34\nhttps://twitter.com/Aiaddict1/status/1503954788257452034\nOperator: John Bernal\nDeveloper: Tesla\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Self-driving system; Computer vision\nIssue: Safety; Accuracy/reliability; Employment\nTransparency: Governance; Black box; Marketing; Legal\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.cnbc.com/2022/03/15/tesla-fired-employee-who-posted-fsd-beta-videos-as-ai-addict-on-youtube.html\nhttps://www.theverge.com/2022/3/16/22980580/tesla-fired-employee-john-bernal-ai-addict-youtube-full-self-driving-beta-reviews\nhttps://www.dailymail.co.uk/news/article-10499145/Full-Self-Driving-clips-Teslas-trying-drive-train-tracks-smashing-bike-lane.html\nhttps://arstechnica.com/tech-policy/2022/03/tesla-fires-employee-who-posted-youtube-videos-of-full-self-driving-accident/\nhttps://www.thedrive.com/news/44788/tesla-fires-autopilot-employee-who-posted-fsd-beta-collision-to-youtube\nhttps://www.businessinsider.com/tesla-employee-self-driving-youtube-reviewer-fired-fsd-collision-video-2022-3\nhttps://nypost.com/2022/03/16/tesla-employee-fired-after-driverless-tech-youtube-reviews\nhttps://futurism.com/tesla-fires-employee-youtube-videos-fsd\nRelated \ud83c\udf10\nTesla Model Y FSD beta crash\nTesla phantom braking\nPage info\nType: Incident\nPublished: March 2022", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ukraine-decision-to-use-clearview-ai-facial-recognition-draws-concerns", "content": "Ukraine use of Clearview AI facial recognition draws concerns\nOccurred: March 2022-\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nUkraine's decision to use Clearview AI's facial recognition technology to identify Russian soldiers, prisoners of war, and undercover saboteurs prompted concerns about what happens if the system makes mistakes.\nUkraine's vice prime minister Mykhailo Fedorov told Reuters that the country's defence ministry is using the controversial system. \nHe also said it would 'dispel the myth of a 'special operation' in which there are 'no conscripts\u2019 and \u2018no one dies,' according to a message posted on Telegram by Federov noted by Forbes cybersecurity writer Thomas Brewster.\nDespite broad support for Ukraine, human and civil rights experts are concerned about what happens if a person is wrongly identified, arrested or worse should Clearview's system make a mistake. In theory, mistakes are more likely given it will have to handle war-scarred faces. \nA 2019 US Department of Energy study concluded that facial decomposition considerably reduced the technology's effectiveness, though a 2021 conference paper was more promising.\nSystem \ud83e\udd16\nClearview AI website\nClearview AI Wikipedia profile\nOperator: Ministry of Defen\u0441e of Ukraine\nDeveloper: Clearview AI\nCountry: Ukraine\nSector: Govt - defence\nPurpose: Identify Russian combatants\nTechnology: Facial recognition\nIssue: Accuracy/reliability; Dual/multi-use\nTransparency: Governance; Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reuters.com/technology/exclusive-ukraine-has-started-using-clearview-ais-facial-recognition-during-war-2022-03-13/\nhttps://www.reuters.com/technology/ukraine-uses-facial-recognition-identify-dead-russian-soldiers-minister-says-2022-03-23/\nhttps://www.msn.com/en-gb/news/world/ukraine-using-facial-recognition-to-id-dead-russian-soldiers-and-tell-families/ar-AAVqwv6\nhttps://www.dailymail.co.uk/sciencetech/article-10614561/Ukraine-using-facial-recognition-technology-uncover-Russian-assailants-identify-dead.html\nhttps://www.zdnet.com/article/ukraine-reportedly-adopts-clearview-ai-to-track-russian-invaders/\nhttps://www.cnbc.com/2022/03/13/ukraine-has-started-using-clearview-ais-facial-recognition-during-war.html\nhttps://www.thestar.com.my/tech/tech-news/2022/03/24/ukraine-says-using-ai-facial-recognition-to-identify-dead-russian-soldiers\nhttps://www.forbes.com/sites/thomasbrewster/2022/03/23/ukraine-starts-using-facial-recognition-to-identify-dead-russians-and-tell-their-relatives/\nhttps://www.washingtonpost.com/technology/2022/03/03/telegram-russian-war-dead-ukraine-pows/\nRelated \ud83c\udf10\nRussian Ukraine disinformation bot farms\nRCMP facial recognition covert surveillance\nPage info\nType: Issue\nPublished: March 2022\nLast updated: February 2024", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tiktok-russiaukraine-war-disinformation", "content": "TikTok exposes new users to Russia/Ukraine war disinformation\nOccurred: March 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nPeople opening new accounts on TikTok are quickly shown misinformation and disinformation about the Russia/Ukraine war.\nAn investigation by anti-misinformation company NewsGuard discovered, by setting up a pair of tests to see how TikTok handled information about the war, that a new account that simply scrolled the app\u2019s For You feature and watched videos about the conflict would be shown false or misleading content within 40 minutes. \nAccording to the Newsguard researchers, 'none of the videos ... contained any information about the trustworthiness of the source, warnings, fact-checks, or additional information that could empower users with reliable information.'\nSystem \ud83e\udd16\nTikTok For You recommendation algorithm\nOperator: ByteDance/TikTok\nDeveloper: ByteDance/TikTok\nCountry: USA; Global\nSector: Media/entertainment/sports/arts; Govt - defence; Politics\nPurpose: Recommend content\nTechnology: Recommendation algorithm\nIssue: Mis/disinformation\nTransparency: Governance; Black box\nResearch, advocacy \ud83e\uddee\nNewsGuard (2022). WarTok: TikTok is feeding war disinformation to new users within minutes \u2014 even if they don\u2019t search for Ukraine-related content\nNewsGuard (2022). NewsGuard Report Shows TikTok Users See False Ukraine War Content Minutes After Signing Up\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theguardian.com/technology/2022/mar/21/tiktok-algorithm-directs-users-to-fake-news-about-ukraine-war-study-says\nhttps://www.newsweek.com/wartok-tiktok-feeding-war-disinformation-new-users-within-minutes-misinformation-monitor-1690808\nhttps://www.nytimes.com/2022/03/05/technology/tiktok-ukraine-misinformation.html\nhttps://fortune.com/2022/03/21/tiktok-misinformation-ukraine/amp/\nhttps://www.euronews.com/my-europe/2022/03/22/new-tiktok-users-exposed-to-fake-news-about-russia-ukraine-war-study-reveals\nhttps://www.republicworld.com/world-news/russia-ukraine-crisis/tiktok-algorithm-misleading-users-fake-news-about-ukraine-crisis-reveals-new-study-articleshow.html\nhttps://www.ansa.it/sito/notizie/tecnologia/internet_social/2022/03/22/ucraina-newsguard-tiktok-indirizza-utenti-su-fake-news_e1b52f07-0926-47d4-b7dd-18da2b1b768a.html\nhttps://www.kompas.tv/article/272949/newsguard-algoritma-tiktok-suapi-pengguna-dengan-konten-disinformasi-soal-konflik-rusia-ukraina\nRelated \ud83c\udf10\nBytedance/TikTok Uyghur censorship\nBeijing Uyghur emotion detection\nPage info\nType: Incident\nPublished: March 2022\nLast updated: November 2023", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/russian-kub-bla-suicide-drone-attacks", "content": "Russian KUB-BLA drones conduct autonomous 'suicide' attacks\nOccurred: March 2022-\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAutonomous Kalashnikov ZALA Aero KUB-BLA loitering drones may have been used for 'suicide attacks' in Russia's war with Ukraine, raising concerns about the ethics and accuracy of so-called 'killer robots'.\nAccording to a Bulletin of the Atomic Scientists article, images of a downed Russian-made Kalashnikov ZALA Aero KUB-BLA drone were found in Ukraine. \nKUB-BLAs are able to select and strike targets through inputted coordinates or autonomously, with the system capable of 'real-time recognition and classification of detected objects' using AI, according (pdf) to Netherlands-based peace organisation Pax for Peace.\nOpen source intelligence together with images released by the Russian government indicate AI technologies are being used widely in the Russia-Ukraine war.\nSystem \ud83e\udd16\nZALA Aero KUB-BLA Wikipedia profile\nOperator: Russian Aerospace Forces\nDeveloper: Rostec/JSC Kalashnikov Concern/ZALA Aero\nCountry: Ukraine; Russia\nSector: Govt - defence\nPurpose: Kill/maim/damage/destroy\nTechnology: Drone; Object recognition\nIssue: Lethal autonomous weapons; Ethics\nTransparency: Governance\nNews, commentary, analysis \ud83d\udcf0\nhttps://twitter.com/RALee85/status/1502550038731497474\nhttps://www.wired.com/story/ai-drones-russia-ukraine/\nhttps://futurism.com/the-byte/suicide-drones-russia-ukraine\nhttps://thebulletin.org/2022/03/russia-may-have-used-a-killer-robot-in-ukraine-now-what/\nhttps://www.dailykos.com/stories/2022/3/17/2086500/-Ukraine-update-Real-terminators-don-t-ask-for-names\nhttps://www.thedrive.com/the-war-zone/44725/proof-of-russia-using-suicide-drones-in-ukraine-emerges\nhttps://www.unilad.co.uk/technology/russias-suicide-drone-raises-fears-over-ai-in-warfare-20220318\nhttps://www.armyrecognition.com/defense_news_march_2022_global_security_army_industry/russia_using_kalashnikov_zala_aero_kub_suicide_drones_in_ukraine.html\nhttps://www.reddit.com/r/UkraineWarVideoReport/comments/tccxi6/downed_russian_kamikaze_drone_kubbla_in_kiev/\nRelated \ud83c\udf10\nUkraine Bayraktar TB2 drone attacks\nHouthi Abu Dhabi drone attack\nPage info\nType: Incident\nPublished: March 2022", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/estee-lauder-employee-performance-assessments", "content": "Est\u00e9e Lauder fires employees after automated performance assessments\nOccurred: March 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nCosmetics company Est\u00e9e Lauder agreed an out-of-court settlement with three make-up artists who were sacked after taking an automated job application assessment.\nHaving had to reapply for their positions, the women had their answers and expressions analysed by recruitment analysis software supplied by HireVue, with the results measured against other data about their job performance. Est\u00e9e Lauder subsidiary MAC Cosmetics had not previously warned any of the three women of performance issues, leading them to conclude they had been unfairly treated and to begin legal proceedings against the company. \nFormer MAC make-up artist Anthea Mairoudhiou later described losing her job as 'the end of the road of that career.' 'And, mentally it massively affected me. I felt very let down, and I thought I was going mad, she added. \nAn Est\u00e9e Lauder spokesman said 'In the situation described, facial recognition accounted for well under 1% (0.25%) of employees\u2019 overall assessment. The company has teams who overlay objective performance-related data and other qualitative feedback, which accounted for the majority of the employment assessment, to make decisions on employment,' he added.\nSystem \ud83e\udd16\nHireVue website\nOperator: Est\u00e9e Lauder/MAC Cosmetics\nDeveloper: HireVue\nCountry: UK\nSector: Cosmetics\nPurpose: Assess employee performance\nTechnology: Facial recognition; Behavioural analysis\nIssue: Accuracy/reliability; Fairness\nTransparency: Governance; Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/iplayer/episode/m0015gvw/computer-says-no\nhttps://www.bbc.co.uk/bbcthree/article/c62bcab6-db6f-4026-90bb-7f508705a65b\nhttps://www.thetimes.co.uk/article/payout-for-estee-lauder-women-sacked-by-algorithm-wnq0ffzt3\nhttps://www.personneltoday.com/hr/estee-lauder-women-sacked-by-algorithm-redundancy-software-hirevue-automation/\nhttps://www.cityam.com/estee-lauder-settles-with-former-staff-over-algorithm-redundancies/\nhttps://bmmagazine.co.uk/news/payout-for-estee-lauder-women-sacked-by-algorithm/\nRelated \ud83c\udf10\nHireVue recruitment facial analysis screening\nXsolla employee monitoring, terminations\nPage info\nType: Incident\nPublished: March 2022\nLast updated: February 2024", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/president-zelenskyy-deepfake-surrender", "content": "Deepfake President Zelenskyy instructs Ukraine army to surrender\nOccurred: March 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA deepfake video circulated online allegedly showing Ukraine president Volodymyr Zelenskyy instructing his army to lay down their arms and surrender. \nAccording to disinformation experts the Atlantic Council\u2019s Digital Forensic Research Lab, the video was first aired on hacked news programme Ukraine 24.\nUkraine 24 quickly posted a warning on Facebook that it had been hacked and that the video was fake, and Facebook removed it from its platform for violating its policy against misleading manipulated media.\nIn a response to the deepfake video posted to Facebook, Zelenksy callied for Russians troops to surrender.\nSystem \ud83e\udd16\nUnknown\n\nDocuments \ud83d\udcc3\nPresident Zelenskyy deepfake video\nMeta response\nOperator:  \nDeveloper:  \nCountry: Ukraine; Russia\nSector: Politics\nPurpose: Scare/confuse/destabilise\nTechnology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning  \nIssue: Mis/disinformation; Ethics\nTransparency: Governance; Privacy\nResearch, advocacy \ud83e\uddee\nStanford HAI (2023). 2023 AI Index Report - 3.2 AI Incidents (pdf)\nInvestigations, assessments, audits \ud83e\uddd0\nDeepfaked. Prebunking Russian deepfake of Ukrainian President\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.atlanticcouncil.org/blogs/new-atlanticist/russian-war-report-hacked-news-program-and-deepfake-video-spread-false-zelenskyy-claims/\nhttps://edition.cnn.com/2022/03/16/tech/deepfake-zelensky-facebook-meta/index.html\nhttps://www.vice.com/en/article/93bmda/hacked-news-channel-and-deepfake-of-zelenskyy-surrendering-is-causing-chaos-online\nhttps://www.snopes.com/news/2022/03/16/zelenskyy-deepfake-shared/\nhttps://futurism.com/deepfaked-video-ukrainian-president-surrender\nhttps://www.dailydot.com/debug/hackers-zelensky-deepfake-surrender-ukraine-war/\nhttps://www.msn.com/en-us/news/world/facebook-removes-e2-80-98deepfake-e2-80-99-of-ukrainian-president-zelenskyy/ar-AAVa4jy\nhttps://techcrunch.com/2022/03/16/facebook-zelensky-deepfake/\nhttps://www.thedailybeast.com/laughably-bad-deepfakes-of-volodymyr-zelensky-could-spiral-into-dangerous-war-disinformation\nhttps://www.npr.org/2022/03/16/1087062648/deepfake-video-zelenskyy-experts-war-manipulation-ukraine-russia\nhttps://www.independent.co.uk/tech/facebook-deepfake-ukraine-zelensky-russia-b2037766.html\nhttps://www.inputmag.com/tech/deepfake-ukraine-president-volodymyr-zelensky\nhttps://news.sky.com/story/ukraine-war-deepfake-video-of-zelenskyy-telling-ukrainians-to-lay-down-arms-debunked-12567789\nRelated \ud83c\udf10\nYoon Suk-yeol presidential election candidate deepfake\nLauren Book deepfake extortion\nPage info\nType: Incident\nPublished: March 2022", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/weight-watchers-child-data-harvesting", "content": "Weight Watchers fined for harvesting US child data\nOccurred: 2018-\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe company formerly known as Weight Watchers collected data on children under 13 through Kurbo, a weight loss app it owns, according to a US Federal Trade Commission (FTC) settlement order. \nThe FTC says hundreds of children were able to sign up to use Kurbo using false birthdays without parental permission and could then change their ages once they had registered. \nKurbo also failed to inform parents about its data collection practices for kids, and kept the data for many years, possibly to help develop other services. \nThe FTC ordered WW International (aka Weight Watchers) to 'destroy any algorithms derived from the data', and to pay a USD 1.5 million penalty.\nSince Kurbo was acquired by Weight Watchers in 2018, the weight loss app has been the subject of regular criticism from dietitions and others.\nSystem \ud83e\udd16\nKurbo Wikipedia profile\nOperator: WW International/Weight Watchers/Kurbo\nDeveloper: WW International/Weight Watchers/Kurbo\nCountry: USA\nSector: Consumer goods\nPurpose: Manage eating habits\nTechnology: Application\nIssue: Privacy; Appropriateness/need\nTransparency: Governance; Privacy; Marketing\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nhttps://www.ftc.gov/news-events/press-releases/2022/03/ftc-takes-action-against-company-formerly-known-weight-watchers\nhttps://www.ftc.gov/system/files/documents/cases/filed_complaint.pdf\nhttps://www.ftc.gov/system/files/documents/cases/wwkurbostipulatedorder.pdf\nResearch, advocacy \ud83e\uddee\nhttps://www.change.org/p/weight-watchers-remove-your-weight-loss-app-for-kids\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theguardian.com/society/2022/mar/04/weight-watchers-kurbo-diet-app-children-data\nhttps://www.theguardian.com/lifeandstyle/2019/aug/25/a-diet-app-for-kids-is-hard-to-swallow\nhttps://www.nytimes.com/2022/03/08/business/weight-watchers-data-children.html\nhttps://www.axios.com/weight-watchers-children-data-ftc-bd9180cf-f672-48bb-af1f-34cf1b004409.html\nhttps://www.cbsnews.com/news/weight-watchers-diet-kids-ftc/\nhttps://www.law360.com/consumerprotection/articles/1465847/ftc-weight-loss-co-ink-deal-to-safeguard-kids-privacy\nhttps://www.jdsupra.com/legalnews/ftc-settles-with-weight-watchers-in-4024870/\nhttps://www.protocol.com/bulletins/weight-watchers-coppa-ftc\nhttps://www.nextgov.com/emerging-tech/2022/03/ftc-justice-say-weight-watchers-parent-group-illegally-collected-child-health-data/362786/\nhttps://www.protocol.com/policy/ftc-algorithm-destroy-data-privacy\nRelated \ud83c\udf10\nTikTok US personal data harvesting\nEveralbum personal facial data harvesting\nPage info\nType: Incident\nPublished: March 2022", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/everalbum-facial-recognition-default-tagging", "content": "Everalbum covertly uses personal data to train facial recognition system\nOccurred: May 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nCloud photo storage company Everalbum used its users' photographs to train its facial recognition system without informing them or letting them turn the system off. \nAccording to an NBC investigation, the photos people shared with the app were used to train Everalbum Inc's facial recognition system, with the system then offered to private companies, law enforcement and the military customers under different names, including \u201cEver AI\u201d and now \u201cParavision.\u201d \nThe company had initially used publicly available technology for basic facial recognition features but later developed its own algorithms using user images as training data.\nEveralbum also stored user data indefinitely, despite telling users that their photos and videos would be deleted if their accounts were deactivated.\nThe finding sparked multiple privacy and civil rights organisations to accuse Everalbum and its brands of egregious privacy abuse and opaque and misleading marketing, and to a wave of complaints by its users.\n\u2795 Ever rebranded as Paravision days after NBC's report.\n\u2795 August 2020. Ever's cloud photo storage app closed, with increased competition from Apple and Google blamed by the company. \n\u2795 January 2021. The furore prompted a January 2021 complaint (pdf) by the US Federal Trade Commission (FTC).\n\u2795 May 2021. Everalbum settled (pdf) with the FTC, with Ever instructed to all user data harvested from its app and to delete 'any models or algorithms developed in whole or in part' using that data. \nSystem \ud83e\udd16\nEveralbum Inc\nOperator: Everalbum/Ever AI/Paravision\nDeveloper: Everalbum/Ever AI/Paravision\nCountry: USA\nSector: Consumer services\nPurpose: Train facial recognition system\nTechnology: Facial recognition\nIssue: Privacy; Surveillance\nTransparency: Governance; Privacy; Marketing\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nFederal Trade Commission (2021). California Company Settles FTC Allegations It Deceived Consumers about use of Facial Recognition in Photo Storage App\nFederal Trade Commission (2021). Everalbum order (pdf)\nFederal Trade Commission (2021). Everalbum complaint (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.nbcnews.com/tech/security/millions-people-uploaded-photos-ever-app-then-company-used-them-n1003371\nhttps://techcrunch.com/2020/08/24/ever-once-accused-of-building-facial-recognition-tech-using-customer-data-shuts-down-consumer-app/\nhttps://www.latimes.com/business/story/2021-01-29/column-facial-recognition-privacy\nhttps://nymag.com/intelligencer/2019/11/the-future-of-facial-recognition-in-america.html\nhttps://www.theregister.com/2021/01/13/paravision_ftc_settlement/\nhttps://news.bloomberglaw.com/privacy-and-data-security/paravision-faces-claim-it-used-cloud-photos-for-face-recognition\nhttps://www.dpreview.com/news/0874174431/paravision-ai-ordered-delete-face-recognition-software-user-pictures-without-permission\nhttps://www.macrumors.com/2020/08/24/everalbum-shutting-down/\nhttps://www.wired.com/story/startup-nix-algorithms-ill-gotten-facial-data/\nhttps://www.theverge.com/2021/1/11/22225171/ftc-facial-recognition-ever-settled-paravision-privacy-photos\nhttps://jolt.law.harvard.edu/digest/everalbum-inc-in-first-facial-recognition-misuse-settlement-ftc-requires-destruction-of-algorithms-trained-on-deceptively-obtained-photos\nhttps://www.protocol.com/policy/ftc-algorithm-destroy-data-privacy\nRelated \ud83c\udf10\nWeight Watchers child data harvesting\nTikTok US personal data harvesting\nPage info\nType: Incident\nPublished: March 2022", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/uber-self-driving-car-pedestrian-fatality", "content": "Uber self-driving car kills Arizona pedestrian\nOccurred: March 2018\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nRafaela Vasquez, the test driver of an Uber autonomous car, crashed into a woman walking her bicycle across a road in Tempe, Arizona. \nOperating in self-drive mode with Vasquez in the driving seat, the Uber fatally struck Elaine Herzberg. The incident was the first known case of a fatality involving a self-driving car.\nUber escaped (pdf) prosecution, but Vasquez was indicted by prosecutors in Arizona in August 2020 on a count of negligent homicide on the basis that she was checking Slack messages from Uber on her work mobile phone and watching a reality show on her personal phone. \nIn July 2023, Vazquez pleaded guilty to one count of endangerment and was sentenced to three years of supervised probation, with no time in prison.\nThe case was seen as a test of with which party legal liability lies. A US National Transportation Safety Board (NTSB) investigation concluded the Uber car had failed to identify Herzberg as a pedestrian and to apply its brakes. It also found that Uber maintained an 'inadequate safety culture.'\nUber suspended its self-driving test programme following the incident, later restarting it in Pittsburgh.\nSystem \ud83e\udd16\nUber USA website\nUber Wikipedia profile\nWikipedia. Death of Elaine Herzberg\nOperator: Uber\nDeveloper: Uber\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Self-driving system\nIssue: Safety; Accuracy/reliability; Liability\nTransparency: Black box\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nCounty of Maricopa. Vasquez v GJ indictment\nNTSB investigation HWY18MH010\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theverge.com/2019/3/5/18252423/uber-wont-be-charged-with-fatal-self-driving-crash-says-prosecutor\nhttps://arstechnica.com/cars/2020/09/arizona-prosecutes-uber-safety-driver-but-not-uber-for-fatal-2018-crash/\nhttps://www.bbc.com/news/world-us-canada-43497364\nhttps://www.bbc.co.uk/news/technology-54175359\nhttps://www.theguardian.com/technology/2018/mar/22/self-driving-car-uber-death-woman-failure-fatal-crash-arizona\nhttps://www.reuters.com/article/us-uber-crash/in-review-of-fatal-arizona-crash-u-s-agency-says-uber-software-had-flaws-idUSKBN1XF2HA\nhttps://qz.com/1566048/uber-not-criminally-liable-in-tempe-self-driving-car-death/\nhttps://eu.azcentral.com/story/news/local/tempe/2019/03/17/one-year-after-self-driving-uber-rafaela-vasquez-behind-wheel-crash-death-elaine-herzberg-tempe/1296676002/\nhttps://www.msn.com/en-us/money/companies/uber-e2-80-99s-fraught-and-deadly-pursuit-of-self-driving-cars-is-over/ar-BB1bItnu\nhttps://www.bloomberg.com/news/articles/2018-05-25/self-driving-uber-investigation-reveals-handoff-problem\nhttps://www.latimes.com/business/autos/la-fi-hy-uber-self-driving-20180319-story.html\nhttps://www.latimes.com/business/story/2021-03-09/elon-musk-wants-it-both-ways-with-telsas-full-self-driving\nhttps://www.consumerreports.org/autonomous-driving/backup-driver-for-uber-test-car-streamed-hulu-at-time-of-fatal-crash/\nhttps://www.wired.com/story/uber-self-driving-car-fatal-crash/\nRelated \ud83c\udf10\nSon Ji-chang Tesla Model X sudden acceleration\nTesla phantom braking\nPage info\nType: Incident\nPublished: March 2022", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/russia-disinformation-bot-farms", "content": "Ukraine shuts down Russian disinformation bot farms\nOccurred: February 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nUkrainian authorities shut down two Russian bot farms that were spreading false rumours about Ukraine, bomb threats and other disinformation designed to sow panic amongst the Ukranian people.\nThe Security Service of Ukraine (SSB) took down a bot farm in Lviv it said was capable of deploying up to 18,000 fake accounts. \nAnother smaller bot farm that reportedly used 7,000 accounts to post fake information on Telegram, WhatsApp and Viber was also discovered.\nFacebook parent company Meta also shut down a fake, pro-Russian influence campaign attempting to spread anti-Ukrainian propaganda using dozens of AI-generated fake profiles.\nSystem \ud83e\udd16\nUnknown\n\nDocuments \ud83d\udcc3\nhttps://ssu.gov.ua/en/novyny/sbu-likviduvala-18ty-tysiachnu-botofermu-u-lvovi-pid-kuratorstvom-rf-siialy-paniku-ta-minuvaly-obiekty-video\nhttps://www.facebook.com/SecurSerUkraine/posts/306364461590471\nOperator: \nDeveloper: \nCountry: Ukraine; Russia\nSector: Politics\nPurpose: Scare/confuse/destabilise\nTechnology: Bot/intelligent agent\nIssue: Mis/disinformation\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.vice.com/en/article/4awq8m/video-ukraine-busts-alleged-russian-bot-farm-using-thousands-of-sim-cards\nhttps://www.latimes.com/world-nation/story/2022-02-17/russia-ukraine-disinformation-campaign\nhttps://www.rferl.org/a/ukraine-says-it-disrupted-bot-network-supported-by-russian-online-services/30441062.html\nhttps://techpolicy.press/researchers-detail-use-of-bots-to-quash-russian-opposition-cheerlead-for-putin/\nhttps://uk.news.yahoo.com/ukraine-busts-alleged-russian-led-161843766.html\nhttps://it.slashdot.org/story/22/02/09/2124236/ukraine-busts-alleged-russian-bot-farm-using-thousands-of-sim-cards\nhttps://www.msn.com/en-us/news/technology/infamous-russian-troll-farm-appears-to-be-source-of-anti-ukraine-propaganda/ar-AAUWWgR\nhttps://www.theguardian.com/media/2022/mar/04/bot-holiday-covid-misinformation-ukraine-social-media\nRelated \ud83c\udf10\n'Kyiv' deepfake influence campaign\nTwitter Ukraine OSINT account suspensions\nPage info\nType: Incident\nPublished: March 2022", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/coupang-eats-star-ratings-system", "content": "Restaurant owner dies after Coupang Eats star rating dispute\nOccurred: June 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA restaurant owner in Seoul, South Korea, collapsed and died from a brain haemorrhage due to unreasonable demands for a refund made by a customer who ordered food through food delivery platform Coupang Eats. \nThe incident resulted in protests by civic groups critical of the company's focus on cutting costs and improving delivery speed, ignoring the ability of restaurant owners and other suppliers to defend themselves, poor transparency and communications, and poor worker safety and working conditions.\nThe customer demanded a refund of 2,000 won (USD 1.77) on the basis that one of the three fried shrimp the restaurant delivered was a 'strange color', verbally abused the owner of the restaurant owner and insulted her parents and, despite receiving a refund, left a one-star rating.\n\nAccording to the owner's family, Coupang Eats did not try to arbitrate in the dispute between the owner and the customer. Instead, it simply asked the owner to comply with the customer's demands.\n\nCoupang Eats later apologised and said it would set up a department dedicated to protecting restaurant owners, in addition to introducing a function whereby owners can respond to poor reviews.\nSystem \ud83e\udd16\nCoupang website\nCoupang Wikipedia profile\nOperator: Coupang/Coupang Eats\nDeveloper: Coupang/Coupang Eats\nCountry: S Korea\nSector: Transport/logistics\nPurpose: Rate products/services\nTechnology: Rating system\nIssue: Robustness; Ethics; Employment\nTransparency: Governance; Complaints/appeals; Black box; Legal; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttp://www.koreaherald.com/view.php?ud=20210622000785\nhttp://www.koreaherald.com/view.php?ud=20210712000928\nhttps://news.mt.co.kr/mtview.php?no=2021062210491042538\nhttps://imnews.imbc.com/replay/2021/nwdesk/article/6280498_34936.html\nhttps://news.mt.co.kr/mtview.php?no=2021062213455644089\nhttps://www.joongang.co.kr/article/24088022#home\nhttps://www.koreatimes.co.kr/www/nation/2021/06/371_310914.html\nhttps://koreajoongangdaily.joins.com/2021/11/06/business/industry/FoodDelivery-App-CoupangEats/20211106070011879.html\nhttp://koreabizwire.com/food-delivery-apps-step-up-to-fix-controversial-star-rating-systems/195463\nhttps://restofworld.org/2022/south-korea-star-ratings-trouble/\nRelated \ud83c\udf10\nCoupang own brand search engine rigging\nNaver own brand search engine rigging\nPage info\nType: Incident\nPublished: March 2022", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/south-korea-presidential-election-candidate-deepfakes", "content": "Yoon Suk-yeol presidential deepfake candidacy prompts concerns\nOccurred: February 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA campaign by South Korean politician Yoon Suk-yeol to become the world's first 'deepfake candidate' to win a presidential election raised concerns that it painted an inaccurate picture of his real character.\n'AI Yoon''s team turned to avatars and short-form videos to explain policy ideas and lampoon his rival in a bid to win over younger voters.\nHowever, some voters have complained that it is an inaccurate physical representation and paints an untrue political picture of Yoon. Others have pointed out the potential for misuse of deepfakes in the form of mis- and disinformation.\nAFP says South Korea's election monitor allowed AI candidates to campaign on the condition deepfake technology is clearly identified, and does not spread misinformation.\nA recent study published in the Proceedings of the National Academy of Sciences USA suggests that real humans can easily fall for deepfake faces and tend to interpret them as more trustworthy than the real thing.\nSystem \ud83e\udd16\nDeepBrain AI website\nOperator: People Power Party\nDeveloper: People Power Party; DeepBrain AI\nCountry: S Korea\nSector: Politics\nPurpose: Communicate with young voters\nTechnology: Deepfake - audio, video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Accuracy/reliability; Mis/disinformation; Ethics\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nNightingale S., Farid H. (2021). AI-synthesized faces are indistinguishable from real faces and more trustworthy\nCommentary, analysis \ud83d\uddde\ufe0f\nhttps://www.wsj.com/articles/these-campaigns-hope-deepfake-candidates-help-get-out-the-vote-11646756345\nhttps://www.scmp.com/news/asia/east-asia/article/3166928/south-korean-worlds-first-official-deepfake-candidate-meet-ai\nhttps://www.msn.com/en-in/news/world/south-korean-presidential-candidate-yoon-suk-yeol-using-deepfake-technology-to-garner-votes/ar-AATPjq9\nhttps://www.scientificamerican.com/article/humans-find-ai-generated-faces-more-trustworthy-than-the-real-thing/\nhttps://www.thetimes.co.uk/article/korea-s-ai-yoon-avatar-stands-in-for-presidential-candidate-8kfr5r7bj\nhttp://www.koreaherald.com/view.php?ud=20211208000709\nhttps://www.koreatimes.co.kr/www/nation/2021/12/356_320192.html\nhttps://www.youtube.com/watch?v=yIUTvPOXkk8&t=47s\nhttps://www.youtube.com/watch?v=ra3SSfiX_24\nhttps://www.reuters.com/world/asia-pacific/skorea-candidates-woo-young-voters-with-deepfakes-hair-insurance-2022-03-03/\nhttps://cmte.ieee.org/futuredirections/2022/02/19/can-you-trust-a-deepfake-presidential-candidate/\nhttps://cacm.acm.org/news/258811-deepfake-democracy-south-korean-candidate-goes-virtual-for-votes/fulltext\nhttps://screenshot-media.com/technology/ai/south-korea-deepfakes-politicians/\nRelated \ud83c\udf10\nPresident Zelenskyy deepfake surrender\nLauren Book deepfake extortion\nPage info\nType: Incident\nPublished: March 2022", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/instacart-personal-shopper-pay-algorithm", "content": "'Unfair' Instacart personal shopper pay algorithm sparks backlash\nOccurred: October 2018-\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nUS grocery and pick-up company Instacart faced a nationwide customer boycott on account of the poor pay doled out to its personal shoppers.\nIn an open letter to Instacart CEO Fidji Simo, the Gig Workers Collective encouraged customers to support delivery workers by boycotting the company until it 'rectifies the genuinely inequitable manner in which it treats its shoppers.'\nThe boycott is the latest in a series of boycotts, walk-offs and strikes driven by low pay, failure to reimburse workers for business expenses, 'stolen' tips subsidising worker pay and other issues that have dogged Instacart in recent years - issues seen to have been aggravated by management greed and an increasing reliance on automation and algorithms.\nThe company revised its personal shopper pay system early 2019 after shoppers walked out over an October 2018 update to the system that resulted in 'substantially' lower pay, and customers complained on social media that their orders were being delayed. \nAnd in November 2019 it controversially withdrew a USD 3 'quality bonus' personal shoppers received for every five-star rating they garnered from customers after a three-day worker pay strike.\nSystem \ud83e\udd16\nInstacart website\nInstacart Wikipedia profile\nDocuments \ud83d\udcc3\nhttps://medium.com/shopper-news/introducing-instacarts-new-earnings-structure-for-shoppers-26a11df53581\nhttps://medium.com/shopper-news/state-of-pay-doing-right-by-our-shoppers-81de4b66580\nhttps://www.instacart.com/company/shopper-community/excited-to-join-the-instacart-family/\nOperator: Instacart\nDeveloper: Instacart\nCountry: USA\nSector: Transport/logistics\nPurpose: Calculate pay\nTechnology: Pay algorithm\nIssue: Employment - pay; Fairness\nTransparency: Governance; Compaints/appeals; Black box\nResearch, advocacy \ud83e\uddee\nhttps://medium.com/@workersboycottic/despite-increased-public-awareness-of-worker-pay-issues-and-the-minimum-wage-instacart-continues-ab4526d50b58\nhttps://medium.com/@workingwa/instacart-heres-our-22-cents-no-more-tip-theft-low-pay-and-black-box-pay-algorithms-8ff1d7c6b66\nhttps://www.hrw.org/news/2020/10/15/grocery-app-workers-rights-are-under-siege\nhttps://actionnetwork.org/petitions/instacart-heres-our-22-cents-no-more-tip-theft-low-pay-and-black-box-pay-algorithms\nhttp://www.workingwa.org/blog/2019/1/14/instacarts-transparent-new-pay-structure-underpayment-tip-theft-and-black-box-algorithms\nhttps://gigworkerscollective.medium.com/open-letter-to-fidgi-simo-instacarts-ceo-e72c0de55e22\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://themarkup.org/2021/10/12/why-are-some-instacart-workers-calling-for-an-app-boycott\nhttps://arstechnica.com/tech-policy/2018/11/instacart-changes-how-it-pays-shoppers-but-many-say-theyre-now-making-less/\nhttps://www.businessinsider.com/instacart-shoppers-blame-delays-on-lower-pay-2018-12\nhttps://www.businessinsider.com/instacart-shoppers-threaten-boycott-claim-lower-pay-2018-12\nhttps://www.chicagotribune.com/business/ct-biz-instacart-shopper-complaints-20181206-story.html\nhttps://www.fastcompany.com/90300962/reeling-from-algorithm-glitch-instacart-institutes-3-minimum-fee-for-drivers\nhttps://www.vox.com/2019/2/6/18213872/gig-economy-instacart-tip-theft-contract-workers\nhttps://www.nytimes.com/2019/02/06/technology/instacart-doordash-tipping-deliveries.html\nhttps://www.theverge.com/2019/2/6/18214335/instacart-reverse-controversial-pay-policy-tip-stealing\nhttps://www.buzzfeednews.com/article/carolineodonovan/after-scrutiny-instacart-will-end-its-controversial-tipping\nhttps://www.vox.com/the-goods/2019/5/14/18566237/instacart-shopper-tip-grocery-delivery-payment\nhttps://www.thedailybeast.com/instacart-workers-sick-of-being-screwed-by-the-algorithm-gear-up-for-strike\nhttps://mashable.com/article/instacart-shoppers-work-stoppage-open-letter-apoorva-mehta\nhttps://inthesetimes.com/article/instacart-gig-economy-strike-mathwashing-algorithm\nhttps://www.vice.com/en/article/v7e9gb/instacart-shoppers-will-stage-nationwide-strike\nhttps://www.vice.com/en/article/j5y3q7/instacart-cuts-quality-bonus-after-workers-go-on-3-day-strike\nhttps://www.vice.com/en_us/article/zmj938/instacart-customers-and-workers-are-revolting-against-the-app\nhttps://onezero.medium.com/instacart-removes-worker-bonus-just-days-after-shoppers-strike-625aa71cf6b4\nhttps://www.latimes.com/business/technology/story/2020-12-21/instacart-shoppers-ratings-returns-missing-orders\nhttps://www.reddit.com/r/InstacartShoppers/\nRelated \ud83c\udf10\nInstacart gig shopper robotisation\nDoordash tip withholding\nPage info\nType: Incident\nPublished: October 2021\nLast updated: February 2024", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/uber-upfront-fares-driver-pay-algorithm", "content": "Uber UpFront Fares algorithm cuts some drivers' earnings\nReleased: February 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn Uber algorithm that allowed drivers in 24 cities across the US to see pay and destinations before accepting a trip resulted in lower overall earnings, and in Uber taking a bigger cut of fares.\nUber said its new Upfront Fare algorithm is based on 'several factors', including base fare, time and distance rates, and real-time demand at the destination, as opposed purely to time and distance, and that the new system is more transparent, gives drivers 'more control and choice', and that drivers are likely to make less money for longer trips but should earn more on shorter trips.\nHowever, some drivers reported lower overall earnings since the introduction of Upfront Fares. Additionally, Uber appeared to be  taking a larger share of fares, although the exact reasoning behind this is unclear, according to The Markup.\nExcept California, where Uber started a similar programme in 2020, the company had resisted offering drivers the ability to see the fare and destination before accepting a trip on the basis that drivers may 'cherry-pick trips' or 'discriminate against riders in disadvantaged neighbourhoods'.\nSystem \ud83e\udd16\nUber USA website\nUber Wikipedia profile\nDocuments \ud83d\udcc3\nUber (2022). More transparency with Upfront Fares\nUber (2022). Introducing Upfront Fares\nOperator: Uber\nDeveloper: Uber\nCountry: USA\nSector: Automotive\nPurpose: Determine pay\nTechnology: Pay algorithm\nIssue: Employment - pay; Fairness\nTransparency: Governance; Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://therideshareguy.com/upfront-pricing-for-drivers/\nhttps://www.reuters.com/business/autos-transportation/exclusive-uber-revamps-driver-pay-algorithm-large-us-pilot-attract-drivers-2022-02-26/\nhttps://themarkup.org/working-for-an-algorithm/2022/03/01/secretive-algorithm-will-now-determine-uber-driver-pay-in-many-cities\nhttps://labsnews.com/en/news/business/uber-tests-algorithm/\nhttps://thepaypers.com/mobile-payments/uber-tests-algorithm-that-changes-us-drivers-payments--1254972\nhttps://www.medianama.com/2022/03/223-uber-pilot-us-cities-fare-drop-details-drivers/\nRelated \ud83c\udf10\nUber, Amazon use AI to pay people different wages for the same work\nDoordash order matching algorithm\nPage info\nType: Incident\nPublished: February 2022\nLast updated: September 2023", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ethiopia-bayraktar-tb2-drone-tigray-school-attack", "content": "Ethiopia uses Bayraktar TB2 drone to attack civilians in school\nOccurred: January 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nEthiopia's government used a Turkish-made Bayraktar TB2 drone in January 2022 to attack civilians sheltering in a school during its 16-month conflict with the Tigray People\u2019s Liberation Front (TPLF). \nAccording to the Washington Post, at least 59 civilians died and 30 were wounded, including children, during the strike. Remnants of weapons recovered from the school by aid workers showed 'internal components and screw configurations matched images of Turkish-made MAM-L munitions'. \nMAM-L weaponry pair exclusively with Bayraktar TB2 drones. Run by Turkey president Recep Tayyip Erdo\u011fan\u2019s son-in-law Sel\u00e7uk Bayraktar, Baykar has supplied Ukraine, Poland, Qatar, Libya, Kyrgyzstan, Kazakhstan, Ethiopia and Azerbaijan with armed autonomous drones.\nThe incident raised concerns about the ethics of using autonomous drones in conflict situations. \nSystem \ud83e\udd16\nBayraktar TB2 drone Wikipedia profile\nOperator: Ethiopian National Defense Force (ENDF)\nDeveloper: Baykar Defence\nCountry: Ethiopia\nSector: Govt - defence\nPurpose: Kill/maim/damage/destroy\nTechnology: Drone; Object recognition\nIssue: Lethal autonomous weapons; Ethics/values\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nhttps://paxforpeace.nl/news/blogs/turkish-drones-join-ethiopias-war-satellite-imagery-confirms\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.politico.eu/article/evidence-civilian-bombing-ethiopia-turkish-drone/\nhttps://www.bbc.co.uk/news/60045176\nhttps://www.washingtonpost.com/world/interactive/2022/ethiopia-tigray-dedebit-drone-strike/\nhttps://www.reuters.com/world/africa/exclusive-us-concerned-over-turkeys-drone-sales-conflict-hit-ethiopia-2021-12-22/\nhttps://www.newsweek.com/turkeys-game-changer-bayraktar-drones-wont-secure-ethiopias-shaky-peace-opinion-1683463\nhttps://stockholmcf.org/turkish-drone-used-by-ethiopia-killed-59-civilians-sheltering-in-a-school-in-tigray-report/\nhttps://www.reuters.com/world/africa/aid-workers-say-ethiopia-air-strike-northwest-tigray-killed-56-people-2022-01-08/\nRelated \ud83c\udf10\nUkraine Bayraktar TB2 drone attacks\nHouthi Abu Dhabi drone attack\nPage info\nType: Incident\nPublished: January 2022\nLast updated: March 2022", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/sama-ethical-data-labeling-content-moderation", "content": "Sama accused of unethical data labeling, content moderation\nOccurred: February 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe team moderating content for Facebook suffered low pay, poor working conditions and alleged union-busting at training data company Sama's office in Nairobi, Kenya. \nTIME found that Kenyan employees for Sama receive a take-home wage equivalent to around USD 1.46 per hour after tax, had to work up to nine hours per day, were continuously monitored, and were measured against metrics for average time spent and quality. The metrics contradicted public statements by Facebook about not setting expectations on its contractors. \nAccording to TIME, at least two Sama content moderators resigned after being diagnosed with mental illnesses including post-traumatic stress disorder (PTSD), anxiety, and depression. Former employee and whistleblower Daniel Motaung told TIME he had been unlawfuly fired for leading over 100 Sama workers in an attempted strike that aimed to secure better pay for staff.\nIn a blog post responding to TIME's investigation, Sama claimed its rate of pay was fair, arguing 'the article falsely alleges that Sama does not compensate its employees fairly'. Two weeks later the company said it would increase salaries by 30 to 50 percent.\nMeta avoided commenting on Sama employment practices, and has not made public its audits of Sama's Kenya office. \nSama reputedly failed to confirm rumours that its managers had attempted to suppress unionisation efforts at its operation in Kenya in 2019.\n\u2795 In January 2023, Time journalist Billy Perrigo revealed that OpenAI used Kenyan workers being paid less than USD 2 an hour to de-toxify Open AI's ChatGPT and GPT-3 large language model. According to Perrigo, 'the work\u2019s traumatic nature eventually led Sama to cancel all its work for OpenAI in February 2022, eight months earlier than planned.'\n\u2795 In May 2023, a judge ruled that Meta could be sued in Kenya after 43 moderators at its Nairobi hub filed a lawsuit against the group and Sama for unfair termination. \nSystem \ud83e\udd16\nUnknown\nSama website\nSama Wikipedia profile\nDocuments \ud83d\udcc3\nSama (2022). What TIME got wrong\nFacebook (2018). Hard Questions: Who Reviews Objectionable Content on Facebook \u2014 And Is the Company Doing Enough to Support Them?\nOperator:  \nDeveloper: Sama AI/Samasource; Meta/Facebook\nCountry: Kenya\nSector: Business/professional services\nPurpose: Label data; Moderate content\nTechnology: Data labeling system; Content moderation system\nIssue: Employment - pay, jobs, working conditions, unionisation\nTransparency: Governance; Marketing - misleading\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nFoxglove (2022). Daniel Motaung launches world-first case to force Facebook to finally put content moderators\u2019 health and wellbeing ahead of cash\nResearch, advocacy \ud83e\uddee\nThe Signals Network (2022). TIME\u2019s revelations on Facebook content moderation made possible by whistleblower support\nInvestigations, assessments, audits \ud83e\uddd0\nTIME (2023). Under Fire, Facebook's 'Ethical' Outsourcing Partner Quits Content Moderation Work\nTIME (2022). Facebook Content Moderators in Kenya to Receive Pay Rise Following TIME Investigation\nTIME (2022). Inside Facebook's African sweatshop\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://fortune.com/2022/02/15/artificial-intelligence-reading-writing-transformers-natural-language-processing/\nhttps://www.msn.com/en-us/money/news/moderator-inside-facebooks-african-sweatshop-says-work-is-mental-torture/ar-AATRB8n\nhttps://futurism.com/facebook-content-mod-sweatshop\nhttps://www.bbc.co.uk/news/technology-46055595\nhttps://www.niemanlab.org/reading/inside-the-nairobi-based-company-facebook-has-hired-to-be-the-emergency-first-responders-of-social-media/\nhttps://techcabal.com/2022/02/14/facebook-fire-for-poor-treatment-of-african-content-moderators/\nhttps://winbuzzer.com/2022/02/14/report-exposes-facebooks-african-sweatshop-where-workers-get-1-50-per-hour-xcxwbn/\nhttps://www.benzinga.com/government/22/02/25623506/moderator-inside-facebooks-african-sweatshop-says-work-is-mental-torture\nhttps://www.niemanlab.org/reading/facebook-content-moderators-in-kenya-are-going-to-get-a-pay-bump-following-a-time-investigation/\nhttps://www.aljazeera.com/news/2023/4/20/court-rules-meta-can-be-sued-in-kenya-over-alleged-unlawful-redundancies\nRelated \ud83c\udf10\nFacebook Australia news, civil society blocks\nFacebook Cross-check\nPage info\nType: Issue\nPublished: March 2022\nLast updated: May 2023", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/zhihu-job-resignation-predictions", "content": "Zhihu job resignation prediction system sparks backlash\nOccurred: February 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA tool used by Zhihu, China's largest question-and-answer platform, to predict employee resignations sparked outrage. \nAn anonymous whistleblower at Zhihu reported that they were fired after their manager discovered their intention to quit, blaming the action on a the system\u2019s predictions. \nThe system concerned was Sangfor Technologies' 'Behavioral Perception System', which purportedly calculated employees\u2019 perceived resignation risk by tracking their browsing history, including visits to recruitment websites, and conversations with coworkers, and flagging people talking about 'bad treatment', 'no prospects', and 'low wages'.\nDespite Zhihu saying it was not using the system for monitoring employees, the incident triggered outrage amongst Chinese internet users, who criticised Zhihu and the technology developer for unreasonable surveillance and abuse of privacy. The system was also accused of being inaccurate and unreliable. \nSangfor removed information about its product from its website after news of the Zhihu firing hit the headlines. \nSystem \ud83e\udd16\nBehavioral Perception System\nOperator: Zhihu; Sina; China Everbright Bank Shenzhen; East China Normal University\nDeveloper: Sangfor Technologies\nCountry: China\nSector: Business/professional services\nPurpose: Predict employee resignations\nTechnology: Prediction algorithm\nIssue: Surveillance; Privacy; Accuracy/reliability; Appropriateness/need\nTransparency: Governance; Privacy; Complaints/appeals; Black box; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://pandaily.com/zhihu-denies-using-behavior-perception-system-to-monitor-employees/\nhttps://min.news/en/news/74e2da852e7989f59e5dc538d54263a5.html\nhttps://min.news/en/economy/a9cbaa4d0f1560e8e82c50b2f5f02f12.html\nhttps://www.airvers.com/zhihu-rumors-of-layoffs-lead-to-a-magical-system-it-can-monitor-employees-turnover-intention-and-only-need-40000-yuan-for-3-years/\nhttps://www.airvers.com/convinced-employee-monitoring-business/\nhttps://www.airvers.com/the-secret-of-resignation-monitoring-system-take-a-screenshot-of-employees-computer-screen-in-30-seconds/\nhttps://finance.sina.com.cn/tech/2022-02-16/doc-ikyamrna1053375.shtml\nhttps://mp.weixin.qq.com/s/VtckCtN5EwHxtGDBAZAUaQ\nhttps://new.qq.com/omn/20220212/20220212A07W5500.html\nhttps://finance.sina.com.cn/tech/2022-02-16/doc-ikyamrna1053375.shtml\nhttps://baijiahao.baidu.com/s?id=1724730759282942515&wfr=spider&for=pc\nRelated \ud83c\udf10\nEstee Lauder employee performance assessments\nXsolla employee monitoring, terminations\nPage info\nType: Incident\nPublished: March 2022", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ukraine-russia-bayraktar-tb2-drone-attacks", "content": "Ukraine uses Bayraktar TB2 drones to hit Russian targets\nOccurred: February 2022-\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nUkraine's air force used Turkish-made automated drones to hit Russian targets during the Russian invasion of the east European country, sparking concerns about the use of AI in military conflicts. \nThe Ukrainian air force confirmed it had struck Russian targets, including a military convoy near Kherson, using Bayraktar TB2 drones during the early days Russia's invasion of Ukraine, according to the Wall Street Journal.\nUkraine\u2019s air force chief Lt. Gen. Mykola Oleshchuk described the drones as 'life-giving'. \nHowever, human rights and non-governmental organisations called for a global ban on lethal autonomous weapons systems (aka 'killer robots' and 'slaughterbots').\nSystem \ud83e\udd16\nBayraktar TB2 Wikipedia profile\nOperator: Ukrainian Air Force\nDeveloper: Baykar Defence\nCountry: Ukraine; Russia\nSector: Govt - defence\nPurpose: Kill/maim/damage/destroy\nTechnology: Drone; Robotics\nIssue: Lethal autonomous weapons; Ethics\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.wsj.com/livecoverage/russia-ukraine-latest-news-2022-02-26/card/ukraine-says-it-uses-turkish-made-drones-to-hit-russian-targets-DrigGO7vkGfDzbBuncnA\nhttps://www.dailymail.co.uk/news/article-10558543/Ukraines-Air-Force-claims-launch-successful-drone-strikes-obliterating-Russian-convoys.html\nhttps://www.middleeasteye.net/news/russia-ukraine-war-turkey-drones-effective-deadly\nhttps://www.bloomberg.com/news/articles/2022-03-01/russia-meets-deadly-turkish-drones-once-more-in-ukraine-invasion\nhttps://time.com/6153197/ukraine-russia-turkish-drones-bayraktar/\nhttps://www.yahoo.com/now/ukraine-drone-strikes-reveal-russian-223151164.html\nhttps://www.thedefensepost.com/2021/10/27/ukraine-deploys-bayraktar-drone/\nhttps://www.forbes.com/sites/davidaxe/2021/12/17/to-hide-from-ukraines-drones-russian-troops-could-lay-smoke-screens/\nhttps://www.forbes.com/sites/davidaxe/2022/02/08/ukraines-got-20-tb-2-drones-it-might-not-matter-in-a-wider-war-with-russia\nhttps://ukdefencejournal.org.uk/ukraine-uses-bayraktar-tb2-drone-in-combat-for-first-time/\nhttps://www.thestar.com.my/news/world/2021/10/31/don039t-blame-us-for-ukraine039s-use-of-turkish-drones--turkish-fm\nhttps://twitter.com/KyivIndependent/status/1497859950256738309\nhttps://www.middleeasteye.net/news/russia-ukraine-war-turkish-drones-strike-troops-tb2\nhttps://www.themoscowtimes.com/2021/10/27/ukraine-destroys-pro-russian-artillery-in-its-first-use-of-turkish-drones-a75420\nhttps://fortune.com/2022/03/04/bayraktar-tb2-drone-ukraine-russia-war/\nRelated \ud83c\udf10\nRussia KUB-BLA 'suicide drone' attacks\nHouthi Abu Dhabi drone attack\nPage info\nType: Incident\nPublished: March 2022", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/kyiv-deepfake-influence-campaign", "content": "Russian 'Kyiv' deepfake influence campaign is dismantled\nOccurred: February 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA fake, AI-powered pro-Russian influence campaign attempted to spread anti-Ukrainian propaganda, sparking concerns about the use of AI to create and spread misinformation and disinformation.\nAccording to Meta, the so-called 'Kyiv' campaign took the form of dozens of fake profiles using AI-generated pictures, including of an aviation engineer, news editors, and scientific authors. \nPosing as independent news desks operating out of Kyiv, the fake profiles were used to publish content on Facebook, Instagram, Twitter, YouTube, Telegram, and Russian social networks Odnoklassniki and VK.\nThe profiles were taken down for violating Meta's coordinated inauthentic behaviour policy.\n\u2795 Ukrainian authorities also shut down a Russian bot farm in Lviv capable of deploying 18,000 fake accounts, and a smaller one that posted fake information about Ukraine on Telegram, WhatsApp and Viber.\nSystem \ud83e\udd16\nUnknown\n\nDocuments \ud83d\udcc3\nhttps://about.fb.com/news/2022/02/metas-ongoing-efforts-regarding-russias-invasion-of-ukraine/\nhttps://about.fb.com/news/tag/coordinated-inauthentic-behavior/\nOperator:    \nDeveloper:  \nCountry: Ukraine; Russia\nSector: Govt - defence\nPurpose: Moderate content\nTechnology: Content moderation system; Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning  \nIssue: Mis/disinformation; Ethics/values\nTransparency: Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://edition.cnn.com/2022/02/28/tech/meta-russia-ukraine-disinformation-network/index.html\nhttps://www.thedailybeast.com/ghostwriter-hackers-targeting-ukraines-army-blocked-by-facebook\nhttps://www.reuters.com/technology/facebook-owner-meta-says-ukraines-military-politicians-targeted-hacking-campaign-2022-02-28/\nhttps://au.finance.yahoo.com/news/facebook-takes-down-fake-accounts-boosting-russian-disinformation-in-ukraine-050028336.html\nhttps://news.bloomberglaw.com/privacy-and-data-security/facebook-curbs-pro-russia-disinformation-as-ukrainians-targeted\nRelated \ud83c\udf10\nPresident Zelenskyy deepfake 'surrender'\nRussian Ukraine disinformation bot farms\nPage info\nType: Incident\nPublished: March 2022", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/roblox-condo-nazi-sex-parties", "content": "Children attend Roblox Condo nazi sex parties\nOccurred: February 2022 \nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nChildren gamers under 13 years-old were found to have been meeting in virtual 'condos' on gaming platform Roblox to talk about and try and have sex, prompting concerns about the platform's safety.\nAccording to the BBC, scantily clad avatars inside the game were giving other users lap dances, while others hung out nude. One avatar was spotted wearing a Nazi uniform. \nA majority of the game's users are children, with even kids under the age of 13 technically allowed to play. \nRoblox responded by saying that offending condos are quickly shut down. But the BBC's report added to growing concerns about the safety of Roblox and other virtual reality-based metaverse platforms. Reports of sexual harassment have also plagued Meta.\nRoblox later published a blog post setting out how it supports and protects its developer and user communities.\nSystem \ud83e\udd16\nRoblox website\nRoblox Wikipedia profile\n\nDocuments \ud83d\udcc3\nhttps://blog.roblox.com/2022/02/supporting-protecting-roblox-developer-user-community/\nOperator: Roblox\nDeveloper: Roblox\nCountry: UK; Global\nSector: Media/entertainment/sports/arts\nPurpose: Manage system safety\nTechnology: Virtual reality; Safety management system\nIssue: Safety; Bias/discrimination - race\nTransparency: Governance; Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/technology-60314572\nhttps://futurism.com/experts-alarmed-roblox\nhttps://www.theweek.co.uk/news/world-news/955775/roblox-childrens-game-hosting-nazi-sex-parties\nhttps://www.bgr.in/gaming/roblox-kids-game-is-filled-with-sex-spaces-people-are-worried-for-their-kids-mental-health-1236113/\nhttps://www.thetimes.co.uk/article/nazi-sex-parties-hosted-on-childrens-game-roblox-s9vktzxx0\nhttps://www.dailymail.co.uk/news/article-10514185/Childrens-online-game-platform-Roblox-infested-sexually-explicit-games.html\nhttps://www.stuff.co.nz/entertainment/games/127789184/bbc-investigation-finds-virtual-sex-parties-happening-in-childrens-computer-game-roblox\nhttps://www.eurogamer.net/articles/2022-02-15-roblox-commits-to-ensuring-a-positive-and-safe-experience-for-its-users\nhttps://www.bloomberg.com/news/articles/2022-02-15/roblox-tumbles-as-results-show-growth-slowing-after-pandemic\nhttps://www.fastcompany.com/90539906/sex-lies-and-video-games-inside-roblox-war-on-porn\nhttps://www.vice.com/en/article/epdzxk/roblox-goes-public-says-child-pornography-is-a-risk-to-its-business\nhttps://www.foxbusiness.com/technology/roblox-discord-teen-gamers-inappropriate\nRelated \ud83c\udf10\nVRChat virtual strip clubs, child grooming\nMeta Horizon Worlds virtual groping, rape\nPage info\nType: Incident\nPublished: February 2022", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/twitter-ukraine-osint-account-suspensions", "content": "Twitter 'mistakenly' suspends Ukraine OSINT accounts before Russian invasion\nOccurred: February 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nTwitter mistakenly suspended several accounts that had been sharing footage and intelligence on Russian military movements, prompting concerns about the platform's content moderation policies and practices. \nTwitter said it had mistakenly suspended around a dozen accounts that had been posting about Russian military movements ater Ukraine had declared a state of emergency before Russia's invasion. The platform said the action was not due to a coordinated bot campaign or mass reporting of the accounts by other users, according to CNBC.\nIn a statement, a Twitter spokesman said 'We\u2019ve been proactively monitoring for emerging narratives that are violative of our policies, and, in this instance, we took enforcement action on a number of accounts in error. We\u2019re expeditiously reviewing these actions and have already proactively reinstated access to a number of affected accounts.'\nYoel Roth, Twitter head of site integrity, tweeted the firm was closely investigating what had happened but said mass reporting was not a factor. 'We do not trigger automated enforcements based on report volume, ever, exactly because of how easily gamed that would be.' \nSystem \ud83e\udd16\nTwitter website\nTwitter Wikipedia profile\nOperator: Twitter\nDeveloper: Twitter\nCountry: Ukraine\nSector: Govt - defence\nPurpose: Moderate content\nTechnology: Content moderation system\nIssue: Mis/disinformation\nTransparency: Governance; Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.cnbc.com/2022/02/23/twitter-mistakenly-took-down-accounts-posting-russian-military.html\nhttps://www.ft.com/content/5b23938b-49b8-426f-97af-640ba40c9922\nhttps://www.businessinsider.com/twitter-mistakenly-blocks-accounts-sharing-video-from-ukraine-2022-2\nhttps://siliconangle.com/2022/02/23/twitter-admits-accidentally-taking-posts-showing-russia-ukraine-conflict/\nhttps://www.theverge.com/2022/2/23/22947769/twitter-osint-russia-ukraine-invasion-suspended-error/\nhttps://www.msn.com/en-gb/news/world/twitter-removes-accounts-tracking-russian-troops-as-putin-launches-ukraine-war/ar-AAUeMa8\nhttps://techcrunch.com/2022/02/23/twitter-russia-ukraine-osint-accounts-suspended/\nRelated \ud83c\udf10\nRussian Ukraine disinformation bot farms\n'Kyiv' deepfake influence campaign\nPage info\nType: Incident\nPublished: February 2022", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/the-book-of-veles", "content": "Author maiBook of Veles photos and text manipulated\nOccurred: October 2021 \nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA book by celebrated Magnum photographer Jonas Bendiksen documenting life in Veles, the 'fake news' capital of North Macedonia, turned out to be a work of disinformation itself.\nPurporting to investigate disinformation production activities in a rural town notorious for its disinformation and misinformation factories, Bendiksen's The Book of Veles had been positively reviewed and screened at film festivals. \nHowever, with Veles' disinformation industry in decline, Bendiksen had manipulated his photos using 3D software to make them more realistic. He also used a large language AI model to generate the accompanying commentary. \nThe author confessed his manipulation after a Veles journalist took to Twitter to expose his fraudulent activities.\nSystem \ud83e\udd16\nUnknown\n\nDocuments \ud83d\udcc3\nJonas Bendikson. The Book of Veles\nMagnum Photos. The Book of Veles\nOperator: Magnum Photos\nDeveloper: Jonas Bendiksen\nCountry: N Macedonia\nSector: Media/entertainment/sports/arts\nPurpose: Expose mis/disinformation\nTechnology: NLP/text analysis\nIssue: Mis/disinformation; Ethics\nTransparency: Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.wired.com/story/true-story-bogus-photos-people-fake-news/\nhttps://www.codastory.com/authoritarian-tech/jonas-bendiksen-book-of-veles/\nhttps://www.washingtonpost.com/photography/2021/10/15/jonas-bendiksen-book-veles/\nhttps://www.niemanlab.org/2021/12/manipulated-media-will-fool-you-yes-you/\nhttps://www.psychologytoday.com/us/blog/misinformation-desk/202110/the-book-veles-disinformation-north-macedonia\nhttps://collectordaily.com/jonas-bendiksen-the-book-of-veles/\nhttps://www.amateurphotographer.co.uk/book_reviews/fake-news-how-jonas-bendiksen-hoodwinked-the-photographic-community-with-the-book-of-veles-160472\nhttps://www.cbc.ca/radio/thecurrent/the-current-for-oct-20-2021-1.6217855/this-photojournalist-faked-an-entire-book-to-highlight-how-hard-it-is-to-spot-misinformation-1.6218251\nRelated \ud83c\udf10\nCambodia torture victims' photo manipulation\nAmnesty fake Colombia national strike images\nPage info\nType: Incident\nPublished: February 2022", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/uk-visa-foreign-language-test-cheating", "content": "Inaccurate ETS test finds most English language test students 'cheated'\nOccurred: February 2014\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn automated test intended to flag cheating by foreign students doing English language tests to quality for UK visas proved highly inaccurate, resulting in political controversy.\nETS, the company that developed and run the language tests, used voice recognition software to detect whether the same voices turned up on multiple test recordings, indicating the same proxy had faked exams for several people. According to the final results, 97 percent of 58,000 Test of English for International Communication (TOEIC) tests taken between 2011 and 2014 were judged suspicious - a figure widely regarded as implausible.\nBBC Panorama first uncovered 'systematic' fraud in the UK's student visa system in 2014, with undercover filming showing entire rooms of candidates having TOIEC tests faked for them. Panorama researchers were also sold fake bank details to demonstrate they had sufficient funds to stay in the UK.  The government-approved system was suspended and thousands of people deported without appeal\nIn 2019, a National Audit Office investigation concluded that the Home Office's course of action against TOEIC students 'carried with it the possibility that a proportion of those affected might have been branded as cheats, lost their course fees, and been removed from the UK without being guilty of cheating or adequate opportunity to clear their names.'\nA 2022 BBC Newsnight investigation found that the UK Home Office continued to remove people discovered to be cheating in English language tests, despite evidence of poor conduct and inaccurate data at ETS.\nSystem \ud83e\udd16\nEducational Testing Service website\nEducational Testing Service Wikipedia profile\nOperator: UK Home Office\nDeveloper: Educational Testing Service (ETS)\nCountry: UK\nSector: Govt - immigration\nPurpose: Detect cheating\nTechnology: Voice recognition\nIssue: Accuracy/reliability; Effectiveness/value\nTransparency: Governance; Complaints/appeals\nInvestigations, assessments, audits \ud83e\uddd0\nBBC (2014). Immigration Undercover: The Student Visa Scandal\nNational Audit Office (2019). Investigation into the response to cheating in English language tests\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/uk-26024375\nhttps://www.bbc.co.uk/news/uk-politics-27993775\nhttps://www.bbc.co.uk/iplayer/episode/p0bnds5y/newsnight-the-english-test-scandal\nhttps://www.theguardian.com/uk-news/2014/feb/10/student-visa-tests-suspended-fraud\nhttps://www.independent.co.uk/news/uk/politics/theresa-may-faces-parliamentary-investigation-over-flimsy-basis-for-student-deportations-a6948796.html\nhttps://www.independent.co.uk/news/uk/politics/home-office-mistakenly-deported-thousands-foreign-students-cheating-language-tests-theresa-may-windrush-a8331906.html\nhttps://www.independent.co.uk/news/uk/home-news/foreign-students-cheating-scandal-english-language-tests-home-office-sajid-javid-a8975886.html\nhttps://www.ft.com/content/2ae9b7d2-4d0c-11e8-8a8e-22951a2d8493\nhttps://edition.cnn.com/2018/07/10/uk/report-uk-foreign-students-cheating-allegations-intl/index.html\nhttps://www.thesun.co.uk/news/9143608/foreign-students-leave-uk-english-tests-cheating/\nRelated \ud83c\udf10\nSouth Korea immigration facial data sharing\nNew Zealand immigration overstayer predictions\nPage info\nType: Incident\nPublished: February 2022\nLast updated: February 2024", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/verus-prison-inmate-call-monitoring", "content": "US prison inmate AI call monitoring plan raises privacy, bias concerns\nOccurred: February 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nUS prisons rolled out out an AI-based system that scans inmates' phone calls, leading rights groups to voice concerns about privacy, surveillance and discrimination.\nIntended to keep prisons and jails safe, Leo Technologies' Verus system uses Amazon speech-to-text technology based on keywords to identify and transcribe prisoners' phone calls. \nHowever, groups such as Access Now, the Electronic Frontier Foundation and Worth Rises expressed concerns that it would abuse the privacy rights of family members and others with whom prisoners are talking. Another issue is discrimination. Research shows voice-to-text tools are significantly more inaccurate for Black voices. And a higher percentage of Black people are detained or jailed in the US.\nSurveillance was also considered a concern, eith Reuters quoting documents that revealed instances of prisons and detention centres using Verus for purposes beyond maintaining safety, for example identifying conversations that would help them win court cases, and monitoring COVID-19. \nThe Intercept had earlier reported that prison officials in several US states had been using Verus to scan inmate calls for COVID-19 mentions and to flag complaints about the quality of prison response to the pandemic.\nSystem \ud83e\udd16\nVerus website\nOperator: Suffolk County Sheriff's Office; Multiple\nDeveloper: LEO Technologies\nCountry: USA\nSector: Govt - justice\nPurpose: Improve safety\nTechnology: Speech-to-text AI\nIssue: Bias/discrimination - race, ethnicity; Privacy; Surveillance\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nStanford HAI (2023). 2023 AI Index Report - 3.2 AI Incidents (pdf)\nSTOP (2022). 55 Civil Rights Groups Demand DOJ, NY Investigate AI Audio Surveillance In Prisons, Jails\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://news.trust.org/item/20220210152812-a16ki/\nhttps://news.trust.org/item/20211122213228-wxsz9\nhttps://news.trust.org/item/20210809090018-c8r11\nhttps://abcnews.go.com/Technology/us-prisons-jails-ai-mass-monitor-millions-inmate/story?id=66370244\nhttps://theintercept.com/2020/04/21/prisons-inmates-coronavirus-monitoring-surveillance-verus/\nhttps://www.inputmag.com/tech/prisons-are-using-amazon-transcribe-ai-to-monitor-inmate-phone-calls\nhttps://www.techdirt.com/articles/20211206/10235648061/ai-surveillance-prison-calls-scooping-up-millions-conversations-producing-little-actionable-info.shtml\nhttps://gcn.com/public-safety/2021/08/ai-on-the-line-monitoring-prisoners-phone-calls-for-criminal-intent/316135/\nhttps://crimeandjusticenews.asu.edu/news/ai-being-used-monitor-prison-phone-calls\nhttps://www.post-gazette.com/opinion/editorials/2020/05/08/Pandemic-opportunism-Prison-call-monitoring-system-exploits-virus-panic/stories/202004290006\nhttps://thecrimereport.org/2020/04/23/officials-scan-inmate-phone-calls-for-virus-mentions/\nRelated \ud83c\udf10\nGoGuardian Beacon student suicide prevention monitoring\nMainz police Luca COVID-19 tracing app data theft\nPage info\nType: Issue\nPublished: February 2022", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/dwp-disability-benefits-fraud-algorithm", "content": "UK DWP sued over 'unfair' disability benefits fraud detection algorithm \nOccurred: February 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe UK Department for Work and Pensions (DWP) faced legal action to reveal details of an 'unfair' 'General Matching Service' algorithm it used to identify benefit fraud.\nThe Greater Manchester Coalition of Disabled People (GMCDP) demanded the DWP reveal how it uses 'AI, algorithmic technology and other forms of automation' when investigating benefit fraud, specifically details of a data matching algorithm that the department is known to use.\nThe GMCDP alleged that disabled people were being unfairly targeted, are subject to 'essential' cash cuts and are given little or no information about why they are being investigated. \nThe legal letter was the second sent by the GMCDP on the subject. The first was issued in December 2021.\nEarly 2021, Privacy International accused the DWP of using 'excessive surveillance techniques' to investigate possible benefit fraudsters.\nSystem \ud83e\udd16\nGeneral Matching Service algorithm\nDepartment of Work and Pensions website\nDepartment of Work and Pensions Wikipedia profile\nOperator: Department for Work and Pensions (DWP)\nDeveloper: Department for Work and Pensions (DWP)\nCountry: UK\nSector: Govt - welfare\nPurpose: Detect fraud\nTechnology: Data matching algorithm; Machine learning\nIssue: Bias/discrimination - disability\nTransparency: Governance; Complaints/appeals; Black box\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nFoxglove (2021). Secret algorithm targets disabled people unfairly for benefit probes \u2013 cutting off life-saving cash and trapping them in call centre hell\nFoxglove (2021). Help us fight the DWP\u2019s secret benefits algorithm\nResearch, advocacy \ud83e\uddee\nPrivacy International (2021). Shedding light on the DWP Part 1 - We read the UK welfare agency\u2019s 995-page guide on conducting surveillance and here are the scariest bits\nPrivacy International (2021). Shedding light on the DWP Part 2 - A Long Day's Journey Towards Transparency\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theguardian.com/society/2021/nov/21/dwp-urged-to-reveal-algorithm-that-targets-disabled-for-benefit\nhttps://www.lawgazette.co.uk/practice-points/algorithmic-transparency-is-not-a-game/5114772.article\nhttps://www.disabilitynewsservice.com/dwp-bosses-quizzed-by-mps-over-secret-benefit-fraud-algorithm/\nhttps://www.disabilitynewsservice.com/legal-letter-asks-dwp-for-information-on-discriminatory-secret-algorithm/\nhttps://www.theoldhamtimes.co.uk/news/19761688.disabled-forced-gruelling-invasive-hamster-wheel-benefits-appeals-dwp/\nhttps://www.manchestereveningnews.co.uk/news/greater-manchester-news/disabled-people-being-forced-gruelling-22349188\nhttps://www.politico.eu/newsletter/ai-decoded/unescos-ai-ethics-framework-some-eu-countries-want-ai-bans-for-tech-companies-uk-benefits-algorithm-under-fire-2/\nhttps://www.messengernewspapers.co.uk/news/19766853.disabled-forced-onto-hamster-wheel-benefits-appeals-dwp/\nhttps://www.vice.com/en/article/y3g9n5/how-the-government-spies-on-welfare-claimants\nhttps://www.mirror.co.uk/news/politics/dwp-faces-legal-action-reveal-26199571\nhttps://www.cambridge-news.co.uk/news/uk-world-news/dwp-faces-legal-action-disclose-23062282\nhttps://www.theguardian.com/society/2021/feb/14/dwp-excessive-surveillance-on-suspected-fraudsters-privacy-international\nRelated \ud83c\udf10\nNetherlands childcare benefits fraud assessments automation\nTrelleborg welfare management automation\nPage info\nType: Incident\nPublished: February 2022\nLast updated: March 2024", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-chemical-food-preservative-suicides", "content": "Amazon sales of suicide chemical compound is questioned\nOccurred: February 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAmazon faced pressure from a bipartisan set of US federal lawmakers to stop sales of a food preservative containing a chemical compound that is being used as a poison in suicides.\n\nThe internet company came under further pressure after a New York Times investigation identified 10 people who had killed themselves after buying a chemical compound through the site in the past two years, and over 300 members of a suicide website who had publicly said they intended to kill themselves using it.\n\nWhile the compound is sold legally in the US and many other countries, Etsy, eBay and some other internet commerce companies have banned its sale on their platforms. Some firms have also stopped making the products containing the compound.\n\nFamilies of suicide victims have been warning Amazon of the danger of selling the compound since it became clear it was being used in suicides in 2017. Nonetheless, it has resisted limiting or stopping its sale. \n\nThe NYT also reports that Amazon's algorithm recommends other items people planning suicide typically purchase. Furthermore, Lori Trahan, a prominent Democrat lawmaker, suggested that reviews warning others about the product have been removed on Amazon.\nSystem \ud83e\udd16\nAmazon recommendation algorithm\nOperator: Amazon\nDeveloper: Amazon\nCountry: USA; India\nSector: Retail\nPurpose: Recommend products\nTechnology: Recommendation algorithm\nIssue: Safety\nTransparency: Governance; Complaints/appeals; Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.nytimes.com/interactive/2021/12/09/us/where-the-despairing-log-on.html\nhttps://www.nytimes.com/2022/02/04/technology/amazon-suicide-poison-preservative.html\nhttps://www.yahoo.com/news/lawmakers-press-amazon-sales-chemical-152416937.html\nhttps://gizmodo.com/amazon-sales-chemical-compound-suicide-1848485532\nhttps://www.businessinsider.com/us-lawmakers-probe-amazon-for-selling-chemical-compound-used-suicide-2022-2\nhttps://www.cnet.com/tech/services-and-software/amazon-challenged-by-lawmakers-over-listings-for-preservative-tied-to-suicides/\nhttps://www.techtimes.com/articles/271473/20220205/amazon-questioned-third-party-sellers-listing-preservatives-tied-self-harm.htm\nhttps://www.inputmag.com/culture/amazon-selling-products-suicide-lawmakers\nhttps://www.reddit.com/r/technology/comments/skzkep/lawmakers_press_amazon_on_sales_of_chemical_used/\nRelated \ud83c\udf10\nAmazon Flex algorithm delivery driver firings\nAmazon algorithmic vaccine misinformation\nPage info\nType: Incident\nPublished: February 2022", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tinder-plus-pricing-algorithm-fairness-discrimination", "content": "Tinder pricing algorithm disciminates against older, gay users\nOccurred: August 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nTinder charged users over 30 and gay and lesbian aged 18-29 more than others in multiple countries, generating accusations of systemic age and sexual discrimination. \nA February 2022 study by Consumer Reports found users aged 30+ were being charged over 65 percent more than others in New Zealand, Brazil, India, the Netherlands, South Korea and the US. The findings were supported by research by Which?, which discovered that the dating app has been increasing prices for gay and lesbian users aged 18-29 in the UK.\nAn August 2020 study by CHOICE had found users in Australia were being charged up to five times as much as others, with older people charged more.\nTinder uses personalised algorithmic pricing for every user, based on an estimation of what they are willing and able to pay. However it refuses to reveal how its pricing system works. \nFacing public petitions and multiple legal threats, Tinder says it stopped the practice of charging users different prices based on how old they are in the US and Australia. \nTinder\u2019s parent company Match Group revealed in an earnings call that it would stop the practice in remaining markets.\nSystem \ud83e\udd16\nTinder pricing algorithm\nDocuments \ud83d\udcc3\nAn update on Tinder's Premium features\nOperator: Match Group/Tinder\nDeveloper: Match Group/Tinder\nCountry: New Zealand; Brazil; India; Netherlands; South Korea; USA\nSector: Media/entertainment/sports/arts\nPurpose: Determine pricing\nTechnology: Pricing algorithm\nIssue: Fairness; Bias/discrimination - age, LGBTQ\nTransparency: Governance; Black box\nResearch, advocacy \ud83e\uddee\nhttps://www.consumersinternational.org/media/369070/personalized_pricing.pdf\nhttps://www.consumerreports.org/consumer-protection/tinder-is-phasing-out-higher-prices-for-users-ages-29-plus-a3881901881/\nhttps://foundation.mozilla.org/en/campaigns/tell-tinder-its-time-for-meaningful-transparency-into-your-pricing-algorithms/\nhttps://www.consumer.org.nz/articles/over-thirty-you-could-be-paying-more-to-swipe-right\nhttps://www.which.co.uk/news/2022/01/tinders-unfair-pricing-algorithm-exposed/\nhttps://www.choice.com.au/consumers-and-data/data-collection-and-use/how-your-data-is-used/articles/tinder-plus-costs-more-if-youre-older\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.scoop.co.nz/stories/BU2202/S00158/over-thirty-you-could-be-paying-more-to-swipe-right.htm\nhttps://www.msn.com/en-us/money/other/tinder-working-to-end-age-based-pricing-for-premium-dating-content/ar-AATCvVc\nhttps://www.choice.com.au/consumers-and-data/data-collection-and-use/how-your-data-is-used/articles/consumers-international-tinder-investigation\nhttps://www.politico.eu/article/uk-consumer-group-tinders-pricing-algorithm-discriminates-against-gay-users-and-over-30s/\nhttps://news.yahoo.com/tinder-charging-people-wildly-different-180510167.html\nhttps://www.msn.com/en-gb/news/world/tinder-charges-gay-and-lesbian-users-more-for-tinder-plus-which-claims/ar-AAT1V0X\nRelated \ud83c\udf10\nAirbnb Smart Pricing algorithm\nYieldStar automated rent-setting\nPage info\nType: Incident\nPublished: February 2022", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/honolulu-homeless-robot-temperature-tests", "content": "Honolulu homeless robot temperature testing slammed as 'inhuman'\nOccurred: January 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nHonolulu's police department use of a robotic dog to take the temperatures of homeless people sparked controversy, with critics describing the practice as 'inhuman' and 'dystopian'. \nUsing a public records request, VICE discovered that the Honolulu Police Department (HPD) deployed a Boston Dynamics Spot robot dog on regular 'temperature' duty of unhoused people living in encampments. The HPD spent USD 150,045 in federal funds earmarked for COVID-19 pandemic relief to acquire the Boston Dynamics Spot robot, according to city spending data first uncovered by Honolulu Civil Beat.\nHonolulu PD officer Mike Lambert described it as 'the most innovative program in the nation' during a city council meeting, while HPD Deputy Director described the robot as 'more than a \u2018thermometer'' in an email interview with Civil Beat. In the same report, another officer described the purchase as 'Toys, toys, toys'.\nHowever, civil rights groups condemned the robot's use as an unacceptable violation of human dignity, the de facto normalisation of dehumanising practices, a violation of privacy due to the lack of consent provided, another example of the ongoing militarisation of policing, and a potential gateway to expanded surveillance and policing of marginalised communities.\nBoston Dynamics' acceptable use guidelines prohibit Spot\u2019s weaponisation or anything that would violate privacy or civil rights laws.\nSystem \ud83e\udd16\nSpot robot website\n\nDocuments \ud83d\udcc3\nBoston Dynamics. Spot Terms and Conditions of Sale (pdf)\nBoston Dynamics Ethical Principles\nOperator: Honolulu Police Department\nDeveloper: Hyundai Motor Group/Boston Dynamics\nCountry: USA\nSector: Govt - police\nPurpose: Detect body temperature\nTechnology: Robotics\nIssue: Bias/discrimination; Human/civil rights; Privacy; Surveillance\nTransparency: Governance\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nHonolulu City Council Public Safety Committee meeting video\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.vice.com/en/article/wx5xym/honolulu-police-used-a-robot-dog-to-patrol-a-homeless-encampment\nhttps://www.vice.com/en/article/v7dz7b/police-outsourcing-human-interaction-with-homeless-people-to-boston-dynamics-robot-dog\nhttps://www.civilbeat.org/2021/01/honolulu-police-spent-150000-in-cares-funds-on-a-robot-dog/\nhttps://www.hawaiinewsnow.com/2021/08/28/hpd-defends-use-pricey-robot-dog-taking-temperatures-homeless-program/\nhttps://www.dailymail.co.uk/news/article-10448379/Cops-Honolulu-use-robot-dog-temperature-homeless-people.html\nhttps://www.ubergizmo.com/2022/01/boston-dynamics-dog-measure-homeless-temperature/\nhttps://www.euronews.com/next/2021/08/06/a-useful-tool-or-dehumanising-robot-police-dog-that-scans-homeless-people-sparks-debate\nhttps://metro.co.uk/2022/01/29/robot-cop-dog-taking-homeless-peoples-temperature-amid-covid-16010881/\nhttps://www.dailywire.com/news/hawaii-using-robot-dog-to-patrol-homeless-community-for-signs-of-covid-19\nhttps://boingboing.net/2022/02/07/robot-dogs-to-patrol-u-s-border.html\nhttps://gizmodo.com/honolulu-police-department-used-150-000-in-cares-funds-1847092760\nhttps://www.inputmag.com/culture/honolulu-police-used-a-robot-dog-to-police-a-homeless-plague-camp\nRelated \ud83c\udf10\nNew York police 'digidog'\nSingapore Xavier patrol robots\nPage info\nType: Incident\nPublished: January 2022\nLast updated: June 2024", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tesla-phantom-braking", "content": "Tesla owners reports multiple instances of phantom braking\nOccurred: February 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nTesla owners using the car maker's automated Full-self driving system complained about an automatic braking issue, with cars suddenly slamming the brakes at high speeds and nearly causing crashes.\nAccording to the Washington Post, the issue could be traced to Tesla's decision to stop using radar sensors in new vehicles in order to move a purely camera-based system known as Tesla Vision, and to an update to its Full-Self Driving beta programme.\nThe carmaker had to recall the update in October 2021 over false positives to its automatic emergency-braking system, acknowledging that these had been triggered by the software update.\nOn February 16, 2021, the US National Highway Traffic Safety Administration (NHTSA) had announced an investigation into Tesla's sudden braking issue, covering an estimated 416,000 vehicles.\nTesla was recently forced to recall and disable a rolling stop function in 53,822 Model S, X, 3 and Y vehicles equipped with FSD. \nSystem \ud83e\udd16\nTesla Vision \nTesla Autopilot, Full-self Driving\nOperator: \nDeveloper: Tesla\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Driver assistance system; Self-driving system; Computer vision\nIssue: Safety\nTransparency: Black box\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nNHTSA (2022). ODI Resume (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.washingtonpost.com/technology/2022/02/02/tesla-phantom-braking/\nhttps://www.reddit.com/r/teslamotors/comments/qeu10x/known_fsd_beta_103_taccautosteer_bug/\nhttps://www.theverge.com/2022/2/2/22914236/tesla-phantom-braking-complaints-nhtsa-fsd\nhttps://www.msn.com/en-us/autos/news/tesla-e2-80-99s-e2-80-98full-self-driving-e2-80-99-phantom-braking-problem-is-getting-worse/ar-AATpOiC\nhttps://www.dailymail.co.uk/sciencetech/article-10470633/U-S-safety-regulator-reviews-Tesla-driver-complaints-false-braking.html\nhttps://www.bloomberg.com/news/articles/2022-02-02/tesla-under-federal-scrutiny-for-phantom-braking-complaints\nhttps://jalopnik.com/tesla-has-a-serious-phantom-braking-issue-on-its-hand-1848469637\nhttps://www.cnbc.com/2021/10/25/tesla-rolled-back-fsd-beta-v-10point3-and-reissued-10point3point1-update.html\nhttps://uk.pcmag.com/cars-auto/138506/tesla-drivers-complain-of-phantom-braking\nhttps://electrek.co/2022/02/02/tesla-tsla-falls-attention-autopilot-serious-phantom-braking-issue/\nhttps://apnews.com/article/technology-business-lifestyle-13dcbfd79de79cb1f4358c3f716e64ae\nRelated \ud83c\udf10\nTesla FSD 'Assertive' Mode\nTesla Smart Summon private jet crash\nPage info\nType: Incident\nPublished: February 2022", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/crisis-text-line-data-sharing", "content": "Crisis Text Line shares users' mental health data with AI company\nOccurred: January 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nUS-based mental health non-profit Crisis Text Line (CLT) shared mental health users' confidential data with an AI customer service company, sparking controversy.\nPolitico discovered customer service company Loris.ai had been using CLT data and insights to develop, market and sell customer service optimisation software, prompting concerns about the non-profit's governance, ethics and transparency, and the commercial nature of the two organisations' relationship.\nVolunteers had earlier expressed concerns about CLT's handling of mental health conversations data, with one volunteer starting a Change.org petition pushing CTL \u201cto reform its data ethics.\u201d It also transpired that CLT was a shareholder in Loris.ai and, according to Politico, at one point shared a CEO with the company. \nUnder pressure from politicians, regulators, privacy experts and mental health practitioners, CLT ended the practice of sharing conversation data with Loris.\nSystem \ud83e\udd16\nCrisis Text Line website\nDocuments \ud83d\udcc3\nCrisis Text Line (2022). On Mental Health, Data, and Why Your Privacy Matters\nCrisis Text Line (2022). An Update on Data Privacy, Our Community and Our Service\nDanah Boyd (2022). Crisis Text Line, from my perspective\nOperator: Crisis Text Line\nDeveloper: Crisis Text Line\nCountry: USA\nSector: NGO/non-profit/social enterprise\nPurpose: Provide mental health support\nTechnology: Chatbot; NLP/text analysis\nIssue: Privacy; Confidentiality; Security; Ethics\nTransparency: Governance; Marketing\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nEPIC (2022). EPIC\u2019s Response to Reports of Crisis Text Line Data Policies\nFCC Commissioner Brendan Carr letter\nResearch, advocacy \ud83e\uddee\nReform Crisis Textline campaign website\nChange.org petition (2022). Ask Crisis Text Line to Reform Its Data Ethics\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.politico.com/news/2022/01/28/suicide-hotline-silicon-valley-privacy-debates-00002617\nhttps://www.politico.com/news/2022/01/31/crisis-text-line-ends-data-sharing-00004001\nhttps://www.theverge.com/2022/1/31/22906979/crisis-text-line-loris-ai-epic-privacy-mental-health\nhttps://www.komando.com/security-privacy/suicide-hotline-caught-selling-caller-data/824248/\nhttps://www.protocol.com/bulletins/crisis-text-line-lorisai\nhttps://www.popsci.com/technology/crisis-text-line-stops-sharing-data-loris-ai/\nhttps://mashable.com/article/crisis-text-line-loris-ai\nRelated \ud83c\udf10\nTamoco location data sharing\nGoogle Deepmind, Royal Free hospital data sharing\nPage info\nType: Incident\nPublished: February 2022", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/lauren-book-deepfake-extortion", "content": "Teenager attempts to extort Lauren Book using deepfake nude photos\nOccurred: December 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nFlorida lawmaker Lauren Book was subjected to an extortion attempt using deepfake nude photos, prompting fears about the use of deepfakes and AI misinformation and disinformation in politics.\nSouth Florida teenager Jeremy Kamperveen attempted to extort Book by demanding USD 5,000 in exchange for not releasing sexually explicit photographs of her showing 'female genitalia and the portrayal of a sexual act' to Fox News.\nStolen graphic images of Book and her husband had reputedy been circulating online since 2020. \nKamperveen pleaded no contest to charges of extortion and cyberstalking in June 2022 and was later sentenced to one year and one day in prison, followed by 10 years of probation.\n\u2795 In 2022, Book responded by introducing legislation would strengthen Florida\u2019s revenge porn law by making it a felony to buy, sell or trade stolen sexually explicit images from someone\u2019s phone or other digital devices. It also made disseminating altered or created sexually explicit images a felony. Florida's Sexually Related Offense law was signed by Governor Ron DeSantis on June 27, 2022 and took effect on October 1, 2022.\nSystem \ud83e\udd16\nUnknown\nOperator: Jeremy Kampervee\nDeveloper: Jeremy Kampervee\nCountry: USA\nSector: Politics\nPurpose: Extortion\nTechnology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning \nIssue: Privacy; Ethics\nTransparency: Governance; Privacy\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nhttps://www.flsenate.gov/Session/Bill/2022/1798/\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://apnews.com/article/arrests-florida-73f7dba8089422a079e7e1126decc419\nhttps://floridianpress.com/2021/12/plot-to-extort-florida-state-sen-lauren-book-leads-to-arrest/\nhttps://people.com/politics/inside-the-case-of-the-florida-lawmaker-who-realized-deepfake-nudes-online/\nhttps://people.com/politics/19-year-old-arrested-for-extorting-cyberstalking-state-sen-lauren-book/\nhttps://miami.cbslocal.com/2021/12/06/plantation-teen-charged-with-extorting-florida-senator-lauren-book-with-explicit-photos/\nhttps://www.miamiherald.com/news/politics-government/state-politics/article256293082.html\nhttps://www.thesun.co.uk/news/17437532/senator-lauren-book-fighting-back-nude-photos/\nhttps://www.independent.co.uk/news/world/americas/lauren-book-nude-photos-porn-florida-b2000715.html\nhttps://www.foxnews.com/politics/florida-senator-extorted-nude-photos-looks-sex-crime\nhttps://www.dailymail.co.uk/ushome/article-10439459/Florida-senator-fights-nude-images-stolen-her.html\nhttps://www.nbcnews.com/news/us-news/florida-sen-lauren-book-fights-back-nude-images-stolen-rcna13492\nhttps://www.local10.com/news/local/2022/06/15/broward-man-accused-of-trying-to-extort-state-sen-lauren-book-takes-plea-deal/\nRelated \ud83c\udf10\nPresident Zelenskyy deepfake surrender\nDeepsukebe nonconcensual nudification\nPage info\nType: Incident\nPublished: January 2022\nLast updated: January 2023", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/houthi-abu-dhabi-drone-attack", "content": "Houthis attack Abu Dhabi oil depot, airport with kamikaze drones\nOccurred: January 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nYemin Houthi rebels used multiple weapon-laden drones to attack Abu Dhabi's main airport and a petrol storage facility, raising concerns about the use of lethal autonomous weapons.\nOn January 17, 2022, 'a large number of' drones struck three oil refueling vehicles at an Abu Dhabi National Oil Company oil refinery, while a simultaeous drone attack hit an extension to Abu Dhabi's International Airport. Three people were killed after the fuel trucks exploded, and six were injured.\nThe UAE\u2019s Ministry of Interior responded by announcing a complete ban on the use of recreational drones across the country, with anyone caught violating the ban facing up to five years in prison and a minimum fine of Dh100,000.\nThe attack was seen to raise concerns about the misuse of drones, and more generally about the nature and use of lethal autonomous and semi-autonomous weapons. \nSystem \ud83e\udd16\nUnknown\nDocuments \ud83d\udcc3\nhttps://en.wikipedia.org/wiki/2022_Abu_Dhabi_attack\nhttps://twitter.com/UAE_PP/status/1486285830301925377\nhttps://www.moi.gov.ae/en/media.center/News/012222n01.aspx\nOperator: Ansar Allah\nDeveloper: Ansar Allah\nCountry: UAE - Abu Dhabi; Yemen\nSector: Govt - energy; Govt - transport\nPurpose: Kill/maim/damage/destroy\nTechnology: Drone\nIssue: Lethal autonomous weapons; Ethics; Dual/multi-use\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://apnews.com/article/business-dubai-united-arab-emirates-abu-dhabi-yemen-8bdefdf900ce46a6fd6c7bc685bf838a \nhttps://www.reuters.com/world/middle-east/uae-grounds-most-private-drones-light-aircraft-month-after-houthi-attack-2022-01-22/\nhttps://www.dailymail.co.uk/news/article-10410249/Abu-Dhabi-attacked-drones-launched-Iran-backed-Houthi-rebels.html\nhttps://www.aljazeera.com/news/2022/1/18/houthi-drone-attack-exposes-uae-vulnerabilities-in-region\nhttps://www.jns.org/houthis-long-range-drone-attack-on-emirates-is-wake-up-call-for-israel/\nhttps://www.cbsnews.com/news/uae-abu-dhabi-suspected-drone-attack-oil-tankers-yemen-houthis-iran/\nhttps://www.ctvnews.ca/world/satellite-photos-show-aftermath-of-abu-dhabi-oil-site-attack-1.5744103\nhttps://gulfnews.com/uae/crime/uae-up-to-five-years-imprisonment-dh100000-fine-for-flying-drones-1.85222414\nhttps://www.theverge.com/2022/1/24/22898614/united-arab-emirates-uae-ban-recreational-drone-attack\nhttps://futurism.com/the-byte/uae-bans-drones-attacks \nRelated \ud83c\udf10\nUkraine Bayraktar TB2 drone attacks\nRussia KUB-BLA 'suicide drone' attacks\nPage info\nType: Incident\nPublished: January 2022", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/voiceverse-nft-voice-theft", "content": "Voiceverse NFT caught plagiarising voice lines from AI service\nOccurred: January 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nVoice NFT start-up Voiceverse was caught appropriating third-party voice lines without permission by the founder of text-to-speech service 15.ai, and labelling it as 'Powered By Voiceverse'.\nVoiceverse later apologised and pinned the blame on its marketing team. The company describes itself on its website as having been 'built with unique, next-generation AI to provide everyone a \"Voice for the Metaverse\"'. \nVoiceverse is the latest NFT company to have attracted unwanted publicity. Others have been dragged over the coals for theft, fraud, lax security, and environmental damage, amongst other issues.\nSystem \ud83e\udd16\nVoiceverse website\nOperator: Voiceverse\nDeveloper: Voiceverse\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Sell voice rights\nTechnology: Voice synthesis; Blockchain\nIssue: Copyright; Hypocrisy\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.kotaku.com.au/2022/01/voiceverse-caught-using-someone-elses-content/\nhttps://www.eurogamer.net/articles/2022-01-17-troy-baker-backed-nft-firm-admits-using-voice-lines-taken-from-another-service-without-permission\nhttps://metro.co.uk/2022/01/17/nft-firm-voiceverse-stole-work-after-announcing-troy-baker-deal-15934440/\nhttps://www.nme.com/news/gaming-news/voiceverse-nft-admits-to-taking-voice-lines-from-non-commercial-service-3140663\nhttps://stevivor.com/news/troy-baker-nft-voiceverse-15-ai/\nRelated \ud83c\udf10\nBytedance/TikTok Bev Standing voice theft\nWitcher 3 AI voice line simulation\nPage info\nType: Incident\nPublished: January 2022", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tesla-fsd-assertive-mode-rolling-stops", "content": "Tesla FSD has 'Assertive' mode that performs illegal rolling stops\nOccurred: October 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nTesla's 'Full-Self-driving' beta test has an 'Assertive' mode that may perform rolling stops. Rolling stops are generally considered illegal under US law.\nAssertive is one of three 'profiles' - 'Chill', 'Average' and 'Assertive' - that dictates how a car will behave in certain circumstances - and will 'have a smaller follow distance, perform more frequent speed lane changes, will not exit passing lanes and may perform more rolling stops'.\nFSD profiles were included in Tesla's October 2021 10.3.1 update. The previous update had been pulled two days after testers complained about false crash warnings and other bugs.\nFollowing meetings with the National Highway Traffic Safety Administration, Tesla recalled (pdf) all 53,822 Model S, X, 3 and Y vehicles with the FSD feature, and says it will disable the rolling stop function. \nSystem \ud83e\udd16\nTesla Autopilot, Full-self Driving\nOperator: \nDeveloper: Tesla\nCountry: USA\nSector: Automotive\nPurpose: Control car behaviours\nTechnology: Self-driving system; Computer vision\nIssue: Safety; Legal - compliance; Ethics/value\nTransparency: Black box\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nNHTSA (2022). Part 573 Safety Recall Report (pdf) \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theverge.com/2022/1/9/22875382/tesla-full-self-driving-beta-assertive-profile\nhttps://gizmodo.com/teslas-assertive-mode-brings-rolling-stops-to-self-driv-1848331537\nhttps://techcrunch.com/2022/01/10/tesla-full-self-driving-beta-features-an-assertive-mode-with-rolling-stops/\nhttps://hypebeast.com/2022/1/tesla-full-self-driving-beta-assertive-mode-perform-rolling-stops\nhttps://www.dailymail.co.uk/sciencetech/article-10386965/amp/Tesla-increase-Self-Driving-package-unveils-update-adds-Assertive-mode.html\nhttps://futurism.com/tesla-fsd-rolling-stops\nhttps://jalopnik.com/teslas-fsd-betas-driving-modes-bring-up-interesting-eth-1848331683\nhttps://www.autoevolution.com/news/elon-musk-announces-fsd-prices-will-rise-to-12000-amid-legal-controversy-178701.html\nhttps://news.yahoo.com/tesla-could-drive-jerk-110039153.html\nhttps://edition.cnn.com/2022/02/01/cars/tesla-fsd-stop-sign/index.html\nhttps://abcnews.go.com/Technology/wireStory/tesla-recall-full-driving-software-runs-stop-signs-82596066\nRelated \ud83c\udf10\nTesla Smart Summon\nTesla phantom braking\nPage info\nType: Incident\nPublished: January 2022", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/mainz-police-luca-covid-19-abuse", "content": "Mainz police under fire for misusing COVID-19 tracing app data\nOccurred: January 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nPolice in the city of Mainz, Germnay, were accused of illegally using data from a COVID-19 tracking app to trace the contacts of a man who died leaving a restaurant, sparking an outcry.\nSeeking witnesses of the incident, Mainz police (Polizeipr\u00e4sidium Mainz) tapped into data from Luca, an app that helps track the COVID-19 pandemic in Germany by registering time spent in restaurants and pubs. The app also logs the patron's full name, address and telephone number - information that is protected under the country's data protection laws.\nThe police and local prosecutors in the case in Mainz successfully appealed to the municipal health authorities to gain access to information about 21 people who visited the restaurant at the same time as the man who died. \nProsecutors apologised in the wake of complaints by local politicians, rights advocates and citizens. \nThe local data protection authority also opened an inquiry. \nSystem \ud83e\udd16\nLuca app\nOperator: Culture4life; Polizeipr\u00e4sidium Mainz\nDeveloper: Culture4life  \nCountry: Germany\nSector: Govt - police\nPurpose: Track COVID-19\nTechnology: Application\nIssue: Privacy; Security\nTransparency: Privacy; Legal\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.washingtonpost.com/world/2022/01/13/german-covid-contact-tracing-app-luca/\nhttps://www.swr.de/swraktuell/rheinland-pfalz/mainz/polizei-ermittelt-ohne-rechtsgrundlage-mit-daten-aus-luca-app-100.html\nhttps://www.tagesschau.de/investigativ/swr/polizei-nutz-luca-app-101.html\nhttps://www.dailymail.co.uk/news/article-10391567/German-police-use-Covid-tracking-data-track-witnesses-investigation.html\nhttps://www.dw.com/en/german-police-under-fire-for-misuse-of-covid-contact-tracing-app/a-60393597\nhttps://www.thelocal.de/20220111/german-police-under-fire-for-using-covid-tracing-app-to-find-witnesses/\nhttps://www.unilad.co.uk/news/police-under-investigation-for-using-covid-tracking-data-to-hunt-down-witness\nhttps://9to5mac.com/2022/01/13/contact-tracing-app-data-misused-by-police/\nhttps://metro.co.uk/2022/01/12/german-police-tracked-down-restaurant-death-witnesses-using-covid-app-15904077/\nRelated \ud83c\udf10\nKFC Germany Kristallnacht marketing automation\nGorillas 'Project Ace' rider work schedule automation\nPage info\nType: Incident\nPublished: January 2022", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tek-fog-political-manipulation", "content": "BJP uses Tek Fog app to manipulate opinion, harass opponents\nReleased: 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nIndia's ruling Bharatiya Janata Party (BJP) was accused of using a software application named Tek Fog to manipulate public opinion and intimidate, harass, and abuse political opponents and journalists.\nA multi-part investigation by The Wire following a whistleblower tip-off indicated that the BJP had been artificially inflating the party's popularity, harassing its critics and manipulating public opinion across a number of major social media and mobile platforms, including WhatsApp, Facebook, and Twitter.\nThe investigation also claimed that the BJP, Persistent Systems and Mohalla Tech were involved in deploying the app, with members of the BJP's youth wing, Bharatiya Janata Yuva Morcha (BJYM), supervising the operators - though each denied being involved.\nThe revelations prompted public condemnation from politicians, rights groups, The Editor's Guild, and The Delhi Union of Journalists, and spurred an investigation by India's Parliamentary Standing Committee on Home Affairs.\nThe incident raised questions about BJP governance and ethics, and the need for vigilance against potential misuse of technology in a political context.\n\u2795 In October 2022, The Wire withdrew its Tek Fog series after revelations about earlier misconduct by journalist Devesh Kumar, and apologised to its readers. \nSystem \ud83e\udd16\nTek Fog Wikipedia profile\nOperator: Bharatiya Janata Party; Mohalla Tech/ShareChat; Persistent Systems\nDeveloper: Bharatiya Janata Party; Persistent Systems\nCountry: India\nSector: Politics\nPurpose: Manipulate public opinion; Harass opponents\nTechnology: Application; NLP/text analysis\nIssue: Safety; Privacy; Bias/discrimination - race, gender; Mis/disinformation\nTransparency: Governance; Privacy\nInvestigations, assessments, audits \ud83e\uddd0\nhttps://thewire.in/tekfog/en/1.html\nhttps://thewire.in/tekfog/en/2.html\nhttps://thewire.in/tekfog/en/3.html\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.business-standard.com/article/current-affairs/tek-fog-app-a-threat-to-national-security-says-derek-o-brien-122011001480_1.html\nhttps://www.thehindu.com/news/national/parliamentary-panel-asks-home-secretary-to-respond-on-tek-fog-app/article38258217.ece\nhttps://www.nationalheraldindia.com/opinion/use-of-tek-fog-by-bjp-immoral-and-unconstitutional-another-tool-to-treat-indian-citizens-as-enemies-of-state\nhttps://www.ndtv.com/opinion/bjp-accused-of-using-app-tek-fog-to-target-intimidate-critics-2705566\nhttps://www.bloomberg.com/opinion/articles/2022-01-12/india-s-tek-fog-shrouds-an-escalating-political-war-against-modi-s-critics\nhttps://qz.com/india/2110005/bjp-supporters-use-tek-fog-to-manipulate-whatsapp-and-twitter/\nhttps://www.thenewsminute.com/article/wire-retracts-meta-stories-tek-fog-investigation-be-reviewed-too-169165\nRelated \ud83c\udf10\nBulli Bai Muslim women auction\nIndia citizenship law protest surveillance\nPage info\nType: System\nPublished: January 2022\nLast updated: December 2022", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/bulli-bai-muslim-women-auction", "content": "100 Muslim women auctioned on Bulli Bai app in India \nOccurred: January 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nOver 100 high-profile Muslim women were 'auctioned' online in India, drawing intense criticism from politicians, rights advocates and others.\nA derogatory term that refers to Muslim women, Bulli Bai in this instance referred to an app that displayed the names and doctored images of influential Muslim women. The images images were obtained without consent from the womens' social media profiles and Hindus were invited to participate in a virtual 'auction' with the intention of humiliating them.\nAccording to police investigating the case, the creators of the app identified as 'Trads' ('Traditionalists'), who promote genocide against minorities.\nOpen source software development platform Github came under fire for hosting Bulli Bai. The platform also hosted Sulli Deals, a near identical app that had run for weeks mid-2021 before being taken down. Owned since 2018 by Microsoft, Github is no stranger to controversy. Amongst other things, the platform hosts libraries of code enabling people to create deepfakes, including for malicious and nefarious purposes.\nA number of individuals were later arrested in connection with Bulli Bai and Sulli Deals.\nThe incident raised concerns about the nature and need to manage online and offline religious hatred and gender discrimination in India. It also raised questions questions about the willingness and ability of Github and Microsoft to police their platform.  \nSystem \ud83e\udd16\nBulli Bai Wikipedia case\nOperator: Microsoft/Github\nDeveloper: \nCountry: India\nSector: Media/entertainment/sports/arts\nPurpose: Moderate content\nTechnology: Content moderation system\nIssue: Accuracy/reliability; Ethics/values; Safety; Bias/discrimination - religion\nTransparency: Governance; Privacy\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.aljazeera.com/news/2022/1/2/bulli-bai-muslim-women-auction-online-india \nhttps://www.aljazeera.com/news/2022/1/10/india-bulli-bai-app-auction-muslim-women-tech-weaponised-abuse \nhttps://www.washingtonpost.com/world/2022/01/04/india-online-auction-muslim-women\nhttps://www.bbc.co.uk/news/world-asia-india-59835674 \nhttps://uk.news.yahoo.com/prominent-muslim-women-made-feel-135239542.html \nhttps://www.dw.com/en/india-auction-of-muslim-women-on-apps-reveals-widespread-online-abuse/a-60379358\nhttps://www.cnn.com/2022/01/03/tech/bulli-bai-india-muslim-women-sale-intl-hnk/index.html \nhttps://www.ndtv.com/india-news/what-is-the-bulli-bai-controversy-explained-in-5-points-2687614 \nhttps://thewire.in/women/opposition-slams-targeting-of-muslim-women-on-bulli-bai-app-demands-strict-action \nhttps://timesofindia.indiatimes.com/india/why-bulli-bai-comes-as-no-surprise/articleshow/88803407.cms \nhttps://restofworld.org/2022/why-anti-muslim-apps-keep-reappearing-on-github/\nhttps://www.technologyreview.com/2022/02/21/1046052/online-auctions-muslim-women-india/\nhttps://www.nationalheraldindia.com/national/committed-crime-face-it-sc-declines-urgent-listing-of-plea-by-sulli-deals-app-accused\nhttps://www.inversejournal.com/2022/02/25/opinion-reflecting-on-the-karnataka-hijab-row-in-india75-by-adwaith-pb/\nRelated \ud83c\udf10\nMicrosoft/Github Copilot 'code laundering'\nIndia citizenship law protest surveillance\nPage info\nType: Incident\nPublished: January 2022\nLast updated: February 2022", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/the-dao-smart-contracts-hack", "content": "USD 50m siphoned in hack of The DAO\nOccurred: June 2016 \nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAutomated, decentralised quasi-venture capital investment fund The DAO was hacked, with USD 50 million of Ether virtual currency siphoned elsewhere. \nThe hack resulted from a critical vulnerability in The DAO\u2019s smart contract code, allowing an attacker to manipulate the code and drain a substantial amount of Ether from The DAO\u2019s funds. The vulnerability was related to the mechanism of splitting, which allowed token holders to exit the organisation and retrieve their Ether. Controversially, The DAO theft prompted Ethereum to do a so-called 'hard fork', in which the Ethereum network split into two as a way to restore the stolen funds. \nLaunched in April 2016, The DAO was an automated quasi-venture capital investment fund with no known management structure based on a digital decentralised autonomous organisation (DAO) that operated on the Ethereum blockchain, as built by German company Slock.it. The DAO raised USD 150 million in what was then the largest crowdfunding campaign, and was seen by enthusiasts as a revolutionary way to manage organisations of all kinds.\nThe incident raised questions about the governance, security and of The DAO, and its regulation, as well as about the governance of decentralised autonomous organisations more broadly. \n\u2795 September 2016. The DAO ceased operations. \n\u2795 July 2017. An SEC investigation concluded: 'The automation of certain functions through this technology, \u2018smart contracts\u2019 or computer code, does not remove conduct from the purview of the U.S. federal securities laws.'\n\u2795 February 2022. In her book The Cryptopians, journalist Laura Shin named then Singapore-based Austrian national Toby Hoenisch as responsible for The DAO hack.\nSystem \ud83e\udd16\nThe DAO Wikipedia profile\nOperator: The DAO; Slock.it; Bity SA; Ethereum Foundation\nDeveloper: The DAO; Slock.it\nCountry: USA; Global\nSector: Banking/financial services\nPurpose: Automate financial contracts\nTechnology: Blockchain; Virtual currency\nIssue: Security\nTransparency: Governance; Marketing\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nSecurities and Exchange Commission (2017). Report of Investigation Pursuant to Section 21(a) of the Securities Exchange Act of 1934: The DAO (pdf) \nResearch, advocacy \ud83e\uddee\nhttps://www.frontiersin.org/articles/10.3389/fbloc.2020.00025/full\nInvestigations, assessments, audits \ud83e\uddd0\nhttps://www.forbes.com/sites/laurashin/2022/02/22/exclusive-austrian-programmer-and-ex-crypto-ceo-likely-stole-11-billion-of-ether/\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.wired.com/2016/06/50-million-hack-just-showed-dao-human/\nhttps://www.coindesk.com/understanding-dao-hack-journalists\nhttps://www.businessinsider.com/dao-hacked-ethereum-crashing-in-value-tens-of-millions-allegedly-stolen-2016-6\nhttps://techcrunch.com/2016/05/16/the-tao-of-the-dao-or-how-the-autonomous-corporation-is-already-here/\nhttps://www.economist.com/finance-and-economics/2016/05/19/the-dao-of-accrue\nhttps://observer.com/2017/07/sec-dao-report-securities/\nhttps://medium.com/swlh/the-story-of-the-dao-its-history-and-consequences-71e6a8a551ee\nhttps://medium.com/@laurashin/who-hacked-the-dao-on-ethereum-heres-how-we-jumped-past-one-critical-step-60aec489a127\nhttps://markets.businessinsider.com/news/currencies/dao-hacker-identity-stole-11-billion-2016-ether-revealed-2022-2\nRelated \ud83c\udf10\nWorldcoin 'field testing'\nVoiceverse NFT voice theft\nPage info\nType: Incident\nPublished: February 2022", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/moviepass-preshow-eye-tracking", "content": "MoviePass to monitor viewers using facial recognition and eye tracking\nOccurred: February 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nMovie ticketing service MoviePass said it would return as an app that would track users\u2019 eyes when they watched adverts, prompting privacy concerns.\nThe new service will reportedly use facial recognition and eye-tracking technology to ensure that viewers are looking directly at adverts in exchange for access to films. To earn movie tickets, users would have to keep their gaze in frame, and look at their screens for the whole ad. A red border appears if they look away, pausing the ads. \nThe announcement prompted commentators to question whether people would be willing to surrender their privacy to watch movies on a small screen, as well as questions about the security of customer data.  \nMoviePass shuttered in 2019 and declared bankruptcy in 2020. In addition to its stuttering financial performance, it had been plagued with allegations of playing fast and loose with customer data.\nSystem \ud83e\udd16\nMoviePass website\nMoviePass Wikipedia profile\n\nDocuments \ud83d\udcc3\nhttps://www.youtube.com/watch?v=xffsK7A4O2A\nhttps://www.kickstarter.com/projects/454129649/preshow-attend-first-run-movies-in-theaters-free\nOperator: MoviePass\nDeveloper: MoviePass\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Earn virtual currency\nTechnology: Facial recognition; Eye tracking\nIssue: Privacy; Security\nTransparency: Privacy\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.vice.com/en/article/akvnba/moviepass-20-wants-to-track-your-eyeballs-to-make-sure-you-watch-ads\nhttps://www.independent.co.uk/tech/moviepass-track-eyes-phone-cameras-b2013273.html\nhttps://www.dailymail.co.uk/sciencetech/article-10511899/MoviePass-eyeball-tracking-make-watch-ads.html\nhttps://www.thegamer.com/moviepass-eye-tracking-ads/\nhttps://www.cbsnews.com/news/moviepass-relaunches-ads-track-eyeballs-face-recognition/\nhttps://www.iflscience.com/technology/eyetracking-moviepass-app-will-pause-ads-if-you-look-away/\nhttps://www.engadget.com/2019-03-21-preshow-free-movie-tickets.html\nhttps://futurism.com/moviepass-eye-tracking-ads\nhttps://www.vulture.com/2022/02/moviepass-2-relaunch-dystopian.html\nhttps://www.thefpsreview.com/2022/02/12/moviepass-relaunching-this-summer-with-eye-tracking-technology-users-watch-ads-to-earn-credits-for-movies/\nhttps://www.tomsguide.com/uk/news/moviepass-20-explained-yes-this-sounds-like-a-black-mirror-episode\nhttps://www.slashgear.com/764386/moviepass-2-0-promises-web3-and-face-tracking-ads-for-summer-relaunch/\nhttps://www.vulture.com/2019/08/moviepass-continues-to-play-fast-and-loose-with-user-data.html\nhttps://techcrunch.com/2019/08/20/moviepass-thousands-data-exposed-leak/\nRelated \ud83c\udf10\nWorldcoin 'field testing'\nSao Paolo Metro advertising facial biometrics\nPage info\nType: Issue\nPublished: February 2022", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/hyderabad-police-facial-recognition", "content": "Hyderabad police force activist to remove COVID-19 mask\nOccurred: May 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nTelangana state, India, faces court action for the allegedly illegal use of facial recognition by its police forces for forcing people  to remove their masks to verify their identities during the COVID-19 pandemic.\nSocial activist SQ Masood claimed he was forced by Hyderabad City Police to remove his mask while he was returning home from work so that they could take his photograph with a mobile phone. \nThe Internet Freedom Foundation issued a legal notice claiming Masood's photographs were likely being fed into a facial recognition database. It also argued that state use of facial recognition 'is not backed by law, is unnecessary, disproportionate, and is being done without any safeguards to prevent misuse'.\nPolice denied using facial recognition in this instance, saying Masood's photograph was not scanned against any database and that facial recognition has only been used during the investigation of a crime or suspected crime. \nTelangana state is said to have amongst the highest density of CCTV cameras in the world, and boasts the most known government facial recognition projects in India. Amnesty International accused Hyderabad of becoming a 'total surveillance city'.\nSystem \ud83e\udd16\nNeoFace Watch facial recognition system\nOperator: Hyderabad City Police\nDeveloper: NEC\nCountry: India\nSector: Govt - police\nPurpose: Reduce crime\nTechnology: Facial recognition\nIssue: Privacy; Surveillance; Dual/multi-use\nTransparency: Governance; Complaints/appeals; Privacy\nResearch, advocacy \ud83e\uddee\nSQ Masood (2021). Legal complaint\nPanoptic Tracker. Hyderbad police\nInternet Freedom Foundation (2021). Hyderabad Police force people to remove their masks before photographing them. We sent a legal notice\nNews, commentary, analysis \ud83d\udcf0\nhttps://www.thenewsminute.com/article/telangana-hc-issues-notice-govt-over-pil-challenging-use-facial-recognition-tech-159432 \nhttps://www.newindianexpress.com/states/telangana/2022/jan/03/telangana-high-court-issues-notice-to-state-government-on-pil-against-facial-recognition-technology-2402618.html\nhttps://www.medianama.com/2022/01/223-facial-recognition-petition-telangana/ \nhttps://cio.economictimes.indiatimes.com/news/corporate-news/telangana-hc-notice-to-state-cops-for-using-face-recognition-tech/88680752\nhttps://www.livelaw.in/news-updates/telangana-high-court-notice-pil-challenging-deployment-facial-recognition-technology-telangana-188646\nhttps://thewire.in/law/telangana-hc-issues-notice-on-pil-challenging-use-of-facial-recognition-technology \nhttps://pulitzercenter.org/stories/police-seize-covid-19-tech-expand-global-surveillance\nhttps://www.biometricupdate.com/202201/law-enforcement-facial-recognition-use-under-scrutiny-in-ireland-india \nRelated \ud83c\udf10\nIndia citizenship law protest surveillance\nHebron Palestinian facial recognition surveillance\nPage info\nType: Incident\nPublished: January 2022\nLast updated: March 2023", "year": "2"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tesla-paris-fatal-crash", "content": "Tesla Model 3 crash kills one, injures twenty in Paris\nOccurred: December 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nOne person was killed and twenty injured, three of them seriously, when an off-duty taxi driver appeared to lose control of his Tesla Model 3 while taking his family to a restaurant in Paris, France. \nViews conflicted on whether the taxi driver was responsible, or if the car had accelerated automatically and its brakes had failed, as claimed by the driver's lawyer. \nThe driver has been placed under formal investigation for suspected involuntary manslaughter by French authorities.\nA 2020 investigation by the US auto safety regulator into the sudden acceleration of Tesla cars concluded that accidents had been caused by 'pedal misapplication'.\nSystem \ud83e\udd16\nTesla Autopilot, Full-self Driving\nOperator: G7\nDeveloper: Tesla\nCountry: France\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Driver assistance system\nIssue: Safety; Accuracy/reliability\nTransparency: Black box\nResearch, advocacy \ud83e\uddee\nTesla Deaths\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reuters.com/business/autos-transportation/paris-taxi-firm-suspends-use-tesla-model-3-after-accident-2021-12-14/\nhttps://www.reuters.com/business/autos-transportation/french-transport-minister-says-not-worried-about-tesla-after-paris-car-accident-2021-12-15/\nhttps://www.reuters.com/world/europe/paris-crash-tesla-driver-says-car-accelerated-its-own-lawyer-2021-12-16/\nhttps://news.sky.com/video/video-fatal-tesla-crash-in-paris-killed-one-person-and-injured-20-others-12496578\nhttps://www.popsci.com/technology/fatal-tesla-crash-paris/\nhttps://thehill.com/policy/technology/585964-paris-taxi-company-suspending-use-of-teslas-after-fatal-accident\nhttps://www.washingtonpost.com/world/2021/12/15/paris-taxi-g7-tesla-accident/\nhttps://www.autoevolution.com/news/tesla-model-3-taxi-cab-accident-hurts-about-20-people-in-paris-due-to-braking-issues-176380.html\nRelated \ud83c\udf10\nTesla Model 3 hits parked police car\nTesla Model 3 hits six children\nPage info\nType: Incident\nPublished: December 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/shanghai-ai-prosecutor", "content": "Shanghai AI prosecutor accused of being inaccurate\nReleased: 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn AI prosecutor in Shanghai, China, was accused of being inaccurate and a threat to freedom of expression. \nChina developed an AI prosecutor able to charge people with eight common crimes, including fraud, theft, dangerous driving, obstructing official duties and 'provoking trouble', with over 97% accuracy, according to the South China Morning Post.\nBuilt and tested by the Shanghai Pudong People\u2019s Procuratorate, the system has been trained on over 17,000 cases and can charge a suspect based on 1,000 'traits' gathered from a human-documented description of a case.\nLauded by some for its accuracy, others are concerned about its potential for errors, and question how appeals can be made against its black box system. Others believe the program will be used to stifle freedom of expression, assembly and other forms of dissent.\nThe AI prosecutor works alongside 'System 206', which was used for the first time in Shanghai in January 2019 and which is said to evaluate evidence, conditions for an arrest, and the degree of danger a suspect poses to the general public. \nSystem \ud83e\udd16\n\nOperator:  \nDeveloper: Shanghai Pudong People\u2019s Procuratorate; Chinese Academy of Sciences\nCountry: China\nSector: Govt - justice\nPurpose: Determine criminal guilt  \nTechnology: NLP/text analysis; Voice to text\nIssue: Accuracy/reliability; Bias/discrimination - multiple; Freedom of expression; Dual/multi-use\nTransparency: Black box; Complaints/appeals\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.scmp.com/news/china/science/article/3160997/chinese-scientists-develop-ai-prosecutor-can-press-its-own \nhttps://www.dailymail.co.uk/news/article-10346933/China-develops-AI-prosecutor-press-charges-97-accuracy.html \nhttps://www.ladbible.com/news/latest-china-develops-ai-that-can-judge-peoples-guilt-with-97-accuracy-20211229 \nhttps://news.yahoo.com/china-develops-ai-prosecutor-charge-220356905.html \nhttps://techwireasia.com/2021/12/china-has-developed-an-ai-prosecutor/ \nhttps://futurism.com/the-byte/china-ai-prosecutor-crimes \nhttps://interestingengineering.com/chinese-scientists-created-an-ai-prosecutor-that-can-press-charges \nhttps://www.chinadaily.com.cn/a/201901/24/WS5c4959f9a3106c65c34e64ea.html\nRelated \ud83c\udf10\nMalaysia AI court sentencing\nDubai deepfake court evidence\nPage info\nType: Issue\nPublished: December 2021\nLast updated: March 2022", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-alexa-penny-challenge", "content": "Amazon Alexa recommends girl touches electric plug \nOccurred: December 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAmazon's Alexa voice assistant recommended a girl touch a coin onto the exposed prongs of a live electric plug, potentially causing her serious injury, or setting off an electric fire.\nHaving been asked by the 10-year old for a physical challenge to do, Alexa recommended that she 'Plug in a phone charger about halfway into a wall outlet, then touch a penny to the exposed prongs'.\nThe penny challenge is thought to have started circulating on TikTok over the past few months. \nAmazon said it removed the challenge from Alexa's database.\nSystem \ud83e\udd16\nAmazon Alexa developer website\nAmazon Alexa Wikipedia profile\nOperator: Kristin Livdahl\nDeveloper: Amazon\nCountry: UK\nSector: Consumer goods\nPurpose: Provide information, services\nTechnology: NLP/text analysis; Natural language understanding (NLU); Speech recognition\nIssue: Accuracy/reliability; Safety\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.com/news/technology-59810383\nhttps://www.dailymail.co.uk/femail/article-10350249/Amazon-apologises-Alexa-challenges-10-year-old-girl-penny-prongs-plug.html\nhttps://arstechnica.com/gadgets/2021/12/alexa-tells-10-year-old-to-try-a-shocking-tiktok-challenge/\nhttps://gizmodo.com/amazon-alexa-told-a-10-year-old-girl-to-play-with-a-liv-1848275928\nhttps://www.cnet.com/home/smart-home/amazons-alexa-reportedly-suggested-10-year-old-stick-a-penny-near-an-outlet/\nhttps://www.businessinsider.com/amazon-fixes-alexa-penny-challenge-error-girl-told-touch-outlet-2021-12\nhttps://metro.co.uk/2021/12/28/alexa-suggests-lethal-tiktok-challenge-to-10-year-old-15831550/\nhttps://www.pcmag.com/news/alexa-suggests-dangerous-outlet-challenge-to-10-year-old\nRelated \ud83c\udf10\nAmazon Alexa plays child pornography\nAmazon automated pricing glitch\nPage info\nType: Incident\nPublished: December 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/pony-ai-driverless-test-crash", "content": "Pony.ai driverless car hits road divider, traffic sign\nOccurred: October 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nPony.ai agreed with the National Highway Traffic and Safety Administration (NHTSA) to issue a recall for three vehicles using versions of its automated driving system.\nThe US DMV had suspended the driverless test permit of the automotive start-up in the wake of a crash of one of its vehicles late October 2021 in Fremont, California. The NHTSA also started an informal inquiry.\nAccording (pdf) to the incident report, the fully autonomous car with no accompanying test driver hit a road center divider and a traffic sign. There were no injuries and other vehicles involved.\nThe agency says this is the first time it has recalled an automated driving system. Pony.ai had registered 10 Hyundai Motor Kona electric vehicles under its driverless testing permit.\nSystem \ud83e\udd16\nPony AI website\nPony AI Wikipedia profile\nOperator: Pony.ai\nDeveloper: Pony.ai; Luminar\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Automated driving system (ADS); Computer vision\nIssue: Safety; Accuracy/reliability\nTransparency: Black box\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nDMV Traffic Collision report (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reuters.com/markets/commodities/california-halts-ponyais-driverless-testing-permit-after-accident-2021-12-14/\nhttps://www.engadget.com/california-suspends-ponyai-driverless-testing-permit-122914859.html\nhttps://www.theverge.com/2021/12/14/22834496/pony-ai-av-test-permit-suspend-california-dmv-crash\nhttps://techcrunch.com/2021/12/14/pony-ai-suspension-driverless-pilot-california/\nhttps://www.autoevolution.com/news/california-dmv-suspends-ponyai-driverless-testing-permit-after-small-crash-176549.html\nhttps://www.therobotreport.com/california-suspends-pony-ai-driverless-testing-after-accident/\nhttps://www.reddit.com/r/SelfDrivingCars/comments/qwuzpg/ponyai_crash_in_fremont_on_october_28/\nhttps://news.ycombinator.com/item?id=29547458\nhttps://www.reuters.com/business/autos-transportation/startup-ponyai-agrees-automated-driving-system-software-recall-2022-03-08/\nhttps://techcrunch.com/2022/03/08/pony-ai-to-issue-recall-of-autonomous-driving-software/\nRelated \ud83c\udf10\nTesla Model 3 Paris fatal crash\nGM Chevrolet Bolt motorbike collision\nPage info\nType: Incident\nPublished: December 2021\nLast updated: March 2022", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/bucheon-covid-19-facial-recognition-tracking", "content": "Bucheon COVID-19 facial recognition tracking pilot triggers privacy backlash\nOccurred: December 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe city of Bucheon in South Korea sparked controversy with a pilot project to use AI-based facial recognition technology and surveillance cameras for contact tracing of COVID-19 patients. \nThe system, funded by the Ministry of Science ICT and the local government, used AI algorithms to analyse footage from over 10,000 security cameras and could track an infected person\u2019s movements, their close contacts, and whether they were wearing a mask. \nThe system aimed to reduce the time and resources needed for contact tracing. The programme prompted digital rights and privacy experts to express concerns hat sensitive personal data was being shared with the unnamed private company building the datasets and developing the algorithm without first obtaining permission from the people appearing in the CCTV footage. \nSome also expressed fears that the system might be used for undeclared purposes.\n\u2796 October 2021. South Korea's justice and immigration authorities came under fire for sharing the facial images and personal details of approximately 170 million travellers without their consent.\nSystem \ud83e\udd16\n\nOperator: City of Bucheon\nDeveloper: Unknown\nCountry: S Korea\nSector: Govt - heath\nPurpose: Track COVID-19 infected individuals\nTechnology: Facial recognition; Gait recognition; Mask recognition\nIssue: Privacy; Scope creep/normalisation; Dual/multi-use; Surveillance\nTransparency: Governance; Privacy\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.hani.co.kr/arti/english_edition/e_national/1019531.html\nhttps://www.biometricupdate.com/202111/south-korean-face-biometrics-data-sharing-scandal-gets-worse\nhttps://www.reuters.com/world/asia-pacific/skorea-test-ai-powered-facial-recognition-track-covid-19-cases-2021-12-13/\nhttps://www.aljazeera.com/economy/2021/12/13/ai-in-korea\nhttps://edition.cnn.com/2021/12/13/asia/south-korea-covid-facial-recognition-intl-hnk/index.html\nhttps://findbiometrics.com/south-korean-city-use-facial-recognition-contact-tracing-efforts-121401/\nhttps://www.nytimes.com/2021/12/13/world/asia/south-korea-facial-recognition-coronavirus.html\nhttps://theweek.com/coronavirus/1008008/a-city-in-south-korea-will-test-facial-recognition-technology-to-track-covid-19\nhttps://www.scmp.com/news/asia/east-asia/article/3159471/coronavirus-south-korea-use-facial-recognition-tech-track-cases\nRelated \ud83c\udf10\nS Korea immigration facial recognition sharing\nFacebook South Korea facial recognition abuse\nPage info\nType: Issue\nPublished: December 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/horizon-worlds-virtual-groping", "content": "Meta Horizon Worlds beta tester groped by stranger\nOccurred: December 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA beta tester of Meta's new social virtual reality platform Horizon Worlds revealed that her avatar was groped by a stranger, raising questions about the safety of metaverses and of Mark Zuckerberg's much-hyped new business direction.\nThe tester, who later revealed herself as Nina Jane Patel, detailed her experience in a lengthy blog post, describing her treatment as 'horrible' and akin to 'virtual gang rape'.\nAccording to the Technology Review, Meta\u2019s internal review of the incident found that the beta tester had not used the platform's 'Safe Zone' tool which enables users to lock themselves in a secure bubble away from other users until they signal they would like to exit it. \nShortly afterwards, Meta announced Personal Boundary, a default setting in the Horizon Worlds creation platform and the Horizon Venues live event service that prevents avatars from coming within a set distance of each other, creating more personal space for people and making it easier to avoid unwanted interactions.\nSystem \ud83e\udd16\nHorizon Worlds website\nHorizon Worlds Wikipedia profile\nOperator: Nina Jane Patel\nDeveloper: Meta/Quest\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Provide virtual social experience\nTechnology: Virtual reality; Safety management system\nIssue: Safety\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nSumOfUs (2022). Metaverse: another cesspool of toxic content (pdf)\nYee N. et al (2009). The Proteus Effect: Implications of Transformed Digital Self-Representation on Online and Offline Behavior\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://medium.com/kabuni/fiction-vs-non-fiction-98aa0098f3b0\nhttps://www.theverge.com/2021/12/9/22825139/meta-horizon-worlds-access-open-metaverse\nhttps://www.technologyreview.com/2021/12/16/1042516/the-metaverse-has-a-groping-problem/\nhttps://www.dailymail.co.uk/sciencetech/article-10320449/Metas-metaverse-app-user-reveals-avatar-virtually-groped.html\nhttps://markets.businessinsider.com/news/stocks/meta-tester-reports-groping-in-virtual-world-1031054255\nhttps://www.msn.com/en-us/news/technology/meta-launched-an-investigation-after-a-woman-said-she-was-groped-by-a-stranger-in-the-metaverse/ar-AARUHuA\nhttps://futurism.com/sexual-assault-metaverse\nhttps://nypost.com/2021/12/17/woman-claims-she-was-virtually-groped-in-meta-vr-metaverse/\nhttps://www.euronews.com/next/2021/12/17/sexual-assault-has-already-started-in-meta-s-horizon-worlds-metaverse\nhttps://www.bbc.co.uk/news/technology-60247542\nhttps://www.theverge.com/2022/2/4/22917722/meta-horizon-worlds-venues-metaverse-harassment-groping-personal-boundary-feature\nhttps://www.vice.com/en/article/3abpg3/woman-says-she-was-virtually-gang-raped-in-facebooks-metaverse\nhttps://nypost.com/2022/02/01/mom-opens-up-about-being-virtually-gang-raped-in-metaverse/\nhttps://nypost.com/2022/02/04/meta-adds-personal-boundary-to-metaverse-after-virtual-gang-rape/\nhttps://www.independent.co.uk/news/uk/home-news/metaverse-gang-rape-virtual-world-b2005959.html\nhttps://www.cnbctv18.com/technology/woman-recalls-gang-rape-in-metaverse-concerns-grow-over-making-vr-platforms-safe-from-sexual-predators-12396992.htm\nhttps://eu.usatoday.com/story/tech/2022/01/31/woman-allegedly-groped-metaverse/9278578002/\nhttps://www.ibtimes.com/woman-recounts-horror-being-virtually-gang-raped-metaverse-it-was-nightmare-3388430\nhttps://eu.usatoday.com/story/tech/2022/01/31/woman-allegedly-groped-metaverse/9278578002/\nhttps://eu.usatoday.com/story/tech/2022/02/04/meta-personal-boundary-horizon-worlds/6663797001/\nhttps://www.washingtonpost.com/technology/2022/02/07/facebook-metaverse-horizon-worlds-kids-safety\nhttps://futurism.com/experts-child-predators-metaverse\nhttps://www.dailymail.co.uk/sciencetech/article-10857551/Woman-21-virtually-RAPED-stranger-Metas-metaverse-app-report-claims.html\nRelated \ud83c\udf10\nVRChat virtual strip clubs, child grooming\nRoblox Condo nazi sex parties\nPage info\nType: Incident\nPublished: December 2021\nLast updated: January 2024", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/xpeng-customer-facial-recognition", "content": "XPeng fined for collecting customer data using facial recognition\nOccurred: December 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nXPeng Motors was fined 100,000 yuan (USD 15,710) by the Shanghai Municipal Administration for Market Regulation for illegally using facial recognition to collect biometric data of visitors to its stores in mainland China. \nXPeng had used 22 cameras to collect over 430,000 facial images over a six-month period, ostensibly to better understand its customers. The company failed to inform visitors about the system, nor gain their consent, thereby violating China's Consumer Rights Protection Law. \nNews of the contravention resulted in a torrent of criticism of the company on local social media.\nThe company said it deleted all 430,000 images and will comply with relevant regulations going forward.\n\nChina's Personal Information Protection Law came into effect on November 1, 2021. \nSystem \ud83e\udd16\nUnknown\nXPeng website\nXPeng Wikipedia profile\nOperator: XPeng Motors\nDeveloper: Unknown\nCountry: China\nSector: Automotive\nPurpose: Understand customers; Improve service\nTechnology: Facial recognition\nIssue: Privacy\nTransparency: Governance; Privacy\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.thepaper.cn/newsDetail_forward_15837238\nhttps://www.globaltimes.cn/page/202112/1241489.shtml\nhttps://www.tianyancha.com/company/3262554286\nhttps://technode.com/2021/12/15/xpeng-motors-fined-by-chinese-watchdog-for-facial-recognition-breach/\nhttps://www.caixinglobal.com/2021-12-15/carmaker-xpeng-deletes-430000-photos-for-misuse-of-facial-recognition-101818294.html\nhttps://www.deccanherald.com/international/world-news-politics/china-fines-ev-firm-xpeng-motors-for-illegally-collecting-visitors-facial-images-1061030.html\nhttps://pandaily.com/xpeng-motors-fined-for-collecting-face-photos/\nhttps://www.sohu.com/a/508284884_116132\nRelated \ud83c\udf10\nKohler, BMW, Max Mara China shopper analysis facial recognition\nTencent 'Midnight Patrol' facial recognition\nPage info\nType: Incident\nPublished: December 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/facebook-political-ads-misidentification", "content": "Study: Facebook misidentifies 83 percent of political ads\nOccurred: December 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nFacebook missed 83 percent of political ads run on Facebook, according to a joint study between researchers at New York University and KU Leuven in Belgium.\nThe researchers examined 33.8 milion ads that ran on Facebook between July 2020 and February 2021, finding that in 189,000 cases when Facebook reviewed an ad to check whether it should be treated as political, it was wrong 83 percent of the time. \nThis included 117,000 cases where Facebook\u2019s detection system failed to flag ads that should have been treated as political, and 40,000 ads that were mistakenly flagged as political when they were not.\nThe researchers also identified over 70,000 political ads running on Facebook during a temporary ban on political advertising that Facebook had imposed during the US presidential election, many of them by organisations that had only run political ads on the platform.\nFacebook\u2019s performance varied across different countries. It had the most success in filtering ads correctly in the US and New Zealand, where only one percent of ads slipped through the net. However, Facebook achieved its worst score in Malaysia, where 45 percent of ads remained under the radar.\nThe researchers warned that these shortcomings could lead to political manipulation, as users who see ads without a political disclaimer may not be aware that their intent is to influence them. \nThey also noted that Facebook\u2019s enforcement of the policy relies heavily on detecting keywords in ads under an automated system, although staff also play a role in moderating the content.\nSystem \ud83e\udd16\nPolitical advertising authorization process\n\nDocuments \ud83d\udcc3\nFacebook (2018). The Authorization Process for US Advertisers to Run Political Ads on Facebook is Now Open\nMeta. Ads about social issues, elections or politics\nOperator: Meta/Facebook\nDeveloper: Meta/Facebook\nCountry: Argentina; Brazil; France; Macedonia; Malaysia; New Zealand; Portugal; Serbia; Turkey; USA\nSector: Politics\nPurpose: Authorise political advertising\nTechnology: Political advertising authorisation process\nIssue: Accuracy/reliability\nTransparency: Governance; Black box\nResearch, advocacy \ud83e\uddee\nCybersecurity for Demoracy (2022). Summary of findings: An audit of Facebook\u2019s political ad policy enforcement\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.straitstimes.com/tech/tech-news/facebook-misidentified-thousands-of-political-ads-study\nhttps://gizmodo.com/facebooks-political-ad-promises-mostly-miss-the-mark-s-1848188441\nhttps://www.brusselstimes.com/business/197144/facebook-very-poor-at-distinguishing-political-ads-ku-leuven-researchers-find\nhttps://www.npr.org/2021/12/09/1062516250/researchers-explain-why-they-believe-facebook-mishandles-political-ads\nhttps://www.ndtv.com/world-news/facebook-misidentified-thousands-of-political-advertisements-study-2644212\nhttps://www.protocol.com/policy/facebook-political-ad-study\nRelated \ud83c\udf10\nFacebook political group recommendations\nFacebook Georgia political partisanship\nPage info\nType: Issue\nPublished: December 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/life360-location-data-sharing", "content": "Life360 sold user location data, sparking controversy\nOccurred: December 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nFamily safety app Life360 collected and sold users' data to at least a dozen data brokers, prompting controversy about the company's transparency, ethics and approach to privacy. \nUsed by approximately 33 million people worldwide, Life360 enables friends and family members to share their exact location. However, it was identified as one of the largest sources of raw data for the location data industry, with X-Mode, Cuebiq, Allstate's Arity, Safegraph and other data brokers packaging and selling Life360 data 'to virtually anyone who wants to buy it'.\nThe controversy led to Life360 announcing that it would stop selling precise location data. This decision came after reports revealed that Life360 was not sufficiently anonymising the data it sold. The company\u2019s CEO, Chris Hulls, announced that Life360 would phase out all of its location data deals, except with Allstate\u2019s Arity. However, the app would continue to sell location data to Placer.ai, but in an aggregated rather than raw, precise form.\nDespite these changes, the company faced a proposed class-action lawsuit alleging it sold users\u2019 location data without permission. The suit was brought on behalf of a Florida minor and his family, who claimed they would not have used Life360 had they known about the data sales.\nSystem \ud83e\udd16\nLife360 website\nDocuments \ud83d\udcc3\nhttps://www.documentcloud.org/documents/21190590-life360-december-2021-quarterly-activities-report\nOperator: Life360; X-Mode; Cuebiq; Allstate/Arity; Safegraph\nDeveloper: Life360\nCountry: Global\nSector: Business/professional services\nPurpose: Track childrens' movements\nTechnology: Location tracking\nIssue: Ethics/values; Privacy; Security\nTransparency: Governance; Privacy\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nE.S. v. Life360 Inc.\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://themarkup.org/privacy/2021/12/06/the-popular-family-safety-app-life360-is-selling-precise-location-data-on-its-tens-of-millions-of-user\nhttps://themarkup.org/privacy/2022/01/27/life360-says-it-will-stop-selling-precise-location-data\nhttps://www.theverge.com/2021/12/9/22820381/tile-life360-location-tracking-data-privacy\nhttps://www.macrumors.com/2021/12/06/life360-selling-location-data-of-millions\nhttps://www.inputmag.com/tech/family-safety-app-life360-selling-exact-location-data-brokers\nhttps://thenextweb.com/news/family-safety-app-life360-selling-location-data-millions-users-syndication\nhttps://9to5mac.com/2021/12/06/tile-owner-life360-reportedly-sells-location-data-of-its-users-to-virtually-anyone/\nhttps://gizmodo.com/life360-the-company-buying-tile-is-purportedly-sellin-1848171116\nhttps://www.pocket-lint.com/apps/news/159314-tile-s-new-owner-life360-reportedly-sells-users-location-data-to-anyone\nRelated \ud83c\udf10\nTamoco location data sharing\nCrisis Text Line data sharing\nPage info\nType: Incident\nPublished: December 2021\nLast updated: February 2022", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/dennys-robot-server", "content": "Denny's robot server sparks employment concerns\nOccurred: November 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA video shot by a customer of a Denny's restaurant robot named 'Janet' serving breakfast went viral.\nThe video, shared by TikTok user @miabellaceo, gained over 470,000 views. It showed Sunny carrying plates of food on in-built shelves. While a waitress took orders and served coffee, Sunny was responsible for delivering the food.\nThe reactions to the video were mixed. Some users saw the benefits of robot servers, such as punctuality, lack of complaints, and absence of interpersonal drama. Others, however, were less receptive. Some users threatened to boycott Denny's or walk out if served by a robot. Concerns were also raised about job security, especially for long-term employees.\nThe use of robot servers came amidst a US national labor shortage, notably in the food industry. Denny's, among other restaurants, had turned to robot servers as a potential solution. \nThe controversy surrounding the robot server at Denny's highlighted the ongoing debate about the role of automation in the service industry.\nSystem \ud83e\udd16\nBear Robotics website\nOperator: Denny's\nDeveloper: Bear Robotics; Softbank\nCountry: USA\nSector: Food/food services\nPurpose: Serve food\nTechnology: Robotics\nIssue: Employment - jobs\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.tiktok.com/@miabellaceo/video/7032987655449218309\nhttps://futurism.com/robot-server-dennys \nhttps://www.foxbusiness.com/lifestyle/dennys-robot-server-goes-viral-tiktok\nhttps://www.dailydot.com/debug/tiktik-robot-dennys-labor-debate/\nhttps://www.newsweek.com/dennys-restaurant-robot-server-tiktok-video-1653569 \nhttps://thespoon.tech/the-media-was-fascinated-with-a-tiktok-video-of-a-robot-at-dennys-heres-what-it-means/ \nhttps://oaklandnewsnow.com/dennys-robot-server-goes-viral-on-tiktok-utah-news/ \nhttps://www.indy100.com/viral/robot-waiter-dennys-restaurant-tiktok-b1964639 \nhttps://thetakeout.com/dennys-robot-waiter-automation-server-replace-humans-la-1848132060 \nRelated \ud83c\udf10\nCaliBurger Flippy robot\nAmazon Astro home robot\nPage info\nType: Incident\nPublished: November 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/henan-foreign-journalist-student-surveillance", "content": "Henan detects and tracks foreign journalists, students, migrant women\nReleased: November 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe government of Henan province, China, was reported to be setting up - or to have set up - a facial recognition system to detect and track 'people of concern', including foreign journalists, students, and migrant women. \nThe system uses facial recognition technology supplied by local company Neusoft, and is connected to over 3,000 CCTV cameras across the province and to a number of national and regional database, as well as to China's national database. \nAccording to the Henan government's 200-page tender document, shared by video research organisation IPVM, the surveillance system uses a traffic light system to categorise targets by those considered to be of 'key concern', 'general concern', and 'not harmful'.\nForeign students will also be assessed and divided into three categories of risk. The safety assessment is made by focusing on the daily attendance of foreign students, exam results, whether they come from key countries, and school-discipline compliance.\nThe finding raised concerns about Chinese government surveillance and loss of privacy.\nSystem \ud83e\udd16\nNeusoft website\nDocuments \ud83d\udcc3\nHunan government tender (pdf)\nOperator: Henan Public Security Department\nDeveloper: Neusoft; Huawei\nCountry: China\nSector: Govt - police; Govt - security\nPurpose: Identify & track foreign journalists, students,'suspicious people'\nTechnology: Facial recognition\nIssue: Surveillance; Privacy\nTransparency: Privacy\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://ipvm.com/reports/henan-neusoft \nhttps://www.reuters.com/technology/exclusive-chinese-province-targets-journalists-foreign-students-with-planned-new-2021-11-29/\nhttps://www.theguardian.com/world/2021/nov/29/china-province-surveillance-system-journalists-students-henan\nhttps://uk.pcmag.com/security/137358/china-reportedly-plans-extensive-surveillance-of-journalists\nhttps://www.bbc.co.uk/news/technology-59441379\nhttps://www.washingtonpost.com/politics/2021/11/30/hackers-wanted-by-us-are-profiting-handsomely-russia/\nhttps://www.thedailybeast.com/dystopian-new-surveillance-system-will-scan-faces-of-journalists-in-china-documents-reveal\nRelated \ud83c\udf10\nBelgrade Safe City surveillance system\nUyghur emotion detection testing\nPage info\nType: Issue\nPublished: November 2021\nLast updated: May 2024", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-dsp-ans-rana-crash-liability", "content": "Amazon sued after DSP van seriously injures Tesla passenger Ans Rana\nOccurred: March 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAmazon was sued after one of its Delivery Service Partner (DSP) vans smashed into a Tesla outside Atlanta, Georgia, leaving the back seat passenger with serious brain and spinal-cord injuries.\nAns Rana accused Amazon of making its delivery drivers and partners work to unrealistic deadlines driven by the suite of algorithms, apps and devices the company uses to manage its logistics business. \nThe lawsuit (pdf) argued that Amazon closely monitored delivery drivers\u2019 every move including 'backup monitoring, speed, braking, acceleration, cornering, seatbelt usage, phone calls, texting, in-van cameras that use artificial intelligence to detect for yawning and more.'\nAmazon denied culpability, pushing the blame on to its delivery partner Harper Logistics and requested that any technological information be sealed by the courts and kept private, claiming that it qualified as protected trade secrets.\nAccording to Bloomberg, Amazon is facing 119 lawsuits involving injuries sustained with Amazon or DSP vehicles in 2021; media reports documented unsafe practices such as forcing Amazon delivery drivers to take dangerous routes.\nSystem \ud83e\udd16\nAmazon Flex website\nAmazon Logistics website\nOperator: Amazon; Harper Logistics\nDeveloper: Amazon\nCountry: USA\nSector: Transport/logistics\nPurpose: Manage package delivery\nTechnology: Amazon Flex app/algorithm\nIssue: Safety; Liability\nTransparency: Governance; Black box; Legal\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nAns Rana v Amazon Logistics, Harper Logistics (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bloomberg.com/news/features/2021-11-12/amazon-com-algorithms-blamed-in-crash-that-paralyzed-aspiring-doctor\nhttps://www.theverge.com/2021/11/14/22781896/go-read-this-amazon-tries-evade-responsibility-delivery-vehicle-crashes\nhttps://arstechnica.com/tech-policy/2021/11/amazon-liable-for-crash-because-software-micromanages-delivery-drivers-victim-says/\nhttps://www.businessinsider.com/amazon-van-crash-diver-surveillance-evidence-liability-lawsuit-2021-11\nhttps://www.manufacturing.net/video/video/21903203/lawsuit-says-amazon-liable-for-crash-that-paralyzed-man\nhttps://www.inputmag.com/culture/amazon-is-being-sued-over-accidents-involving-rushed-delivery-drivers\nhttps://www.wbur.org/hereandnow/2021/11/19/amazon-delivery-crash-lawsuit\nhttps://www.bnnbloomberg.ca/amazon-sued-over-crashes-by-drivers-rushing-to-make-deliveries-1.1680831\nhttps://www.insurancejournal.com/news/national/2021/11/22/642209.htm\nRelated \ud83d\uddde\ufe0f\nUber self-driving car pedestrian fatality\nSon Ji-chang Tesla Model X sudden acceleration\nPage info\nType: Incident\nPublished: November 2021\nLast updated: February 2023", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/zillow-offers-ibuying-zestimate-algorithm", "content": "Zillow Offers automated house flipping system is shut down\nOccurred: November 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nProperty and technology company Zillow Group shuttered its i-Buying home flipping business, saying it had lost USD 881 million on the business.\nZillow Offers used big data and automated valuation algorithms to make offers on homes across the US, with the aim of selling acquistions quickly for a profit. \nHowever, the company appeared to lose faith in the ability of its algorithm to make reliable predictions, including during so-called 'black swan' events such as COVID-19.\nIn a letter to shareholders, CEO Rich Barton explained Zillow Offers was 'too risky, too volatile to our earnings and operations', and provided 'too low of a return on equity opportunity, and too narrow in its ability to serve our customers.'\nZillow said it would lay off 2,000+ employees. The company was also hit with two class action lawsuits claiming it had misled investors about the true nature of its financial performance.\nSystem \ud83e\udd16\nZillow Wikipedia profile\n\nDocuments \ud83d\udcc3\nZillow Group Q3 2021 Shareholder Letter (pdf)\nZillow Group Q4 2021 Shareholder Letter (pdf)\nOperator: Zillow Group\nDeveloper: Zillow Group\nCountry: USA\nSector: Real estate\nPurpose: Estimate and predict real estate value\nTechnology: Automated valuation model\nIssue: Accuracy/reliability\nTransparency: Black box; Marketing\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nhttps://www.classaction.org/news/zillows-decision-to-wind-down-zillow-offers-after-q3-losses-sparks-stock-drop-class-action\nhttps://www.pr-inside.com/zillow-group-inc-nasdaq-z-zg-shareholder-class-action-alert-bernstein-r4862244.htm\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.cnbc.com/2021/11/02/zillow-shares-plunge-after-announcing-it-will-close-home-buying-business.html\nhttps://www.bloomberg.com/news/articles/2021-11-08/zillow-z-home-flipping-experiment-doomed-by-tech-algorithms\nhttps://www.cnet.com/personal-finance/mortgages/what-happened-at-zillow-how-a-prized-real-estate-site-lost-at-ibuying/\nhttps://www.wired.co.uk/article/zillow-ibuyer-real-estate\nhttps://www.economist.com/finance-and-economics/2021/11/13/a-whodunnit-on-zillow\nhttps://www.geekwire.com/2021/ibuying-algorithms-failed-zillow-says-business-worlds-love-affair-ai/\nhttps://www.fool.com/investing/2021/11/11/the-biggest-reason-zillow-went-sideways-in-ibuying/\nhttps://edition.cnn.com/2021/11/09/tech/zillow-ibuying-home-zestimate/index.html\nhttps://www.washingtonpost.com/opinions/2021/11/09/zillow-sent-its-algorithm-take-housing-market-housing-market-won/\nhttps://www.wsj.com/articles/zillow-offers-real-estate-algorithm-homes-ibuyer-11637159261\nhttps://www.americamagazine.org/politics-society/2021/11/17/zillow-housing-algorithm-241864\nhttps://www.businessinsider.com/zillow-offers-pause-ibuyers-homes-atlanta-phoenix-dallas-houston-minneapolis-2021-10\nhttps://www.foxbusiness.com/lifestyle/zillow-federal-lawsuit-home-flipping-business\nhttps://www.wsj.com/articles/zillows-shuttered-home-flipping-business-lost-881-million-in-2021-11644529656\nhttps://www.technologyreview.com/2022/04/13/1049227/house-flipping-algorithms-are-coming-to-your-neighborhood/\nRelated \ud83c\udf10\nYieldStar automated rent-setting\nUpstart automated consumer lending\nPage info\nType: Incident\nPublished: November 2021\nLast updated: February 2022", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tesla-model-y-fsd-beta-crash", "content": "Self-driving Tesla Model Y crashes in Brea, California\nOccurred: November 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe first crash of a Tesla using the company\u2019s Full Self-Driving (FSD) beta software took place, according to the US National Highway Traffic Safety Administration (NHTSA). \nAccording to the report, the crash took place on November 3 in Brea, California, and involved a Model Y vehicle in FSD mode that crashed after mistakenly turning into the wrong lane. The Tesla was then struck, leading to the car being 'severely damaged' on the driver\u2019s side. \nNobody was injured in the crash, according to the NHTSA. \nTesla\u2019s FSD Beta testing programme resulted in significant criticism with the company rolling out multiple updates as a result of reported bugs, a recall, and its decision to stifle untrained test drivers with NDAs. \nSystem \ud83e\udd16\nTesla Autopilot, Full-self Driving\nOperator: \nDeveloper: Tesla\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Self-driving system; Computer vision\nIssue: Safety; Accuracy/reliability\nTransparency: Black box; Legal\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nhttps://www.nhtsa.gov/vehicle/2021/TESLA/MODEL%252520Y%252520%2525205-SEAT/SUV/RWD\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.nbcnews.com/news/us-news/regulators-looking-complaint-teslas-full-self-driving-software-rcna5537\nhttps://jalopnik.com/the-first-crash-of-a-tesla-using-fsd-beta-may-have-happ-1848049816\nhttps://www.theverge.com/2021/11/12/22778135/tesla-full-self-driving-beta-crash-fsd-california\nhttps://electrek.co/2021/11/12/tesla-owner-claims-first-full-self-driving-beta-crash-strange-nthsa-complaint/\nhttps://gizmodo.com/teslas-full-self-driving-beta-appears-to-have-caused-it-1848049965\nhttps://www.teslarati.com/tesla-fsd-beta-crash-details-nhtsa/\nhttps://hypebeast.com/2021/11/tesla-model-y-crash-full-self-driving-beta-mode-cause-california\nRelated \ud83c\udf10\nSon Ji-chang Tesla Model X sudden acceleration\nUber self-driving car pedestrian fatality\nPage info\nType: Incident\nPublished: November 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/faciliti-automated-accessibility", "content": "FACIL\u2019iti legal threats result in automated website accessibility backlash\nOccurred: October 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nFrench web accessibility firm FACIL\u2019iti filed legal threats against web accessibility expert Julie Moynat and consultancy Koena, both of whom had been publicly critical of the company.\nMoynat and Koena had questioned the effectiveness of FACIL\u2019iti's automated accessibility overlay product, which promised instant, full accessibility and compliance with Web Content Accessibility Guidelines. They argued the company's marketing claims fail to match reality.\nThe legal threats, issued by FACIL\u2019iti's law firm HAAS Avocates, took the form of 'poursuites b\u00e2illons' (or legal gagging orders, roughly equivalent to a US SLAPP suit), and are typically used to intimidate or silence critics or opponents with the prospect of full legal proceedings.\nThe move sparked a backlash against the company and raised further questions about the efficacy of automated web accessibility products and the industry in general. It also prompted French trade union Cinov Numerique to issue a 'Motion of support' for Koena.\nSystem \ud83e\udd16\nFACIL\u2019iti website\nOperator: Julie Moynat, Koena\nDeveloper: FACIL\u2019iti\nCountry: France\nSector: Business/professional services\nPurpose: Improve website accessibility  \nTechnology: Web accessibility overlay\nIssue: Accuracy/reliability\nTransparency: Legal; Marketing\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nhttps://www.lalutineduweb.fr/en/help-lawyer-fees-faciliti-lawsuit/\nhttps://koena.net/en/koena-given-formal-notice-by-faciliti/\nhttps://koena.net/3-questions-a-yves-cornu-facil-iti/\nhttps://www.cinov-numerique.fr/motion-de-soutien-a-lentreprise-koena/\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.lflegal.com/2021/11/overlay-legal-update/\nhttps://adrianroselli.com/2021/11/overlays-underwhelm-web-directions-aaa-2021.html\nhttps://blog.empreintedigitale.fr/2021/06/01/outils-de-surcouche-daccessibilite-que-valent-ils-vraiment/\nhttps://seenthis.net/sites/7282461336007709057\nhttps://seenthis.net/sites/7282461336007709034\nRelated \ud83c\udf10\nAccessiBe automated web accessibility\nPage info\nType: Incident\nPublished: November 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/accessibe-automated-accessibility", "content": "EyeBobs settles over controversial AccessiBe AI accessibility overlay\nOccurred: October 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nOnline eyewear retailer Eyebobs settled with a customer who had sued the company for failing to provide users of its website with screen readers of equal access to its services, and for misleading marketing.\nPlaintiff Anthony Hammond Murphy, a blind man, alleged that Eyebobs\u2019 website was inaccessible to blind individuals using screen reader software, which he claimed violated the effective communication and equal access requirements of Title III of the Americans with Disabilities Act (ADA).\nBut Eyebobs had installed an accessibility overlay on their digital platform developed by AccessiBe, an Israel-based web accessibility start-up that claimed to make websites for people with disabilities easier to use by providing an 'automated, state-of-the-art AI technology'. The overlay was supposed to automatically bring the website into compliance with the ADA by resolving underlying accessibility issues. However, the plaintiff claimed that the overlay failed to provide full and equal access to screen reader users.\nThe lawsuit was filed as a class action, representing an entire class of people who may have been similarly limited by Eyebobs\u2019 website inaccessibility, and was settled with a consent decree from the United States District Court for the Western District of Pennsylvania2. As part of the settlement, Eyebobs agreed to an extensive accessibility programme with mandatory reporting and was required to pay USD 16,000 in legal fees.\nThis case had significant implications for digital accessibility and the use of AI technology like AccessiBe\u2019s overlay solution.\n\u2795 Over 600 blind people, accessibility advocates and software developers had previously signed an open letter calling on website operators to stop using AccessiBe and similar companies.\nSystem \ud83e\udd16\nAccessiBe website\nAccessiBe Wikipedia profile\nOperator: Eyebobs, Masterbuilt Manufacturing\nDeveloper: AccessiBe\nCountry: USA, Israel\nSector: Business/professional services\nPurpose: Improve website accessibility\nTechnology: Web accessibility overlay\nIssue: Accuracy/reliability\nTransparency: Legal; Marketing\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nhttps://www.adatitleiii.com/wp-content/uploads/sites/121/2021/01/Murphy-v.-Eyebobs.pdf\nhttps://www.eyebobsadasettlement.com/docs/Eyebobs%20-%20Settlement%20Agreement.pdf\nhttps://www.scribd.com/document/490740167/Exhibit-A-for-21-cv-00017\nResearch, advocacy \ud83e\uddee\nOverlay Fact Sheet\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.law.com/thelegalintelligencer/2021/10/07/judge-approves-ada-class-action-settlement-against-eyewear-company-over-websites-lack-of-visibility/\nhttps://www.nbcnews.com/tech/innovation/blind-people-advocates-slam-company-claiming-make-websites-ada-compliant-n1266720\nhttps://www.wired.com/story/company-tapped-ai-website-landed-court/\nhttps://www.forbes.com/sites/gusalexiou/2021/10/28/why-automated-tools-alone-cant-make-your-website-accessible-and-legally-compliant/\nhttps://www.lflegal.com/2021/11/overlay-legal-update/\nhttps://uxdesign.cc/important-settlement-in-an-ada-lawsuit-involving-an-accessibility-overlay-748a82850249\nRelated \ud83c\udf10\nFACIL\u2019iti automated web accessibility\nPage info\nType: Incident\nPublished: November 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/hebron-palestinian-facial-recognition-surveillance", "content": "Israel use of facial recognition to monitor Hebron Palestinians raises human rights concerns\nReleased: November 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe use of facial recognition and other AI tools and techniques by Israel's defence forces to monitor Palestinians violates their basic rights, according to a 82-page report by human rights organisation Amnesty.\nIsrael was found to be using AI-based surveillance technologies including 'Red Wolf', a facial recognition system installed at checkpoints which automatically dentifies Palestinians, and 'Blue Wolf', a smartphone-based facial recognition tool rolled out late 2020 that performs much the same function across Hebron and east Jerusalem. \nThe two systems link to 'Wolf Pack', a database containing profiles of Palestinians in the West Bank and east Jerusalem, including photographs, family histories, education, and security ratings. \nWolf Pack enables Israeli soldiers to identify and detain Palestinians before they have presented their ID cards, and allows Israel to 'consolidate existing practices of discriminatory policing, segregation, and curbing freedom of movement,' Amnesty said.\n\u2795 November 2021. The Washington Post reported the existence of the 'Wolf Pack' and 'Blue Wolf' systems, suggesting they may form part of 'Hebron Smart City', a broader real-time system comprising closed-circuit TV, computer vision, and movement sensors that tracks Palestinians across the city and, according to a former IDF soldier, in their homes. \nHebron Smart City also reputedly includes 'White Wolf', an app used by security officials in West Bank settlements to identify Palestinians before they enter 'illegal' settlements to work.\nSystem \ud83e\udd16\nRed Wolf\nBlue Wolf\nDocuments \ud83d\udcc3\nIsrael Defence Forces (2020). Hebron Smart City\nOperator: Israel Defense Forces (IDF)\nDeveloper: \nCountry: Israel; Palestine\nSector: Govt - defence; Govt - security; Govt - police\nPurpose: Identify & track Palestinians\nTechnology: Facial recognition\nIssue: Privacy; Surveillance\nTransparency: Governance; Privacy\nResearch, advocacy \ud83e\uddee\nAmnesty International (2023). Automated Apatheid\nWho Profits (2018). \u201cBig Brother\u201d in Jerusalem\u2019s Old City: Israel\u2019s Militarized Visual Surveillance System in Occupied East Jerusalem (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.nytimes.com/2023/05/01/technology/israel-palestine-facial-recognition.html\nhttps://foreignpolicy.com/2022/02/21/palestine-israel-ai-surveillance-tech-hebron-occupation-privacy/\nhttps://www.newarab.com/analysis/how-ai-big-tech-and-spyware-power-israels-occupation\nhttps://www.thedailybeast.com/israels-tech-spying-on-palestinians-would-shame-dictatorships\nhttps://www.washingtonpost.com/world/middle_east/israel-palestinians-surveillance-facial-recognition/2021/11/05/3787bf42-26b2-11ec-8739-5cb6aba30a30_story.html\nhttps://forward.com/opinion/478846/israel-new-surveillance-technology-occupation-palestinians/\nhttps://www.jpost.com/israel-news/more-details-published-on-idf-use-of-facial-recognition-in-west-bank-684379\nhttps://www.timesofisrael.com/idf-building-facial-recognition-database-of-palestinians-in-hebron-report/\nhttps://www.yenisafak.com/en/world/israel-deploys-sweeping-facial-recognition-tech-in-west-bank-report-3583763\nhttps://www.haaretz.com/israel-news/israel-surveils-palestinians-in-west-bank-in-massive-facial-recognition-program-1.10363514\nRelated \ud83c\udf10\nOosto/AnyVision facial recognition drones\nHyderabad police facial recognition\nPage info\nType: Incident\nPublished: November 2021\nLast updated: November 2023", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/s-korea-immigration-facial-recognition-sharing", "content": "South Korea immigration shares travellers' facial data without consent\nOccurred: October 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nDocuments released by South Korea's government revealed the country's Ministry of Justice was sharing the facial images and personal details of travellers using Seoul's Incheon International Airport with local commercial companies.\nThe data involved approximately 170 million travellers, and was obtained without their consent. It was being shared to help develop a government system for screening and identifying travellers.\nSouth Korea's Personal Information Protection Act determined that the sharing of sensitive personal data with third parties requires the specific and separate consent of the data subject.\nLocal civil liberty activists threatened to take the government to court on the issue.\nSystem \ud83e\udd16\nUnknown\nOperator: Ministry of Justice (MOJ); Ministry of Science and ICT (MSIT); Incheon International Airport\nDeveloper: CUbox  \nCountry: South Korea\nSector: Govt - immigration\nPurpose: Identify travellers; Predict security breaches\nTechnology: Facial recognition\nIssue: Privacy; Dual/multi-use\nTransparency: Governance; Privacy\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.hani.co.kr/arti/english_edition/e_national/1016107.html\nhttps://english.hani.co.kr/arti/english_edition/e_national/1016279.html\nhttp://english.hani.co.kr/arti/english_edition/e_national/1018763.html\nhttps://www.vice.com/en/article/xgdxqd/south-korea-is-selling-millions-of-photos-to-facial-recognition-researchers\nhttps://www.biometricupdate.com/202110/seoul-shares-face-biometrics-of-170m-travelers-with-private-firms\nhttps://www.biometricupdate.com/202111/rights-groups-demand-halt-to-south-korea-facial-recognition-surveillance-project\nhttps://www.reddit.com/r/tech/comments/qfe996/s_korean_government_provided_170m_facial_images/\nhttp://world.kbs.co.kr/service/news_view.htm?Seq_Code=165030\nhttps://aitopics.org/doc/news:9ED1FC50\nhttps://www.20minutes.fr/high-tech/3162131-20211101-la-coree-du-sud-fournit-170-millions-d-images-faciales-a-des-entreprises-privees\nRelated \ud83c\udf10\nCBSA Toronto Pearson airport facial recognition\nWellington International Airport facial recognition\nPage info\nType: Incident\nPublished: October 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tesla-fsd-beta-software-glitch-recall", "content": "Tesla recalls 11,700 cars due to FSD beta software glitch\nOccurred: November 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nTesla was forced to recall over 11,700 cars in the US due to a glitch in the beta version of its Full Self-Driving (FSD) system. \nThe software glitch could cause a false forward-collision warning or unexpected activation of the automatic emergency braking (AEB) system, potentially increasing the risk of a collision. \nTesla stated that it was not aware of any crashes or injuries relating to the issue, and released an over-the-air software update to address it. As of October 29, 2021, more than 99.8 pecent of vehicles had installed the update. \nTesla had uninstalled FSD 10.3 following driver reports of inadvertent activation of their cars' automatic emergency braking system.\nThe recall highlighted ongoing concerns about the robustness and safety of Tesla's self-driving software.\nSystem \ud83e\udd16\nTesla Autopilot, Full-self Driving\nOperator: \nDeveloper: Tesla\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Self-driving system\nIssue: Robustness; Safety\nTransparency: Black box; Legal\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nNHTSA (2021). Part 573 Safety Recall Report (pdf) \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://gizmodo.com/tesla-forced-to-recall-11-704-vehicles-over-full-self-d-1847981141\nhttps://www.theguardian.com/technology/2021/nov/02/tesla-recall-nearly-12000-us-vehicles-software-glitch\nhttps://www.reuters.com/business/autos-transportation/tesla-recalling-nearly-12000-us-vehicles-over-software-communication-error-2021-11-02/\nhttps://www.thedrive.com/news/42968/tesla-recalls-nearly-12000-cars-after-faulty-fsd-beta-update-causes-braking-glitch\nhttps://www.unilad.co.uk/technology/tesla-safety-concerns-forces-safety-recall-of-12000-cars/\nhttps://techxplore.com/news/2021-11-tesla-bug-self-driving-alarms.html\nhttps://www.silicon.co.uk/e-innovation/artificial-intelligence/tesla-recalls-12000-cars-425140\nhttps://uk.news.yahoo.com/tesla-recalls-11-704-vehicles-133556674.html\nRelated \ud83c\udf10\nTesla FSD Assertive Mode\nTesla FSD beta test car hits bollard, driver fired\nPage info\nType: Issue\nPublished: November 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/wellington-international-airport-facial-recognition", "content": "Wellington International Airport facial recognition trial is exposed\nOccurred: October 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nNew Zealand's Wellington International Airport quietly trialled a facial recognition system despite being warned not to by the country's Office of the Privacy Commissioner of New Zealand. \nHaving started the trial in June, the airport reportedly used the system to count how many passengers passed through security, and how long they spend queuing.\nThe Privacy Commissioner argued the tool's impact on privacy outweighed its benefits, and could lead to its use by other agencies, including law enforcement or intelligence agencies.\nThe Commissioner also criticised New Zealand's Aviation Security Service's decision not to issue a press release about the new technology, or to include information about it on the airport website.\nSystem \ud83e\udd16\nAvsec website\nOperator: Wellington International Airport\nDeveloper: Aviation Security (Avsec)\nCountry: New Zealand\nSector: Transport/logistics\nPurpose: Assess security queues\nTechnology: Facial recognition\nIssue: Privacy; Appropriateness/need; Dual/multi-use\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.stuff.co.nz/national/politics/300420063/secretive-facial-recognition-trial-at-wellington-airport-went-against-privacy-commissioners-advice\nhttps://www.rnz.co.nz/national/programmes/middayreport/audio/2018814656/facial-recognition-trial-not-needed-says-privacy-commissioner\nhttps://www.biometricupdate.com/202110/nz-airport-face-biometrics-trial-deployed-against-privacy-commissioner-advice\nhttps://www.biometricupdate.com/202110/face-biometrics-roll-out-at-airports-on-three-continents-amid-ethics-oversight-debates\nhttps://www.stuff.co.nz/technology/digital-living/126691808/privacy-watch-how-to-keep-big-brother-at-bay\nhttps://iapp.org/news/a/nz-airport-defies-privacy-commissioner-with-facial-recognition-test/\nhttps://www.reddit.com/r/newzealand/comments/pyt2r1/secretive_facial_recognition_trial_at_wellington/\nRelated \ud83c\udf10\nCBSA Toronto Pearson airport facial recognition\nS Korea immigration facial recognition data sharing\nPage info\nType: Incident\nPublished: October 2021\nLast updated: January 2023", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/adobe-sensei-project-morpheus", "content": "Adobe Project Morpheus slammed for deepfake uses\nReleased: October 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nProject Morpheus, Adobe's prototype video version of the Neural Filters machine learning-based image-editing tools that the company released as part of Photoshop 22.0, raised concerns due to its potential misuse in creating lifelike deepfake videos.\nLike Neural Filters, Morpheus enables users to alter someone's age and facial expressions, whilst adding the ability to change facial hair and glasses.\nBut the software raised concerns about the ease with which life-like deepfake videos could be created to malign, undermine or otherwise damage the interests or reputation of an individual, group or society.\nIn a promotional video, Project Morpheus software developer Han Gao claimed that Morpheus was an experiment, and may not end up as part of Photoshop or another Adobe product.\nSystem \ud83e\udd16\nProject Morpheus website\n\nDocuments \ud83d\udcc3\nAdobe MAX #MorpheusSneak\nOperator: Adobe\nDeveloper: Adobe\nCountry: USA\nSector: Technology\nPurpose: Manipulate video\nTechnology: Deepfake - video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning \nIssue: Dual/multi-use\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theverge.com/2021/10/27/22748508/adobe-deepfake-tool-max-project-morpheus\nhttps://en.brinkwire.com/technology/experts-say-adobes-new-project-morpheus-is-a-deepfake-tool-heres-why-this-editing-software-might-be-bad/\nhttps://www.inputmag.com/tech/adobe-is-toying-around-with-deepfake-tech-for-photoshop\nhttps://www.engadget.com/adobe-max-sneaks-project-morpheus-183702995.html\nhttps://www.techtimes.com/articles/267199/20211027/adobes-new-project-morpheus-deepfake-tool-experts-claim-heres-why.htm\nhttps://www.europe1.fr/emissions/L-innovation-du-jour/projet-morpheus-dadobe-il-sera-bientot-aussi-simple-de-retoucher-des-videos-que-de-simples-photos-4074091\nhttps://feber.se/pc/adobe-visar-upp-project-morpheus/431145/\nhttps://www.silicon.co.uk/e-regulation/surveillance/location-breach-huq-424261/amp\nhttps://onezero.medium.com/the-future-of-images-is-trust-nothing-5c20b95ea0d1\nRelated \ud83c\udf10\nCambodia torture victims' photo manipulation\nLinkedIn deepfake salespeople\nPage info\nType: Issue\nPublished: October 2021\nLast updated: May 2024", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/santo-robot-catholic-priest", "content": "SanTO robot Catholic priest accused of heresy, blasphemy\nOccurred: March 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nSanTo, a seventeen-inch tall 'Catholic robot' equipped with a computer, microphone, sensors and a facial recognition-enabled camera prompted accusations of heresy and blasphemy. \nDeveloped by Gabriele Trovato, a roboticist and assistant professor at Japan\u2019s Waseda University, SanTO is a repurposed domestic robot that typically perform religious rites such as delivering sermons and providing basic religious advice  in churches, temples and other religious institutions.\nSanTo's introduction prompted accusations of heresy and blasphemy and sparked ethical concerns about the dangers of nudging, AI bias, and privacy. Others scoffed at SanTo's ability to answer anything other than the most basic questions. \nThe controversy highlighted the challenges and debates surrounding the integration of advanced technology into religion. \n\u2795 In October 2021, SanTo was featured in a BBC documentary in St. John Paul II Catholic Church in in the Bemowo district of Warsaw.\nSystem \ud83e\udd16\nSanTO robot website\nOperator: St. John Paul II Church, Warsaw\nDeveloper: Gabriele Trovato\nCountry: Poland  \nSector: Religion\nPurpose: Teach prayer; Provide advice\nTechnology: Robotics; Facial recognition\nIssue: Appropriateness/need; Ethics/values\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://waseda.pure.elsevier.com/en/publications/communicating-with-santo-the-first-catholic-robot\nhttps://www.wsj.com/articles/deus-ex-machina-religions-use-robots-to-connect-with-the-public-11553782825\nhttps://www.catholicnewsagency.com/news/42302/in-robota-christi-why-robots-can-never-be-catholic-priests%C2%A0\nhttps://www.irishcatholic.com/robot-priests-and-other-heresies/\nhttps://www.eurasiareview.com/19092019-in-robota-christi-why-robots-can-never-be-catholic-priests/\nhttps://www.vox.com/future-perfect/2019/9/9/20851753/ai-religion-robot-priest-mindar-buddhism-christianity\nhttps://www.youtube.com/watch?v=JE85PTDXARM\nhttps://www.thefirstnews.com/article/sermon-giving-robotic-priest-arrives-in-poland-to-support-faithful-during-pandemic-25688\nhttps://www.lifesitenews.com/news/746892/\nhttps://medium.com/in-our-times/meet-your-maker-the-robot-priests-taking-the-world-by-storm-32b2e398383\nhttps://www.analyticsinsight.net/ai-and-robotics-next-generation-religious-priests-for-worshippers/\nResearch, advocacy \ud83e\uddee\nTrovato G. et al (2023). Retrospective Insights on the Impacts of the Catholic Robot SanTO\nRelated \ud83c\udf10\nS\u00e3o Geraldo Magela drone Eucharist delivery\nMicrosoft reincarnation chatbot\nPage info\nType: Issue\nPublished: October 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/s%C3%A3o-geraldo-magela-drone-delivery", "content": "Eucharist delivery drone sparks religious controversy\nOccurred: April 2018\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe use of a drone fitted with a monstrance to deliver the Eucharist to the altar at a church in Brazil caused religious outrage.\nA priest at S\u00e3o Geraldo Magela church in Soracaba, Brazil, has used a drone fitted with a monstrance to deliver the Eucharist to the altar at S\u00e3o Geraldo Magela in in Soracaba, Brazil.\nThe congregation reputedly supported the novelty, but, once shared on Facebook, religious conservatives complained that the act was 'inappropriate, 'scandalous', and a 'profanation'. Priest John Zuhlsdorf labelled the stunt 'sacrilegious silliness', according to the Catholic Herald.\nThe incident highlighted questions about the appropriate use of AI in religion.\nSystem \ud83e\udd16\n\nOperator: S\u00e3o Geraldo Magela church, Sorocaba\nDeveloper: Unknown\nCountry: Brazil\nSector: Religion\nPurpose: Deliver Eucharist\nTechnology: Drone\nIssue: Ethics/values\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.aciprensa.com/noticias/polemica-por-procesion-de-la-eucaristia-con-un-dron-en-brasil-video-31699\nhttp://www.catholicherald.co.uk/news/2018/04/03/parish-criticised-after-flying-blessed-sacrament-from-drone/\nhttps://boingboing.net/2018/04/04/people-pissed-at-parish-for-us.html\nhttps://futurism.com/drone-deliver-eucharist-brazil\nhttps://inews.co.uk/news/priests-backlash-eucharist-church-drone-141283\nhttps://geekologie.com/2018/04/so-weve-come-to-this-video-of-a-quadroco.php\nhttps://www.churchpop.com/2018/04/05/sacrilegious-eucharistic-procession-with-drone-caught-on-video-goes-viral/\nhttps://usa.inquirer.net/12396/holy-mass-jesus-sacrifice-truly-one\nhttps://www.bostonglobe.com/ideas/2018/04/06/innovation-week-eucharist-drone/Z2hquYGpMcaw9jsTRTRMlL/story.html\nhttps://cathnews.co.nz/2018/04/12/drone-delivers-host/\nhttps://www.nine.com.au/entertainment/viral/drone-holy-sacrament-church-brazil/4a7c84ab-7acb-4179-891f-26de163684aa\nRelated \ud83c\udf10\nSanTO robot Catholic priest\nMicrosoft reincarnation chatbot\nPage info\nType: Incident\nPublished: March 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/india-citizenship-law-protest-surveillance", "content": "New Delhi police use facial recognition to monitor India citizenship law protests\nOccurred: February 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nNew Delhi police used facial recognition and drones to identify and detain protestors marching against India's Citizenship Law, prompting accusations of privacy abuse and unnecessary surveillance.\nCritics argued that the use of the system for profiling and surveillance at public congregations wass illegal and unconstitutional, and that it directly impaired the rights of ordinary Indians from assembly, speech, and political participation.\nThe Delhi Police defended their use of the technology, stating it was based on credible intelligence inputs about possible disruptions. They assured that best industry standard checks and balances against any potential misuse of data were in place.\nThis event occurred amidst nationwide protests against a new citizenship law, during which at least 26 people were killed. The law was said to marginalise Muslims.\nSystem \ud83e\udd16\nAutomated Facial Recognition System (AFRS)\nOperator: Delhi Police; Uttar Pradesh Police\nDeveloper: Innefu Labs, Staqu\nCountry: India\nSector: Govt - police\nPurpose: Identify criminals, protestors\nTechnology: Facial recognition\nIssue: Privacy; Surveillance\nTransparency: Governance; Privacy\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reuters.com/article/us-india-citizenship-protests-technology-idUSKBN20B0ZQ\nhttps://www.thehindu.com/news/cities/Delhi/1100-rioters-identified-using-facial-recognition-technology-amit-shah/article31044548.ece\nhttps://www.dw.com/en/protesters-in-india-object-to-facial-recognition-expansion/a-52412455\nhttps://www.smh.com.au/world/asia/controversy-over-india-s-use-of-facial-recognition-during-protests-20200217-p541pp.html\nhttps://www.biometricupdate.com/202003/minister-says-more-than-1900-delhi-rioters-identified-by-facial-biometrics\nRelated \ud83c\udf10\nHyderabad police facial recognition\nTek Fog political manipulation, harassment\nPage info\nType: Incident\nPublished: February 2020", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/huq-gps-location-data-sharing", "content": "Huq GPS location data sharing\nOccurred: October 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nUK-based docation data merchant Huq received GPS coordinates even when mobile users have explicitly opted-out of the collection of this data on individual Android apps, according to app analysis company AppCensus and Vice's Motherboard.\nHuq collects and processes over one billion 'mobility events' collected from apps on people's phones apps every day and sells it to clients, including dozens of English and Scottish city councils.\nThe apps in question measured Wi-Fi strength, scanned barcodes, amongst other things, but appeared to have shared user data for purposes other than those stated, according a Vice report.\nThe company appeared unaware of the vulnerability, but the findings raised questions about the nature and effectiveness of it's customer/partner compliance checks, which the company said it ran monthly.\nCivil rights and privacy advocates suggested the problem reflected poor data sharing practices and governance by the advertising and marketing industry.\nHuq, a British firm that sells people\u2019s location data, admitted to a privacy breach where some of its data was obtained without user permission1. \nIn two instances, Huq\u2019s app partners did not seek consent from users1. The company stated that these were \u201ctechnical breaches\u201d of data privacy requirements and that the issues had been rectified1. Huq asked the app partners to correct their code and republish their apps1.\nThe apps in question, one measuring Wi-Fi strength and another scanning barcodes, were highlighted in a story published by Vice1. The story questioned the clarity for users that apps downloaded for one purpose were sharing information for a completely different one1.\nHuq did not rule out the possibility that other apps may have failed to ask for proper consent1. The company emphasized the importance of consent as a vital pillar of data collection and stated that they always act swiftly in case of a breach1.\nAppCensus, a company that analyses the privacy of apps, found variations in how users are notified about their GPS location tracker and home Wi-Fi router data being collected across apps that include Huq1. According to analysis from Danish TV2, Android apps are more likely to pass on location data than those on iPhones1.\n\nSystem \ud83e\udd16\nHuq website\nOperator: Huq Industries\nDeveloper: Huq Industries, Kaibits Software, AppSourceHub\nCountry: UK\nSector: Technology\nPurpose: Track user location\nTechnology: Location tracking algorithms\nIssue: Privacy\nTransparency: Governance; Privacy\nInvestigations, assessments, audits \ud83e\uddd0\nAppCensus (2021). What the Huq?\nVICE News (2021). Location Data Firm Got GPS Data From Apps Even When People Opted Out\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/technology-59063766\nhttps://www.heise.de/news/Populaere-Apps-leaken-Bewegungsprofile-6228973.html\nhttps://informationsecuritybuzz.com/expert-comments/location-data-collection-firm-admits-privacy-breach/\nhttps://iapp.org/news/a/location-data-firms-technical-breach-from-lack-of-user-consent/\nhttps://yro.slashdot.org/story/21/10/28/1919238/location-data-firm-got-gps-data-from-apps-even-when-people-opted-out\nRelated \ud83c\udf10\nTamoco location data sharing\nCrisis Text Line data sharing\nPage info\nType: Incident\nPublished: October 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tamoco-location-data-sharing", "content": "Tamoco sale of Norwegians' location data enables citizen, military tracking\nOccurred: May 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nNUK location data broker Tamoco sold large volumes of personally identifiable data, sparking controversy about the intrusiveness of its practices. \nNorwegian broadcaster NRK bought 460 million rows of raw location data for 140,000 mobile devices for GBP 3,000, enabling the broadcaster to identify individuals and even track military personnel.\nNorwegian data protection authority Datatilsynet subsequently condemned the sale of Norwegians' location data, and alerted other EU data protection authorities, including the UK Information Commissioner's Office ('ICO'), which subsequently announced its own investigation into Tamoco.\nTamoco says on on its website that it is a data privacy-aware geospatial company that accurately understands device location with unprecedented accuracy. The controversy raised questions about the effectiveness of these privacy measures. \n\u2795 October 2021. The ICO reprimanded Tamoco for 'failing to provide sufficient privacy information to UK citizens'.\nSystem \ud83e\udd16\nTamoco website\n\nDocuments \ud83d\udcc3\nhttps://www.tamoco.com/blog/tamocos-full-response-to-questions-on-articles-published-in-nrk/\nOperator: Tamoco\nDeveloper: Tamoco\nCountry: Norway, UK\nSector: Technology\nPurpose: Assess & enhance location data\nTechnology: Location tracking\nIssue: Privacy\nTransparency: Governance; Privacy\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nhttps://www.datatilsynet.no/aktuelt/aktuelle-nyheter-2020/datatilsynet-ser-narmere-pa-lokasjonssporing/\nhttps://www.datatilsynet.no/aktuelt/aktuelle-nyheter-2020/datatilsynene-i-norge-og-storbritannia-samarbeider-om-sporingssak/\nInvestigations, assessments, audits \ud83e\uddd0\nhttps://www.nrk.no/norge/xl/avslort-av-mobilen-1.14911685\nhttps://www.nrk.no/norge/datatilsynet-opnar-gransking-etter-nrk-avsloring-1.15011535\nhttps://www.nrk.no/norge/mobilsporing_-britisk-dataselger-varsler-intern-gransking-etter-nrk-avsloring-1.15031158\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.nationen.no/motkultur/leder/for-darlig-personvern/\nhttps://www.dr.dk/nyheder/penge/norsk-medie-afsloerede-datafirmaers-handel-med-nordmaends-personlige-data-nu-gaar-det\nhttps://www.forbrukerradet.no/side/asking-companies-to-clarify-surveillance-of-consumers/\nhttps://www.dataguidance.com/news/norway-datatilsynet-issues-statement-selling-location\nhttps://www.dday.it/redazione/37823/lincredibile-storia-di-un-uomo-che-ha-spiato-le-app-che-lo-spiavano-la-denuncia-siamo-tutti-spiati\nhttps://news.bloomberglaw.com/privacy-and-data-security/norway-probes-u-s-u-k-companies-for-alleged-privacy-breaches\nhttps://www.bbc.co.uk/news/technology-59063766\nhttps://themarkup.org/privacy/2021/09/30/theres-a-multibillion-dollar-market-for-your-phones-location-data\nRelated \ud83c\udf10\nHuq GPS location data sharing\nMeituan location tracking\nPage info\nType: Incident\nPublished: April 2021\nLast updated: October 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/meituan-location-tracking", "content": "Meituan criticised for 'intensive' location tracking\nOccurred: October 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nChinese e-commerce company Meituan came under fire for 'intensive' location tracking of its users. \nA local gadget review blogger discovered that the app had been tracking his location even when he was not using it, and had taken to social media to complain. His post quickly went viral.\nMeituan was also dragged into the spotlight by Wang Sicong, online influencer and son of Dalian Wanda Group\u2019s founder. Wang claims he was locked out of his account on Meituan's Dazhong Dianping restaurants and review service, when it was linked to another user\u2019s mobile phone number.\nThe company\u2019s practices sparked controversy about its approach to user privacy, and a broader discussion about data rights and the extent to which technology companies should be allowed to track user location. \n\u2795 October 2021. Meituan was fined Yuan 3.44 billion (USD 534 million) by Chinese regulators for antitrust violations.\nSystem \ud83e\udd16\nMeituan website\nMeituan Wikipedia profile\nOperator: Meituan Dazhong\nDeveloper: Meituan Dazhong\nCountry: China\nSector: Technology\nPurpose: Track user location\nTechnology: Location tracking\nIssue: Privacy; Security  \nTransparency: Governance; Privacy\nInvestigations, assessments, audits \ud83e\uddd0\nhttps://weibo.com/5299071024/KC2TtiVce?from=page_1005055299071024_profile&wvr=6&mod=weibotime&type=comment#_rnd1633921677591\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.ithome.com/0/579/893.htm\nhttps://technode.com/2021/10/11/meituan-faces-data-privacy-controversy-after-antitrust-fine/\nhttps://www.verdict.co.uk/china-meituan-clickfood-crackdown-533m-fine-sends-shares-up-by-8/\nhttps://kr-asia.com/major-mobile-apps-in-china-extensively-mine-personal-data-sparking-social-media-outrage\nhttps://thewisemarketer.com/loyalty-newswire/loyalty-newswire-october-11th-2021/\nhttps://www.scmp.com/tech/big-tech/article/3151942/chinese-tycoons-socially-influential-son-adds-meituans-antitrust-woes\nRelated \ud83c\udf10\nHuq GPS location data sharing\nTamoco location data sharing\nPage info\nType: Incident\nPublished: October 2021\nLast updated: June 2024", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/cruise-driverless-cars-traffic-blocking", "content": "Cruise driverless cars block traffic for two hours\nOccurred: June 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nNearly twenty Cruise driverless cars stood still and blocked traffic for two hours on a street in central San Francisco, USA, causing tailbacks, making the area impassable and inhibiting night street-sweeping. \nThe cars were reputedly disabled after they had lost touch with a Cruise server. The incident was resolved when the cars were manually moved out of the way by Cruise employees.\nPer WIRED, a Cruise employee later sent an anonymous letter to the California Public Utilities Commission, claiming that Cruise loses communication with the automated vehicles 'with regularity', blocking traffic and potentially hindering emergency vehicles.\n\u2795 April 2022. Twitter users posted videos of a Cruise vehicle moving away from San Francisco police after they had tried to find put whether the car had a driver.  \nSystem \ud83e\udd16\nCruise website\nCruise Wikipedia profile\nOperator: GM Cruise\nDeveloper: GM Cruise; General Motors/Chevrolet\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Self-driving system; Computer vision\nIssue: Accuracy/reliability; Safety\nTransparency: Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.sfexaminer.com/news/fleet-of-cruise-driverless-cars-blocked-traffic-for-hours-tuesday-night/article_57e8d2c8-f8c4-11ec-b99d-33fd6c31cb3e.html\nhttps://www.reddit.com/r/sanfrancisco/comments/vnmpf1/bunch_of_cruise_cars_stuck_on_gough_by_robin/\nhttps://www.sfgate.com/local/article/Cruise-driverless-cars-block-traffic-SF-17279744.php\nhttps://www.wired.com/story/cruises-robot-car-outages/\nhttps://www.cnbc.com/2022/07/01/self-driving-cars-from-gms-cruise-block-san-francisco-streets.html\nhttps://sfist.com/2022/07/08/gm-cruise-robotaxis-froze-and-blocked-sf-streets-far-more-frequently-than-we-knew-one-time-nearly-60-cars-stopped-at-once/\nhttps://techcrunch.com/2022/06/30/cruise-robotaxis-blocked-traffic-for-hours-on-this-san-francisco-street/\nhttps://www.engadget.com/cruise-driverless-taxis-blocked-san-francisco-traffic-for-hours-robotaxi-gm-204000451.html\nhttps://www.thedrive.com/news/a-swarm-of-self-driving-cruise-taxis-blocked-san-francisco-traffic-for-hours\nhttps://www.bloomberg.com/news/articles/2022-07-14/gm-s-cruise-faces-government-board-scrutiny-after-snafus\nRelated \ud83c\udf10\nCruise driverless car police inspection\nPony.ai driverless test crash\nPage info\nType: Incident\nPublished: November 2022", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/facebook-meaningful-social-interactions-algorithm", "content": "Facebook pushes users into seeing more provocative, negative content\nOccurred: October 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nLeaked documents reveal Facebook deliberately pushed users into seeing more emotional, provocative, and negative content in order to deepen engagement and drive more revenue.\nBetween 2017 and 2020, an update to Facebook's 'Meaningful Social Interactions' (MSI) algorithm saw the weighting of angry emoji substantially increased.\nThe update was made despite Facebook researchers having warned senior leadership at the company, including CEO Mark Zuckerberg, that it would likely lead to higher levels of spam, misinformation and disinformation, and abuse.\nThe update also had the effect of undermining efforts by the company's content moderators and integrity teams to manage toxic and harmful content.\nThe leaked documents were shared with the SEC and the US Congress by whistleblower Frances Haugen. \nSpeaking in the UK parliament, Haugen argued 'anger and hate is the easiest way to grow on Facebook.'\nSystem \ud83e\udd16\nMeaningful Social Interactions\nFacebook Feed/News Feed\nFacebook Feed Wikipedia profile\nOperator: Meta/Facebook\nDeveloper: Meta/Facebook\nCountry: USA; Global\nSector: Technology\nPurpose: Increase engagement, revenue\nTechnology: Content ranking system\nIssue: Ethics/values; Hate speech/violence; Mis/disinformation\nTransparency: Governance; Black box; Marketing\nInvestigations, assessments, audits \ud83e\uddd0\nhttps://www.washingtonpost.com/technology/2021/10/25/what-are-the-facebook-papers/\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.washingtonpost.com/technology/2021/10/26/facebook-angry-emoji-algorithm/\nhttps://www.wsj.com/articles/facebook-algorithm-change-zuckerberg-11631654215\nhttps://www.dailymail.co.uk/news/article-10132759/Facebooks-algorithm-promoted-toxic-hateful-content.html\nhttps://www.poynter.org/commentary/2021/the-most-damning-facebook-story-yet/\nhttps://theweek.com/facebook/1006422/facebook-reportedly-gave-the-angry-emoji-5-times-as-much-weight-as-a-like\nhttps://thehill.com/policy/technology/578548-facebook-formula-gave-anger-five-times-weight-of-likes-documents-show\nhttps://nymag.com/intelligencer/2021/10/what-was-leaked-in-the-facebook-papers.html\nhttps://www.inputmag.com/culture/facebook-weighed-emoji-reactions-much-heavier-than-a-like\nhttps://www.niemanlab.org/2021/10/more-internal-documents-show-how-facebooks-algorithm-prioritized-anger-and-posts-that-triggered-it/\nhttps://fortune.com/2021/10/26/facebook-zuckerberg-why-were-so-afraid/\nhttps://mashable.com/article/facebook-reactions-news-feed\nhttps://www.forbes.com/sites/amitchowdhry/2017/03/02/facebook-confirms-emoji-reactions-affects-your-news-feed/\nRelated \ud83c\udf10\nFacebook downranking system failure\nAmazon US own brand search engine rigging\nPage info\nType: Incident\nPublished: October 2021\nLast updated: January 2022", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/twitter-right-wing-content-amplification", "content": "Twitter algorithms boost right-wing news, commentary\nOccurred: October 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nTwitter discovered that its algorithms disproportionately boosted right-wing news and commentary content. \nCovering politicians\u2019 tweets and political content from Canada, France, Germany, Japan, Spain, the UK and US from April 1 to August 2020, Twitter found that right-wing content was more amplified algorithmically relative to a reverse-chronological timeline in every country, other than Germany.\nTwitter director of software engineering Rumman Chowdury and machine learning researcher Luca Belli admitted it is unclear why its algorithms behaved this way. \n'Further root cause analysis', they say, 'is required in order to determine what, if any, changes are required to reduce adverse impacts by [Twitter's] Home timeline algorithm'. \nSystem \ud83e\udd16\nTwitter website\nTwitter Wikipedia profile\nDocuments \ud83d\udcc3\nTwitter (2021). Examining algorithmic amplification of political content on Twitter\nTwitter (2021). Algorithmic Amplification of Politics on Twitter (pdf)\nOperator: Twitter\nDeveloper: Twitter\nCountry: Canada; France; Germany; Japan; Spain; UK; USA\nSector: Politics\nPurpose: Recommend content\nTechnology: Recommendation algorithm\nIssue: Bias/discrimination - politics\nTransparency: Governance; Black box; Complaints/appeals\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.washingtonpost.com/business/2021/10/22/twitter-algorithm-right-leaning/\nhttps://www.inputmag.com/tech/twitter-has-no-clue-why-its-algorithms-amplify-right-leaning-content\nhttps://www.theguardian.com/technology/2021/oct/22/twitter-admits-bias-in-algorithm-for-rightwing-politicians-and-news-outlets\nhttps://www.irishtimes.com/business/media-and-marketing/twitter-admits-bias-in-algorithm-for-right-wing-sources-1.4707981\nhttps://www.newsweek.com/twitter-reveals-algorithm-amplified-right-wing-content-despite-claims-liberal-bias-1641780\nhttps://www.businessinsider.com/twitter-says-algorithm-biased-toward-right-wing-politicians-conservatives-2021-10\nhttps://www.euronews.com/2021/10/22/twitter-admits-its-algorithms-amplify-right-wing-politicians-and-news-content\nhttps://www.theregister.com/2021/10/25/twitters_political_bias/\nRelated \ud83c\udf10\nFacebook Georgia political partisanship\nInstagram, Twitter Palestinian discrimination\nPage info\nType: Incident\nPublished: October 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/the-facetag", "content": "Facial recognition-based FaceTag student networking app prompts backlash\nReleased: October 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA networking app that enabled users to scan fellow students' faces using facial recognition and to exchange contact details prompted a backlash amongst Harvard students, social media users, digital rights advocates and commentators.\nHarvard student Yuen Ler Chow created The Facetag, an imitation Facebook app intended to facilitate student networking. Despite providing a privacy feature whereby a user must give permission before someone else accesses their profile, and storing user data on Google Cloud, the app has raised privacy, ethics, and security concerns.\n'It's kinda weird how I see so many people scared over the fact that I'm collecting this data, but almost all the other social media apps collect way, way more' he argues.\nChow claims 'FaceTag is the next Facebook.' 'I\u2019m Zuck, but better' he claims, saying he aims to expand FaceTag to other colleges and communities.\nThe app closed shortly after the fracas.\nSystem \ud83e\udd16\nThe FaceTag website\nOperator: Yuen Ler Chow\nDeveloper: Yuen Ler Chow  \nCountry: USA\nSector: Technology\nPurpose: Scan human faces\nTechnology: Facial recognition\nIssue: Privacy; Security\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.businessinsider.com/harvard-freshman-facetag-stokes-facial-recognition-debate-tiktok-mark-zuckerberg-2021-10\nhttps://www.thecrimson.com/article/2021/10/21/facetag-better-than-zuck/\nhttps://inside.com/campaigns/inside-ai-2021-10-22-29891/sections/256085\nhttps://baomoi.com/mot-ung-dung-cua-sinh-vien-harvard-dang-gay-tranh-cai-ve-dao-duc/c/40653595.epi\nRelated \ud83c\udf10\nFacebook South Korea facial recognition abuse\nPimEyes facial recognition search engine\nPage info\nType: Incident\nPublished: October 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/gorillas-rider-work-schedule-automation", "content": "Gorillas 'Project Ace' rider work schedule automation backfires\nOccurred: June-December 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGerman grocery delivery service Gorillas use of algorithmic automation to intensify work schedules and get delivery riders to fulfill more orders contributed directly to worker discontent, strikes, and firings. \nGorillas 'Project Ace' used an algorithm to calculate the times in which most workers were needed, resulting in shorter, more irregular shifts. It also ignored the eleven-hour rest time mandated under German law.\nGorillas described itself as a 'counter-model to the gig economy'. Unlike some of its competitors, it employed riders and warehouse staff. But wages were low, salaries often paid late, safety regarded as inadequate, and employees' probationary period was a full six months - the maximum allowed under German law. \nFurthermore, Gorillas workers limited access to management, and 'Rider Support', which was supposed to take care of employee concerns, has no telephone number and, according to employees, left emails unanswered for days.\nOver 350 employees were reportedly fired for striking.\nSystem \ud83e\udd16\nGorillas website\nGorillas Wikipedia profile\nOperator: Gorillas\nDeveloper: Gorillas\nCountry: Germany\nSector: Transport/logistics\nPurpose: Automate work scheduling\nTechnology: Scheduling algorithm\nIssue: Accuracy/reliability; Fairness\nTransparency: Governance; Complaints/appeals \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.klassegegenklasse.org/gorillas-in-unlimited-strike/\nhttps://www.tagesspiegel.de/berlin/arbeitskampf-beim-berliner-start-up-streikende-gorillas-rider-legen-zwei-lagerhaeuser-lahm/27676182.html\nhttps://www.jungewelt.de/artikel/411648.arbeitskampf-pranke-von-gorillas.html\nhttps://techxplore.com/news/2021-06-gorilla-tactics-berlin-delivery-riders.html\nhttps://braveneweurope.com/gig-economy-project-berlin-mass-firings-at-gorillas-in-response-to-wild-cat-strikes\nhttps://www.berliner-zeitung.de/en/unrest-in-unicorn-country-li.177200\nhttps://sifted.eu/articles/gorillas-workers-tensions/\nhttps://sifted.eu/articles/gorillas-wework/\nhttps://www.dw.com/en/gorillas-delivery-service-fires-back-over-workers-strike/a-59445809\nhttps://www.902.gr/eidisi/kosmos/273642/apergies-ton-dianomeon-stin-etaireia-gorillas\nhttps://www.vice.com/en/article/7kvgmd/gorillas-delivery-app-fires-workers-for-striking\nhttps://www.euronews.com/next/2021/10/08/gorillas-delivery-app-fires-hundreds-of-berlin-workers-for-strikes-over-pay-and-working-co\nRelated \ud83c\udf10\nUber UpFront Fares driver pay algorithm\nAmazon Mentor delivery driver scoring\nPage info\nType: Incident\nPublished: October 2021\nLast updated: June 2024", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-india-search-rigging", "content": "Amazon India accused of rigging search engine to promote 'own' brands\nOccurred: October 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAmazon India was accused of running a 'systematic' programme of copying other companies' products and promoting them by manipulating its own search results. \nReuters obtained thousands of pages of internal Amazon documents that appear to show Amazon India employees used 'search seeding', 'search sparkles' and other techniques so that the company\u2019s own label product lines, such as AmazonBasics and Solimo, would appear 'in the first 2 or three \u2026 search results' on Amazon.in.\nThe documents reveal how Amazon\u2019s private-brands team in India secretly exploited internal data from Amazon.in to copy products sold by other companies, and then offered them on its platform. The employees also manipulated Amazon\u2019s search results so that the company\u2019s products would appear in the first 2 or three search results when customers were shopping on Amazon.in.\nA popular shirt brand in India, John Miller, was a victim of Amazon's strategy, with the company deciding to \u201cfollow the measurements of\u201d John Miller shirts down to the neck circumference and sleeve length.\nAmazon denied the accusations and stated that their policy strictly prohibits the use or sharing of non-public, seller-specific data for the benefit of any seller, including sellers of private brands.\n\u2795 \nSystem \ud83e\udd16\nAmazon India website\nAmazon Wikipedia profile\nOperator: Amazon\nDeveloper: Amazon\nCountry: India\nSector: Retail\nPurpose: Rank content/search results\nTechnology: Search engine algorithm\nIssue: Ethics/values; Competition/price fixing; Copyright\nTransparency: Governance; Complaints/appeals; Black box; Marketing\nInvestigations, assessments, audits \ud83e\uddd0\nReuters (2021). Amazon copied products and rigged search results to promote its own brands, documents show\nThe Markup (2021). Amazon puts its own brands above better rated products\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.msn.com/en-us/money/companies/amazon-copied-products-rigged-search-to-push-own-brands-reuters/ar-AAPvqQZ\nhttps://www.cnbc.com/2021/10/13/amazon-india-reportedly-copied-products-and-rigged-search-results.html\nhttps://www.theverge.com/2021/10/13/22724152/amazon-india-search-result-rigging-reference-product-seller-data-report\nhttps://www.forbes.com/sites/lisakim/2021/10/13/amazon-reportedly-copied-products-and-manipulated-search-results-to-benefit-its-own-products-in-india/\nhttps://www.cnet.com/tech/amazon-copied-products-and-manipulated-search-results-in-india-report-says/\nhttps://uk.finance.yahoo.com/news/u-senator-warren-urges-amazon-090209461.html\nhttps://www.engadget.com/amazon-india-products-copy-search-results-manipulation-174030250.html\nhttps://itwire.com/strategy/amazon-accused-of-copying-products,-rigging-search-results-in-india.html\nhttps://www.tribuneindia.com/news/business/amazon-india-copied-products-and-rigged-search-results-to-promote-its-own-brands-documents-show-324066\nRelated \ud83c\udf10\nAmazon US own brand search engine rigging\nNaver own brand search engine rigging\nPage info\nType: Incident\nPublished: October 2021\nLast updated: March 2022", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/dubai-usd-35m-voice-cloning-fraud", "content": "Scammers use cloned voice to steal USD 35m from Dubai company\nOccurred: October 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nDubai investigators discovered an elaborate scam in which deepfake technology was used to clone the voice of a company director and defraud his company of USD 35 million.\nAbout to make an acquisition, the company needed its bank to authorise a series of transfers. A bank employee was fooled by the deepfaked voice into authorising the transfer of the cash, believing it was a legitimate business transaction. \nUAE authorities reckon the scheme involved at least 17 people. \nIt is the second known case of fraudsters using deep voice tools to carry out a heist, and much the largest. \nSystem \ud83e\udd16\nUnknown\nOperator:  \nDeveloper:  \nCountry: UAE/Dubai\nSector: Banking/financial services\nPurpose: Defraud\nTechnology: Deepfake - audio; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning \nIssue: Ethics; Security\nTransparency: Governance\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nhttps://www.documentcloud.org/documents/21085009-hackers-use-deep-voice-tech-in-400k-theft\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.forbes.com/sites/thomasbrewster/2021/10/14/huge-bank-fraud-uses-deep-fake-voice-tech-to-steal-millions/\nhttps://screenrant.com/ai-deepfake-cloned-voice-bank-scam-theft-millions/\nhttps://www.unite.ai/deepfaked-voice-enabled-35-million-bank-heist-in-2020/\nhttps://gizmodo.com/bank-robbers-in-the-middle-east-reportedly-cloned-someo-1847863805\nhttps://gadgettendency.com/fraudsters-steal-35-million-from-a-bank-in-the-uae-with-the-help-of-a-diplomatic-voyage/\nhttps://www.unite.ai/deepfaked-voice-enabled-35-million-bank-heist-in-2020/\nRelated \ud83c\udf10\nDubai deepfake court evidence\nChina taxation department deepfake fraud\nPage info\nType: Incident\nPublished: October 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/nhgsfp-school-meal-fingerprint-biometrics", "content": "NHGSFP collection of Nigerian students' fingerprints sparks controversy\nOccurred: August 2021-\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe collection of fingerprints of 642,000 primary school pupils in Nigeria sparked controversy about the programme's necessity and intrusiveness.\nPart of the roll-out of the Nigeria's federal government's National Home Grown School Feeding Programme (NHGSFP), the programme aimed to provide school meals to students and accelerate school enrollment and retention, whilst boosting local food production and economies. \nCritics asked by why fingerprinting was necessary, especially for children under 5 years old, other than to verify pupils' identities. \nQuestions were also asked why the collection of biometric data started in Borno State, which at the time was plagued by Boko Haram violence.\nSystem \ud83e\udd16\nHID Global website\nOperator: Ministry of Humanitarian Affairs; Disaster Management and Social Development\nDeveloper: HID Global; Plovtech\nCountry: Nigeria\nSector: Education\nPurpose: Verify identity\nTechnology: Fingerprint biometrics\nIssue: Privacy; Security; Dual/multi-use\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.vanguardngr.com/2021/08/fg-commences-biometric-data-capture-of-642000-pupils-into-home-grown-school-feeding/\nhttps://guardian.ng/news/fg-embarks-on-biometric-enumeration-of-pupils-on-school-feeding-programme/\nhttps://www.biometricupdate.com/202110/nigeria-collects-biometrics-of-elementary-school-children-for-meal-program\nhttps://www.biometricupdate.com/202107/nigeria-to-add-6m-students-to-biometric-database-for-school-meal-program\nhttps://promptnewsonline.com/school-feeding-fg-begins-verification-of-196873-pupils-in-nasarawa/\nRelated \ud83c\udf10\nGdansk Primary School No. 2 meal payment verification\nNorth Ayrshire school meal payment verification\nPage info\nType: Issue\nPublished: August 2021\nLast updated: June 2024", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/gdansk-primary-school-no-2-meal-payment-verification", "content": "Gdansk Primary School fined for using fingerprint data to verify meal payments\nOccurred: March 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGdansk Primary School No. 2, in northern Poland, was fined EUR 4,600 by UODO, the country's data privacy regulator, for processing students' fingerprint data to verify their school meal payments. \nThe school had been using fingerprints to verify whether pupils had paid for their meals since 2015, with system being used on 680 children in the current academic year, while four children were processed using 'an alternative identification system'.\nAccording to UODO, the system was unfair, and 'significantly disproportionate' to the task. It also said that other, more appropriate, forms of identification could have bene used.  \nThe President of UODO ordered the erasure of the students' fingerprint processed and the cessation of any further collection of personal data.\n\u2795 October 2021. Several schools in North Ayrshire, Scotland, attracted the ire of the UK data privacy regulator for using facial recognition to verify school meal payments.\nSystem \ud83e\udd16\n\nOperator: Gdansk Primary School No. 2\nDeveloper: Unknown\nCountry: Poland\nSector: Education\nPurpose: Verify meal payments\nTechnology: Fingerprint biometrics\nIssue: Privacy; Appropriateness/need\nTransparency: Governance\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nhttps://edpb.europa.eu/news/national-news/2020/fine-processing-students-fingerprints-imposed-school_en\nhttps://www.uodo.gov.pl/en/553/1102\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.dataguidance.com/news/poland-uodo-fines-school-pln-20000-processing-childrens-biometric-data\nhttps://venturebeat.com/2020/03/06/polish-school-hit-with-gdpr-fine-for-using-fingerprints-to-verify-students-lunch-payments/\nhttps://identityweek.net/polish-school-fined-for-processing-students-fingerprints/\nhttps://www.itgovernance.eu/blog/en/polish-school-fined-for-processing-childrens-biometric-data\nRelated \ud83c\udf10\nNHGSFP school meal fingerprint biometrics\nNorth Ayrshire school meal payment verification\nPage info\nType: Incident\nPublished: March 2020\nLast updated: November 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/masayuki-nakamoto-deepfake-uncensored-pornography", "content": "Masayuki Nakamoto arrested for selling deepfaked uncensored pornography\nOccurred: October 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nJapanese police arrested a man who admitted to sharing and selling pornography that had been deepfaked to clarify censored, pixellated genitalia. \nAccording to The Mainichi, Masayuki Nakamoto sold over 2,500 doctored video files for 11 million yen (roughly USD 96,000) using TecoGAN, a deep learning video clarification tool, to unblur genitalia.\nNakamoto was arrested for breaking Japan's copyright law and obscenity law, which bans the display of 'indecent materials'.\nSystem \ud83e\udd16\nTecoGAN\nOperator: Masayuki Nakamoto\nDeveloper: Masayuki Nakamoto\nCountry: Japan\nSector: Media/entertainment/sports/arts\nPurpose: Drive sales\nTechnology: Deepfake - audio; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Copyright; Ethics/values; Privacy\nTransparency: Privacy\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://mainichi.jp/english/articles/20211019/p2a/00m/0na/008000c\nhttps://www.inputmag.com/culture/japan-arrested-a-man-who-de-pixelated-porn-using-deepfake-tech\nhttps://gizmodo.com/deepfaking-genitalia-into-blurred-porn-leads-to-mans-ar-1847893570\nhttps://nextshark.com/japanese-man-unpixelate-porn-deepfake/\nhttps://www.vice.com/en/article/xgdq87/deepfakes-japan-arrest-japanese-porn\nhttps://techstory.in/japanese-man-arrested-for-using-deepfake-to-de-pixelate-porn-content/\nhttps://yro.slashdot.org/story/21/10/20/2113246/man-arrested-for-uncensoring-japanese-porn-with-ai-in-first-deepfake-case\nRelated \ud83c\udf10\nXiao Yu deepfake pornography\nLauren Book deepfake nude extortion\nPage info\nType: Incident\nPublished: October 2021\nLast updated: October 2022", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/xiao-yu-deepfake-pornography", "content": "Taiwanese arrested, jailed for creating and selling deepfake pornography\nOccurred: October 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nChu Yu-chen (\u6731\u7389\u5bb8), 26, aka 'Xiaoyu', and his assistant, Chuang Hsin-jui (\u838a\u7098\u777f), were arrested for creating, selling and distributing over 100 porn videos of Taiwanese celebrities and politicians. \nAccording to Taiwan's Criminal Investigation Bureau, the three made over NTD 13 million in a past year by superimposing peoples' faces without their consent on to existing pornographic videos using deepfake technology.\nIn addition to selling deepfakes of public figures on a series of Telegram groups, Chu is said to have accepted commissions to make deepfake videos of lesser-known individuals as a form of 'revenge porn.'\nKaohsiung City Councilor Huang Jie (\u9ec3\u6377), one of the people whose faces were used in the videos without their consent, told the Taipei Times that she \"felt disgusted and afraid when she saw the videos herself, and indicated that the videos had caused real harm and humiliation to the victims.\"\n\u2795 July 2022. Chu and Chuang were sentenced to five years and six months and three years and eight months in prison respectively for violations of Taiwan's Personal Data Protection Act. The sentences were commutable to fines. \nTwenty-one other victims filed civil lawsuits seeking financial compensation from Chu and Chuang for misuse of their likenesses. \n\u2795 January 2023. Taiwan's national legislature passed legislation that made the production and spread of fake or manipulated images and video for profit a crime punishable by up to seven years in prison. \nThe amendments to the Criminal Code included an additional article dedicated to a new form of crime using artificial intelligence \u2014 deepfakes \u2014 which involve inserting the likeness of a person into an existing image or video.\nThe legislation had been proposed by politician Kao Chia-yu (\u9ad8\u5609\u745c), also a victim of Xiaoyu.\nSystem \ud83e\udd16\nUnknown\nOperator: Chu Yu-chen ('Xiaoyu')\nDeveloper: Chu Yu-chen ('Xiaoyu')\nCountry: Taiwan\nSector: Media/entertainment/sports/arts\nPurpose: Create entertainment\nTechnology: Deepfake - video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning \nIssue: Privacy; Ethics\nTransparency: Privacy\nInvestigations, assessments, audits \ud83e\uddd0\nMirror Media. Deepfake Taiwan\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.ettoday.net/news/20211018/2103658.htm%23ixzz79cLID5Jw\nhttps://newbloommag.net/2021/10/19/deepfake-arrest/\nhttps://www.taipeitimes.com/News/front/archives/2021/10/20/2003766430\nhttps://focustaiwan.tw/society/202110190012\nhttps://www.taiwannews.com.tw/en/news/4318972\nhttps://www.icrt.com.tw/info_details.php?mlevel1=6&mlevel2=12&news_id=212039\nhttps://hype.my/2021/250030/youtuber-xiao-yu-deepfakes-videos-porno-internet-celebrities/\nhttps://thediplomat.com/2021/12/domestic-abuse-incident-highlights-taiwans-struggles-with-misogyny/\nhttps://focustaiwan.tw/society/202301050022\nhttps://www.cna.com.tw/news/asoc/202301050129.aspx\nhttps://www.taipeitimes.com/News/front/archives/2023/01/08/2003792190\nRelated \ud83c\udf10\nLauren Book deepfake nude extortion\nMasayuki Nakamoto deepfake uncensored porn\nPage info\nType: Incident\nPublished: October 2021\nLast updated: January 2023", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/north-ayrshire-school-meal-payment-verification", "content": "North Ayrshire schools rapped for facial recognition meal payments\nOccurred: October 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe use of facial recognition for take payments for school lunches by nine schools in Ayrshire, Scotland, attacted controversy as intrusive and disproportionate.\nCBD Cunninghams, the company that developed the system, claimed it speeded up queues and protected students better against COVID-19 than the card payments and fingerprint scanners the schools used previously.\nPrivacy advocates responded by saying the new system was not needed, operated without explicit consent, and amounted to the de facto normalisation of facial recognition.\nThe North Ayshire Council (NAC) programme was suspended after the UK Information Commissioner\u2019s Office (ICO) responded to the controversy by encouraging schools to take a 'less intrusive' approach where possible. \n\u2795 February 2023. The ICO informed (pdf) NAC that it 'is likely to have infringed data protection law'.\nSystem \ud83e\udd16\nCRB Cunninghams website\n\nDocuments \ud83d\udcc3\nhttps://www.north-ayrshire.gov.uk/Documents/EducationalServices/facial-recognition-parental-flyer.pdf\nOperator: North Ayrshire Council (NAC)\nDeveloper: Vesta Software Group/CRB Cunninghams\nCountry: UK\nSector: Education\nPurpose: Verify meal payments\nTechnology: Facial recognition\nIssue: Appropriateness/need; Privacy\nTransparency: Privacy\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nInformation Commissioner's Office (2021). North Ayrshire Council\u2019s use of Facial Recognition Technology in its schools (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.ft.com/content/af08fe55-39f3-4894-9b2f-4115732395b9\nhttps://metro.co.uk/2021/10/17/scotland-facial-recognition-software-being-used-in-north-ayrshire-schools-15437868/\nhttps://www.theverge.com/2021/10/18/22732330/uk-schools-facial-recognition-lunch-payments-north-ayrshire\nhttps://www.euronews.com/next/2021/10/18/schools-in-scotland-start-using-facial-recognition-on-children-paying-for-lunch\nhttps://news.sky.com/story/facial-recognition-used-to-take-payments-from-school-children-12437234\nhttps://www.irvinetimes.com/news/19654518.north-ayrshire-facial-recognition-schools-pay-lunch/\nhttps://www.politico.eu/article/facial-recognition-cameras-in-uk-schools-raises-concerns-about-divergence-from-eu-rules/\nhttps://www.theguardian.com/education/2021/oct/18/privacy-fears-as-schools-use-facial-recognition-to-speed-up-lunch-queue-ayrshire-technology-payments-uk\nhttps://www.bbc.co.uk/news/technology-59037346\nhttps://www.biometricupdate.com/202302/scottish-schools-canteen-facial-recognition-likely-infringed-gdpr-ico\nRelated \ud83d\uddde\ufe0f\nNHGSFP school meal fingerprint biometrics\nGdansk Primary School No. 2 meal payment verification\nPage info\nType: Incident\nPublished: October 2021\nLast updated: February 2023", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-us-own-brand-search-engine-rigging", "content": "Amazon US accused of rigging search engine to promote 'own' brands\nOccurred: October 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAmazon regularly placed its own brand products above those of competitor products, according to an investigation by The Markup. \nBased on data provided by 3,400 product searchers in January 2021, the report found that Amazon brands and exclusives received an 'outsized portion of the top spot on search results' while only accounting for a small proportion of products on the company's platform.\nThe Markup also conducted a survey showing that most US consumers are unable to identify Solimo, Pinzon, and other top-selling house brands, many of which are not clearly labelled as owned by Amazon. \nOver 150 Amazon house brands and 137,000 unique house brand and exclusive products were identified during the investigation.\n\u2796  June 2019. The US House Judiciary Committee antitrust subcommittee investigated (pdf) competitive practices at Amazon, Alphabet, Apple, and Facebook. During the hearings, Amazon associate general counsel Nate Sutton argued that Amazon algorithms 'are optimised to predict what customers want to buy regardless of the seller.'\n\u2795 October 2021. The day before The Markup's investigation was published, Reuters revealed Amazon documents that show Amazon has been copying products on its marketplace in India and covertly rigging its search engine algorithm to boost them. \n\u2795 March 2022. the US House of Representatives\u2019 antitrust subcommittee accused (pdf) Amazon of lying to Congress about how it treats third-party sellers, including whether it preferences its own brands and exclusive products in search results. It also asked (pdf) the Department of Justice to investigate Amazon for 'potentially criminal' obstruction of Congress.\n\u2795 April 2022. The WSJ reported that the US SEC is investigating how Amazon discloses its business practices, including its use of third-party-seller data for its private-label business.\nSystem \ud83e\udd16\nAmazon US website\nAmazon Wikipedia profile\nDocuments \ud83d\udcc3\nAmazon (2019). Responses to Questions for the Record following the July 16, 2019, Hearing of the Subcommittee on Antitrust, Commercial, and Administrative Law, Committee on the Judiciary, Entitled \u201cOnline Platforms and Market Power, Part 2: Innovation and Entrepreneurship\u201d (pdf)\nOperator: Amazon\nDeveloper: Amazon\nCountry: USA\nSector: Retail\nPurpose: Rank content/search results\nTechnology: Search engine algorithm\nIssue: Ethics; Competition/price fixing; IP abuse/misuse\nTransparency: Governance; Complaints/appeals; Black box; Marketing\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nhttps://judiciary.house.gov/uploadedfiles/competition_in_digital_markets.pdf\nhttps://docs.house.gov/meetings/JU/JU05/20200729/110883/HHRG-116-JU05-20200729-SD008.pdf\nhttps://joeyzwillinger.medium.com/dear-mr-bezos-e691f6d6d705\nhttps://judiciary.house.gov/uploadedfiles/hjc_referral_--_amazon.pdf\nhttps://judiciary.house.gov/uploadedfiles/letter_-_amazon_misrepresentations_-_10.18.21.pdf\nInvestigations, assessments, audits \ud83e\uddd0\nhttps://themarkup.org/amazons-advantage/2021/10/14/amazon-puts-its-own-brands-first-above-better-rated-products\nhttps://themarkup.org/amazons-advantage/2021/10/14/how-we-analyzed-amazons-treatment-of-its-brands-in-search-results\nhttps://themarkup.org/amazons-advantage/2021/10/14/when-amazon-takes-the-buy-box-it-doesnt-give-it-up\nhttps://twitter.com/themarkup/status/1448619835966038018\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.cnet.com/tech/amazon-reportedly-favors-its-own-brands-in-search-results/\nhttps://www.seattletimes.com/business/amazon/u-s-senators-to-introduce-antitrust-legislation-after-reports-say-amazons-marketplace-is-unfair-to-sellers/\nhttps://www.theverge.com/2021/10/14/22726897/amazon-apple-google-app-stores-marketplace-antitrust-competition-klobuchar-grassley\nhttps://www.cnbc.com/video/2021/10/14/being-an-amazon-brand-is-the-most-important-factor-to-get-top-ranking-on-amazon.html\nhttps://www.wsj.com/articles/members-of-congressional-committee-question-if-amazon-executives-misled-congress-11634551201?mod=djemalertNEWS\nhttps://www.cnbc.com/2019/09/25/allbirds-co-ceo-joey-zwillinger-suing-amazon-over-shoes-is-risky.html\nhttps://www.bbc.co.uk/news/business-58961836\nhttps://www.wsj.com/articles/amazon-scooped-up-data-from-its-own-sellers-to-launch-competing-products-11587650015\nhttps://themarkup.org/amazons-advantage/2022/03/09/house-antitrust-committee-accuses-amazon-of-lying-to-congress-asks-doj-to-investigate\nhttps://themarkup.org/amazons-advantage/2021/10/18/citing-markup-investigation-lawmakers-demand-answers-from-amazon#:~:text=In%20the%20wake%20of%20an,Congress%E2%80%9D%20and%20demanding%20that%20the\nhttps://www.wsj.com/articles/sec-is-investigating-how-amazon-disclosed-business-practices-11649271819?mod=djemalertNEWS\nRelated \ud83c\udf10\nAmazon India own brand search rigging\nCoupang own brand search engine rigging\nPage info\nType: Incident\nPublished: October 2021\nLast updated: March 2022", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-ring-video-doorbell-neighbour-privacy-invasion", "content": "Amazon Ring video doorbell ruled to invade neighbour privacy\nOccurred: October 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA judge has ruled that Amazon-manufactured CCTV cameras and a Ring doorbell installed by Jon Woodard, a houseowner in Oxfordshire, UK, 'unjustifiably invaded' the privacy and contributed to the harassment of his neighbour Dr Mary Fairhurst.\nDr Fairhurst had claimed that the devices installed on Mr Woodard's house and garden shed captured images of her house, garden and parking space, and recorded personal conversations. \nAmazon customers were unable to switch off the audio recording facility on Ring doorbells until 2020, with the company requested that customers 'respect their neighbours' privacy, and comply with any applicable laws when using their Ring device.'\n\nSystem \ud83e\udd16\nAmazon Ring website\nAmazon Ring Wikpedia profile\nOperator: Amazon\nDeveloper: Amazon\nCountry: UK\nSector: Consumer goods\nPurpose: Strengthen security\nTechnology: CCTV; Computer vision\nIssue: Privacy; Surveillance\nTransparency: Privacy\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nFairhurst v Woodard\nInvestigations, assessments, audits \ud83e\uddd0\nNYU School of Law Policing Project. Ring Neighbors Civil Rights & Civil Liberties Audit (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/technology-58911296\nhttps://www.oxfordmail.co.uk/news/19644337.amazon-ring-doorbell-breached-thame-neighbours-privacy-judge-rules/\nhttps://www.msn.com/en-gb/news/world/amazon-ring-doorbells-e2-80-98unjustifiably-invaded-e2-80-99-neighbour-e2-80-99s-privacy-judge-rules/ar-AAPz4Zi\nhttps://www.msn.com/en-gb/money/technology/ring-doorbell-is-the-smart-camera-doorbell-a-breach-of-privacy-uk-ring-doorbell-court-case-explained/ar-AAPyW8M\nhttps://gizmodo.com/amazons-ring-doorbell-can-violate-your-neighbors-privac-1847868545\nhttps://www.techradar.com/uk/news/court-rules-that-ring-video-doorbell-invaded-neighbors-privacy\nhttps://www.theguardian.com/uk-news/2021/oct/14/amazon-asks-ring-owners-to-respect-privacy-after-court-rules-usage-broke-law\nhttps://uk.finance.yahoo.com/news/amazon-ring-doorbell-how-data-breach-ruling-may-impact-you-130237079.html\nhttps://www.theregister.com/2021/10/13/amazon_ring_audio_recording_data_protection/\nhttps://www.dailymail.co.uk/news/article-10085561/A-victory-privacy-Woman-100k-damages-neighbours-doorbell-cameras.html\nRelated \ud83c\udf10\nAmazon Ring Always Home Cam\nGoogle Nest Hub 2 sleep tracking\nPage info\nType: Incident\nPublished: October 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/waymo-cars-get-stuck-in-cul-de-sac", "content": "Multiple Waymo self-driving cars get stuck in cul-de-sac\nOccurred: October 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nResidents of a a quiet cul-de-sac in San Francisco were 'plagued' by an influx of Waymo self-driving cars, prompting questions about the nature of the company's automated system and concerns about the effect on the environment.\n\nAccording to local news station KPIX, Residents complain that up to 50 cars a day make their way down the street, only to have to turn around, resulting in more traffic and noise pollution over a period of several weeks.\n\nThe cause of the issue was unclear. Local residents told KPIX that they asked the drivers but were told that the cars were 'programmed' and that they were just doing their jobs. Noboby reported seeing one of the cars dropping off or picking up a passenger. \n\nWaymo responded by saying the vehicles were simply 'obeying road rules' designed to limit traffic in certain residential streets.\n\nThe incident to calls for the more effective regulation of autonomous vehicles.\nSystem \ud83e\udd16\nWaymo Driver website\nOperator: Alphabet/Waymo\nDeveloper: Alphabet/Waymo\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Waymo Driver\nIssue: Accuracy/reliability; Environment\nTransparency: Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://sanfrancisco.cbslocal.com/2021/10/14/dead-end-sf-street-plagued-with-confused-waymo-cars-trying-to-turn-around-every-5-minutes/\nhttps://www.bbc.co.uk/news/technology-58928706\nhttps://www.msn.com/en-us/news/technology/waymo-e2-80-99s-autonomous-vehicles-keep-getting-stuck-in-a-dead-end-street-in-san-francisco/ar-AAPwT7x\nhttps://thenextweb.com/news/confused-waymo-robotaxis-flood-dead-end-street-15th-avenue-san-francisco\nhttps://www.yahoo.com/entertainment/self-driving-waymo-cars-flocking-110804847.html\nhttps://gizmodo.com/waymos-self-driving-cars-are-mysteriously-flocking-to-a-1847862042\nhttps://www.sfgate.com/sf-culture/article/Why-Waymo-vehicles-are-getting-stuck-in-SF-16533647.php\nhttps://www.dailymail.co.uk/sciencetech/article-10092593/Confused-Waymo-self-driving-cars-flooding-dead-end-street-San-Francisco.html\nhttps://www.businessinsider.co.za/trending/self-driving-cars-waymo-google-stuck-end-street-san-francisco-2021-10\nRelated \ud83c\udf10\nCruise driverless cars traffic blocking\nTesla Model 3 hits parked police car\nPage info\nType: Incident\nPublished: October 2021\nLast updated: May 2024", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/vision-60spur-quadrupedal-war-robot", "content": "Vision 60/SPUR quadrupedal war robot prompts AI weaponisation fears\nOccurred: October 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA robot dog armed with a sniper rifle capable of hitting targets from 3,940 feet away was criticised as 'terrifying', 'dystopian' and a 'nightmare'.\nManufactured by Ghost Robotics and arms manufacturer SWORD International and unveiled at a US Army trade show, the 'Special Purpose Unmanned Rifle' (SPUR) can be remotely instructed to load, unload and fire.\nSPUR is 'the future of unmanned weapons system \u2014 and that future is now', according to SWORD International.\nCritics have long voiced concerns over the weaponisation of robot quadrupeds.\nSystem \ud83e\udd16\nGhost Robotics Vision60\nSword Defence Systems SPUR\nOperator: Ghost Robotics\nDeveloper: Ghost Robotics, Sword International\nCountry: USA\nSector: Aerospace/defence\nPurpose: Kill/main adversaries\nTechnology: Robotics\nIssue: Appropriateness/need; Ethics/values\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.dailymail.co.uk/sciencetech/article-10091493/Killer-bot-Terrifying-robot-dog-fitted-6-5mm-sniper-RIFLE-unveiled-Army-trade-show.html\nhttps://www.theverge.com/2021/10/14/22726111/robot-dogs-with-guns-sword-international-ghost-robotics\nhttps://futurism.com/sniper-rifle-robot-dog\nhttps://gizmodo.com/robot-murder-dog-new-thing-to-worry-about-as-you-fall-a-1847867174\nhttps://interestingengineering.com/yes-robot-dogs-can-now-carry-sniper-rifles-on-their-backs\nhttps://www.ladbible.com/news/latest-robotics-company-attaches-high-tech-sniper-rifle-to-robot-dog-20211012\nhttps://www.iflscience.com/technology/robot-dogs-just-got-a-lethal-dystopian-upgrade/\nhttps://metro.co.uk/2021/10/14/lethal-robot-dogs-now-have-assault-rifles-attached-to-their-backs-15420004/\nRelated \ud83c\udf10\nNYPD digidog\nMohsen Fakhrizadeh assassination\nPage info\nType: Issue\nPublished: October 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/china-taxation-department-id-system-hack", "content": "Chinese government facial recognition system hacked by tax fraudsters\nOccurred: 2018-2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nTwo men in China tricked the country's State Taxation Administration's facial recognition identity verification system into creating fake invoices valued at USD 76.2 million.\nAccording to a Xinhua report, the pair manipulated high-definition photographs with an app available on the black market that turns photos into videos and then used a smartphone to bypass its camera during facial authentication, instead feeding their doctored videos into the system. \nThe men set up a shell company that issued fake tax invoices that defrauded the taxation department by 500 million yuan (approximately USD 76.2 million). \nThe fraudsters started their operation in 2018, and have since been captured and prosecuted in Shanghai.  \nSystem \ud83c\udf10\nState Taxation Administration facial recognition identity verification system \nOperator: State Taxation Administration\nDeveloper: State Taxation Administration\nCountry: China\nSector: Govt - finance\nPurpose: Verify identity\nTechnology: Facial recognition; Deepfake - image; Deepfake - video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Security\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttp://www.xinhuanet.com/2021-03/30/c_1127270651.htm\nhttps://www.scmp.com/tech/tech-trends/article/3127645/chinese-government-run-facial-recognition-system-hacked-tax\nhttps://findbiometrics.com/fraudsters-use-deepfake-biometrics-hack-chinas-taxation-system-040103/\nhttps://www.theregister.com/2021/03/31/tax_scammers_fool_ai_facial_recognition/\nhttps://geekwire.eu/2021/03/31/tax-scammers-in-china-turn-photos-into-vids-to-crack-tax-dept-facial-recognition-system/\nhttps://www.biometricupdate.com/202103/hackers-spoofed-biometric-authentication-videos-to-steal-millions-in-china\nhttps://www.thestar.com.my/tech/tech-news/2021/03/31/report-chinese-government-run-facial-recognition-system-hacked-by-tax-fraudsters\nRelated \ud83c\udf10\nDubai USD 35m voice cloning fraud\nDubai deepfake court evidence\nPage info\nType: Incident\nPublished: March 2021\nLast updated: February 2024", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/facebook-marketplace-amazon-rainforest-sales", "content": "Amazon rainforest illegally for sale on Facebook Marketplace\nOccurred: February 2021-\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nSwathes of Brazil's Amazon rainforest, including national forests and land reserved for indigenous peoples, were being illegally bought and sold on Facebook's Marketplace commerce site.\nThe protected areas included national forests and land reserved for indigenous peoples, with some plots listed on Facebook\u2019s classified ads service reportedly as large as 1,000 football pitches.\nMany sellers openly admitted they did not have a land title, which is the only document that proves ownership of land under Brazilian law. The illegal activity is thought to have been primarily fueled by Brazil\u2019s cattle ranching industry.\nThe issue raised concerns among environmentalists and indigenous communities, who claimed Brazil's government is unwilling to halt the sales. The Amazon rainforest is often described as the lungs of the Earth, and plays a crucial role in the global climate system. \nThe revelations were provoked an inquiry by Brazil\u2019s Supreme Court, though Facebook said it would allow the sales to continue. \n\u2795 October 2021. Facebook announced that it was updating its commerce policies to prohibit the sale of land in ecological conservation areas across Facebook, Instagram, and WhatsApp.\nSystem \ud83e\udd16\nFacebook Marketplace website\nDocuments \ud83d\udcc3\nFacebook (2021). Curbing Amazon Land Listings on Marketplace\nOperator: Meta/Facebook\nDeveloper: Meta/Facebook\nCountry: Brazil\nSector: Retail\nPurpose: Sell products/services, advertising\nTechnology: Database\nIssue: Ethics; Legality\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/technology-56168844\nhttps://www.bbc.co.uk/iplayer/episode/m000st9n/our-world-selling-the-amazon\nhttps://www.bbc.co.uk/news/technology-58843166\nhttps://edition.cnn.com/2021/10/08/tech/facebook-marketplace-amazon-land/index.html\nhttps://www.businessinsider.com/facebook-block-illegal-sales-of-amazon-rainforest-on-marketplace-bbc-2021-10\nhttps://www.thehindu.com/sci-tech/technology/internet/facebook-to-curb-sale-of-amazon-rainforest-land-on-marketplace/article36938861.ece\nhttps://www.eco-business.com/news/facebook-to-block-illegal-sales-of-protected-amazon-rainforest-areas/\nhttps://news.mongabay.com/2021/03/facebook-enabling-amazon-land-grabbing-deforestation-investigation/\nhttps://gizmodo.com/facebook-will-no-longer-let-you-monsters-sell-protected-1847828781\nhttps://news.trust.org/item/20211008150510-ncre5\nRelated \ud83c\udf10\nFacebook Marketplace gun sales\nPage info\nType: Incident\nPublished: February 2021\nLast updated: June 2024", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/narxcare-drug-addition-risk-assessment", "content": "NarxCare drug addiction assessment system wrongly denies patients opioids\nOccurred: August 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nNarxCare, a software system that uses patient data, drug use data, and metadata to determine the risk of drug addiction, was accused of denying users opioids on the basis it wrongly reckoned they were at risk of addiction. \nDeveloped by Bamboo Health (formerly named Appriss), NarxCare is used across several US states to track prescriptions for drugs such as opioids and purports to identify and flag patients with \u2018drug-shopping\u2019 behaviours - people who might be lying to doctors about the pain they\u2019re in in order to get opioids.\nHowever, in some cases, the system recommended denying painkillers to people who suffer from severe pain and have no history of drug abuse, according to WIRED, thereby denying patients health benefits, worsening their health and leading to accusations that it can be inaccurate and unfair. \nThe system was also accused of reinforcing existing racial and gender biases. \nSystem \ud83e\udd16\nNarxCare drug addiction assessment system\nOperator: US Department of Justice; Rite Aid; Walmart; Sam's Club\nDeveloper: Appriss\nCountry: USA\nSector: Health\nPurpose: Assess & predict drug abuse\nTechnology: Risk assessment algorithm\nIssue: Accuracy/reliability; Bias/discrimination - race, gender; Fairness\nTransparency: Governance; Black box; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.wired.com/story/opioid-drug-addiction-algorithm-chronic-pain/\nhttps://www.sicknote.co/p/the-algorithm-that-narcs-on-you\nhttps://read.deeplearning.ai/the-batch/issue-106/\nhttps://www.painnewsnetwork.org/stories/2021/8/31/the-tangled-mess-of-prescription-opioid-guidelines\nhttps://filtermag.org/pain-patients-opioids-fear/\nhttps://www.reddit.com/r/medicine/comments/p2ofkc/a_sweeping_drug_addiction_risk_algorithm_has/\nRelated \ud83c\udf10\nEpic Systems sepsis prediction model\nEpic Systems Epic Deterioration Index\nPage info\nType: Incident\nPublished: September 2021\nLast updated: June 2024", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/google-misidentifies-engineer-as-serial-killer", "content": "Google misidentifies engineer as serial killer\nOccurred: June 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA software engineer based in Switzerland discovered that a Google search of his name confused him with a serial killer of the same name.\nHristo Georgiev believes the error was caused by Google\u2018s knowledge graph, which generates informational boxes adjacent to its search results. He  reckoned the algorithm matched his picture to the Wikipedia entry because the now-dead killer shared his name.\nGoogle removed Georgiev's image from the killer's infobox after the engineer had reported the issue.\nThe incident raised concerns about the accuracy and reliability of Google\u2019s Knowledge Graph, and the potential for such errors to have troubling consequences.\nSystem \ud83e\udd16\nGoogle Knowledge Graph\nGoogle Knowledge Graph Wikipedia profile\nOperator: Alphabet/Google  \nDeveloper: Alphabet/Google\nCountry: Switzerland\nSector: Technology\nPurpose: Enhance search engine results\nTechnology: Knowledge Graph\nIssue: Accuracy/reliability; Privacy\nTransparency: Governance; Black box\nResearch, advocacy \ud83e\uddee\nHristo Georgiev (2021). Google turned me into a serial killer\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://thenextweb.com/news/google-search-algorithm-matches-software-engineer-photo-to-wikipedia-serial-killer-entry\nhttps://futurism.com/the-byte/google-algorithm-serial-killer\nhttps://www.searchenginejournal.com/google-knowledge-panel-shows-wrong-man-as-serial-killer/411676/\nhttps://www.digit.in/news/general/botched-google-search-card-serial-killer-59946.html\nhttps://gadgets.ndtv.com/internet/news/google-algorithm-hristo-georgiev-zurich-engineer-image-bulgarian-serial-killer-2472823\nhttps://news.ycombinator.com/item?id=27622100\nRelated \ud83c\udf10\nApple/SIS misidentification, wrongful arrest\nPage info\nType: Incident\nPublished: June 2021\nLast updated: June 2024", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/facebook-marketplace-gun-sales", "content": "Guns disguised as cases for sale on Facebook Marketplace\nOccurred: August 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGunsellers were found to be secretly posting assault rifles and other guns for sale on Facebook Marketplace by disguising them as posts for gun cases or empty boxes.\nThe practice was spotted by the Wall Street Journal, who discovered over-priced cases listed in several US states. When contacted, sellers revealed that the sale price included the firearm that belongs to the case. \nThe practice is in defiance of Facebook's ban on the sale of private guns and ammunition. Facebook uses a combination of artificial intelligence and human moderators to police sales on Marketplace, calling into question the workings and effectiveness of its system.\nSystem \ud83e\udd16\nFacebook Marketplace\nFacebook Marketplace Wikipedia profile\nOperator: Meta/Facebook\nDeveloper: Meta/Facebook\nCountry: USA\nSector: Technology\nPurpose: Moderate content\nTechnology: Content moderation system\nIssue: Accuracy/reliability\nTransparency: Governance; Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.wsj.com/articles/gun-sellers-are-sneaking-onto-facebooks-booming-secondhand-marketplace-11566315198\nhttps://www.wsj.com/articles/democratic-senators-question-facebook-on-hidden-gun-sales-11567781443\nhttps://www.wsj.com/articles/gun-sellers-use-new-tactic-to-deal-on-facebook-marketplace-11598270872\nhttps://www.businessinsider.com/gun-owners-use-facebook-marketplace-sell-without-background-checks-2019-8\nhttps://www.independent.co.uk/life-style/gadgets-and-tech/news/facebook-guns-sale-marketplace-loophole-a9073311.html\nhttps://www.businessinsider.com/guns-sold-online-facebook-marketplace-mark-zuckerberg-sticker-listings-workaround-2020-8\nhttps://abc13.com/facebook-sellng-guns-on-gun-sales-troubleshooter/5706821/\nhttps://www.protocol.com/facebook-gun-sales-ban\nRelated \ud83c\udf10\nFacebook Marketplace Amazon rainforest sales\nChatGPT makes up research claiming guns are not harmful to kids\nPage info\nType: Incident\nPublished: August 2019\nLast updated: December 2023", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/met-police-retrospective-facial-recognition", "content": "Met Police retrospective facial recognition system raises privacy concerns\nReleased: September 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA retrospective facial recognition system ordered by London's police force raised concerns about its purpose and potential bias.\nLondon's Metropolitan Police Service is to use a new, retrospective facial recognition system that will enable them to process historic CCTV, social media, and other images when identifying and tracking down suspects, according to WIRED.\nCritics expressed concerns that the system, which extended the Met's existing facial recognition capabiilities, could violate privacy, entrench racially and otherwise discriminatory policing, and easily be used for other purposes.\nThe Met's contract was discovered when the Mayor of London's office published an approved proposal for the system, which formed part of a GBP 3 million, four-year deal with NEC Corporation's UK subsidiary Northgate Public Services.\nIn an interview with The Register, the UK government\u2019s Surveillance Camera Commissioner Professor Fraser Sampson argued 'We need as a minimum a single set of clear principles by which those using the biometric and surveillance camera systems will be held to account, transparently and auditably.'\nSystem \ud83e\udd16\nNeoFace Watch facial recognition system\nDocuments \ud83d\udcc3\nMayor of London (2021). Retrospective Facial Recognition System\nOperator: Metropolitan Police Service\nDeveloper: NEC\nCountry: UK\nSector: Govt - police\nPurpose: Identify criminals\nTechnology: Facial recognition\nIssue: Privacy; Surveillance; Dual/multi-use; Bias/discrimination - race, ethnicity\nTransparency: Governance; Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.wired.co.uk/article/met-police-facial-recognition-new\nhttps://www.biometricupdate.com/202109/ice-drops-3-9m-for-trust-stamps-facial-recognition-london-met-taps-nec-for-4-2m\nhttps://www.computing.co.uk/news/4037761/met-police-retrospective-facial-recognition-technology\nhttps://www.forbes.com/sites/emmawoollacott/2021/09/28/londons-met-police-buying-retrospective-facial-recognition-technology/\nhttps://www.politico.eu/newsletter/ai-decoded/politico-ai-decoded-germanys-new-ai-era-londons-ai-vision-fighting-covid-19/\nhttps://findbiometrics.com/london-police-implement-retrospective-facial-recognition-system-092801/\nhttps://www.computerweekly.com/news/252507569/Met-Police-purchase-new-retrospective-facial-recognition-system\nhttps://www.theregister.com/2021/09/21/uk_surveillance_commissioner_facial_recog_warning/\nRelated \ud83c\udf10\nUK Met Police Gangs Violence Matrix\nRCMP facial recognition covert surveillance\nPage info\nType: Issue\nPublished: September 2021\nLast updated: June 2024", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/google-deepmind-royal-free-data-sharing", "content": "Google DeepMind, Royal Free London rapped for patient data sharing\nOccurred: April 2016\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGoogle's DeepMind AI unit and the Royal Free London NHS Foundation Trust shared sensitive data, including mental health records and HIV diagnosis, of 1.6 million patients.\nIn 2015, Google\u2019s AI firm DeepMind was given the personal records of 1.6 million patients at the Royal Free London NHS Foundation Trust to help create Streams, an alert, diagnosis, and detection system that could spot when patients were at risk of developing acute kidney injury.\nHowever, the deal was the focus of public outrage following a New Scientist report that vast amounts of data had been accessed by DeepMind.\nThe incident raised questions about Google/Deepmind ethics and the legaility of its actions. \n\u2795 May 2016. The New Scientist revealed that Deepmind had failed to secure approval from the Confidentiality Advisory Group of the Medicines and Healthcare Products Regulatory Agency. \n\u2795 July 2017. The UK Information Commissioner's Office ruled that the Royal Free hospital had failed to comply with the UK Data Protection Act when it shared the data, though it did not issue a fine on the basis that there was a lack of guidance for the sector. \n\u2795 September 2021. Law firm Mishcon de Reya announced it was to bring a class action lawsuit against Google on behalf of the 1.6 million individuals whose medical records were shared.\n\u2795 October 2021. DeepMind apologised and stated that it had focused on building tools for clinicians, rather than considering how the project should have been shaped by the needs of patients.\n\u2795 May 2022. The action was later discontinued and resurrected as a legal action against Google for using the NHS data of 1.6 million Britons 'without their knowledge or consent'. \n\u2795 May 2023. The lawsuit was again (pdf) dismissed.\nSystem \ud83e\udd16\nStreams\nDocuments \ud83d\udcc3\nRoyal Free London (2019). Information Commissioner\u2019s Office (ICO) investigation\nRoyal Free London (2018). Royal Free London publishes audit into Streams app\nGoogle Deepmind (2018). Scaling Streams with Google\nGoogle Deepmind (2017). The Information Commissioner, the Royal Free, and what we\u2019ve learned\nOperator: Royal Free London NHS Foundation Trust\nDeveloper: Alphabet/Google/Deepmind; NHS\nCountry: UK\nSector: Health\nTechnology: Prediction algorithm\nPurpose: Detect & predict acute kidney disease\nIssue: Privacy; Security; Ethics/values\nTransparency: Governance; Black box; Privacy\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nPrismall v Google (pdf)\nMishcon (2022). New claim against Google and DeepMind Technologies for unauthorised use of confidential medical records\nLitigation Capital Management (2022). Litigation Finance Agreement for new representative claim against Google and DeepMind Technologies\nUK Information Commissioner's Office (2017). Royal Free London undertaking (pdf)\nUK Information Commissioner's Office (2017). ICO letter to Royal Free London (pdf)\nInvestigations, assessments, audits \ud83e\uddd0\nLinklaters (2018). Audit of the acute kidney injury detection system known as Streams (pdf)\nResearch, advocacy \ud83e\uddee\nShaping AI - University of Warwick (2023). Shifting AI controversies (pdf)\nPowls P., Hodson H. (2017). Google DeepMind and healthcare in an age of algorithms\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.newscientist.com/article/2086454-revealed-google-ai-has-access-to-huge-haul-of-nhs-patient-data/\nhttps://www.newscientist.com/article/2088056-did-googles-nhs-patient-data-deal-need-ethical-approval/\nhttps://news.sky.com/story/google-received-16-million-nhs-patients-data-on-an-inappropriate-legal-basis-10879142\nhttps://techcrunch.com/2018/06/13/audit-of-nhs-trusts-app-project-with-deepmind-raises-more-questions-than-it-answers\nhttps://www.zdnet.com/article/deepmind-and-the-nhs-what-its-really-like-to-use-googles-kidney-health-app/\nhttps://www.digitalhealth.net/2016/11/google-deepmind-and-royal-free-in-five-year-deal/\nhttps://www.theguardian.com/technology/2017/jul/03/google-deepmind-16m-patient-royal-free-deal-data-protection-act\nhttps://www.insider.com/nhs-discloses-how-much-its-paying-google-deepmind-2017-6\nhttps://www.digitalhealth.net/2017/03/deepmind-health-royal-frees-criticised-lack-patient-involvement/\nhttps://www.newscientist.com/article/2289101-google-is-shutting-down-controversial-data-sharing-project-with-nhs/\nhttps://www.bbc.co.uk/news/technology-40483202\nhttps://uk.news.yahoo.com/uk-class-action-style-suit-150419945.html\nhttps://www.bbc.co.uk/news/technology-58761324\nhttps://techcrunch.com/2022/05/16/google-deepmind-nhs-misuse-of-private-data-lawsuit\nRelated \ud83c\udf10\nGoogle/HCA Healthcare patient data sharing\nCrisis Text Line data sharing\nPage info\nType: Incident\nPublished: November 2021\nLast updated: June 2024", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-mentor-dsp-delivery-driver-scoring", "content": "Amazon Mentor delivery driver scoring criticised as invasive and inaccurate\nOccurred: February 2021-\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn app used by Amazon to track and score the safety of its delivery drivers was criticised for being invasive and often miscalculating dangerous driving behaviour.\nThe Mentor app, developed by eDriving, promised to improve driver safety by generating a 'FICO' score each day that measured their driving performance. It also provided micro-training modules to the driver. \nHowever, drivers reported being docked points for using a phone while driving even when they did not answer a ringing phone. Another driver was flagged for distracted driving at every delivery stop she made. \nHowever, drivers complained the app was often inaccurate and led to unfair disciplinary action, including the loss of restricted payouts, bonuses and perks, as well as jobs. \nMentor was also seen as unnecessarily invasive, tracking drivers' location after they clocked out from work, according to a report by CNBC.\nSystem \ud83e\udd16\neDriving. Mentor DSP Driver Guide\nOperator: Amazon\nDeveloper: Solera/eDriving\nCountry: USA\nSector: Transport/logistics\nPurpose: Assess delivery driver performance\nTechnology: Performance scoring algorithm\nIssue: Accuracy/reliability; Fairness; Privacy; Surveillance\nTransparency: Governance; Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.cnbc.com/2021/02/12/amazon-mentor-app-tracks-and-disciplines-delivery-drivers.html\nhttps://mashable.com/article/amazon-mentor-delivery-driver-monitoring-app\nhttps://www.businessinsider.com/amazon-scores-delivery-workers-driving-skills-using-tracking-app-2019-12\nhttps://www.theverge.com/2021/2/12/22280585/amazon-mentor-app-delivery-drivers-location-tracking-performance\nhttps://www.wired.com/story/some-amazon-drivers-have-had-enough-can-they-unionize/\nhttps://www.engadget.com/amazon-mentor-app-edriving-delivery-driver-tracking-surveillance-gps-194346304.html\nhttps://www.youtube.com/watch?v=E3r8_z1f60U\nhttps://www.reddit.com/r/AmazonDSPDrivers/comments/gydrct/mentor_has_many_flaws/\nhttps://slashdot.org/story/21/02/12/2129213/amazon-uses-an-app-called-mentor-to-track-and-discipline-delivery-drivers\nRelated \ud83c\udf10\nAmazon Flex algorithm delivery driver firings\nAmazon Flex delivery driver routing safety\nAmazon delivery driver safety cameras\nPage info\nType: Incident\nPublished: February 2021\nLast updated: June 2024", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-flex-delivery-driver-routing-safety", "content": "Amazon Flex algorithm forces delivery drivers to take unsafe rouets\nOccurred: June 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAmazon delivery drivers were forced to take dangerous routes and run across four-lane highways at night with multiple packages and boxes.\nVice News reported that Amazon's routing algorithms sometimes group deliveries on both sides of a street into a single stop, forcing to risk their personal safety in order to satisfy the demands of the company's cost and speed-focused Flex routing algorithm.\nOther drivers bring another person with them to park and look after their vehicles when they make deliveries. \nApproximately 85,000 contracted delivery drivers across the US and Europe purportedly use Amazon's Flex algorithm. \nSystem \ud83e\udd16\nAmazon Flex website\nOperator: Amazon\nDeveloper: Amazon\nCountry: USA; EU; UK; Australia\nSector: Transport/logistics\nPurpose: Manage package delivery\nTechnology: Routing algorithm\nIssue: Safety; Fairness\nTransparency: Governance; Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.vice.com/en/article/5db95k/amazons-cost-saving-routing-algorithm-makes-drivers-walk-into-traffic\nhttps://www.cnet.com/news/amazon-delivery-drivers-risk-write-ups-and-injuries-as-they-race-to-your-door/\nhttps://www.breitbart.com/tech/2021/06/05/report-amazons-delivery-algorithm-directs-drivers-to-walk-into-traffic/\nhttps://tech.slashdot.org/story/21/06/03/2032202/amazons-cost-saving-routing-algorithm-makes-drivers-walk-into-traffic\nhttps://www.abc.net.au/news/2021-08-29/amazon-flex-delivery-drivers-voice-safety-concerns/100404498\nhttps://www.abc.net.au/indonesian/2021-08-30/kesulitan-yang-dialami-pekerja-kurir-di-australia/100417724\nhttps://www.abc.net.au/chinese/2021-09-01/amazon-flex-delivery-drivers-voice-safety-concerns-australia/100423446\nhttps://www.cnbc.com/2021/07/30/amazon-dsps-tell-drivers-to-bypass-safety-inspections.html\nRelated \ud83c\udf10\nAmazon Driveri delivery driver safety monitoring\nAmazon DSP Ans Rana driver liability\nPage info\nPublished: June 2021\nLast updated: October 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/apple-iphone-depression-detection-study", "content": "Apple, Biogen depression, dementia detection study raises privacy concerns\nOccurred: September 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA study by Apple, UCLA and Biogen using facial recognition, speech patterns and other behavioural data to detect stress, depression and cognitive decline raised concerns about privacy and the validity of emotion recognition technology.\nFirst announced in August 2020, the study was initially limited to health data such as heart rate and sleep, and how a person interacts with their iPhone, Apple Watch or Beddit sleep-tracker to understand their mental health. \nHowever, the study was extended to monitor people\u2019s vital signs, movements, speech, sleep, typing habits and frequency of typos, raising concerns from digital rights advocates about the validity of emotion AI/affective computing, privacy, and scope creep, according to the Wall Street Journal.\nApple had sought to position itself as a privacy leader, including insisting that apps in the iOS App Store add privacy 'nutrition labels' to inform users what type of sensitive information the app collects. \nSystem \ud83e\udd16\nUCLA (2020). UCLA launches study in collaboration with Apple to discover insights about depression\nOperator: \nDeveloper: Apple; Biogen; UCLA\nCountry: USA\nSector: Health\nPurpose: Detect anxiety, depression, autism, dementia\nTechnology: Emotion recognition; Facial recognition\nIssue: Accuracy/reliability; Privacy; Scope creep/normalisation\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.wsj.com/articles/apple-wants-iphones-to-help-detect-depression-cognitive-decline-sources-say-11632216601\nhttps://www.fastcompany.com/90678993/apple-depression-study-iphone-data-emotion-ai-flaws\nhttps://www.emarketer.com/content/apple-s-depression-detecting-iphone-tool-butts-heads-with-its-privacy-first-marketing\nhttps://www.independent.co.uk/life-style/gadgets-and-tech/apple-iphone-health-depression-study-b1924213.html\nhttps://gizmodo.com/apple-working-on-depression-detection-for-iphones-repo-1847714458\nhttps://www.dailymail.co.uk/sciencetech/article-10013133/Apple-working-technology-help-diagnose-depression-report-reveals.html\nhttps://thenextweb.com/news/iphone-depression-anxiety-feature-rumor-analysis\nhttps://www.cnbc.com/2020/08/04/apple-ucla-to-study-depression.html\nhttps://www.engadget.com/apple-ucla-depression-study-225118236.html\nhttps://en.brinkwire.com/news/researchers-will-use-apple-gadgets-in-study-to-detect-and-treat-depression/\nRelated \ud83c\udf10\nIntel AI student emotion monitoring\nZoom AI emotion recognition\nPage info\nType: Issue\nPublished: September 2021\nLast updated: June 2024", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/xpeng-p7-crashes-into-truck", "content": "Xpeng P7 on auto navigation crashes into truck, injuring driver\nOccurred: September 2021 \nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA driver of an Xpeng P7 is reported to have collided with a truck whilst using the car's Navigation Guided Pilot (NGP) automatic navigation driver assistance system. The accident resulted in minor injuries to the driver, who was hospitalised. \nThe cause of the collision was unclear; however, the system appeared not to have recognised the truck, which was a flatbed vehicle with an extended tail. It was not carrying cargo. \nXpeng says it would investigate the crash, and that its initial analysis showed the car\u2019s assistant driving features were 'functioning normally' before the collision.\nXpeng reputedly failed to contact the owner after the accident, and then directed him to contact the police. The company said it would improve its 'seemingly inhumane' customer service.\nSystem \ud83e\udd16\nXpeng P7 website\nXPeng P7 Wikipedia profile\nDocuments \ud83d\udcc3\nhttps://www.youtube.com/watch?v=KJyg6c4H4jg\nOperator: Xpeng\nDeveloper: Xpeng\nCountry: China\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Driver assistance system\nIssue: Safety; Accuracy/reliability\nTransparency: Governance; Black box; Complaints/appeals\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.aicaijing.com.cn/article/9355\nhttps://tech.ifeng.com/c/89pWnOcmi3R\nhttps://carnewschina.com/2021/09/24/xpeng-p7-crashed-into-the-rear-of-the-truck-ngp-was-activated/\nhttps://pandaily.com/xpeng-p7-crashes-into-rear-of-truck-while-using-ngp-automatic-navigation-assisted-driving-system/\nhttps://pandaily.com/xpeng-responds-to-recent-accident-says-the-ngp-functioning-normally/\nhttps://www.autoevolution.com/news/xpeng-p7-on-ngp-doesn-t-detect-a-truck-crashes-in-china-170224.html\nhttps://autotech.news/xpeng-p7-crashes-into-rear-of-truck-while-using-ngp/\nhttps://technode.com/2021/09/26/xpeng-p7-driver-got-into-collision-while-using-assistant-driving-feature/\nhttps://cryptopress.network/xpeng-p7-driver-got-into-collision-while-using-assistant-driving-feature-%C2%B7-technode/\nRelated \ud83c\udf10\nNIO ES8 fatal crash\nXPeng customer facial recognition\nPage info\nType: Incident\nPublished: September 2021\nLast updated: June 2024", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/washington-dc-schools-teacher-value-added-scoring", "content": "Sarah Wysocki fired after inaccurate teacher effectiveness assessment\nOccurred: February 2012\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\n5th grade Washington DC public school teacher Sarah Wysocki was fired for receiving a poor evaluation by an algorithm, resulting in a heated controversy about the accuracy, fairness, value, and transparency of the region's teacher performance evaluation system. \nAt the time, District of Columbia Public Schools' IMPACT teacher evaluation system used test scores from schools under investigation for cheating to calculate so-called 'value-added scores' that would be incorporated into teacher evaluations. \nWyosecki had earned excellent observation ratings and was highly regarded by peers and parents. But she received an uncharacteristically low value-added score and was subsequently fired, despite evidence the evaluation generated by the algorithm was based on falsified student scores.\nHer appeal against her dismissal failed, though she was quickly offered a position with another school system.\nSystem \ud83e\udd16\nDCPS IMPACT teacher evaluation system\nDocuments \ud83d\udcc3\nDistrict of Colombia Public Schools. IMPACT: The DCPS Evaluation and Feedback System for School-Based Personnel\nOperator: District of Columbia Public Schools\nDeveloper: Mathematica Policy Research\nCountry: USA\nSector: Education\nPurpose: Assess and rank teacher performance\nTechnology: Value-added model\nIssue: Accuracy/reliability; Bias/discrimination - income, geography; Fairness; Effectiveness/value\nTransparency: Governance; Complaints/appeals; Black box\nResearch, advocacy \ud83e\uddee\nVarma A., Dawkins C., Chaudhuri K. (2023). Artificial intelligence and people management: A critical assessment through the ethical lens\nAlon-Barkat S., Busuioc M. (2023). Human\u2013AI Interactions in Public Sector Decision Making: \u201cAutomation Bias\u201d and \u201cSelective Adherence\u201d to Algorithmic Advice \nYan S. (2021). Algorithms are not bias-free: Four mini-cases\nEducation Writers Assocation (2012). One Teacher Feels Impact of \u2018Value-Added\u2019 Evaluations\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.washingtonpost.com/local/education/creative--motivating-and-fired/2012/02/04/gIQAwzZpvR_story.html\nhttps://www.washingtonpost.com/blogs/answer-sheet/post/firing-of-dc-teacher-reveals-flaws-in-value-added-evaluation/2012/03/07/gIQAtmlGxR_blog.html\nhttps://www.huffpost.com/entry/teacher-evaluations_b_1328456\nhttps://www.edweek.org/leadership/opinion-the-problem-with-one-size-fits-all-approaches-to-teacher-quality/2012/03\nhttps://www.edweek.org/teaching-learning/when-value-added-scores-dont-make-sense/2012/03\nhttps://www.dailykos.com/stories/2012/3/8/1072427/-Was-a-highly-regarded-Washington-D-C-teacher-fired-because-of-someone-else-s-cheating\nhttps://qz.com/819245/data-scientist-cathy-oneil-on-the-cold-destructiveness-of-big-data/\nhttps://towardsdatascience.com/its-time-to-optimize-data-algorithms-with-fairness-considerations-9bfe68c7ed38\nhttps://blogs.ischool.berkeley.edu/w231/2018/12/11/impact-of-algorithmic-bias-on-society/\nhttps://www.econtalk.org/cathy-oneil-on-weapons-of-math-destruction/\nRelated \ud83c\udf10\nSheri G. Lederman NYC teacher effectiveness assessment\nIntel AI student emotion monitoring\nPage info\nType: Incident\nPublished: November 2021\nLast updated: June 2024", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tesla-model-x-crashes-into-five-police-officers", "content": "Tesla Model X on Autopilot crashes into five police officers\nOccurred: February 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Tesla Model X slammed into five police officers while they were operating a traffic stop in Splendora, Texas. \nFootage of the incident obtained by the Wall Street Journal showed a Tesla Model X driving at 54 miles per hour in the right-hand lane of a highway. After passing multiple parked emergency vehicles, a police SUV with its lights flashing appeared stationary in the center of the rightmost lane, which the Tesla fails to detect. The Tesla then rammed into one of the police SUVs, injuring five officers and hospitalising the subject of the original traffic stop.\nThe five officers filed a lawsuit against Tesla, claiming that 'design and manufacturing defects' were 'known to Tesla', were responsible for the crash and that Autopilot 'failed to detect the officers\u2019 cars or to function in any way to avoid or warn of the hazard and subsequent crash.' \nThe lawsuit included Pappas Restaurants Inc., the owner of Pappasito\u2019s Cantina, where the driver of the Tesla was allegedly overserved alcohol before the accident. It requested damages in excess of USD 1 million with maximum damages of USD 20 million from Tesla and Pappas Restaurants.\nTesla maintains that drivers are responsible for their vehicle at all times and need to keep their hands on the steering wheel and be ready to take control at all times. \n\u2795 August 2021. The US National Highway Traffic Safety Administration (NHTSA) opened a preliminary investigation into 11 instances of Teslas crashing into emergency vehicles.\nSystem \ud83e\udd16\nTesla Autopilot, Full-self Driving\nTesla Autopilot, Full-Self Driving misleading marketing\nOperator:  \nDeveloper: Tesla\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Driver assistance system\nIssue: Safety; Accuracy/reliability\nTransparency: Black box; Legal\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nhttps://s3.documentcloud.org/documents/21069638/lawsuit-filed-against-tesla-and-pappas-restaurants.pdf\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theverge.com/2021/9/28/22698388/tesla-texas-lawsuit-cops-autopilot-crash-injury\nhttps://www.click2houston.com/news/local/2021/09/27/lawsuit-filed-against-tesla-after-accident-that-injured-5-police-officers/\nhttps://electrek.co/2021/09/28/tesla-sued-by-police-officers-suffered-injuries-after-drunk-model-x-owner-crash-autopilot/\nhttps://arstechnica.com/tech-policy/2021/09/tesla-on-autopilot-slammed-into-police-cars-despite-flashing-lights-lawsuit-says/\nhttps://www.chron.com/news/houston-texas/transportation/article/tesla-lawsuit-autopilot-elon-musk-police-sue-16493284.php\nhttps://ca.news.yahoo.com/texas-cops-suing-tesla-car-141259213.html\nhttps://www.law360.com/articles/1425649/texas-cops-sue-tesla-over-autopilot-crash-at-traffic-stop\nRelated \ud83c\udf10\nSon Ji-chang Tesla Model X sudden acceleration\nTesla Model S crashes into fire engine\nPage info\nType: Incident\nPublished: September 2021\nLast updated: June 2024", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/china-facial-image-criminal-inference", "content": "'Inaccurate' Chinese facial image criminality system accused of phrenology\nOccurred: November 2016\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nSoftware developed by Chinese researchers was said to automatically detect with 85.5 percent certainty that a human is a criminal by analysing their facial features, prompting accusations of physiognomy, phrenology, and pseudoscience.\nXiaolin Wu and Xi Zhang, researchers at Shanghai Jiao Tong University, found that people with smaller mouths, curvier upper lips and closer-set eyes are more likely to be criminals. The research looked at 1,856 faces of Chinese men aged 18-55 with no facial hair, facial scars or other markings, of which 730 belonged to criminals.\nWu and Zhang stated in their study that they they do not intend to or are not 'qualified to discuss or debate on societal stereotypes'. They also said their system cultivated 'no biases whatsoever due to past experience, race, religion, political doctrine, gender, age, etc,' before acknowleding it should be tested using a dataset of different races, genders and facial expressions before it could be implemented on a broader scale. \nGiven the software's potential for inaccuracy and bias, concerns were expressed that the system could wrongly identify criminals if used in the real-world.\nSystem \ud83e\udd16\nUnknown\nOperator:\nDeveloper: Xiaolin Wu; Xi Zhang; Shanghai Jiao Tong University\nCountry: China\nSector: Research/academia\nPurpose: Recognise/predict criminality\nTechnology: Facial analysis; Computer vision; Deep learning; Neural network; Machine learning\nIssue: Accuracy/reliability; Bias/discrimination - race, gender, age, income; Ethics\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nWu X., Zhang X. (2016). Automated Inference on Criminality using Face Images (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.globaltimes.cn/content/1026928.shtml\nhttps://www.technologyreview.com/2016/11/22/107128/neural-network-learns-to-identify-criminals-by-their-faces/\nhttps://theintercept.com/2016/11/18/troubling-study-says-artificial-intelligence-can-predict-who-will-be-criminals-based-on-facial-features/\nhttps://www.dailymail.co.uk/sciencetech/article-3956826/Do-face-criminal-Controversial-AI-judges-law-abiding-based-look-like.html\nhttps://www.telegraph.co.uk/technology/2016/11/24/minority-report-style-ai-learns-predict-people-criminals-facial/\nhttps://medium.com/@blaisea/physiognomys-new-clothes-f2d4b59fdd6a\nhttps://www.biometricupdate.com/201611/shanghai-researchers-create-ai-system-detecting-criminals-from-face-features\nhttps://news.ycombinator.com/item?id=12983827\nRelated \ud83c\udf10\nHarrisburg University criminality prediction study\nIntel AI student emotion monitoring\nPage info\nType: Issue\nPublished: October 2021\nLast updated: June 2024", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/jr-east-facial-recognition", "content": "JR East suspends facial recognition system after scope creep is revealed\nOccurred: September 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nEast Japan Railway Co. (JR East) used thousands of facial recognition-enabled cameras in metro stations across Tokyo to identify criminals, prompting concerns about privacy.\nJapan's largest passenger rail company used facial recognition at 110 major railway stations and other facilities in the Tokyo metropolitan area during the Tokyo Olympics and Paralympics to identify released criminals, wanted suspects and people acting suspiciously.\nHowever, the company's Victim Notification Service, which informed victims of crime of the identity of their attacker, came under fire from privacy advocates concerned that the system may not be accurate and should not include released convicts and parolees.\nThe fracas also triggered a public debate about privacy and the unregulated use of such technology in public spaces.\nJR East suspended the addition of released prisoners to its database on the basis of 'insufficient social consensus building', though it continued to feed facial data about people acting suspiciously into the system\u2019s database.\nSystem \ud83e\udd16\nUnknown\nOperator: East Japan Railway Co.\nDeveloper: East Japan Railway Co.\nCountry: Japan\nSector: Transport/logistics\nPurpose: Identify criminals and suspects\nTechnology: Facial recognition\nIssue: Privacy; Surveillance; Ethics/values\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://soranews24.com/2021/09/25/japanese-train-operator-scraps-plan-to-identify-past-offenders-with-facial-recognition-camera/\nhttps://www.japantimes.co.jp/news/2021/09/23/national/crime-legal/jr-east-released-prisoners-facial-tracking/\nhttps://www.asahi.com/ajw/articles/14445523\nhttps://www.nippon.com/en/news/yjj2021092200784/\nhttps://www.yomiuri.co.jp/national/20210921-OYT1T50240\nhttp://otakomu.jp/archives/27476941.html\nhttps://www.bengo4.com/c_1009/n_13598/\nRelated \ud83c\udf10\nXPeng customer facial recognition\nKohler, BMW, Max Mara China facial recognition\nPage info\nType: Incident\nPublished: September 2021\nLast updated: June 2024", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/queensland-high-risk-domestic-violence-predictions", "content": "Queensland domestic violence predictive policing trial prompts concerns\nReleased: TBC\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA trial of predictive policing software in Queensland, Australia, drew concerns that minority groups would be unfairly targeted. \nThe Queensland Police Service (QPS) initiated a trial using artificial intelligence (AI) to predict the future risk posed by known domestic violence perpetrators. The AI system identified \"high risk\" perpetrators based on previous calls to an address, past criminal activity, and other police-held data. These individuals will be visited at home by police before domestic violence escalates, and before any crime has been committed.\nHowever, the approach sparked controversy due to its potential for unintended consequences. Critics argued that the AI system could reinforce existing biases in the criminal justice system by creating an endless feedback loop between police and those members of the public who have the most contact with police.\nQPS claimed it removed ethnicity and geographic location attributes before training the AI model. However, human and civil rights advocates expressed concerns that Aboriginal and Torres Strait Islander people would be unfairly targeted, as they are in real life.\nThis initiative raised questions about the role of police in preventing domestic violence incidents and the ethical implications of using AI in this context. While the aim of policing AI-based strategies is to prevent or reduce crime through an assessment of the risk of future offending, there are concerns that this approach may inadvertently create crime.\nSystem \ud83e\udd16\n\nOperator: Queensland Police Service (QPS)\nDeveloper: Queensland Police Service (QPS)\nCountry: Australia\nSector: Govt - police\nPurpose: Identify high-risk domestic violence offenders\nTechnology: Prediction algorithm; Risk processing analysis\nIssue: Bias/discrimination - race, ethnicity; Ethics\nTransparency: Black box\nResearch, advocacy \ud83e\uddee\nAustralian Strategic Policy Institute (). AI and policing: what a Queensland case study tells us\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theguardian.com/australia-news/2021/sep/14/queensland-police-to-trial-ai-tool-designed-to-predict-and-prevent-domestic-violence-incidents\nhttp://www.australasianscience.com.au/article/science-and-technology/qld-police-will-use-ai-predict-domestic-violence-it-happens-beware-un\nhttps://theconversation.com/qld-police-will-use-ai-to-predict-domestic-violence-before-it-happens-beware-the-unintended-consequences-167976\nhttps://www.abc.net.au/radionational/programs/downloadthisshow/can-ai-prevent-crime/13553112\nhttps://eveningreport.nz/2021/09/17/qld-police-will-use-ai-to-predict-domestic-violence-before-it-happens-beware-the-unintended-consequences-167976/\nhttps://thelatch.com.au/ai-domestic-violence/\nhttps://mydroll.com/qld-police-will-use-ai-to-predict-domestic-violence-before-it-happens-beware-the-unintended-consequences/\nhttps://happymag.tv/queensland-police-artificial-intelligence-domestic-violence/\nRelated \ud83c\udf10\nUK Met Police retrospective facial recognition\nBelgrade Safe City surveillance\nPage info\nType: Issue\nPublished: February 2022\nLast updated: June 2024", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/xiaomi-5g-mobile-communications-tracking-censorship", "content": "Study: Xiaomi 5G mobile phone censors anti-Beijing terms\nOccurred: September 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nXiaomi mobile phones were found to have a built-in ability to automatically detect and censor over 449 terms, prompting accusations of censorship. \nLithuania's National Cyber Security Centre discovered that Xiaomi's Mi 10T 5G phone could dtect and censor phrases including 'Free Tibet', 'Long live Taiwan independence' and 'democracy movement'. The investigators said the software had been turned off on Xiaomi's Mi 10T 5G phone for the 'European Union region', but could be turned on remotely at any time.\nThe Lithuanian government recommended its citizens not to buy new Chinese phones and quickly to get rid of those already purchased. Germany\u2019s cybersecurity watchdog, the Federal Office for Information Security (BSI), initiated a technical examination on a smartphone model from Xiaomi.\nXiaomi has denied thee allegations, stating that it has never and will never restrict or block any personal behaviour of its smartphone users. However, the controversy raised concerns about Xiaomi, Chinese government interference, and the rapid expansion of Chinese technology companies across Europe.\nSystem \ud83e\udd16\nXiaomi Mi 10T 5G\nOperator: Xiaomi\nDeveloper: Xiaomi\nCountry: Lithuania\nSector: Technology\nPurpose: Detect & censor sensitive terms\nTechnology: Content moderation\nIssue: Freedom of expression - censorship; Security\nTransparency: Governance\nInvestigations, assessments, audits \ud83e\uddd0\nNSCS (2023). Assessment of cybersecurity of mobile devices supporting 5G technology sold in Lithuania (pdf) \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reuters.com/business/media-telecom/lithuania-says-throw-away-chinese-phones-due-censorship-concerns-2021-09-21/\nhttps://www.bbc.co.uk/news/technology-58652249\nhttps://news.sky.com/story/lithuanian-defence-ministry-urges-people-to-throw-away-chinese-phones-after-discovering-censorship-tools-12414319\nhttps://www.nationalreview.com/2021/09/americans-invest-in-chinese-censorship/\nhttps://www.theguardian.com/world/2021/sep/22/lithuania-tells-citizens-to-throw-out-chinese-phones-over-censorship-concerns\nhttps://thenextweb.com/news/xiaomi-censorship-phone-lithuanian-report\nhttps://www.dw.com/en/lithuania-cybersecurity-agency-warns-against-chinese-made-phones/a-59266470\nRelated \ud83c\udf10\nHenan foreign journalist, student surveillance\nBytedance Uyghur censorship\nPage info\nType: Issue\nPublished: September 2021\nLast updated: June 2024", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/facebook-balkan-troll-farms", "content": "Facebook data leak exposes Balkan troll farm political disinformation\nOccurred: September 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nTroll farms reached 140 million US Facebook users per month and nearly half of all Americans in the run-up to the 2020 presidential election, according to a leaked document.\nAuthored by former Facebook data scientist Jeff Allen and leaked to the Technology Review, the document revealed that 75 percent of US users consumed troll content not because they followed a page but because Facebook\u2019s recommendation engine proactively served it to them.\nLargely based in the Balkans, the troll farms were seen to have targeted four primary groups - American Indians, Black Americans, Christian Americans, and American women - with a mixture of propaganda, misinformation and disinformation.\nAccording to the Technology Review, Allen left Facebook shortly after writing the document, partly because the company 'effectively ignored' his research.\nSystem \ud83e\udd16\nFacebook recommendation system\nDocuments \ud83d\udcc3\nFacebook (2019). How Communities are Exploited on our Platform: A Final Look at the 'Troll Farm' Pages\nOperator: Meta/Facebook\nDeveloper: Meta/Facebook\nCountry: US\nSector: Politics\nPurpose: Scare/confuse/destabilise\nTechnology: Content recommendation system\nIssue: Accuracy/reliability; Mis/disinformation\nTransparency: Governance; Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.technologyreview.com/2021/09/16/1035851/facebook-troll-farms-report-us-2020-election/\nhttps://arstechnica.com/tech-policy/2021/09/facebook-forced-troll-farm-content-on-over-40-of-all-americans-each-month/\nhttps://www.businessinsider.com/facebook-troll-farms-peddling-misinformation-reached-nearly-half-of-americans-2021-9\nhttps://theconversation.com/facebooks-algorithms-fueled-massive-foreign-propaganda-campaigns-during-the-2020-election-heres-how-algorithms-can-manipulate-you-168229\nhttps://drudge.com/news/253808/facebook-forced-troll-farm-content-40\nhttps://reaction.life/how-facebooks-algorithms-are-exploited-to-spread-propaganda/\nhttps://www.theverge.com/2021/9/23/22688976/facebook-research-scandals\nRelated \ud83c\udf10\nFacebook COVID-19 misinformation ad approvals\nFacebook downranking system failure\nPage info\nType: Incident\nPublished: October 2021\nPage updated: June 2024", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/nhs-digitaliproov-facial-recognition-data-sharing", "content": "NHS Digital, iProov facial recognition deal raises transparency concerns\nOccurred: September 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA contract between the UK's NHS Digital and private tech company, iProov, to collect and store facial verification data via the NHS app raised concerns about transparency and accountability.\niProov\u2019s technology is used to ensure people are genuinely present when using NHS login to access the NHS App. The process involves new users recording a video of their face, which is then sent to iProov to compare the facial data with anonymised photo IDs held by the government.\nHowever, privacy campaigners called for transparency on how data is used and stored. The contract between iProov and NHS Digital was not published, reportedly for \"security reasons\". This decision was criticised as \"unnecessarily undermining trust\".\nDespite assurances from both iProov and NHS Digital that app users\u2019 biometric data is anonymised and guarded via the best possible security protection, digital rights and privacy campaigners expressed concerns about the secrecy surrounding iProov\u2019s use of data. They argued that anyone who sends personal information to a private company, at the encouragement of the NHS, has a right to know exactly what happens to their data.\nThis controversy highlighted the ethical implications of data privacy and the need for transparency and accountability in the use of facial recognition technology, especially in the public service.\n\u2795 September 2021. The Guardian reported on iProov's close links with the UK's ruling Conservative Party, to which two of its directors donated.\nSystem \ud83e\udd16\nhttps://www.iproov.com/\nhttps://digital.nhs.uk/\nOperator: NHS Digital\nDeveloper: iProov\nCountry: UK\nSector: Gov - Health\nPurpose: Store facial verification data\nTechnology: Facial recognition\nIssue: Privacy; Security\nTransparency: Governance; Privacy\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.digitalhealth.net/2021/09/nhs-app-storing-facial-recognition-data-with-private-tech-company/\nhttps://www.business-humanrights.org/en/latest-news/uk-campaigners-raise-privacy-concerns-about-nhs-app-storing-facial-verification-data-via-contract-with-iproov/\nhttps://www.computing.co.uk/news/4037142/privacy-concerns-raised-nhs-deal-iproov-facial-collection\nhttps://www.telecompaper.com/news/privacy-groups-concerned-by-nhs-app-collecting-facial-recognition-data--1397160\nhttps://www.theguardian.com/society/2021/sep/15/nhs-app-storing-facial-verification-data-via-contract-with-firm-linked-to-tory-donors\nRelated \ud83c\udf10\nGoogle DeepMind, Royal Free data sharing\nGoogle/HCA Healthcare patient data sharing\nPage info\nType: Issue\nPublished: September 2021\nLast updated: June 2024", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-delivery-driver-safety-cameras", "content": "Amazon Driveri delivery driver safety monitoring slammed as inaccurate, unfair\nOccurred: February 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe use of an 'innovative' AI-enabled video camera system by Amazon was slammed by critics as inaccurate, unfair, and unnecessary.\n\nAccording to The Information, the Netradyne-supplied cameras were able to access drivers' location, movement, and biometric data to detect risky driver behaviour, with verbal warnings issued when drivers appear distracted, ignore signposts, or drive too fast. \n\nVice later reported that drivers who refuse to sign forms allowing Amazon to collect, store and use their facial and other biometric data lose their jobs, and that drivers are being unfairly punished for mistakes they had not made.\n\nDrivers, digital rights advocates and others complained that Amazon was running an inaccurate, unfair, and unnecessary system with inadequate security and privacy protection. \n\nThey also argued that Amazon deliberately makes it deliberately difficult for people being monitored to lodge complaints and appeals in a meaningful manner.\nSystem \ud83e\udd16\nNetradyne website\nOperator: Amazon\nDeveloper: Netradyne\nCountry: USA\nSector: Transport/logistics\nPurpose: Improve safety\nTechnology: CCTV; Computer vision\nIssue: Accuracy/reliability; Fairness; Surveillance; Security; Privacy; Employment - jobs, pay\nTransparency: Governance; Black box; Complaints/appeals\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theinformation.com/articles/amazon-plans-ai-powered-cameras-to-monitor-delivery-van-drivers\nhttps://www.cnbc.com/2021/02/03/amazon-using-ai-equipped-cameras-in-delivery-vans.html\nhttps://www.bbc.co.uk/news/technology-55938494\nhttps://www.independent.co.uk/life-style/gadgets-and-tech/amazon-ai-cameras-yawn-drivers-b1797528.html\nhttps://news.trust.org/item/20210205132207-c0mz7\nhttps://www.cnbc.com/2021/02/12/amazon-mentor-app-tracks-and-disciplines-delivery-drivers.html\nhttps://news.trust.org/item/20210319120214-n93hk/\nhttps://www.vice.com/en/article/dy8n3j/amazon-delivery-drivers-forced-to-sign-biometric-consent-form-or-lose-job\nhttps://www.businessinsider.com/amazon-delivery-driver-camera-ai-bezos-2021-3\nhttps://www.theverge.com/2021/2/3/22265031/amazon-netradyne-driveri-survelliance-cameras-delivery-monitor-packages\nhttps://www.vice.com/en/article/88npjv/amazons-ai-cameras-are-punishing-drivers-for-mistakes-they-didnt-make\nRelated \ud83c\udf10\nAmazon Flex delivery driver routing safety\nAmazon DSP Ans Rana driver liability\nPage info\nType: Incident \nPublished: February 2021\nLast updated: June 2024", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/alexei-navalny-smart-voting-bot", "content": "Social media platforms block Alexei Navalny smart voting chatbot\nOccurred: September 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nApple, Google and Telegram blocked a smart voting chatbot devised by jailed Russian opposition leader Alexei Navalny, sparking discussion about freedom of expression in Russia. \nLaunched on the day the country started voting in its 2021 parliamentary elections, Navalny's bot was seen as a way for voters opposed to Vladimir Putin to identify candidates able to defeat pro-Kremlin United Russia party candidates.\nThe bot was removed from the Telegram messenger app, following similar actions by Apple and Google. Telegram founder Pavel Durov said the service would abide by Russia\u2019s 'election silence', a law practiced in other countries that prohibits campaigning during the elections and that Telegram would 'plan' to limit the functioning of bots associated with election campaigns in the future. \nNavalny\u2019s allies accused Apple and Google of \u201ccensorship\u201d and claimed that these companies faced public threats from the Russian government and private threats of serious criminal charges and incarceration of local staff.\nSystem \ud83e\udd16\nTelegram website\nTelegram Wikipedia profile\nDocuments \ud83d\udcc3\nhttps://t.me/durov_russia/32\nOperator: Apple, Google, Telegram  \nDeveloper: FBK\nCountry: Russia\nSector: Politics\nPurpose: Facilitate tactical voting\nTechnology: Chatbot\nIssue: Freedom of expression - censorship  \nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theglobeandmail.com/world/article-navalnys-app-removed-from-apple-and-google-stores-as-russians-head-to/\nhttps://www.bbc.co.uk/news/world-europe-58593940\nhttps://edition.cnn.com/2021/09/17/tech/navalny-app-google-facebook/index.html\nhttps://www.newsweek.com/navalny-app-deleted-google-apple-putin-1630113\nhttps://www.nytimes.com/2021/09/17/world/europe/russia-navalny-app-election.html\nhttps://www.reuters.com/world/europe/google-apple-remove-navalny-app-stores-russian-elections-begin-2021-09-17/\nhttps://www.dw.com/en/google-apple-remove-navalnys-tactical-voting-app-as-russian-polls-open/a-59209275\nhttps://www.latimes.com/world-nation/story/2021-09-17/navalny-app-removed-russia-elections\nhttps://www.rferl.org/a/telegram-navalny-smart-voting/31466263.html\nhttps://www.engadget.com/telegram-blocks-russia-navalny-chat-bot-during-vote-211544623.html\nhttps://www.aljazeera.com/news/2021/9/20/navalny-allies-accuse-telegram-and-other-platforms-of-censorship\nRelated \ud83c\udf10\nMoscow arrests Navalny funeral attendees using facial recognition\nMoscow Metro Face Pay facial recognition\nPage info\nType: Issue\nPublished: September 2021\nLast updated: June 2024", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/mohsen-fakhrizadeh-assassination", "content": "Mohsen Fakhrizadeh assassinated with robot machine gun\nOccurred: November 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nMohsen Fakhrizadeh, the head of Iran's nuclear programme, was assassinated by a remote-controlled machine gun as he was being driven from Tehran to his weekend villa in Absard.\nThe first known autonomous assassination, the attack promptedpoliticians, activists and others In Iran and other countries to voice their fears about the nature and use of lethal autonomous weapons. \nThe New York Times later published a report confirming that Fakhrizadeh was assassinated using a modified Belgian FN Mag machine gun incorporating robotics and various forms of artificial intelligence, including facial recognition. The weapon was reportedly capable of firing at Fakhrizadeh without hitting his wife, who was beside him.\nAccording to the Times, the attack was controlled by a Mossad team operating in a command centre outside Iran.\nIsrael has not publicly commented on the allegations of its involvement.\nSystem \ud83e\udd16\nFN MAG website\nFN MAG Wikipedia profile\nOperator: Mossad\nDeveloper: FN MAG\nCountry: Israel  \nSector: Govt - military\nPurpose: Kill/maim/damage/destroy  \nTechnology: Robotics; Facial recognition\nIssue: Dual/multi-use; Autonomous lethal weapons  \nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/world-middle-east-55214359\nhttps://www.theguardian.com/world/2020/dec/07/mohsen-fakhrizadeh-iran-says-ai-and-satellite-controlled-gun-used-to-kill-nuclear-scientist\nhttps://www.thetimes.co.uk/article/mohsen-fakhrizadeh-nuclear-chief-assassinated-by-ai-machinegun-60x2dghnx\nhttps://www.msn.com/en-us/news/world/iran-nuclear-scientist-mohsen-fakhrizadeh-assassinated-shot-with-remote-controlled-machine-gun-news-agency-says/ar-BB1btCtK\nhttps://apnews.com/article/iran-israel-killed-mohsen-fakhrizadeh-88c2173048f77695af864d2055af54c6\nhttps://edition.cnn.com/2020/12/06/middleeast/iran-nuclear-scientist-mohsen-fakhrizadeh-satellite-intl/index.html\nhttps://www.forbes.com/sites/davidhambling/2020/12/22/3d-modeling-finds-suprising-source-of-shots-that-killed-iranian-scientist\nhttps://onezero.medium.com/was-an-iranian-scientist-assassinated-with-an-a-i-weapon-50ec9d5b1206\nhttps://www.nytimes.com/2021/09/18/world/middleeast/iran-nuclear-fakhrizadeh-assassination-israel.html\nhttps://www.jpost.com/middle-east/mossad-assassinated-irans-chief-nuke-scientist-with-remote-ai-gun-report-679751\n\nRelated \ud83c\udf10\nVision 60/SPUR quadrupedal war robot\nIsrael AI robot machine guns fire tear gas at Palestinian protestors\nPage info\nType: Incident\nPublished: December 2020\nLast updated: June 2024", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/facebook-google-abortion-reversal-ads", "content": "Facebook, Google run millions of unsafe 'abortion reversal' ads\nOccurred: September 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA research study found Facebook and Google had been running millions of ads for unsafe and potentially dangerous 'abortion reversal' pills on their platforms, despite policies banning the treatment.\nSo-called 'abortion reversal' pills promote the theoretical use of high doses of the hormone progesterone to \u2018reverse\u2019 the effects of taking mifepristone, the first of a pair of drugs used in a medical abortion. \nHowever, there is a lack of medical evidence demonstrating the safety and efficacy of the treatment, which is deemed unsafe and unscientific by The American College of Obstetricians and Gynecologists.\nThe Center for Countering Digital Hate (CCDH) discovered that Facebook ran 92 abortion reversal ads between January 1, 2020, and September 8, 2021 from from Live Action News, Live Action, and Heartbeat International worth between USD 115,400 and USD 140,667. \nIt also found the ads were seen 18.4 million times since January 2020, and 700,000 times by 13 to 17 year-olds. Facebook\u2019s own analytics show that as many as 1.5 million users in the UK and 3 million in the Republic of Ireland could have been targeted by these ads.\nBoth companies' policies banned 'unsafe' or 'misleading' advertising. Google had rules that banned advertisers from promoting misleading information about products, while Facebook banned the ads from being targeted to 13- to 17-year-olds.\nGoogle removed the ads after the CCDH published its report. \nSystem \ud83e\udd16\nGoogle AdSense website\nFacebook Ads Manager\nOperator: Meta/Facebook; Alphabet/Google\nDeveloper: Meta/Facebook; Alphabet/Google\nCountry: USA\nSector: Health\nPurpose: Sell advertising\nTechnology: Facebook Ads; Google Ads\nIssue: Ethics; Mis/Disinformation\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nCenter for Countering Digital Hate (2021). Endangering Women for Profit\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.businessinsider.com/facebook-profits-from-abortion-reversal-ads-seen-184-million-times-2021-9\nhttps://www.theguardian.com/world/2021/sep/15/facebook-and-google-condemned-over-ads-for-abortion-pill-reversal\nhttps://www.msn.com/en-gb/news/world/facebook-accused-of-cashing-in-on-abortion-reversal-adverts/ar-AAOvILV\nhttps://www.catholicnewsagency.com/news/249021/senators-ask-google-why-it-removed-ads-for-abortion-pill-reversal\nhttps://gizmodo.com/facebook-profits-from-showing-teens-unethical-abortion-1847675230\nhttps://www.msn.com/en-us/news/technology/horrifying-and-utterly-unsurprising-the-shady-abortion-reversal-ads-running-on-facebook/ar-AAOryLe\nhttps://www.msn.com/en-us/news/us/anti-abortion-groups-ad-on-abortion-pill-reversal-among-over-a-dozen-blocked-by-google/ar-AAOuyHD\nhttps://www.thedailybeast.com/facebook-is-raking-it-in-with-anti-abortion-ads-from-live-action\nRelated \ud83c\udf10\nFacebook teen alcohol, drug, gambling ads approvals\nFacebook political ads misidentification\nPage info\nType: Issue\nPublished: September 2021\nLast updated: June 2024", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/new-south-wales-victoria-covid-19-facial-recognition-trials", "content": "Australia police COVID-19 quarantine facial recognition trial draws concerns\nOccurred: September 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA trial of facial recognition software by Australian provinces South Australia, Victoria, and New South Wales prompted concerns from civil rights groups and others over a lack of privacy safeguards. \nPolice in the two states trialled facial recognition software that enabled them to check whether people were quarantining at home during COVID-19. In the trial, people had to respond to random check-in requests by taking a photo of themselves at their designated home quarantine address. \nSofware manufacturer Genvis claimed the trial was voluntary. However, the authorities ere entitled to follow up with a visit to the location to confirm the person's whereabouts should the software not verify the image against their 'facial signature'. \nRights advocates warned the technology may be inaccurate, threaten people's privacy, and be used by the police for other purposes.\n\u2796 September 2021. South Australia state trialled a similar, non-Genvis technology, sparking warnings from privacy advocates around the world about potential surveillance overreach. \nSystem \ud83e\udd16\nVictoria Department of Health    \nOperator: NSW Health; NSW Police; Department of Health Victoria\nDeveloper: Genvis Pty\nCountry: Australia\nSector: Gov - police\nPurpose: Enforce COVID-19 quarantine\nTechnology: Facial recognition\nIssue: Privacy; Surveillance; Dual/multi-use\nTransparency: Governance; Privacy\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reuters.com/world/asia-pacific/australias-two-largest-states-trial-facial-recognition-software-police-pandemic-2021-09-16/\nhttps://uk.news.yahoo.com/australian-police-facial-recognition-sure-095719829.html\nhttps://www.itnews.com.au/news/nsw-to-trial-facial-recognition-geolocation-app-for-home-quarantine-569950\nhttps://uk.news.yahoo.com/australia-trials-home-quarantine-vaccinated-090751505.html\nhttps://www.unilad.co.uk/life/australia-introduces-orwellian-police-enforced-facial-recognition-covid-app/\nhttps://www.thetimes.co.uk/article/australia-will-use-facial-recognition-to-help-reopen-borders-pn0bwjfvl\nhttps://www.theguardian.com/australia-news/2021/sep/04/south-australia-facial-recognition-trial-covid-app-blasted-by-fox-and-breitbart-criticised-over-lack-of-safeguards\nhttps://www.abc.net.au/news/2021-08-23/how-will-south-australias-home-quarantine-trial-work/100398878\nhttps://www.theatlantic.com/ideas/archive/2021/09/pandemic-australia-still-liberal-democracy/619940/\nhttps://www.adelaidenow.com.au/coronavirus/us-news-magazine-claims-sas-voluntary-covid19-home-quarantine-tracking-app-is-as-orwellian-as-any-in-the-free-world/news-story/c053b4f3108314929dfc6d2a5ebd5277     \nRelated \ud83c\udf10\nBucheon COVID-19 facial recognition tracking\nMainz police Luca COVID-19 abuse\nPage info\nType: Incident\nPublished: September 2021\nLast updated: June 2024", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chinese-study-uses-facial-recognition-to-identify-uyghurs-tibetans", "content": "Chinese research study uses facial recognition to identify Uyghurs, Tibetans\nOccurred: September 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA research study by a Chinese professor at Curtin University, Australia, on 'Chinese ethnical groups' that used facial recognition to identify Uyghurs, Tibetans and Koreans was found to have breached the university's ethics standards. \nWanquan Liu published the study (pdf), which he co-authored with three other academics at Chinese universities and which was co-funded by the Chinese government, without the consent of its subjects, and apparently without the university's knowledge. \nLiu resigned his post and moved to a university in China. The university said it would overhaul its informal academic research approval procedures and has asked publisher Wiley to retract the study.\nSystem \ud83e\udd16\nUnknown\nOperator: Wanquan Liu; Cunrui Wang; Qingling Zhan; Yu Li; Lixin Miao\nDeveloper: Wanquan Liu; Cunrui Wang; Qingling Zhan; Yu Liu; Lixin Miao\nCountry: Australia, China\nSector: Research/academia\nPurpose: Identify Uyghur & Tibetan minorities\nTechnology: Facial recognition\nIssue: Ethics/values; Privacy; Surveillance\nTransparency: Governance; Privacy\nResearch, advocacy \ud83e\uddee\nWang C., Zhang Q., Liu W., Liu Y., Lixin Miao L. (2018). Facial feature discovery for ethnicity recognition (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.abc.net.au/news/2021-09-15/curtin-university-lobby-remove-unethical-uyghur-ai-study/100463996\nhttps://www.abc.net.au/news/2019-07-16/australian-unis-to-review-links-to-chinese-surveillance-tech/11309598\nhttps://www.youtube.com/watch?v=t-axd1Ht_J8\nhttps://news.yahoo.com/australian-university-says-chinese-uyghur-094136979.html\nhttps://www.bbc.co.uk/news/world-australia-58571618\nhttps://www.timeshighereducation.com/news/chinese-facial-recognition-scholar-ignored-questions-went-home\nhttps://ipvm.com/reports/eth-rec-ethics\nRelated \ud83c\udf10\nHuawei Uyghur-spotting patent\nBytedance/TikTok Uyghur censorship\nPage info\nType: Issue\nPublished: September 2021\nLast updated: June 2024", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/instagram-teen-girls-mental-health-harms", "content": "Meta/Facebook 'aware of' Instagram impact on teen girls' mental health\nOccurred: September 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nFacebook was aware for years that teenagers blame Instagram for increased levels of anxiety and depression, made little effort to address the issue, and played it down in public, according to leaked documents. \nInternal research, presentations and emails by the social network obtained by the Wall Street Journal revealed Facebook (since renamed Meta) knew its photo-sharing app Instagram had had harmful effects on many of its young users, particularly teenage girls. Over 40 percent of Instagram\u2019s users are 22 years-old or younger.\nThe documents spurred US Senators Richard Blumenthal and Marsha Blackburn to say they 'will use every resource at [their] disposal to investigate what Facebook knew and when they knew it.'\nFacebook CEO Mark Zuckerberg responded by saying that he did not reckon 'the research is conclusive' on the extent to which social media impacts childrens' declining mental health. \nAccording to Forbes, the social network has refused many requests from members of Congress to share its research on children\u2019s mental health, arguing it is 'kept confidential to promote frank and open dialogue and brainstorming internally.'\nSystem \ud83e\udd16\nInstagram website\nInstagram Wikipedia profile\nDocuments \ud83d\udcc3\nInstagram (2021). Using research to improve your experience\nInstagram (2021). What Our Research Really Says About Teen Well-Being and Instagram\nOperator: Meta/Facebook  \nDeveloper: Meta/Facebook\nCountry: USA\nSector: Technology\nPurpose: Moderate content\nTechnology: Content moderation system\nIssue: Ethics; Hypocrisy  \nTransparency: Governance; Black box; Marketing; Legal\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nEdward J. Markey letter to Mark Zuckerberg (pdf)\nFrancis Haugen testimony to US Senate Commerce Subcommittee\nResearch, advocacy \ud83e\uddee\nPew Research (2018). Teens\u2019 Social Media Habits and Experiences\nInvestigations, assessments, audits \ud83e\uddd0\nWall Street Journal (2021). Facebook Knows Instagram Is Toxic for Teen Girls, Company Documents Show\nWall Street Journal (2021). Facebook\u2019s Documents About Instagram and Teens, Published\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/technology-58570353\nhttps://www.forbes.com/sites/jemimamcevoy/2021/09/14/facebook-internal-research-found-instagram-can-be-very-harmful-to-young-girls-report-says/\nhttps://www.cnbc.com/2021/09/14/facebook-documents-show-how-toxic-instagram-is-for-teens-wsj.html\nhttps://www.aljazeera.com/economy/2021/9/14/facebook-knows-instagram-is-harmful-to-teen-girls-wsj\nhttps://www.thestar.com.my/tech/tech-news/2021/09/15/us-senators-vow-to-probe-facebooks-knowledge-of-instagrams-risks-to-girls\nhttps://www.theguardian.com/technology/2021/sep/14/facebook-aware-instagram-harmful-effect-teenage-girls-leak-reveals\nhttps://www.bloombergquint.com/onweb/senators-vow-to-probe-facebook-s-knowledge-of-risks-to-girls\nhttps://www.cnet.com/news/politics/facebook-exec-grilled-by-lawmakers-over-mental-health-impact-on-children/\nRelated \ud83c\udf10\nCrisis Text Line data sharing\nGaggle student behavioural monitoring\nPage info\nType: Issue\nPublished: September 2021\nLast updated: June 2024", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/lapd-social-media-data-collection", "content": "LAPD collects personal social media data of every citizen it interviews\nOccurred: September 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe Los Angeles Police Department (LAPD) collected the social media details of every citizen it interviewed, including people who had not been arrested or accused of a crime.\nThousands of documents obtained by non-profit organisation The Brennan Center for Justice revealed that LAPD officers must record a civilian\u2019s Facebook, Instagram, Twitter and other social media accounts on 'field interview cards', alongside basic biographical information.\nIn a memo, LAPD police chief Michael Moore said interview cards would be reviewed by supervisors to ensure they are complete, so that they could later be used in 'investigations, arrests, and prosecutions'.\nThe findings raised concerns about civil rights and mass surveillance without justification, as well as potential privacy abuse. The LAPD told the Guardian that the field interview card policy was being updated, but 'declined to provide further details.'\nThe documents had come to light after the LAPD refused to hand over documents in a records request filed by The Brennan Center and was sued.\nSystem \ud83e\udd16\nDataminr website\nGeofeedia Wikipedia profile\nOperator: Los Angeles Police Department (LAPD)\nDeveloper: Geofeedia; Dataminr\nCountry: USA\nSector: Govt - police\nPurpose: Monitor individuals\nTechnology: Social media monitoring\nIssue: Human/civil rights; Privacy; Surveillance\nTransparency: Governance; Legal\nResearch, advocacy \ud83e\uddee\nhttps://www.brennancenter.org/our-work/research-reports/lapd-social-media-monitoring-documents\nhttps://www.brennancenter.org/sites/default/files/2021-04/2020-12-08%20First%20Amended%20Complaint.pdf\nhttps://www.brennancenter.org/sites/default/files/2020-02/20200130%20LA%20SMM%20CPRA%20Request_0.pdf\nFreedom of information requests \nCity of Los Angeles Public Records Request 20-719\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theguardian.com/us-news/2021/sep/08/revealed-los-angeles-police-officers-gathering-social-media\nhttps://thehill.com/changing-america/respect/equality/571399-lapd-officers-instructed-to-get-social-media-data-on-every\nhttps://www.dailymail.co.uk/news/article-9970801/LAPD-orders-officers-collect-social-media-civilian-interview-not-arrested.html\nhttps://www.dailydot.com/debug/lapd-collect-social-media-civilians/\nhttps://news.yahoo.com/lapds-mass-collection-social-media-202536192.html\nhttps://gizmodo.com/the-lapds-spy-tactics-detailed-in-over-6-000-pages-of-n-1847639084\nhttps://www.msn.com/en-us/news/technology/the-lapd-built-collecting-social-media-info-into-its-interview-process-for-civilians/ar-AAOeMZ6\nRelated \ud83c\udf10\nUS Postal Inspection Service iCOP covert monitoring and surveillance\nPimEyes facial recognition search engine\nPage info\nType: Incident\nPublished: September 2021\nLast updated: June 2024", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tiktok-recommends-adult-content-to-children", "content": "TikTok USA recommends drugs, alcohol to children\nOccurred: September 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nTikTok actively recommended drugs, alcohol, and sexual content to children as young as 13, according to a Wall Street Journal investigation.\nThe WSJ set up a series of fake accounts to see how TikTok's algorithm worked. TikTok served one account registered as a 13-year-old at least 569 videos about drug use, references to cocaine and meth addiction, and promotional videos for online sales of drug products and paraphernalia. \nDespite their age settings, the more sexual content the teenagers viewed, the more they were given in return, with hundreds of similar videos appearing in the feeds of the Journal\u2019s other minor accounts.\nA spokesperson told the Journal that it had removed some of the reported videos, and restricted distribution of others. It also said it does not currently differentiate between content served to adults and children, but that it was working to on a new filter for younger users.  \nSystem \ud83e\udd16\nTikTok For You recommendation algorithm\nOperator: ByteDance/TikTok\nDeveloper: ByteDance/TikTok\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Recommend content\nTechnology: Recommendation algorithm\nIssue: Safety; Accuracy/reliability\nTransparency: Governance; Black box\nInvestigations, assessments, audits \ud83e\uddd0\nWall Street Journal (2021). How TikTok serves up drug videos to minors\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.dailymail.co.uk/sciencetech/article-9970435/TikToks-algorithm-promoting-sexual-content-children-young-13.html\nhttps://www.nationalreview.com/corner/tiktok-is-evil/\nhttps://www.christianpost.com/news/tiktok-served-videos-with-adult-content-to-minors-report.html\nhttps://www.thesun.co.uk/news/16087636/tiktok-investigation-sex-drugs-booze/\nhttps://www.msn.com/en-gb/news/newsbirmingham/warning-issued-to-any-tiktok-user-who-is-aged-between-13-and-15/ar-AAOfODu\nhttps://www.businessinsider.com/tiktok-explicit-videos-sex-and-drugs-to-minors-report-2021-9\nRelated \ud83c\udf10\nTikTok spliced beheading video\nTikTok #intersex 'censorship'\nPage info\nType: Incident\nPublished: September 2021\nLast updated: June 2024", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/nio-es8-fatal-crash", "content": "NIO ES8 crashes into highway patrol vehicle, killing driver\nOccurred: August 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Nio ES8 crashed into a highway maintenance vehicle in Putian, a city in eastern China, killing the Nio owner 31-year-old Lin Wenqin. \nReports state Nio's Navigate on Pilot (NOP) driver-assistance feature had been activated. \nAfterwards, a Beijing law company hired by Lin Wenqin's family accused Nio of tampering with the car's data and accessing and deleting potentially incriminating evidence without the approval of the traffic police. \nNio responded that the intervention was motivated by safety and that the operation performed on the vehicle would not cause data loss.\nThe incident raised concerns about Nio's the safety of Navigate on Pilot system, and about autonomous driving systems in general. It also raised questions about the company's integrity, transparency and accountability.\nSystem \ud83e\udd16\nNio Navigate on Pilot\nOperator: \nDeveloper: Nio\nCountry: China\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Self-driving system\nIssue: Accuracy/reliability; Safety\nTransparency: Governance; Complaints/appeals; Black box; Legal\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://technode.com/2021/09/07/drive-i-o-fatal-crash-threatens-nios-reputation-and-expansion-plans/\nhttps://insideevs.com/news/527231/nio-es8-fatal-crash-china/\nhttps://asia.nikkei.com/Spotlight/Caixin/Nio-denies-tampering-with-data-after-fatal-crash\nhttps://cnevpost.com/2021/08/22/family-of-car-owner-in-fatal-crash-accuses-nio-of-helping-to-destroy-evidence/\nhttps://finance.sina.com.cn/tech/2021-08-22/doc-ikqciyzm2920991.shtml\nhttps://www.msn.com/en-us/money/news/nio-faces-formal-complaint-from-family-of-deceased-entrepreneur-over-allegations-it-helped-destroy-falsify-evidence-report/ar-AANCEzT\nhttps://technode.com/2021/08/23/police-investigate-claims-ev-maker-nio-tampered-with-car-data-after-crash/\nhttps://cnevpost.com/2021/08/24/nio-begins-requiring-users-to-take-test-before-using-assisted-driving-features\nhttps://www.carscoops.com/2021/08/nio-is-requiring-owners-to-pass-a-test-before-using-its-semi-autonomous-system/\nhttps://electrek.co/2021/08/24/nio-now-requires-a-test-before-using-assisted-driving-following-fatal-crash/\nRelated \ud83c\udf10\nXpeng P7 crashes into truck\nTesla China Autopilot cruise control activation\nPage info\nType: Incident\nPublished: August 2021\nLast updated: June 2024", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/singapore-xavier-patrol-robots", "content": "Singapore Xavier patrol robots fuel surveillance state concerns\nOccurred: September 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA test of 'patrol robots' looking out for 'undesirable social behaviours' during the COVID-19 pandemic fueled concerns about a surveillance state in Singapore.\nSingapore government's Home Team Science and Technology Agency (HTX) announced a three-week test of robots that would scan for 'undesirable social behaviours' at a housing estate and shopping centre in the central Toa Payoh neighbourhood.\nThese behaviours included the congregation of more than five people, smoking in prohibited areas, illegal hawking, improperly parked bicycles, and riding motorised active mobility devices and motorcycles on footpaths.\nThe agency said the robots would be used for surveillance and public education during the trial, and would not be used for law enforcement. It went on to argue that robots are needed to address a labour crunch in an ageing society with a smaller workforce.\nSystem \ud83e\udd16\nXavier patrol robot\nDocuments \ud83d\udcc3\nMeet our newest patrol pal Xavier!\nOperator: Home Team Science and Technology Agency (HTX)  \nDeveloper: HTX, A*STAR\nCountry: Singapore  \nSector: Govt - interior\nPurpose: Manage 'undesirable behaviour'\nTechnology: Computer vision; Facial recognition; Neural network; Robotics\nIssue: Privacy; Surveillance\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reuters.com/technology/singapore-trials-patrol-robots-deter-bad-social-behaviour-2021-09-06/\nhttps://www.engadget.com/singapore-robots-patrol-065009631.html\nhttps://in.mashable.com/tech/24568/robots-patrol-the-streets-of-singapore-to-thwart-bad-social-behaviour\nhttps://www.thesun.co.uk/news/16064858/surveillance-robot-patrols-singapore-streets/\nhttps://www.livemint.com/news/world/singapore-trials-robots-to-detect-poor-social-behaviour-on-streets-11630936942517.html\nhttps://www.thestar.com.my/aseanplus/aseanplus-news/2021/09/05/autonomous-robots-check-on-bad-behaviour-in-singapore039s-heartland\nhttps://www.msn.com/en-in/news/techandscience/meet-xavier-singapores-robot-cop-that-will-penalise-human-bad-behaviour/ar-AAOaTud\nhttps://www.theguardian.com/world/2021/oct/06/dystopian-world-singapore-patrol-robots-stoke-fears-of-surveillance-state\nRelated \ud83c\udf10\nSingapore TraceTogether COVID-19 contact tracing data sharing\nMalaysia minister discourages Singaporeans from visiting Malaysia\nPage info\nType: Issue\nPublished: September 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/facebook-labels-black-men-primates", "content": "Facebook labels black men 'primates'\nOccurred: September 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nFacebook's AI-powered recommendation system asked users watching a Daily Mail video featuring Black men whether they like to '[k]eep seeing videos about 'Primates', prompting significant controversy.\nThe video was published by the Daily Mail on its Facebook page and depicted white people, including police officers, confronting Black men. No primates were shown. \nThe discovery led to accusations of racism, condemnation of Facebook's apparent inability to manage inappropriate content on its platform, including its over-reliance on technology to detect and manage unsafe content. \nThe company apologised for its 'unacceptable error', said it had disabled its topic recommendation feature responsible for the message, and would look to ensure it does not happen again.\nSystem \ud83e\udd16\nFacebook content recommendation system\nFacebook content moderation system\nOperator: Meta/Facebook\nDeveloper: Meta/Facebook\nCountry: USA\nSector: Technology\nPurpose: Recommend topics\nTechnology: Topic recommendation system\nIssue: Bias/discrimination - race; Safety\nTransparency: Governance; Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.nytimes.com/2021/09/03/technology/facebook-ai-race-primates.html\nhttps://engadget.com/facebook-ai-mislabels-video-black-men-primates-044255703.html\nhttps://www.telegraph.co.uk/news/2021/09/04/facebook-sorry-labelling-black-men-primates-video/\nhttps://eu.usatoday.com/story/tech/2021/09/03/facebook-video-black-men-primates-apology/5721948001/\nhttps://www.thedailybeast.com/facebook-ai-slaps-primates-label-on-daily-mail-video-of-black-men\nhttps://www.thewrap.com/facebook-apology-primates-label-video-black-men/\nhttps://www.news.com.au/breaking-news/facebook-mistakenly-labels-black-men-primates/news-story/48d4bb3956b64eaf58b71d1a7953256b\nhttps://www.theverge.com/2021/9/4/22657026/facebook-mislabeling-video-black-men-primates-algorithm\nhttps://www.forbes.com/sites/edwardsegal/2021/09/04/facebook-apologizes-for-embarassing-mistake-caused-by-ai/\nRelated \ud83c\udf10\nGoogle Photos mislabels black Americans as gorillas\nFacebook job ad delivery gender discrimination\nPage info\nType: Incident\nPublished: September 2021\nLast updated: June 2024", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/toyota-paralympics-self-driving-bus-hits-athlete", "content": "Toyota Paralympics self-driving bus hits disabled athlete\nOccurred: August 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Toyota e-Palette self-driving bus used to ferry athletes during the 2020 Paralympic Games in Tokyo hit a Japanese visually impaired athlete. \nJudo specialist Aramitsu Kitazono had been attempting to cross a street at a designated crossing within the Athletes Village when he was hit. He was suffered bruising to his head and leg, which required about two weeks of recovery time, and was left unable to compete. \nThe autonomous bus had two human operators on board who were tasked with supervising its operation. The bus had stopped automatically just before the incident when it detected a security guard close to an intersection. Kitazono was hit as the bus made a right turn onto that intersection because the operators had assumed he would stop walking and pressed the start button to resume operation of the vehicle.\nThe Toyota e-Palette self-driving vehicle was being used as a shuttle bus for athletes but failed to stop as Kitazono was using a pedestrian crossing. The following day Toyota president Akio Toyoda apologised, before adding that 'It shows that autonomous vehicles are not yet realistic for normal roads.'\nThe incident raised questions about the safety of Toyota self-driving vehicles.\nSystem \ud83e\udd16\nToyota e-Palette website\n\nDocuments  \ud83d\udcc3\nToyota. Resumption of Services of the Toyota e-Palette Vehicle and Additional Safety Measures at the Tokyo 2020 Paralympic Athletes' Village\nOperator:  \nDeveloper: Toyota\nCountry: Japan\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Self-driving system\nIssue: Safety; Accuracy/reliability  \nTransparency: Governance; Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.japantimes.co.jp/sports/2021/08/28/paralympics/summer-paralympics/aramitsu-kitazono-athletes-village-injury/\nhttps://www.theguardian.com/technology/2021/aug/28/toyota-pauses-paralympics-self-driving-buses-after-one-hits-visually-impaired-athlete\nhttps://www.msn.com/en-us/autos/news/toyota-restarts-self-driving-shuttles-after-paralympics-crash-with-pedestrian/ar-AANU9lg\nhttps://news.sky.com/story/tokyo-paralympics-visually-impaired-athlete-forced-to-withdraw-after-accident-with-self-driving-shuttle-bus-12392806\nhttps://www.telegraph.co.uk/paralympic-sport/2021/08/28/paralympic-athlete-ruled-games-run-self-driving-bus-village/\nhttps://www.businessinsider.com/toyota-suspends-self-driving-buses-after-paralympic-accident-2021-8\nhttps://www.yahoo.com/now/toyota-suspends-self-driving-vehicles-185215001.html\nhttps://english.kyodonews.net/news/2021/08/4529830fbb83-sight-impaired-paralympian-hit-by-autonomous-bus-in-athletes-village.html\nhttps://www.forbes.com/sites/peterlyon/2021/08/28/collision-with-paralympic-athlete-forces-toyota-to-halt-self-driving-bus-service-in-olympic-village/\nhttps://www.reuters.com/business/autos-transportation/toyota-halts-all-self-driving-e-pallete-vehicles-after-olympic-village-accident-2021-08-27/\nhttps://www.euronews.com/next/2021/08/30/toyota-halts-autonomous-e-palette-buses-after-one-hits-paralympic-athlete-in-tokyo-olympic\nRelated \ud83c\udf10\nTesla FSD beta test car hits bollard, driver fired\nTesla phantom braking\nPage info\nType: Incident\nPublished: August 2021\nLast updated: June 2024", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/hour-one-character-clones", "content": "Hour One AI 'character' clones accused of allowing misuse\nOccurred: August 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn AI system that enables anyone to create a full digital clone of themselves speaking on camera in any language was accused of providing inadequate usage and protection controls.\nTel Aviv-based 'video transformation company' Hour One pays people to deepfake their face so that their 'characters' can be used in promotional, commercial, and educational videos. The company said it has a library of around 100 characters.\nCritics pointed out that Hour One's ethics policy provides little information or guidance about how personal clones should be used and misused by its customers - such as for fraud, extortion and other crimes. It also said little about how it would protect the privacy of people whose characters they were synthesising.\nOthers pointed out that the system posed a threat to voice artists and other creative professionals, whose voices could be appropriated in ways that could jeopardise their reputation and ability to earn income.\nSystem \ud83e\udd16\nHour One website\nDocuments \ud83d\udcc3\nHour One ethics policy\nOperator: Hour One\nDeveloper: Hour One\nCountry: Israel\nSector: Business/professional services\nPurpose: Market products/services\nTechnology: Computer vision\nIssue: Employment; Ethics/values; Privacy; Security\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.technologyreview.com/2021/08/27/1033879/people-hiring-faces-work-deepfake-ai-marketing-clones/\nhttps://futurism.com/the-byte/company-deepfake-advertising-clones\nhttps://www.inputmag.com/tech/were-begging-you-to-not-turn-yourself-into-ai-powered-clone\nhttps://www.fastcompany.com/90694393/hour-one-is-building-an-army-of-deepfake-like-talking-heads-maybe-including-you\nhttps://mixed.de/hour-one-deepfake-verleihservice-fuer-gesichter/\nhttps://petapixel.com/2021/02/16/ai-can-now-turn-you-into-a-fully-digital-realistic-talking-clone/\nhttps://interestingengineering.com/advertising-company-wants-deepfake-clones-of-your-face\nhttps://wonderfulengineering.com/this-company-wants-to-put-your-face-in-advertisements-by-making-deepfake-clones-with-ai/\nhttps://www.diyphotography.net/this-tool-turns-you-into-a-creepily-realistic-ai-clone/\nRelated \ud83c\udf10\nDeepfake 'Pan Africanists' support Burkina Faso junta\nCruzcampo Lola Flores deepfake ad\nPage info\nType: Incident\nPublished: August 2021\nLast updated: October 2023", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tesla-model-3-hits-parked-police-car", "content": "Tesla Model 3 hits parked police car with emergency lights activated\nOccurred: August 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Tesla Model 3 reportedly with its Autopilot driver assistance system engaged slammed into a parked police car with its emergency lights activated on an interstate road in Orlando, Florida. \nThe 2019 Tesla narrowly missed the police car driver, who was helping the driver of a Mercedes, and collided with the other vehicle. The trooper whose cruiser was hit shortly had activated his emergency lights.\nThe Tesla driver and the driver of the disabled vehicle suffered minor injuries. The trooper was unhurt.\nThe driver claimed he was using Autopilot, Tesla's partially automated driving system, at the time of the crash. \n\u2796 August 2021. The US government announced a sweeping investigation into Tesla's Autopilot driving system.\nSystem \ud83e\udd16\nTesla Autopilot, Full-self Driving\nOperator: \nDeveloper: Tesla\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Driver assistance system\nIssue: Accuracy/reliability; Safety\nTransparency: Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://apnews.com/article/technology-business-florida-001915f68f2327a42eb5b6f5c4ccd2b7\nhttps://www.msn.com/en-us/news/other/a-tesla-model-3-hit-a-parked-police-car-in-orlando-driver-said-she-was-in-autopilot/ar-AANQI1Y\nhttps://thehill.com/policy/transportation/automobiles/569860-tesla-car-on-autopilot-hits-parked-police-car-in-florida\nhttps://jalopnik.com/a-tesla-that-the-driver-says-was-on-autopilot-crashed-i-1847581492\nhttps://uk.news.yahoo.com/tesla-autopilot-crashes-police-car-003000315.html\nhttps://metro.co.uk/2021/08/29/us-tesla-on-autopilot-crashes-into-police-and-nearly-hits-officer-15170672\nhttps://www.whichcar.com.au/car-news/another-tesla-autopilot-police-crash\nhttps://gizmodo.com/a-tesla-model-3-with-autopilot-activated-crashes-into-t-1847579288\nRelated \ud83c\udf10\nTesla Model S crashes into fire engine\nTesla Model X crashes into five police officers\nPage info\nType: Incident\nPublished: August 2021\nLast updated: June 2024", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tesla-model-3-hits-six-children-adult", "content": "Tesla Model 3 hits six children, adult\nOccurred: August 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Tesla Model 3 was involved in an accident at Ardingly College, West Sussex, where it hit six children and an adult. \nThe incident occurred during a holiday camp pick-up. One of the children, an eight-year-old boy, was seriously injured and airlifted to St George\u2019s Hospital in London. The other five children and the adult received treatment for minor injuries. \nThe driver of the Tesla, a 47-year-old woman, was uninjured. An investigation law launched into the cause of the collision, including with the car's Autopilot driver assistance system had been activated at the time of the crash.\nSystem \ud83e\udd16\nTesla Autopilot, Full-self Driving\nOperator: \nDeveloper: Tesla\nCountry: UK\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Driver assistance system\nIssue: Safety; Accuracy/reliability\nTransparency: Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.dailymail.co.uk/news/article-9904367/Police-probe-Tesla-Model-3-autopilot-mowed-six-schoolchildren-parent.html\nhttps://www.itv.com/news/meridian/2021-08-16/car-collides-with-pedestrians-in-sussex\nhttps://www.theargus.co.uk/news/19517369.air-ambulance-attends-accident-college-road-ardingly/\nhttps://www.bbc.co.uk/news/uk-england-sussex-58234999\nhttps://www.telegraph.co.uk/news/2021/08/16/five-children-injured-tesla-crashes-school-car-park/\nhttps://www.businessinsider.com/tesla-model-3-crash-school-england-ardingly-college-children-2021-8\nhttps://www.thetimes.co.uk/article/children-hurt-in-tesla-crash-at-west-sussex-school-qfqc3htr2\nhttps://www.nasdaq.com/articles/six-children-and-one-adult-injured-in-tesla-crash-2021-08-17\nRelated \ud83c\udf10\nTesla Model 3 hits parked police car\nTesla Model 3 Paris fatal crash\nPage info\nType: Incident\nPublished: August 2021\nLast updated: June 2024", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/mater-dei-hospital-medicine-robots", "content": "'Worthless' Mater Dei Hospital medicine robots cause mass resignations\nOccurred: August 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nTwo robots tasked with distributing medicine across Mater Dei Hospital in Malta were criticised by the country's nurses union for repeatedly breaking down, stocking the wrong drugs and providing incorrect medicine dosages to patients.\nThe Deenova robotic system was introduced by the hospital in 2019 to dispense medicines and reduce human error in the distribution and dosage of medicines. The robots, nicknamed Mario, were part of a multi-million-euro investment.\nThe Malta Union of Midwives and Nurses (MUMN) described the robots as 'worthless' and a 'complete failure', wuth nurses having to double-check the work done by the machines, borrow drugs from adjacent wards, and revert back to the old manual system of medicine distribution.\nFurthermore, the union claimed eight nurses in one ward resigned since the robots' introduction, with the rest demanding to be transferred. It demanded that the system was removed.\nMalta health minister Chris Fearne defended the introduction of the machines by saying they would increase efficiency and they were meant to help and not replace nurses.\nSystem \ud83e\udd16\nMario robot\nOperator: Mater Dei Hospital\nDeveloper: Deenova\nCountry: Malta\nSector: Gov - health\nPurpose: Distribute medicines\nTechnology: Robotics\nIssue: Accuracy/reliability; Employment - jobs\nTransparency: Governance; Complaints/appeals \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.maltatoday.com.mt/news/national/111610/medicine_dispensing_robot_worthless_total_failure_nurses_union_claims#.YSoxQY5Kg2w\nhttps://lovinmalta.com/news/teething-problems-in-mater-deis-medicine-robots-that-cost-e23-million-have-been-addressed-sources-say/\nhttps://maltadaily.mt/chris-fearne-defends-criticised-robots-at-mater-dei-hospital/\nhttps://www.maltatoday.com.mt/news/national/111638/robot_at_centre_of_nurses_complaints_has_increased_efficiency_health_minister_says#.YSoxc45Kg2w\nhttp://www.independent.com.mt/articles/2021-08-23/local-news/25-million-robotics-equipment-a-complete-failure-and-waste-of-taxpayer-s-money-MUMN-6736236164\nhttps://newsbook.com.mt/en/mater-dei-medicine-distribution-system-a-complete-failure-mumn/\nRelated \ud83c\udf10\nMedical robot tells man he is dying\nHonolulu homeless robot temperature tests\nPage info\nType: Incident\nPublished: August 2021\nLast updated: June 2024", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/us-mortgage-approval-algorithm-discrimination", "content": "US mortgage approval algorithm more likely to reject people of colour \nOccurred: August 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nUS mortgage lenders were much more likely to turn down Latino, Asian, Native American and Black applicants than White ones, according to an investigation. \nAn assessment of mortgage data by The Markup, co-published by the AP, discovered that mortgage loan applicants of colour were 40-80 percent more likely to be denied than their White counterparts across the US, and that the disparity was greater than 250 percent in some urban areas.\nThe complex statistic analysis of more than two million conventional mortgage applications for home purchases found that lenders were 40 percent more likely to turn down Latino applicants for loans, 50 percent more likely to deny Asian/Pacific Islander applicants, and 70 percent more likely to deny Native American applicants than similar White applicants.\nThe investigation also found that lenders in 2019 were more likely to deny home loans to people of colour than to white people with similar financial characteristics.\nThe algorithms used by lenders were mostly mandated and mortgage applications approved by Freddie Mac and Fannie Mae, whose own automated underwriting algorithms are a closely held secret.\nThe findings suggested that there was a hidden bias in mortgage-approval algorithms, leading to significant racial disparities in loan approvals. \n\u2795 February 2024. The Markup's investigation was cited by Senator Ron Wyden as a reason for the introduction of the US Algorithmic Accountability Act of 2022. \nSystem \ud83e\udd16\nClassic FICO Score\nFICO Score Wikipedia profile\nDocuments \ud83d\udcc3\nhttps://www.documentcloud.org/documents/21011405-mortgage-bankers-trade-groups-letter-to-cfpb-10-29-14\nhttps://www.documentcloud.org/documents/21010234-cfpb-final-policy-guidance-disclosure-of-loan-level-hmda-data\nOperator: Freddie Mac; Fannie Mae\nDeveloper: Freddie Mac; Fannie Mae\nCountry: USA\nSector: Banking/financial services\nPurpose: Assess mortgage applications\nTechnology: Underwriting algorithms\nIssue: Bias/discrimination - race, ethnicity\nTransparency: Governance; Black box\nInvestigations, assessments, audits \ud83e\uddd0\nThe Markup. The secret bias in mortgage approval algorithms\nThe Markup. Dozens of mortgage lenders showed significant disparities. Here are the worst\nThe Markup. How we investigated racial disparities in federal mortgage data\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://apnews.com/article/lifestyle-technology-business-race-and-ethnicity-mortgages-2d3d40d5751f933a88c1e17063657586\nhttps://www.marketplace.org/2021/08/25/housing-mortgage-algorithms-racial-disparities-bias-home-lending/\nhttps://www.dailymail.co.uk/news/article-9925561/The-secret-bias-hidden-mortgage-approval-algorithms.html\nhttps://www.forbes.com/sites/korihale/2021/09/02/ai-bias-caused-80-of-black-mortgage-applicants-to-be-denied\nhttps://www.culturebanx.com/post/a-i-bias-caused-80-of-black-mortgage-applicants-to-be-denied\nhttps://greensboro.com/news/local_news/report-minorities-in-greensboro-high-point-more-likely-to-be-denied-mortgage-loans/article_13f07052-0505-11ec-a8a7-bfb9809ff675.html\nhttps://journalnow.com/news/state-and-regional/racial-disparity-in-mortgage-loan-denials-less-pronounced-in-winston-salem-area-compared-with-n/article_8fd6af44-043b-11ec-9c73-cb6f46a4ba52.html\nhttps://www.nationalmortgagenews.com/news/fannie-mae-freddie-macs-automated-underwriting-changes-irk-lenders\nhttps://themarkup.org/denied/2022/02/04/markup-mortgage-industry-investigation-cited-in-support-of-algorithmic-accountability-bill\nRelated \ud83c\udf10\nUpstart consumer lending discrimination\nUS mortgage credit score data economic, racial bias\nPage info\nType: Incident\nPublished: August 2021\nLast updated: June 2024", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/credit-score-algorithm-data-economic-racial-bias", "content": "Study: US mortgage loans assessment tools suffer from economic, racial bias \nOccurred: August 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nUS mortgage loan assessment tools may suffer from economic and racial bias, according to Stanford Graduate School of Business researchers.\nLaura Blattner and Scott Nelson used artificial intelligence to test alternative credit-scoring models, finding that the predictive tools were between 5 and 10 percent less accurate for lower-income families and minority borrowers than for higher-income and non-minority groups.\nThe researchers pointed out that the issue was not that the credit score algorithms themselves are biased against disadvantaged borrowers. Instead, the underlying data was less accurate in predicting creditworthiness for these groups, often because these borrowers had limited credit histories. \nA \u201cthin\u201d credit history will in itself lower a person\u2019s score, because lenders prefer more data than less. But it also means that one or two small dings, such as a delinquent payment many years in the past, can cause outsized damage to a person\u2019s score.\nThe study highlighted the need for further investigation and potential reform in the mortgage lending industry to ensure fairness and equality.\nSystem \ud83e\udd16\nMultiple\nOperator:  \nDeveloper: \nCountry: USA\nSector: Banking/financial services\nPurpose: Calculate credit score; Predict loan default\nTechnology: Credit score algorithms\nIssue: Accuracy/reliability; Bias/discrimination - economic, racial\nTransparency: Black box\nResearch, advocacy \ud83e\uddee\nBlattner L., Nelson S. How Costly is Noise? Data and Disparities in Consumer Credit\nStanford HAI. How Flawed Data Aggravates Inequality in Credit\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.technologyreview.com/2021/06/17/1026519/racial-bias-noisy-data-credit-scores-mortgage-loans-fairness-machine-learning/\nhttps://www.weforum.org/agenda/2021/07/ai-machine-learning-bias-discrimination/\nhttps://www.futurity.org/noisy-data-ai-credit-risk-mortgages-2617872-2/\nhttps://www.heise.de/hintergrund/Kreditwuerdigkeit-in-den-USA-Wie-KI-gegen-Voreingenommenheit-helfen-koennte-6112379.html\nRelated \ud83c\udf10\nUpstart consumer lending discrimination\nUS mortgage approval data algorithm racial discrimination\nPage info\nType: Incident\nPublished: August 2021\nLast updated: June 2024", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/facebook-south-korea-facial-recognition-abuse", "content": "Facebook fined for violating privacy of 200,000 South Koreans\nOccurred: August 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nFacebook was fined USD 5.5m by South Korea's data privacy regulator PIPC for creating and storing facial recognition templates of 200,000 local users without consent. \nThe South Korean government\u2019s data protection watchdog, the Personal Information Protection Commission (PIPC), ordered Facebook to pay a fine of 6.46 billion won (approximately USD 5.5 million) for creating and storing facial recognition templates of 200,000 local users without proper consent between April 2018 and September 2019.\nIn addition, Facebook was issued a penalty of 26 million won (approximately USD 22,000) for illegally collecting social security numbers, not issuing notifications regarding personal information management changes, and other missteps.\nFacebook was ordered to destroy the facial information it had collected and was prohibited from processing identity numbers without a legal basis. \nThe fine was the second-largest ever issued by the PIPC.\n\u2796 November 2020. South Korea's PIPC fined Facebook for passing on personal data to other operators without user permission.\nSystem \ud83e\udd16\nFacebook website\nOperator: Meta/Facebook\nDeveloper: Meta/Facebook\nCountry: South Korea\nSector: Technology\nPurpose: Collect facial biometrics\nTechnology: Facial recognition\nIssue: Privacy\nTransparency: Privacy\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nPIPC. Personal Information Commission fines foreign companies for violating personal information protection laws\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttp://koreabizwire.com/facebook-netflix-fined-over-privacy-violations-in-s-korea/197567\nhttps://www.theregister.com/2021/08/26/facebook_fined_by_south_korea/\nhttp://www.koreaherald.com/view.php?ud=20210825000954\nhttps://www.cpomagazine.com/data-privacy/south-korean-regulator-fines-facebook-for-privacy-violations-social-media-giant-shared-personal-data-without-user-consent/\nhttps://koreajoongangdaily.joins.com/2021/08/25/business/tech/Google-Netflix-Facebook/20210825191400440.html\nhttps://www.kedglobal.com/newsView/ked202108260007?lang=1\nhttps://slashdot.org/story/21/08/26/2125219/facebook-used-facial-recognition-without-consent-200k-times-says-watchdog\nRelated \ud83c\udf10\nS Korea immigration facial recognition sharing\nBucheon COVID-19 facial recognition tracking\nPage info\nType: Incident\nPublished: August 2021\nLsst updated: June 2024", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/unity-govtech-ai-military-applications", "content": "Unity secretly develops AI products for US military\nOccurred: August 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGame development tool company Unity's GovTech unit secretly worked for the US Department of Defense, causing the firm's employees to complain of poor integrity and internal transparency. \nAccording to VICE, the GovTech unit developed technologies across Unity\u2019s products to help the government adapt AI and machine learning, with some of the technologies ending up with military clients without the knowledge of the employees who developed them.\nInternal Unity documents revealed the company is struggled to explain why its employees, who had joined to create tools for game makers, were developing technologies for military purposes. \nThe company attempted to manage the situation by issuing a memo titled \u201cGovTech Projects - Communication Protocol,\u201d which instructs managers on how to discuss these contracts with stakeholders. The memo emphasizes that Unity\u2019s work will not be used in live warfighting and that the company is providing a service or solution to Department of Defense companies.\nHowever, the lack of transparency and the potential military applications of their work have left some Unity employees uncomfortable, and calling for more clarity around the company\u2019s government contracts.\nSystem \ud83e\udd16\nUnity website\nUnity Wikipedia profile\nOperator: US Department of Defense\nDeveloper: Unity\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Develop military products\nTechnology: Machine learning\nIssue: Ethics/values\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.vice.com/en/article/y3d4jy/unity-workers-question-company-ethics-as-it-expands-from-video-games-to-war\nhttps://www.nme.com/news/gaming-news/unity-employees-demand-better-transparency-over-military-contracts-3027454\nhttps://kotaku.com/report-unity-employees-not-thrilled-their-work-is-supp-1847541134\nhttps://www.gamesindustry.biz/articles/2021-08-23-report-unitys-transparency-with-its-military-work-causes-employee-concerns\nhttps://techstory.in/employees-question-unitys-ethics-as-firm-ventures-into-war/\nhttps://www.gamasutra.com/view/news/387238/Report_Unity_staff_concerned_by_lack_of_transparency_over_military_projects.php\nhttps://www.pcgamer.com/unity-employees-reportedly-arent-happy-about-the-companys-military-dealings/\nhttps://www.thegamer.com/unity-workers-worried-miliatry/\nRelated \ud83c\udf10\nMohsen Fakhrizadeh automated assassination\nUkraine war Clearview AI facial recognition\nPage info\nType: Issue\nPublished: August 2021\nLast updated: June 2024", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/goguardian-student-monitoring", "content": "GoGuardian Beacon student suicide prevention software raises privacy, bias concerns\nOccurred: October 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nBaltimore City Schools monitored the laptops of local school students for indications of self-harm and suicide during the COVID-19 pandemic, raising questions about privacy and equity.\nUsing GoGuardian\u2019s Beacon suicide prevention software, the school system identified nine students as having a severe mental health crisis. \nHowever, reports suggested the software was also being used for disciplinary purposes and could result in LGBTQ students being unintentionally outed, or student expression hampered. In addition, it appeared less wealthy students may be tracked more regularly since school-owned laptops may be their only devices. \nOther reports state school police monitor GoGuardian software after school hours, including on weekends and holidays, raising concerns about students enduring more or less constant surveillance.\nWhile some people saw this as a necessary safeguard, others worried about the potential invasion of privacy and the implications of having school police involved in mental health checks. \nSystem \ud83e\udd16\nGoGuardian Beacon website\nOperator: Baltimore City Public Schools; Pekin Community High School  \nDeveloper: Liminex Inc/GoGuardian\nCountry: USA\nSector: Education\nPurpose: Detect & categorise at-risk behaviour\nTechnology: Suicide prevention algorithm\nIssue: Privacy; Surveillance; Bias/discrimination - economic\nTransparency: Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.baltimoresun.com/education/bs-md-laptops-monitoring-20211012-a2j3vsytijhhjj36n57ri5zdhi-story.html\nhttps://apnews.com/article/technology-baltimore-health-education-ead9cecb355600b3bf8ff9aad98b5eb6\nhttps://news.yahoo.com/baltimore-city-student-laptops-monitored-080000798.html\nhttps://theconversation.com/school-surveillance-of-students-via-laptops-may-do-more-harm-than-good-170983\nhttps://www.forbes.com/sites/lisakim/2021/10/12/school-issued-laptops-in-baltimore-are-monitoring-students-for-risk-of-self-harm-as-concern-mounts-nationwide-over-surveillance/\nhttps://baltimore.cbslocal.com/2021/10/12/baltimore-schools-monitor-student-laptops-for-suicide-signs/\nhttps://wtop.com/baltimore/2021/10/baltimore-schools-monitor-student-laptops-for-suicide-signs/\nhttps://therealnews.com/cops-in-baltimore-schools-are-monitoring-students-laptops\nhttps://www.washingtonpost.com/local/education/baltimore-school-laptops-monitored/2021/10/24/be2c6b6e-2d2a-11ec-8ef6-3ca8fe943a92_story.html\nhttps://www.bloomberg.com/news/features/2021-10-28/how-goguardian-ai-spyware-took-over-schools-student-devices-during-covid\nhttps://slate.com/technology/2021/11/goguardian-school-pandemic-surveillance.html\nRelated \ud83c\udf10\nGaggle student behavioural monitoring\nSeoul bridge suicide detection, prevention\nPage info\nType: Issue\nPublished: October 2021\nLast updated: June 2024", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/us-postal-inspection-service-icop-covert-monitoring-and-surveillance", "content": "US Postal Inspection Service runs covert protestor monitoring programme\nOccurred: May 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe US Postal Inspection Service (USP IS) ran a secret programme tracking social media users, including racial justice protestors, and creating fake identities, prompting concerns about its legality.\nAccording to Yahoo!, USP IS' Internet Covert Operations Program (iCOP) used social media monitoring software to identity and track investigation targets, and shared that data with law enforcement agencies. The programme was carried out by the United States Postal Inspection Service (USPIS), the law enforcement arm of the USPS.\nThe report alleged that iCOP analysts began monitoring social media to track potential violence at racial justice protests following the death of George Floyd, and that the programme enabled USP IS staff to assume fake identities online and employ facial recognition software, including Clearview AI's facial recognition system.\nThe incident raised questions about the ethics and legality of USPS' targeting and collecting information on US citizens not suspected of any crime and with no connection to the post office.\n\u2795 August 2021. Privacy group EPIC filed a lawsuit against the US Postal Service to block the use of facial recognition and social media monitoring tools under iCOP. \n\u2795 March 2022. EPIC's lawsuit was dismissed on the basis that EPIC did not suffer a 'cognizable injury in fact' from the Service\u2019s unlawful refusal to disclose information about the programme.\n\u2795 March 2022. A US Inspector General audit (pdf) of the iCOP programme concluded that the USP IS did not have the legal authority to conduct the sweeping intelligence collection and surveillance of American protesters and others between 2018 and 2021.\nSystem \ud83e\udd16\nClearview AI website\nOperator: US Postal Inspection Service (USP IS)\nDeveloper: Clearview AI; Zignal Labs; Nfusion\nCountry: USA\nSector: Govt - postal\nPurpose: Identify crime suspects; Identify protestors\nTechnology: Facial recognition; Social media monitoring\nIssue: Privacy; Surveillance\nTransparency: Governance; Privacy\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nEPIC (2022). Postal Service Surveillance Program Targeted in EPIC Lawsuit \u2018Exceeded\u2019 Legal Authority, Inspector General Finds\nUS District Court for the District of Colombia (2022). Opinion (pdf)\nEPIC (2022). Court Won\u2019t Hear Merits of EPIC Suit Challenging Secret Postal Service Surveillance Program\nEPIC (2021). EPIC vs U.S. Postal Service \nEPIC (2021). EPIC Sues Postal Service to Halt Use of Facial Recognition, Social Media Monitoring\nInvestigations, assessments, audits \ud83e\uddd0\nUS Office of Inspector General (2022). U.S. Postal Inspection Service\u2019s Online Analytical Support Activities\nYahoo News (2022). Inspector general says post office surveillance program exceeded legal authority\nYahoo! News (2021). Facial recognition, fake identities and digital surveillance tools: Inside the post office's covert internet operations program\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.vice.com/en/article/m7enk3/us-postal-inspection-service-icop-presentation\nhttps://www.salon.com/2021/04/21/is-the-post-office-spying-on-you-usps-covert-operations-may-monitor-social-media-posts/\nhttps://www.businessinsider.com/usps-running-covert-program-that-monitors-americans-social-media-per-report\nhttps://www.biometricupdate.com/202108/neither-ai-inaccuracy-nor-foia-requests-nor-human-rights-stays-usps-surveillance\nhttps://www.dailymail.co.uk/news/article-9595879/USPS-uses-facial-recognition-Clearview-AI-fake-identities-online-snoop-Americans.html\nhttps://www.infosecurity-magazine.com/news/usps-reportedly-uses-clearview-ai\nhttps://www.dailydot.com/debug/post-office-facial-recognition-clearview-surveillance/\nhttps://www.thesun.co.uk/news/14728133/us-postal-service-is-running-secret-program-called-icop/\nhttps://www.foxbusiness.com/politics/republicans-usps-amorphous-intelligence-collecting-operation\nRelated \ud83c\udf10\nLAPD social media data collection\nNYPD Domain Awareness System\nPage info\nType: Incident\nPublished: May 2021\nLast updated: June 2024", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tiktok-lgbtq-shadowbanning", "content": "TikTok LGBTQ hashtags shadowbanned in Bosnia, Jordan, Russia\nOccurred: September 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nTikTok admitted it censored LGBTQ hashtags in Bosnia, Jordan, and Russia, after being accused of doing so by the Australian Strategic Policy Institute (ASPI) think-tank.\nShadow banning involves limiting the discovery of content without communicating that a specific hashtag is on a ban list. \nAccording to TikTok, some hashtags were restricted to comply with local laws, others were limited as they were often used to discover pornographic content, whilst some English and Arabic phrases were banned by mistake.\nThe episode raised questions about TikTok's approach to censorship and concerns about the transparency of its decision-making.\nSystem \ud83e\udd16\nTikTok website\nTikTok Wikipedia profile\nOperator: ByteDance/TikTok  \nDeveloper: ByteDance/TikTok  \nCountry: Bosnia; Jordan; Russia\nSector: Media/entertainment/sports/arts\nPurpose: Block/reduce user/content visibility\nTechnology: Recommendation algorithm\nIssue: Freedom of expression - censorship\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nAustralian Strategic Policy Institute (2020). TikTok and WeChat\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://qz.com/1900530/tiktok-shadow-bans-lgbt-hashtags-in-russian-and-arabic/\nhttps://www.pinknews.co.uk/2020/09/12/tiktok-admits-it-enacted-shadow-ban-censoring-some-lgbt-hashtags-including-i-am-a-gay-lesbian/\nhttps://www.starobserver.com.au/news/tiktok-censors-lgbtqi-hashtags/197643\nhttps://www.bbc.co.uk/news/technology-54102575\nhttps://www.smh.com.au/politics/federal/tiktok-censoring-lgbtq-issues-uighur-crackdown-report-20200908-p55tjn.html\nhttps://www.reuters.com/article/britain-tech-lgbt-idUSL5N2GJ459\nhttps://www.spiegel.de/netzwelt/apps/tiktok-unterdrueckt-lgbtq-hashtags-in-mehreren-sprachen-a-649574ff-3ce7-4857-a8e4-ae7a96052e32\nhttps://netzpolitik.org/2020/shadowbanning-tiktok-zensiert-lgbtq-themen-und-politische-hashtags/\nhttps://www.rnd.de/digital/shadowbanning-tiktok-verbannt-lgbtq-themen-und-politische-hashtags-MDK3MFCM7BHCVHY4UPSWUGIX5A.html\nRelated \ud83c\udf10\nTikTok creators hate speech detection racial bias\nTikTok #intersex hashtag 'censorship'\nPage info\nType: Incident\nPublished: April 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/us-police-perpetual-facial-line-up", "content": "US law enforcement able to access facial photos of 117 million Americans\nOccurred: October 2016\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nLaw enforcement agencies across the US were able to access the facial photos of over 117 million US adults stored in government datasets, raising serious questions about privacy and transparency.\nAfter a year-long investigation based on over 100 Freedom of Information (FOI/FOIA) requests and dozens of interviews, researchers at Georgetown Law\u2019s Center on Privacy and Technology discovered that half of American adults were enrolled in a facial recognition network searchable by law enforcement.\nThey found that FBI, state and local police departments were using or building facial recognition systems to compare the faces of suspected criminals to their driver\u2019s license and ID photos in a mostly opaque and unregulated manner, and that few agencies had instituted 'meaningful protections' to prevent misuse of the technology.\nThe report recommended that the use of facial recognition for law enforcement should be stopped. \nThe findings raised concerns about privacy and civil liberties, and led to legislative and policy changes in many US states. \nSystem \ud83e\udd16\nMultiple\nOperator: Chicago PD; Dallas PD; LAPD\nDeveloper: Federal Bureau of Investigation (FBI)\nCountry: USA\nSector: Govt - police\nPurpose: Identify criminals\nTechnology: Facial recognition\nIssue: Privacy; Accuracy/reliability; Bias/discrimination - racial, ethnicity\nTransparency: Black box; Governance\nResearch, advocacy \ud83e\uddee\nPerpetual line-up website\nGeorgetown Law. The Perpetual Line-Up: Unregulated Police Face Recognition in America\nACLU (2016). Coalition Letter to the Department of Justice Civil Rights Division Calling for an Investigation of the Disparate Impact of Face Recognition on Communities of Color\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://arstechnica.com/tech-policy/2016/10/the-perpetual-lineup-half-of-us-adults-in-a-face-recognition-database/\nhttps://www.theguardian.com/world/2016/oct/18/police-facial-recognition-database-surveillance-profiling\nhttps://www.theatlantic.com/technology/archive/2016/04/the-underlying-bias-of-facial-recognition-systems/476991/\nhttps://www.securityindustry.org/2021/07/23/what-science-really-says-about-facial-recognition-accuracy-and-bias-concerns/\nhttps://www.biometricupdate.com/202107/sia-blasts-misrepresentation-of-facial-recognition-studies\nhttps://www.newscientist.com/article/mg21528804-200-fbi-launches-1-billion-face-recognition-project/\nhttp://openbiometrics.org/publications/klare2012demographics.pdf\nRelated \ud83c\udf10\nMinnesota Operation Safety Net\nUK Met Police Gangs Violence Matrix\nPage info\nType: Issue\nPublished: November 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/nfl-concussion-settlement-discrimination", "content": "Black players sue NFL over algorithmic 'race-norming'\nOccurred: August 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA group of Black former National Football League (NFL) players filed a federal lawsuit claiming that an evaluation system used for the NFL's USD 1 billion player concussion settlement favoured White players. \nThe players argued that the system - named Heaton norms - blocked hundreds of Black claimants from securing payouts by assuming they had lower cognitive functioning when healthy than white players, making it harder for them to be eligible for a payout.\nNFL stars stood to receive up to USD 5 million in compensation if they could prove they suffered brain trauma while competing in the League. But some ended up with nothing, resulting in loss of income, homelessness, anxiety and, in some cases, suicide.\nAccording to The Guardian, 'Under the settlement, [..] the NFL has insisted on using a scoring algorithm on the dementia testing that assumes Black men start with lower cognitive skills. They must therefore score much lower than whites to show enough mental decline to win an award. The practice, which went unnoticed until 2018, has made it harder for Black former players to get awards.'\n\u2795 June 2021. The NFL agreed to halt the use of 'race-norming' and stated it would make changes to the concussion settlement. \nSystem \ud83e\udd16\nHeaton norms\nOperator: National Football League (NFL)\nDeveloper: National Football League (NFL), BrownGreer\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Evaluate dementia claims\nTechnology: Scoring algorithm\nIssue: Bias/discrimination - race\nTransparency: Governance; Complaints/appeals\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nNFL Concussion Settlement\nResearch, advocacy \ud83e\uddee\nChange.org. STOP RACIAL DISCRIMINATION IN THE NFL CONCUSSION SETTLEMENT\nInvestigations, assessments, audits \ud83e\uddd0\nPerils of Race-Based Norms in Cognitive Testing. The Case of Former NFL Players\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.wsj.com/articles/lawsuit-alleges-nfls-concussion-settlement-discriminates-against-black-players-11598371843\nhttps://abcnews.go.com/Sports/clinicians-fear-nfls-concussion-settlement-program-protocols-discriminate/story?id=75646704\nhttps://www.newsweek.com/black-men-have-lower-cognitive-skills-white-men-nfl-asserts-brain-injury-lawsuits-1591752\nhttps://www.msn.com/en-us/sports/nfl/how-e2-80-98race-norming-e2-80-99-was-built-into-the-nfl-concussion-settlement/ar-AAMQJSr\nhttps://www.marketwatch.com/story/retired-black-nfl-players-and-their-families-call-for-race-norming-practice-to-end-01621018741\nhttps://www.theguardian.com/sport/2021/may/14/nfl-race-norming-concussion-settlement\nhttps://abcnews.go.com/Sports/clinicians-fear-nfls-concussion-settlement-program-protocols-discriminate/story\nhttps://www.nytimes.com/2021/06/02/sports/football/nfl-concussion-settlement-race.html\nhttps://www.sfchronicle.com/sports/49ers/article/NFL-abandons-crude-proxy-of-race-norming-in-1-16223812.php\nhttps://thehill.com/regulation/court-battles/553482-nfl-accused-of-systemic-racism-in-handling-black-ex-players-brain\nhttps://www.bloomberg.com/opinion/articles/2021-03-17/nfl-concussion-case-illustrates-a-deadly-form-of-racism\nhttps://www.insurancejournal.com/news/national/2021/05/18/614610.htm\nhttps://www.insurancejournal.com/news/national/2021/10/22/638491.htm\nRelated \ud83c\udf10\nAirbnb 'Smart Pricing' algorithm racism\nUS mortgage approval algorithm discrimination\nPage info\nType: Incident\nPublished: August 2020", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tesla-model-s-crashes-into-fire-engine", "content": "Tesla Model S crashes into fire engine with Autopilot 'engaged'\nOccurred: January 2018\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Tesla Model S travelling at 65 mph crashed into the back of a stationary fire engine on a California highway. \nNo injuries were recorded, and the driver subsequently claimed his car was on Autopilot at the time of the accident. \nBloomberg reported that the federal government is \u201cgathering information\u201d on the accident, but had yet to decide to formally open an investigation.\nThe incident raised questions about the safety of Tesla's Autopilot system and the adequacy of its communication about its limitations to customers. \n\u2795 August 2021. The US government opened a formal investigation into Tesla\u2019s Autopilot after a series of collisions with emergency vehicles, including fire engines and ambulances.\nSystem \ud83e\udd16\nTesla Autopilot, Full-self Driving\nOperator: \nDeveloper: Tesla\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Driver assistance system\nIssue: Safety; Accuracy/reliability\nTransparency: Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://electrek.co/2018/01/22/tesla-model-s-autopilot-crash-fire-truck/\nhttps://www.bbc.co.uk/news/technology-42801772\nhttps://www.theverge.com/2018/1/23/16923800/tesla-firetruck-crash-autopilot-investigation\nhttps://www.bloomberg.com/news/articles/2018-01-23/tesla-crash-in-california-draws-interest-from-u-s-investigators\nhttps://eu.usatoday.com/videos/news/nation/2018/01/24/tesla-car-autopilot-crashes-into-fire-truck/109773214/\nhttps://apnews.com/article/technology-business-61557d668b646e7ef48c5543d3a1c66c\nhttps://www.yahoo.com/entertainment/tesla-autopilot-face-investigation-series-123819434.html\nRelated \ud83c\udf10\nTesla Model 3 hits parked police car\nTesla Autopilot cruise control activation\nPage info\nType: Incident\nPublished: December 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/gm-chevrolet-bolt-motorbike-collision", "content": "GM Chevrolet Bolt collides with motorbike, injures rider\nOccurred: January 2018\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nMotorcyclist Oscar Nillson sued General Motors after an autonomous Chevrolet Bolt electric prototype collided in a street in San Francisco, knocking him off his bike and leaving him with injuries to his neck and shoulder.\nGM said in its accident report that its vehicle had been changing lanes when the gap ahead closed, forcing it to re-centre itself in the original lane. Meantime the motocyclist, who was coming from behind at a faster speed, moved into the car's way. \nAfter the collision, the motorcyclist was seen tumbling to the ground. \nNillson subsequently filed a lawsuit against General Motors.\n\u2795 June 2018. General Motors settled with Nilsson, without admitting fault. Details of the settlement were not disclosed.\nSystem \ud83e\udd16\nChevrolet Bolt EV website\nChevrolet Bolt Wikipedia profile\nOperator: General Motors  \nDeveloper: General Motors  \nCountry: USA  \nSector: Automotve\nPurpose: Automate steering, acceleration, braking\nTechnology: Self-driving system\nIssue: Safety\nTransparency: Governance; Black box; Legal\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nhttps://www.scribd.com/document/369841173/Nilsson-Suit-V-GM\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.mercurynews.com/2018/01/23/motorcyclist-hit-by-self-driving-car-in-s-f-sues-general-motors/\nhttps://www.bbc.co.uk/news/technology-42801772\nhttps://abcnews.go.com/US/gm-sued-motorcyclist-injured-crash-involving-driving-car/story?id=52630445\nhttps://www.caranddriver.com/news/a15872851/motorcyclist-suing-gm-after-accident-with-autonomous-chevrolet-bolt-ev/\nhttps://www.theverge.com/2018/1/23/16925396/gm-cruise-automation-self-driving-car-crash-lawsuit\nhttps://www.mercurynews.com/2018/01/23/motorcyclist-hit-by-self-driving-car-in-s-f-sues-general-motors/\nhttps://www.washingtonpost.com/news/innovations/wp/2018/01/25/after-crash-injured-motorcyclist-accuses-robot-driven-vehicle-of-negligent-driving/\nhttps://www.theguardian.com/technology/2018/jan/24/general-motors-sued-motorcyclist-first-lawsuit-involve-autonomous-vehicle\nhttps://www.thesun.co.uk/motors/5421363/will-driverless-cars-really-make-our-roads-safer-tesla-and-gm-under-investigation-after-self-driving-crashes/\nhttps://jalopnik.com/gm-settles-lawsuit-with-motorcyclist-over-crash-with-se-1826492276\nhttps://www.caranddriver.com/news/a15872851/motorcyclist-suing-gm-after-accident-with-autonomous-chevrolet-bolt-ev/\nRelated \ud83d\uddde\ufe0f\nCruise driverless cars traffic blocking\nTesla Model 3 Paris fatal crash\nPage info\nType: Incident\nPublished: January 2022", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/defra-biodiversity-net-gain-metric", "content": "UK Biodiversity Net Gain algorithm accused of not being fit for purpose\nOccurred: July 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA UK biodiversity algorithm was accused of being inaccurate, unreliable, and easy to manipulate.\nBiodiversity Net Gain (BNG), a biodiversity metric for new developments introduced in the UK government's 2021 Environment Bill, was intended to determine how new houses, roads, and other construction projects must achieve no net loss of biodiversity, or, if nature is damaged on the construction site, achieve a 10 percent net gain elsewhere.\nHowever, BNG was found not to value scrubby landscapes such as sand pits or those used for rewilding programmes, which it logged as a sign of 'degradation' and would not therefore qualify for compensation, resulting in complaints from academics and conservationists that the system was not fit for purpose. \nThe UK government has a target of building 300,000 new homes a year by the mid-2020s.\nSystem \ud83e\udd16\nhttp://nepubprod.appspot.com/publication/6049804846366720\nhttps://www.gov.uk/guidance/biodiversity-metric-calculate-the-biodiversity-net-gain-of-a-project-or-development\nhttps://naturalengland.blog.gov.uk/2021/09/21/biodiversity-net-gain-more-than-just-a-number/\nDeveloper: Natural England\nOperator: Department for Environment, Food and Rural Affairs (DEFRA)\nCountry: UK\nSector: Govt - environment\nPurpose: Manage conservation  \nTechnology: Biodiversity Metric 3\nIssue: Accuracy/reliability; Bias/discrimination - rewilding  \nTransparency: Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theguardian.com/environment/2021/jul/21/biodiversity-metric-algorithm-natural-england-developers-blight-valuable-habitats-aoe\nhttps://www.endsreport.com/article/1722893/dangerous-ecologists-warn-against-serious-flaws-new-biodiversity-metric\nhttps://www.naturebasedsolutionsinitiative.org/news/biodiversity-algorithm-risks/\nhttps://www.agg-net.com/news/mpa-calls-on-defra-to-think-again\nhttps://www.endsreport.com/article/1723401/works-fiction-scientists-disparage-developers-biodiversity-gain-plans\nhttps://www.thetimes.co.uk/article/wildlife-rules-too-easy-to-manipulate-by-builders-llg5snvnk\nhttps://www.fwi.co.uk/business/business-management/biodiversity-net-gain-what-is-it-how-will-it-work\nhttps://www.aggbusiness.com/news/mpa-fears-new-net-gain-metric-will-deliver-worse-outcomes-nature-quarries\nhttps://www.planningresource.co.uk/article/1723853/ecologists-warn-serious-flaws-metric-used-developers-deliver-biodiversity-gains\nhttps://www.propertyweek.com/insight/no-biodiversity-pain-no-gain/5117034.article\nRelated \ud83c\udf10\nToronto beach water quality predictions\nDubai drone weather engineering\nPage info\nType: Issue\nPublished: August 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/twitter-photo-crop-algorithm-age-weight-bias", "content": "Twitter photo crop algorithm dropped due to age, weight bias\nOccurred: August 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn algorithm used by Twitter to crop photos and other images on user timelines came under fire for perceived gender and racial bias.\nThe company's 'saliency' algorithm decided how images would be cropped in Twitter previews. However, researcher Bogdan Kulynyc discovered that the algorithm preferred to show faces that were slimmer, younger, and with lighter skin, and seemed to favour stereotypically feminine facial traits. \nKulynyc also discovered that the \u201csaliency\u201d of a face in an image could be increased, making it less likely to be hidden by the cropping algorithm, by making the person\u2019s skin lighter or warmer and smoother.\nEarlier, Twitter\u2019s own research had found the algorithm had a bias towards cropping out black faces. When two faces were in the same image, the preview crop appeared to favor white faces, hiding the black faces until users clicked through. Twitter\u2019s own subsequent analysis showed a \"4 percent difference from demographic parity, in favor of white individuals\".\nIn response, Twitter decided to scrap the image-cropping system; Twitter\u2019s then director of software engineering, Rumman Chowdhury, stated that \"how to crop an image is a decision best made by people\". \nThis controversy highlighted the challenges and potential pitfalls of using algorithms in social media platforms and the importance of continually testing and refining these systems to ensure they are fair and unbiased.\nSystem \ud83e\udd16\nTwitter website\nTwitter Wikipedia profile\nDocuments \ud83d\udcc3\nhttps://blog.twitter.com/engineering/en_us/topics/insights/2021/algorithmic-bias-bounty-challenge\nhttps://blog.twitter.com/engineering/en_us/topics/insights/2021/sharing-learnings-about-our-image-cropping-algorithm\nhttps://blog.twitter.com/en_us/topics/product/2020/transparency-image-cropping\nOperator: Twitter\nDeveloper: Twitter\nCountry: USA\nSector: Technology\nPurpose: Crop images\nTechnology: Saliency algorithm\nIssue: Bias/discrimination - gender, race, age, weight\nTransparency: \nResearch, advocacy \ud83e\uddee\nhttps://twitter.com/hiddenmarkov/status/1424472328197525505\nhttps://github.com/bogdan-kulynych/saliency_bias\nhttps://hackerone.com/h1c-twitter-algorithmic-bias\nhttps://github.com/twitter-research/image-crop-analysis\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.wired.com/story/twitters-photo-cropping-algorithm-favors-young-thin-females/\nhttps://www.cnet.com/tech/mobile/twitter-offers-bug-bounty-to-spot-and-fix-ai-bias-in-its-algorithms/\nhttps://www.cnet.com/tech/mobile/twitter-ai-bias-contest-shows-beauty-filters-hoodwink-the-algorithm/\nhttps://www.theguardian.com/technology/2021/aug/10/twitters-image-cropping-algorithm-prefers-younger-slimmer-faces-with-lighter-skin-analysis\nhttps://www.msn.com/en-us/money/other/twitter-e2-80-99s-photo-cropping-algorithm-prefers-young-beautiful-and-light-skinned-faces/ar-AAN93Wf\nhttps://www.msn.com/en-us/money/other/twitter-e2-80-99s-racist-algorithm-is-also-ageist-ableist-and-islamaphobic-researchers-find/ar-AAN7cV5\nhttps://www.marktechpost.com/2021/08/10/twitter-algorithmic-bias-challenge-winner-finds-beauty-filters-can-fool-twitters-ai/\nRelated \ud83c\udf10\nTinder Plus pricing algorithm discrimination\nTikTok mandatory beauty filtering\nPage info\nType: Incident\nPublished: October 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/xsolla-employee-monitoring-terminations", "content": "Xsolla uses secret monitoring system to fire employees \nOccurred: August 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nRussian payment services company Xsolla came under fire for terminating 150 employees who had been subjected to secretive, ongoing monitoring and 'big data' analysis of their activity at work.\nIn a leaked letter to the 150 employees, Xsolla CEO and founder Aleksandr Agapitov said 'You received this email because my big data team analyzed your activities in Jira, Confluence, Gmail, chats, documents, dashboards and tagged you as unengaged and unproductive employees.'\n'Many of you might be shocked, but I truly believe that Xsolla is not for you. Nadia and her care team partnered with seven leading HR agencies, as we will help you find a good place, where you will earn more and work even less.'\nAgapitov's move resulted in an employee backlash, and the prospect of legal action against Xsolla was raised. Agapitov later blamed the mass lay-off on the company's slowing growth.\nThe incident raised questions about the ethics and transparency of Xsolla decision-making, and about the use of AI in the workplace more generally.\nSystem \ud83e\udd16\nUnknown\nDocuments \ud83d\udcc3\nhttps://gameworldobserver.com/wp-content/uploads/2021/08/xsolla-screen-letter.jpg\nhttps://twitter.com/agapitovs/status/1422987608229961730\nOperator: Xsolla  \nDeveloper: Xsolla  \nCountry: Russia\nSector: Banking/financial services  \nPurpose: Assess productivity\nTechnology: Online/social media monitoring\nIssue: Employment; Ethics/values\nTransparency: Governance; Complaints/appeals\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://gameworldobserver.com/2021/08/04/xsolla-fires-150-employees-using-big-data-and-ai-analysis-ceos-letter-causes-controversy\nhttps://gameworldobserver.com/2021/08/05/xsolla-cites-growth-rate-slowdown-as-reason-for-layoffs-ceos-tweet-causes-further-controversy\nhttps://app2top.ru/industry/xsolla-uvolila-okolo-150-sotrudnikov-kompaniya-ottalkivalas-ot-analiza-ih-aktivnosti-v-rabochih-prilozheniyah-188953.html\nhttps://vc.ru/hr/277507-xsolla-uvolila-chast-sotrudnikov-permskogo-ofisa-posle-analiza-ih-aktivnosti-v-rabochih-chatah\nhttps://www.forbes.ru/newsroom/biznes/436639-nichego-ne-izmenitsya-esli-ih-ne-budet-glava-permskogo-startapa-obyasnil\nhttps://www.mcvuk.com/business-news/xsolla-fires-150-employees-based-on-big-data-analysis-of-their-activity-many-of-you-might-be-shocked-but-i-truly-believe-that-xsolla-is-not-for-you/\nhttps://properm.ru/news/business/198398/\nhttps://english.elpais.com/usa/2021-10-14/one-second-150-dismissals-inside-the-algorithms-that-decide-who-should-lose-their-job.html\nhttps://www.tecmundo.com.br/mercado/222501-startup-demite-empregados-big-data-ceo-polemiza-improdutivos.htm\nhttps://www.gamesindustry.biz/articles/2021-12-23-2021-the-year-of-living-ridiculously-this-year-in-business\nRelated \ud83c\udf10\nEstee Lauder employee performance assessments\nZhihu job resignation predictions\nPage info\nType: Incident\nPublished: January 2022", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/apple-neuralhash-csam-scanning", "content": "Apple NeuralHash CSAM scanning system raises privacy concerns\nOccurred: August 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA plan by Apple to start automatically scanning iPhones and iCloud accounts in the US for child sexual abuse material (CSAM) using perceptual hashing was heavily criticised by privacy advocates and others as overly intrusive. \nAn August 2021 Financial Times article revealed Apple's NeuralHash system would have automatically scanned devices to identify if they contain photos featuring child sexual abuse before the images are uploaded to iCloud. Matches were to be reported to the US National Centre for Missing and Exploited Children (NCMEC).\nThe plan had been seen as potentially helpful for law enforcement in criminal investigations, but critics feared it might open the door to unnecessary or disproprotionate legal and government demands for user data.\nWidespread hostility persuaded Apple to delay the rollout of its CSAM detection system.\n\u2795 December 2021. Apple deleted all reference to the plan on its website.\n\u2795 December 2022. Apple publicly dropped the system.\nSystem \ud83e\udd16\nNeuralHash\n\nDocuments \ud83d\udcc3\nApple. Expanded protections for children\nApple (2023). Letter to Heat Initiative \nApple (2021). CSAM Detection - Technical Summary (pdf)\nOperator: Apple\nDeveloper: Apple\nCountry: USA\nSector: Technology\nPurpose: Detect child pornography\nTechnology: Hash matching; Deep learning; Neural network; Machine learning\nIssue: Security; Privacy; Surveillance; Accuracy/reliability\nTransparency: \nResearch, advocacy \ud83e\uddee\nHeat Initiative\nBhatia J.S., Meng K. (2022). Exploiting and Defending Against the Approximate Linearity of Apple\u2019s NEURALHASH (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.ft.com/content/14440f81-d405-452f-97e2-a81458f5411f\nhttps://www.bbc.co.uk/news/technology-58109748\nhttps://www.cnbc.com/2021/08/05/apple-will-report-child-sexual-abuse-images-on-icloud-to-law.html\nhttps://www.theverge.com/2021/8/5/22611721/apple-csam-child-abuse-scanning-hash-system-ncmec\nhttps://www.forbes.com/sites/thomasbrewster/2021/08/06/apple-is-trying-to-stop-child-abuse-on-iphones-so-why-do-so-many-privacy-experts-hate-it/\nhttps://www.theverge.com/2021/12/15/22837631/apple-csam-detection-child-safety-feature-webpage-removal-delay\nhttps://www.theverge.com/2022/12/7/23498588/apple-csam-icloud-photos-scanning-encryption\nhttps://www.wired.com/story/apple-photo-scanning-csam-communication-safety-messages/\nhttps://www.wired.com/story/apple-csam-scanning-heat-initiative-letter/\nRelated \ud83c\udf10\nGoogle flags medical images of groin as CSAM\nHackney Early Help Profiling System\nPage info\nType: Issue\nPublished: January 2022\nLast updated: September 2023", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-one-palmprint-biometrics", "content": "Reports: Amazon One raises palmprint biometric privacy, opacity concerns\nOccurred: August 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAmazon One, a service that allows customers to pay using their handprint, was accused of using sensitive personal data to improve its system. \nThe system uses a palm scanner to register an image of the user\u2019s palm, allowing them to pay by hovering their hand over a sensor in Amazon checkout-free stores. \nDespite promising to secure biometric data using encryption, data isolation and secure zones, Amazon came under fire for allegedly using an unspecified sub-set of anonymous palm data to improve its system, raising concerns about unstated potential commercial and other uses.\nBy linking a customer's palm print to their Amazon account, the company could use the data it collected, like shopping history, to target ads, offers and recommendations over time, according to TechCrunch.\nThe controversy prompted US lawmakers to write to Amazon CEO Andy Jassy expressed privacy and competition concerns over Amazon One, and asked the company to provide information about how it keeps users\u2019 data safe. \n\u2796 August 2021. Amazon\u2019s decision to offer a USD 10 credit for new users who enrolled their palm prints in the programme was criticised by privacy advocates who saw it as a tactic to coerce people into handing over sensitive personal data.\nSystem \ud83e\udd16\nAmazon One website\nAmazon One Wikipedia profile\nOperator: Amazon\nDeveloper: Amazon  \nCountry: USA\nSector: Technology\nPurpose: Verify identity; Authorise transactions\nTechnology: Palm print scanning\nIssue: Privacy; Dual/multi-use\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://techcrunch.com/2021/08/02/amazon-credit-palm-biometrics/\nhttps://www.biometricupdate.com/202108/amazon-sees-profit-in-your-palmprint-opponents-see-harmful-surveillance-capitalism\nhttps://www.msn.com/en-us/money/companies/want-a-2410-credit-from-amazon-just-let-it-scan-your-palm/ar-AAMTK8D\nhttps://www.yahoo.com/now/amazon-is-offering-10-in-credit-for-your-palm-print-data-114014151.html\nhttps://hypebeast.com/2021/8/amazon-register-palm-print-data-biometric-data-payment\nhttps://www.wsbtv.com/news/trending/amazon-wants-read-your-palm-will-pay-10-some-are-wary-about-technology/OVIRJVWKEJEKDBVSHR54DRWQCY/\nhttps://www.techtimes.com/articles/263659/20210802/amazon-one-palm-print-biometrics-guarantees-10-promotional-credit-self-checkout-store.htm\nhttps://nypost.com/2021/08/03/amazon-will-pay-you-10-to-scan-your-palm-report/\nRelated \ud83c\udf10\nNHGSFP school meal fingerprint biometrics\nMoscow Metro Face Pay\nPage info\nType: Incident\nPublished: January 2022", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/mercadona-facial-recognition", "content": "Mercadona fined for facial recognition privacy violations\nOccurred: July 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nSpanish supermarket chain Mercadona's was fined EUR 2.5m by AEPD, the country's data protection regulator, for illegally collecting and processing childrens' and employees' biometric data. \nThe stated aim of the programme was to detect known criminals and people with restraining orders issued against them for attacking Mercadona employees, with cameras equipped with facial recognition identifying relevant transgressors, who were then reported to the police.\nAEPD ruled that Mercadona had failed to appreciate that its system processed sensitive data of anyone who entered its supermarkets, including childen and its own employees. \nThe regulator also found that Mercadona had violated GDPR Article 12 and 13 transparency requirements, including the ability of those affected to complain or appeal.\nMercadona's facial recognition system was supplied by Israeli company AnyVision (now Oosto). Oosto also manufactures military drones equipped with facial recognition.\nSystem \ud83e\udd16\nOosto website\n\nOperator: Mercadona\nDeveloper: Oosto/AnyVision Interactive Technologies  \nCountry: Spain\nSector: Retail\nPurpose: Detect criminals\nTechnology: Facial recognition\nIssue: Privacy\nTransparency: Marketing; Complaints/appeals\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nAEPD (2021). RESOLUCI\u00d3N DE TERMINACI\u00d3N DEL PROCEDIMIENTO POR PAGO VOLUNTARIO (pdf)#\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://elpais.com/economia/2021-07-22/mercadona-paga-una-sancion-de-25-millones-de-euros-a-proteccion-de-datos.html\nhttps://www.thelocal.es/20200702/spains-mercadona-supermarkets-install-facial-recognition-systems-to-keep-thieves-at-bay/\nhttps://elpais.com/tecnologia/2020-07-06/proteccion-de-datos-abre-una-investigacion-sobre-las-camaras-de-vigilancia-facial-de-mercadona.html\nhttps://www.natlawreview.com/article/spanish-dpa-fines-supermarket-chain-2520000-eur-unlawful-use-facial-recognition\nhttps://www.theolivepress.es/spain-news/2021/07/23/mercadona-gets-e2-5-million-fine-for-installing-facial-recognition-cameras-in-their-supermarkets-in-spain/\nhttps://www.businessinsider.com/httpswwwbusinessinsideresdrones-reconocimiento-facial-cerca-ser-realidad-812285\nhttps://www.businessinsider.es/jefe-europeo-privacidad-cuestiona-vigilancia-mercadona-733389\nhttps://www.businessinsider.es/reconocimiento-facial-instala-mercadona-polemico-670827\nhttps://www.lexology.com/library/detail.aspx?g=921c67d8-104c-42a0-81c8-debb6d637c40\nhttps://www.itpro.co.uk/policy-legislation/general-data-protection-regulation-gdpr/356436/supermarket-chain-mercadona-under\nRelated \ud83c\udf10\nKohler, BMW, MaxMara China facial recognition\nPimEyes facial recognition search engine\nPage info\nType: Incident\nPublished: November 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/nato-warships-ais-spoofing", "content": "NATO warships' locations are spoofed\nOccurred: July 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe locations of over 100 warships, including the Royal Navy's HMS Queen Elizabeth aircraft carrier strike group, were manipulated and their positions falsified.\nResearchers at environmental NGOs SkyTruth and Global Fishing Watch discovered that the automatic identification systems (AIS) of Royal Navy, Swedish Navy and US Navy warships had been altered in order to suggest they were close to a Russian naval base. \nThe spoofed tracks included one that seemed to show the American destroyer USS Roosevelt four nautical miles inside Russian territorial waters.\nOnboard AIS systems broadcast a ship's location, course and speed, and show the same data from other vessels. Their manipulation increases the risk of safety incidents, and creates disinformation about warship positions and operations. \nCommentators said the culprit and motivation was unknown, though Russia was fingered as the most likely culprit.\nThe incident underscored the importance of advanced countermeasures against AIS spoofing. \nSystem \ud83e\udd16\nAutomatic Identification System (AIS)\nAIS Wikipedia profile\nOperator: Royal Navy; Swedish Navy; United States Navy\nDeveloper: International Maritime Organization\nCountry: Russia; Sweden; UK; USA\nSector: Govt - defence\nPurpose: Track vessel movements\nTechnology: Automatic identification system (AIS)\nIssue: Security; Safety; Mis/disinformation; Dual/multi-use\nTransparency: Governance\nInvestigations, assessments, audits \ud83e\uddd0\nSkyTruth (2021). Systematic data analysis reveals false vessel tracks\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.wired.com/story/fake-warships-ais-signals-russia-crimea/\nhttps://www.bbc.co.uk/news/technology-58027363\nhttps://www.portsmouth.co.uk/news/defence/gps-data-showing-royal-navy-warship-hms-defender-charging-towards-russian-naval-base-was-faked-say-mod-3287074\nhttps://nationalinterest.org/blog/reboot/dozens-nato-warship-positions-near-russia-being-faked-why-190927\nhttps://www.msn.com/en-us/news/technology/over-100-warship-locations-have-been-faked-in-one-year/ar-AAMNmlU\nhttps://futurezone.at/digital-life/phantom-kriegsschiffe-ais-gefahr-sicherheit-konfliktzonen/401459071\nhttps://www.euronews.com/next/2021/06/28/hms-defender-ais-spoofing-is-opening-up-a-new-front-in-the-war-on-reality\nhttps://news.usni.org/2021/06/21/positions-of-two-nato-ships-were-falsified-near-russian-black-sea-naval-base\nhttps://www.dn.se/sverige/falska-svenska-marina-fartyg-pa-natet-pekas-ut-pa-positioner-nara-ryssland\nRelated \ud83c\udf10\nRussia disinformation bot farms\n'Kyiv' deepfake influence campaign\nPage info\nType: Incident\nPublished: January 2022", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/facebook-google-anti-semitic-failure-to-act", "content": "Study: Social media companies fail to take down anti-Semitic content\nOccurred: August 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nFacebook, Google, TikTok, Twitter, and other major social media platforms are routinely failing to take down anti-Semitic posts on their platforms, despite being flagged, according to a study by the Center for Countering Digital Hate (CCDH).\nAnalysing over 'anti-Jewish' 700 posts invoking Holocaust denial and forms of neo-Nazism which had collectively been viewed 7.3 million times, CCDH discovered that 84 percent had not been not acted upon. \nFacebook was the worst offender, failing to remove 89 percent of anti-Semitic posts. Facebook says 97 percent of hate speech was discovered by its system before it was reported by users.\nThe finding raised questions about the effectiveness of the companies' respective content management systems, which had become increasingly powered by AI. \nSystem \ud83e\udd16\nFacebook content moderation system\nTikTok content moderation system \nTwitter content moderation system\nYouTube content moderation system\nOperator: Meta/Facebook; Alphabet/Google/YouTube; Bytedance/TikTok; Twitter  \nDeveloper: Meta/Facebook; Alphabet/Google/YouTube; Bytedance/TikTok; Twitter\nCountry: Global\nSector: Technology\nPurpose: Detect hate speech\nTechnology: Content moderation system\nIssue: Bias/discrimination - religion; Safety - harassment\nTransparency: Governance; Complaints/appeals\nResearch, advocacy \ud83e\uddee\nCenter for Countering Digital Hate (2021). Failure to Protect\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/technology-58058428\nhttps://www.theguardian.com/media/2021/aug/01/a-safe-space-for-racists-antisemitism-report-criticises-social-media-giants\nhttps://capx.co/waiting-for-social-media-companies-to-tackle-anti-semitism-is-utterly-pointless/\nhttps://www.telegraph.co.uk/news/2021/08/01/social-media-firms-fail-remove-84pc-anti-semitic-posts-report/\nhttps://ca.movies.yahoo.com/social-media-firms-failing-act-090153273.html\nhttps://finance.yahoo.com/news/almost-antisemitic-posts-stay-online-121523212.html\nhttps://www.standard.co.uk/news/uk/centre-for-countering-digital-hate-tiktok-instagram-facebook-twitter-b948749.html\nRelated \ud83c\udf10\nEngland footballers' racism Instagram moderation\nGalactica large language model\nPage info\nType: Incident\nPublished: November 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/apple-watch-heart-rate-variability-inconsistencies", "content": "Apple Watch heart rate variability data found to be inconsistent\nOccurred: July 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nHeart rate variability (HRV) data collected by the Apple Watch was found to be inconsistent, prompting questions about the system's reliability.\nHarvard biostatistics researcher JP Onnela discovered that it is not possible to use Apple Watches for research into heart rate variability after finding the data collected was inconsistent. \nAccording to Onnela, updates to Apple Watch algorithms mean 'the data from the time period can change without warning'. \n'These algorithms are what we would call black boxes \u2014 they\u2019re not transparent. So it\u2019s impossible to know what\u2019s in them' he argued.\nThe finding highlighted the need for careful interpretation of Apple Watch HRV data.\nSystem \ud83e\udd16\n\nOperator: Apple\nDeveloper: Apple\nCountry: USA\nSector: Healthcare\nPurpose: Detect heart rate\nTechnology: Heart rate variability algorithm\nIssue: Accuracy/reliability\nTransparency: Black box\nResearch, advocacy \ud83e\uddee\nhttps://www.beiwe.org/exporting-the-same-data-from-a-wearable-twice-doesnt-give-you-the-same-data/\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theverge.com/2021/7/27/22594178/apple-watch-data-research-heart-rate-reliability\nhttps://9to5mac.com/2021/07/27/apple-watch-black-box-issue-health-studies/\nhttps://appleinsider.com/articles/21/07/27/apple-watch-black-box-algorithms-unreliable-for-medical-research\nhttps://www.macrumors.com/2021/07/27/researchers-struggle-to-use-apple-watch\nRelated \ud83c\udf10\nApple Watch blood oximeter racial bias\nApple Cycle Tracking fertility predictions\nPage info\nType: Issue\nPublished: January 2022", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/dubai-deepfake-court-evidence", "content": "Deepfake audio recording used in Dubai child custody battle\nOccurred: January 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nDeepfake technology was used to create a fake audio recording of a lawyer's client during a UK child custody battle in Dubai. \nDubai-based family lawyer Byron James revealed that a 'heavily doctored' recording of his client appearing to utter 'violent' threats towards his wife had been presented in court, threats he claimed had not been uttered.\nExperts examining the deepfake's metadata concluded the recording had been manipulated. If the piece of evidence had not been challenged, it would have negatively affected the client\u2019s case by portraying him as a violent and aggressive man.\nThis incident raised concerns about about the reliability of legal evidence in the age of AI. It also pointed to the need for judicial training for judges and litigators to identify manipulated evidence.\nSystem \ud83e\udd16\nUnknown\nOperator: \nDeveloper:  \nCountry: UAE/Dubai; UK\nSector: Govt - justice\nPurpose: Damage reputation\nTechnology: Deepfake - audio; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Mis/disinformation\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.thenationalnews.com/uae/courts/deepfake-audio-evidence-used-in-uk-court-to-discredit-dubai-dad-1.975764\nhttps://www.telegraph.co.uk/news/2020/01/31/deepfake-audio-used-custody-battle-lawyer-reveals-doctored-evidence/\nhttps://www.legalcheek.com/2020/01/watch-out-for-deepfake-evidence-forgery-family-lawyer-warns/\nhttps://www.abajournal.com/web/article/courts-and-lawyers-struggle-with-growing-prevalence-of-deepfakes\nhttps://www.lawscot.org.uk/members/journal/issues/vol-65-issue-03/deepfakes-and-how-to-avoid-them/\nhttps://www.lexology.com/library/detail.aspx?g=be75a3a5-595b-4dc8-ac4e-7e6f0523257f\nRelated \ud83c\udf10\nDubai USD 35m voice cloning fraud\nLauren Book deepfake nude extortion\nPage info\nType: Incident\nPublished: December 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/dubai-drone-weather-engineering", "content": "Dubai drone cloud seeding raises flooding, weaponisation concerns\nOccurred: July 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA programme operated by the Dubai government to generate rain by seeding clouds raised concerns about dangerous flooding, damage to public health, and its possible weaponisation.\nFirst reported by the Washington Post, the state's National Center of Meteorology (NCM) conducted 126 cloud seeding operations since 2021 using drones developed by the University of Reading working in conjunction with the University of Bath. Catapults launch the drones which zap clouds with an electric charge, charging the droplets inside. \nRegular heatwaves and a lack of freshwater meant the programme largely received a positive response by Dubai citizens. But scientists expressed concerns that chemicals used to generated rain are classified as 'possible carcinogens' to humans by the World Health Organisation\u2019s International Agency for Research on Cancer.\nConcerns were also expressed about the geo-political ramifications of 'rain stealing' and other potential weaponisations of the technology.\nSystem \ud83e\udd16\nDubai National Center of Metereology. Cloud Seeding\nOperator: National Center of Meteorology (NCM)\nDeveloper: University of Reading; University of Bath\nCountry: UAE/Dubai\nSector: Govt - agriculture\nPurpose: Seed clouds\nTechnology: Drone\nIssue: Environment; Geopolitics; Public health\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nVittoria D'alessio (2021). A new method to trigger rain where water is scarce\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.washingtonpost.com/nation/2021/07/21/uae-dubai-fake-rain/\nhttps://futurism.com/the-byte/uae-rainstorms-cloud-seeding-drones\nhttps://www.dailystar.co.uk/news/world-news/dubai-creates-rain-break-heatwave-24586732\nhttps://interestingengineering.com/the-uae-is-using-drones-to-control-dubais-weather\nhttps://wired.me/science/environment/cloud-seeding-uae-dubai-rain-floods/\nhttps://www.independent.co.uk/climate-change/news/dubai-fake-rain-heat-b1887596.html\nhttps://www.unilad.co.uk/technology/dubai-is-creating-fake-rain-to-battle-50c-heat/\nhttps://metro.co.uk/2021/07/21/weather-dubai-makes-it-own-fake-rain-with-drones-to-tackle-50c-heat-14963675/\nhttps://www.dailymail.co.uk/news/article-9809529/Dubai-creates-RAIN-tackle-122F-heat-Drones-blast-clouds-electrical-charge.html\nhttps://www.9news.com.au/world/dubai-news-drones-make-rain-middle-east-weather/1231bf6e-8856-408a-a819-aab9239fe076\nhttps://www.bbc.co.uk/news/technology-56428984\nhttps://www.arabnews.com/node/1826651/middle-east\nRelated \ud83c\udf10\nToronto beach water quality predictions\nUK Biodiversity Net Gain metric\nPage info\nType: Incident\nPublished: January 2022\nLast updated: November 2023", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/cbsa-toronto-pearson-airport-facial-recognition", "content": "Toronto Pearson airport secretly tests facial recognition system on travellers\nOccurred: July 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGovernment of Canada officials secretly tested a facial recognition system at Toronto's Pearson International Airport in 2016, prompting concerns about privacy and transparency.\nThe Globe and Mail revealed that the Canada Border Services Agency (CBSA) had run \u201cFaces on the Move,\u201d a project involving the secret testing of facial recognition technology on millions of unsuspecting travellers.\nRunning for about six months, from July to December of 2016,  Ottawa-based technology company Face4 Systems, on behalf of the CBSA, monitored approximately 15,000 and 20,000 travellers per day in order to identify people the CBSA suspected might try to enter the country using fake identification.\nHowever, the CBSA failed to inform travellers that their faces were being scanned, nor did it specify which airport was being used for the trial. Details of the trial were obtained by The Globe through a freedom of information (FOI/FOIA) request.\nThe finding sparked concerns about potential human rights violations and privacy breaches, with experts highlighting the need to balance national security with personal privacy.\nSystem \ud83e\udd16\nFace4 Recognition System website\nOperator: Canada Border Services Agency (CBSA)\nDeveloper: Face4 Systems\nCountry: Canada\nSector: Govt - immigration\nPurpose: Identify deported travellers\nTechnology: Facial recognition\nIssue: Privacy\nTransparency: Governance; Marketing; Privacy\nInvestigations, assessments, audits \ud83e\uddd0\nCBSA (2016). Faces on the Move: Mulllti-ca:mera Screening Privacy Impact Assessment (PIIA) (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theglobeandmail.com/canada/article-ottawa-tested-facial-recognition-on-millions-of-travellers-at-torontos/\nhttps://www.thestar.com/opinion/contributors/2021/07/27/reports-find-that-pearson-airport-is-once-again-using-surveillance-this-time-through-facial-recognition-this-threatens-our-human-rights.html\nhttps://findbiometrics.com/canadas-border-agency-quietly-tested-facial-recognition-torontos-pearson-airport-report-072005/\nhttps://www.blogto.com/tech/2021/07/canada-secretly-using-facial-recognition-toronto-pearson-airport/\nhttps://www.hilltimes.com/2021/07/28/facial-recognition-technology-fundamentally-undemocratic-says-angus-as-critics-wary-of-political-use/308694\nhttps://iapp.org/news/a/facial-recognition-pilot-caught-millions-of-travelers-at-ottawa-airport/\nhttps://www.mcgill.ca/newsroom/channels/news/experts-ottawa-tested-facial-recognition-millions-travellers-torontos-pearson-airport-332052\nRelated \ud83c\udf10\nWellington International Airport facial recognition\nS Korea immigration facial recognition sharing\nPage info\nType: Incident\nPublished: January 2022", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/japan-pm-critics-twitter-suspension", "content": "Twitter suspension of Japan PM critics prompts censorship accusations \nOccurred: July 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nTwitter Japan suspended the accounts of several prominent critics of Japan Prime Minister Suga Yoshihide and his government, prompting accusations of politcally-motivated censorship.\nOne of the suspended accounts was promoting an animated satire parodying Prime Minister Suga Yoshihide. The account, named \u201cChecking the Prime Minister\u2019s Pancakes for Poison\u201d, was suspended after tweeting about an upcoming film. \nAnother prominent critic, Nomachi Mineko, was suspended and then reinstated after making a series of tweets using the hashtag \u201cCancel the Tokyo Olympics\u201d.\nIn each case, the user\u2019s Twitter account was frozen after criticising the Japanese government. However, the accounts were reactivated after a public outcry, with no reason provided by Twitter Japan for the suspension or the subsequent restoration of the accounts.\nTwitter later blamed the company's AI-powered content moderation system.\nSystem \ud83e\udd16\nTwitter website\nTwitter Wikipedia profile\nOperator: Twitter\nDeveloper: Twitter\nCountry: Japan\nSector: Politics\nPurpose: Moderate content\nTechnology: Content moderation system\nIssue: Accuracy/reliability; Freedom of expression\nTransparency: Governance; Black box; Complaints/appeals\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.asahi.com/ajw/articles/14395800\nhttps://www.asahi.com/ajw/articles/14380956\nhttps://mainichi.jp/english/articles/20210626/p2a/00m/0na/018000c\nhttps://globalvoices.org/2021/07/19/twitter-japan-appears-to-suspend-government-critics/\nhttps://www.huffingtonpost.jp/entry/news_jp_609207fee4b05af50dc91c83\nhttps://lite-ra.com/2018/06/post-4086.html\nhttps://togetter.com/li/1660256\nRelated \ud83c\udf10\nEngland footballers' racism Instagram moderation\nTwitter right-wing content amplification\nPage info\nType: Incident\nPublished: January 2022", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/henn-na-hotel-robot-security", "content": "Henn-na Hotel robots found to have security vulnerability\nOccurred: October 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nRobots running units of the Henn-na hotel chain in Japan were discovered to have a security vulnerability that could expose guests to unwanted surveillance and abuse.\nSecurity engineer Lance R. Vick discovered that Henn-na hotel group's Tapia robots 'can be converted to offer anyone remote camera/mic access to all future guests'. \nH.I.S Hotel Group admitted that people could gain unauthorised access to its 100 Tapia devices and apologised 'for any uneasiness caused', but only after Vick had given it 90 days warning and had taken to Twitter in exasperation to publicise his findings. \nThe finding raised concerns about the security of Tapia robots and the organisations that use them, and the safety of their customers.\nSystem \ud83e\udd16\nTapia AI companion robot\nOperator: H.I.S. Hotel Group\nDeveloper: MJI Robotics\nCountry: Japan\nSector: Travel/hospitality\nPurpose: Interact with humans\nTechnology: Robotics\nIssue: Security; Safety; Dual-/multi-use\nTransparency: Governance; Marketing; Complaints/appeals\nResearch, advocacy \ud83e\uddee\nhttps://twitter.com/lrvick/status/1182823213736161280\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.tokyoreporter.com/business/robot-hotel-operator-announces-modification-to-prevent-hacks-by-guests/\nhttps://www.zdnet.com/article/hotels-in-room-assistants-could-have-been-used-to-spy-on-guests/\nhttps://gizmodo.com/how-to-ethically-hack-the-hotel-bedside-robots-a-gui-1839303207\nhttps://www.dailymail.co.uk/news/article-7608443/Japanese-robot-hotel-apologises-security-expert-exposes-hacking-flaw.html\nhttps://www.hotelmanagement.net/tech/japanese-hotel-bedside-robots-hacked\nhttps://www.fastcompany.com/90421411/wacky-robot-hotel-admits-its-bedside-cameras-could-have-exposed-guests-to-peeping-hackers\nhttps://www.securitymagazine.com/articles/91157-japanese-hotel-apologizes-for-robots-that-allowed-video-and-sound-to-be-hacked\nhttps://www.newsweek.com/quirky-japanese-hotel-replaces-room-robots-when-they-turn-out-hackable-can-turned-peeping-toms-1467386\nRelated \ud83c\udf10\nMarty grocery store robot\nDenny's robot server\nPage info\nType: Issue\nPublished: December 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/softbank-pepper-robot-security-vulnerabilities", "content": "Softbank Pepper robot discovered to have extensive security vulnerabilities\nOccurred: May 2018\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nSoftBank's Pepper robot has significant security issues, notably unauthenticated administrative capabilities, according to a paper published by a group of researchers. \nAlberto Giaretta of Sweden's \u00d6rebro University and Michele De Donno and Nicola Dragoni of the Technical University of Denmark found that the Pepper anthropomorphic robot suffered from significant security vulnerabilities, including allowing for unauthenticated root-level access, having a default root password, being susceptible to brute-force attack as it did not have any protections against unlimited password attempts.\nThe researchers alleged that SoftBank Robotics 'extensively neglected any sort of security assessments before commercializing their product', and that it would be 'a breeze to remotely turn [Pepper] into a 'cyber and physical weapon', exposing malicious behaviours'.\n\u2795 June 2021. Production of Pepper was stopped due to weak demand.\nSystem \ud83e\udd16\nPepper robot website\nPepper robot Wikipedia profile\nOperator: Softbank Robotics\nDeveloper: Softbank Robotics\nCountry: Japan\nSector: Technology\nPurpose: Interact with humans\nTechnology: Robotics\nIssue: Security; Safety; Dual/multi-use\nTransparency: \nResearch, advocacy \ud83e\uddee\nGiaretta A. et al. Adding Salt to Pepper: A Structured Security Assessment over a Humanoid Robot\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.techrepublic.com/article/softbank-invested-almost-nothing-in-pepper-robot-security-creating-huge-business-risk/\nhttps://www.theregister.com/2018/05/29/softbank_pepper_robot_multiple_basic_security_flaws/\nhttps://internetofbusiness.com/softbank-pepper-robot-astonishingly-insecure-and-a-cyber-weapon/\nhttps://www.lemondeinformatique.fr/actualites/lire-le-robot-pepper-nid-a-vulnerabilites-de-securite-71896.html\nhttps://www.heise.de/security/meldung/Roboter-Pepper-kaempft-mit-massiven-Sicherheitsproblemen-4060743.html\nhttps://www.golem.de/news/roboter-pepper-ist-voller-sicherheitsluecken-1805-134648.html\nhttps://www.techzine.nl/nieuws/security/405204/pepper-de-menselijke-robot-is-eenvoudig-te-hacken\nRelated \ud83c\udf10\nHenn-na hotel robot security\nTesla Optimus robot\nPage info\nType: Incident\nPublished: January 2022", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ocado-robot-collision", "content": "Ocado warehouse robots collide, causing fire and evacuation\nOccurred: July 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThree robots collided at an Ocado warehouse in Erith, south-east London, causing a fire and the facility's evacuation. \nThe incident disrupted operations, led to the evacuation of about 800 staff members, and resulted in the cancellation of thousands of orders. Around 100 firefighters worked overnight to contain the blaze. \nNo injuries were reported. At the time, the warehouse was serviced by around 3,500 delivery-packing robots. \nThe damage, which Ocado estimated to cost GBP 35 million, was limited to a small section of the warehouse. The company\u2019s shares slipped by about 3 percent. \nThe incident raised questions about the safety of Ocado's automated packing system.\n\u2796  In July 2019, an Ocado robot malfunctioned in its Andover warehouse, resulting in a major fire. The warehouse had to re-built at an estimated cost of GBP 110 million.\nSystem \ud83e\udd16\nOcado website\nOcado Wikipedia profile\nDocuments \ud83d\udcc3\nStatement regarding incident at Erith Customer Fulfilment Centre\nOperator: Ocado\nDeveloper: Ocado\nCountry: UK\nSector: Transport/logistics\nPurpose: Pick groceries\nTechnology: Robotics\nIssue: Safety\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://inews.co.uk/news/uk/ocado-fire-warehouse-orders-cancelled-disruption-robots-1109261\nhttps://www.ft.com/content/aaddf4b1-a78b-4289-b42f-fd3f5cd7f176\nhttps://www.thetimes.co.uk/article/robot-wars-break-out-at-ocado-warehouse-bh3wxg8x8\nhttps://www.msn.com/en-gb/news/uknews/tens-of-thousands-of-ocado-deliveries-are-cancelled-after-blaze/ar-AAMgZGl\nhttps://www.telegraph.co.uk/business/2021/07/18/ocado-cancels-orders-warehouse-fire/\nhttps://uk.finance.yahoo.com/news/ocado-expects-disruption-warehouse-fire-181914463.html\nhttps://www.bloomberg.com/news/articles/2021-07-18/ocado-sees-operations-disrupted-after-robots-collide-cause-fire\nhttps://www.dailyecho.co.uk/news/17410117.100-firefighters---including-crews-southampton---battle-blaze-ocado-warehouse/\nhttps://www.newsshopper.co.uk/news/19582811.erith-warehouse-fire-cost-ocado-estimated-35-million\nRelated \ud83c\udf10\nOcado robot charger malfunction\nStarship Technologies delivery robots\nPage info\nType: Incident\nPublished: November 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ocado-robot-charger-malfunction", "content": "Ocado robot charger malfunctions, 370 jobs eliminated\nOccurred: February 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe malfunctioning of a robot charger resulted in a major fire at an Ocado distribution centre in Andover, UK, and in the cancellation of customer deliveries. \nAn electrical fault in a battery charging unit led to the plastic lid on the top of a grocery-carrying robot catching alight. The fire resulted in substantial damage to the warehouse and required nearby houses to be evacuated. \nThe blaze lasted four days. It was later discovered that an automated detection system had failed and that employees had turned off the sprinklers and tried to tackle the fire, only calling the fire brigade after one hour. \nFollowing the incident, Ocado took steps to minimise the risk of such an event occurring again, including installing additional smoke detectors and removing the plastic lid on its robots.\nThe incident cost the firm an estimated GBP 110 million, excluding insurance cover, and led to the elimination of 370 jobs.\n\u2795 July 2021. Three robots collided at an Ocado warehouse in Erith, south-east London, resulting in the facility's evacuation and the cancellation of thousands of customer orders\n\u2795 August 2021. Ocado's Andover facility re-opened.\nSystem \ud83e\udd16\nOcado website\nOcado Wikipedia profile\nOperator: Ocado\nDeveloper: Ocado\nCountry: UK\nSector: Transport/logistics\nPurpose: Pick groceries\nTechnology: Robotics\nIssue: Employment; Safety\nTransparency: \nInvestigations, assessments, audits \ud83e\uddd0\nHampshire Fire and Rescue Authority. Ocado Fire Contributory Factors (pdf) \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/uk-england-hampshire-47127814\nhttps://www.bbc.co.uk/news/technology-47160448\nhttps://www.bbc.co.uk/news/uk-england-hampshire-49071456\nhttps://www.telegraph.co.uk/news/2019/02/06/ocado-cancels-orders-fire-rips-andover-warehouse/\nhttps://www.thetimes.co.uk/article/ocado-robots-at-andover-warehouse-hampered-firefighters-gwtj0zqd7\nhttps://www.msn.com/en-gb/news/uknews/major-fire-breaks-out-at-ocado-warehouse-after-three-robots-collide/ar-AAMfQ4N \nhttps://www.independent.co.uk/news/business/news/ocado-losses-warehouse-fire-robots-andover-hampshire-a9329301.html\nhttps://www.andoveradvertiser.co.uk/news/17760741.ocado-says-andover-warehouse-blaze-cost-company-110m/\nRelated \ud83c\udf10\nOcado warehouse robot collisions\nInstacart gig shopper robotisation\nPage info\nType: Incident\nPublished: November 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/anthony-bourdain-voice-deepfake", "content": "Anthony Bourdain Roadrunner voice deepfake results in backlash\nOccurred: July 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe unauthorised use of AI to recreate the voice of Anthony Bourdain in a documentary resulted in a backlash by film critics and others.\nRoadrunner, a documentary about the life and death of chef and TV personality Anthony Bourdain, used deepfake technology to re-create Bourdain's voice without disclosure, and without the permission of Bourdain's widow Ottavia.\nIn an interview with the New Yorker, filmmaker Neville Morgan described how he had supplied an AI company with a dozen hours of Bourdain speaking in order to manufacture three quotes he had wanted the TV host to say. \nThe interview led to a backlash by film critics, film makers and others, who accused Morgan of underhand, manipulative and unethical behaviour. 'We can have a documentary-ethics panel about it later', Morgan quipped.\nSystem \ud83e\udd16\nUnknown\nDocuments \ud83d\udcc3\nRoadrunner official trailer\nOperator: Focus Features\nDeveloper: Morgan Neville\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Entertain\nTechnology: Deepfake - audio; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning  \nIssue: Cheating/plagiarism; Copyright; Ethics/values; Privacy\nTransparency: Governance; Privacy; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.newyorker.com/culture/annals-of-gastronomy/the-haunting-afterlife-of-anthony-bourdain\nhttps://www.gq.com/story/anthony-bourdain-morgan-neville-roadrunner-documentary\nhttps://www.businessinsider.com/anthony-bourdain-documentary-uses-deepfake-of-his-voice-2021-7\nhttps://hypebeast.com/2021/7/roadrunner-a-film-about-anthony-bourdain-documentary-deepfake-audio\nhttps://www.theverge.com/2021/7/15/22578707/anthony-bourdain-documentary-deepfake-voice\nhttps://www.vulture.com/2021/07/posthumous-anthony-bourdain-doc-used-a-i-voice-model.html\nhttps://www.vice.com/en/article/m7e54b/roadrunner-director-deepfaked-anthony-bourdains-voice\nhttps://variety.com/2021/artisans/news/anthony-bourdain-fake-voice-roadrunner-documentary-backlash-1235020878/\nhttps://www.bbc.co.uk/news/technology-57842514\nhttps://www.npr.org/2021/07/16/1016838440/ai-brought-anthony-bourdains-voice-back-to-life-should-it-have\nRelated \ud83c\udf10\nKim Kwang-Seok voice recreation prompts concerns\nTom Cruise deepfakes\nPage info\nType: Incident\nPublished: December 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/livonia-skating-rink-misidentifies-black-teenager", "content": "Black teenager misidentified, barred by Livonia skating rink AI system\nOccurred: July 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA black teenager was barred from entering an ice skating rink in Livonia, Michigan, after she was misidentified by a facial recognition system. \nAccording to Fox 2 Detroit, The Riverside Arena skating rink in stopped Lamya Robinson from entering its premises after her face was scanned and the results indicated she had previously been involved in a brawl there. However, Robinson had never been to the skating rink.\nRobinson's mother told Fox 2 Detroit, 'To me, it\u2019s basically racial profiling,' said the girl\u2019s mother. 'You\u2019re just saying every young Black, brown girl with glasses fits the profile and that\u2019s not right.'\nThe incident called into question the accuracy and effectiveness of the (unnamed) system, and underscored perceptions that facial recognition systems are 'racist'.\nThe skating rink apologised 'if there was a mistake.'\nSystem \ud83e\udd16\nUnknown\nOperator: Riverside Arena, Livonia, Michigan\nDeveloper: Unclear/unknown\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Strengthen security\nTechnology: Facial recognition\nIssue: Accuracy/reliability; Bias/discrimination - race\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://youtu.be/Cnp45VHyFRs\nhttps://www.fox2detroit.com/news/teen-kicked-out-of-skating-rink-after-facial-recognition-camera-misidentified-her\nhttps://petapixel.com/2021/07/15/facial-recognition-misidentifies-black-teen-ignites-debate-over-its-ethics/\nhttps://www.dailydot.com/debug/facial-recognition-misidentified-black-girl-skating-rink/\nhttps://www.theverge.com/2021/7/15/22578801/black-teen-skating-rink-inaccurate-facial-recognition\nhttps://www.msn.com/en-us/news/crime/black-teen-barred-from-skating-rink-after-facial-recognition-camera-misidentified-her/ar-AAMc9qg\nhttps://thegrio.com/2021/07/15/facial-recognition-misidentification-lamya-robinson-michigan/\nhttps://blavity.com/black-teen-barred-from-skating-rink-after-being-misidentified-by-facial-recognition-system\nhttps://www.zdnet.com/article/backlash-to-retail-use-of-facial-recognition-grows-after-michigan-teen-kicked-out-of-skating-rink-after-false-match/\nhttps://www.theregister.com/2021/07/16/facial_recognition_failure/\nhttps://www.deadlinedetroit.com/articles/28392/teen_barred_from_livonia_roller_rink_based_on_incorrect_facial_recognition\nRelated \ud83c\udf10\nApple/SIS facial recognition misidentifies 'shoplifter' Ousmane Bah\n'Racist' Uber Eats facial ID check gets Pa Edrissa Manjang fired\nPage info\nType: Incident\nPublished: January 2022\nLast updated: October 2023", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/russia-facial-recognition-ethnicity-analytics", "content": "Russian companies found to have built facial recognition ethnicity analytics\nOccurred: July 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn investigation by video surveillance company IVPM discovered that Russian technology companies AxxonSoft, Tevian, VisionLabs and NtechLab had built facial recognition powered tools to identify people based on their race or ethnicity. \nThe finding prompted digital rights advocates to express concerns that facial recognition technologies could be used to deepen existing racial and ethnic discrimination in the country, and elsewhere.\nAccording to Reuters, there was no indication that Russian police had targeted minorities using the firms' software.\nIn response, AxxonSoft disabled its ethnicity analytics feature, claiming it was not interested in promoting technologies that could be a basis for ethnic segregation, Tevian denied that current uses of its tool by police could entrench discrimination, and VisionLabs stated its ethnicity analytics software was developed for internal research purposes only.\nThe Russian government is a customer of each company; NtechLab is partially funded and owned by the Russian government.\nSystem \ud83e\udd16\nAxxonSoft website\nNtechLab website\nTevian website\nVisionLabs website\nOperator: Moscow Department of Technology\nDeveloper: AxxonSoft; Tevian; VisionLabs; NtechLab\nCountry: Russia\nSector: Govt - transport\nPurpose: Identify ethnicity\nTechnology: Facial recognition\nIssue: Surveillance; Bias/discrimination - race, ethnicity\nTransparency: Governance; Marketing\nResearch, advocacy \ud83e\uddee\nhttps://ipvm.com/reports/russia-ethnicity-analytics\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reuters.com/article/uk-russia-tech-race-idUSKCN2EB0BC\nhttps://www.biometricupdate.com/202107/ethnicity-recognition-found-among-russian-face-biometrics-providers-features\nhttps://www.voanews.com/europe/racist-facial-recognition-sparks-ethical-concerns-russia-analysts-say\nhttps://www.wionews.com/videos/russian-firms-using-ai-tools-to-classify-faced-based-on-race-396191\nhttps://www.hrw.org/news/2020/10/02/russia-expands-facial-recognition-despite-privacy-concerns\nhttps://www.codastory.com/authoritarian-tech/russia-facial-recognition-networks/\nRelated \ud83c\udf10\nHuawei Uyghur-spotting patent\nChina facial image criminal inference\nPage info\nType: Incident\nPublished: December 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/england-footballers-racism-instagram-moderation", "content": "Twitter, Instagram fail to stop racist abuse of England footballers\nOccurred: July 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nInstagram, Twitter and other social media platforms failed to stop a torrent of racial abuse hurled at black England footballers after the national team was knocked out of the Euro 2020 tournament. \nThe abuse aimed at Bukayo Saka, Marcus Rashford and Jadon Sancho, all of whom missed penalties in England's Euro 2020 final loss to Italy, was condemned by then Prime minister Boris Johnson and the Football Association, amongst others. \nTwitter said it had removed over 1,000 posts over 24 hours and suspended a number of accounts for violating its rules. Instagram CEO Adam Mosseri blamed its 'mistaken' content moderation technology for failing to remove thousands of racist comments and images.\nReports indicated that many posts and accounts reported for being offensive or abuse were not taken down or suspended due to the high volume of comments and apparent confusion about whether they violated the platforms' policies and terms.\nSystem \ud83e\udd16\nInstagram content moderation system\nTwitter content moderation system\nDocuments \ud83d\udcc3\nTwitter hateful conduct policy\nInstagram CEO Adam Mosseri response\nOperator: Meta/Facebook; Twitter\nDeveloper: Meta/Facebook; Twitter\nCountry: UK\nSector: Media/entertainment/sports/arts\nPurpose: Detect toxic content\nTechnology: Content moderation system\nIssue: Accuracy/reliability; Bias/discrimination - race, ethnicity\nTransparency: Governance; Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/technology-57848106\nhttps://www.standard.co.uk/sport/football/instagram-racist-comments-emjojis-bukayo-saka-rashford-sancho-b945969.html\nhttps://www.theguardian.com/world/2021/jul/14/social-networks-anti-racism-policies-belied-by-users-experience\nhttps://inews.co.uk/news/technology/instagram-racist-abuse-posts-england-players-after-euros-1102896\nhttps://inews.co.uk/news/racist-abuse-england-players-euro-2020-final-instagram-accounts-still-active-1101659\nhttps://inews.co.uk/news/england-players-racist-abuse-social-media-legal-action-euros-1102867\nhttps://www.msn.com/en-us/news/technology/racist-posts-and-accounts-are-still-live-on-instagram-e2-80-93-despite-site-insisting-it-e2-80-99s-cracking-down-after-euros/ar-AAM9sMG\nhttps://www.lbc.co.uk/news/instagram-under-fire-as-racist-monkey-and-banana-posts-do-not-qualify-for-ban/\nRelated \ud83c\udf10\nTikTok beheading video splicing\nTikTok creators hate speech detection\nPage info\nType: Incident\nPublished: December 2021\nLast updated: May 2023", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/userviz-video-game-cheating-system", "content": "Userviz video game cheating system is shut down after legal threat\nOccurred: July 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nUserviz, an 'undetectable' AI system that could be used to aim automatically at an enemy while gaming, was shut down after a legal threat.\nUserviz used machine learning to feed gameplay video from a console via network streaming or capture card, and fed it into a nearby computer, which displayed information the cheater could use. It was supposedly able to cheat any game on any platform or device and be undetectable.\nThe system started getting attention after a well-known cheat hunter group tweeted about it, prompting game publisher Activision to issue a legal threat. Activision's Call of Duty: Warzone game was being used to market the system.\nUserViz was later shut down, with it\u2019s developer, 'User101', deleting all content on the system's site and replacing it with a statement claiming the software had never been published and that its development would not be continued.\nThe incident raised questions about the ethics of using systems - covert and otherwise - that cheat online games. \nSystem \ud83e\udd16\nUserViz\nOperator: Activision\nDeveloper: User 101  \nCountry: USA; Global\nSector: Media/entertainment/sports/arts\nPurpose: Cheat video games\nTechnology: Deep learning; Machine learning\nIssue: Ethics/values\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.vice.com/en/article/m7e5bv/undetectable-console-cheat-shuts-down-after-activision-request\nhttps://arstechnica.com/gaming/2021/07/cheat-maker-brags-of-computer-vision-auto-aim-that-works-on-any-game/\nhttps://kotaku.com/cheat-maker-new-aimbot-undetectable-on-consoles-pc-1847245460\nhttps://techraptor.net/gaming/news/activision-shuts-down-ai-assisted-call-of-duty-warzone-cheat\nhttps://www.vg247.com/2021/07/14/undetectable-call-of-duty-warzone-cheat-activision-takedown/\nhttps://www.everyeye.it/notizie/cod-warzone-lotta-cheat-bloccato-software-basato-machine-learning-529401.html\nhttps://www.pcguru.hu/hirek/az-activision-lecsapott-a-legujabb-csalas-fejlesztojere/64242\nRelated \ud83c\udf10\nWitcher 3 AI voice line simulation\nElite Dangerous AI spaceships create superweapons\nPage info\nType: Incident\nPublished: December 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tencent-midnight-patrol-facial-recognition", "content": "Critics question Tencent 'Midnight Patrol' effectiveness, instrusiveness \nOccurred: July 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA facial recognition system designed to enforce a government curfew on gaming for minors in China prompted complaints about its effectiveness and accusations that it intruded on the privacy of families.\nTencent\u2019s \u2018Midnight Patrol\u2019 was intended to stop children playing online games between 22:00 and 08:00 in line with a Chinese government national crackdown on video game addiction. Tencent is the largest maker and distributer of online video games in China.\nThe ban required gamers to register with their official IDs, linked to a national database. However, it transpired that children were using adults' IDs instead of their own to get around the law, bringing into question the system's effectiveness and value. \nIn addition, the system, which is linked to China's central public security system, involved the mass collection of personal data, which could lead to security breaches. Critics also pointed out it could be used beyond its original purpose, including home surveillance, and could result in having psychological impacts on minors, who might feel overly monitored or restricted.\nSystem \ud83e\udd16\nMidnight Patrol\n\nDocuments \ud83d\udcc3\nhttps://mp.weixin.qq.com/s/Cr17TKWQ3XcuNzDw085OLw\nOperator: Tencent\nDeveloper: Tencent\nCountry: China\nSector: Technology\nPurpose: Restrict gaming hours\nTechnology: Facial recognition\nIssue: Effectiveness/value\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.sixthtone.com/news/1007915/tencent-deploys-facial-recognition-to-detect-minors-gaming-at-night-\nhttps://www.nytimes.com/2021/07/08/business/video-game-facial-recognition-tencent.html\nhttps://www.abc.net.au/news/2021-07-22/chinese-gaming-company-midnight-patrol-using-facial-recognition-/100306444\nhttps://www.bloomberg.com/news/articles/2021-07-08/tencent-uses-facial-recognition-to-ban-kids-gaming-past-bedtime\nhttps://www.pcgamer.com/tencent-is-now-using-facial-recognition-in-china-to-stop-children-from-gaming-all-night/\nhttps://fortune.com/2021/07/09/tencent-facial-recognition-video-games-children/\nhttps://beebom.com/tencent-facial-recongnition-stop-children-china-gaming-at-night/\nhttps://www.digitaltrends.com/gaming/tencent-facial-recognition-china/\nhttps://www.biometricupdate.com/202107/tencent-low-light-facial-recognition-to-check-night-time-gaming-among-children\nhttps://www.bbc.co.uk/news/technology-57752782\nRelated \ud83c\udf10\nKohler, BMW, MaxMara China facial recognition\nXPeng customer facial recognition\nPage info\nType: Issue\nPublished: July 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tiktok-uses-bev-standing-voice-to-train-ai", "content": "TikTok uses Bev Standing voice without consent to train AI\nOccurred: May 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nTikTok owner Bytedance settled out of court with voice actor Bev Standing for allegedly using her voice without her permission for its new text-to-speech function.\nHired to read thousands of English sentences for translation by China's Institute of Acoustics, Standing sued Bytedance on the basis that her recordings came into TikTok's possession and her voice used to generate TikTok's new text-to-speech function.\nWith her voice appearing in 'foul and offensive' viral videos across the world, Standing also accuses TikTok of causing her reputation 'irreparable harm'. \nTikTok has yet to acknowledge in public that it had been using her voice.\nSystem \ud83e\udd16\nTikTok text-to-speech voice generator\nOperator: ByteDance/TikTok\nDeveloper: ByteDance/TikTok\nCountry: USA; Global\nSector: Media/entertainment/sports/arts\nPurpose: Convert speech to text\nTechnology: Text-to-speech\nIssue: Copyright; Employment\nTransparency: Governance; Privacy; Marketing; Legal\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nStanding v Bytedance\nGoFundMe. StandwithBevStanding\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.vice.com/en/article/z3xqwj/this-tiktok-lawsuit-is-highlighting-how-ai-is-screwing-over-voice-actors\nhttps://www.technologyreview.com/2021/07/09/1028140/ai-voice-actors-sound-human\nhttps://www.hitc.com/en-gb/2021/05/25/who-is-beverly-standing/\nhttps://www.bbc.co.uk/news/technology-57063087\nhttps://www.theverge.com/2021/5/13/22435058/tiktok-voice-sues-unauthorized-usage-text-to-speech\nhttps://www.inputmag.com/culture/tiktok-is-being-sued-by-the-voice-of-its-viral-text-to-speech-feature\nhttps://www.cbc.ca/news/business/canadian-actor-suing-tiktok-1.6024369\nhttps://www.telegraph.co.uk/technology/2021/05/09/voice-tiktok-sues-company-emotional-distress/\nhttps://screenrant.com/tiktok-text-speech-app-feature-voice-actor-lawsuit-explained/\nhttps://www.theverge.com/2021/9/29/22701167/bev-standing-tiktok-lawsuit-settles-text-to-speech-voice\nRelated \ud83c\udf10\nVoiceVerse NFT voice theft\nWitcher 3 AI voice line simulation\nPage info\nType: Incident\nPublished: June 2021\nLast updated: May 2023", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ena-emergency-severity-index", "content": "ENA Emergency Severity Index accused of racial, economic bias\nOccurred: June 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA number of healthcare delivery and planning algorithms widely used across the US, notably The Emergency Nurses Association (ENA)'s Emergency Severity Index, actively reinforced existing racial and economic biases. \nResearchers at The University of Chicago Booth School of Business' Center for Applied Artificial Intelligence assessed (pdf) algorithms that help emergency rooms triage patients and predict diabetes, amongst other uses. \nThe Emergency Nurses Association (ENA)'s Emergency Severity Index was found to underestimate the severity of Black and Hispanic peoples' problems, perpetuating inequitable treatment in areas in which they reside for at least a decade, STAT reported.\nThe finding was seen to highlight concerns about racial, ethnic and geographic bias in the assignment of ESI scores to patients.\nSystem \ud83e\udd16\nEmergency Severity Index Wikipedia profile \n\nDocuments \ud83d\udcc3\nhttps://www.chicagobooth.edu/-/media/project/chicago-booth/centers/caai/docs/algorithmic-bias-playbook-june-2021.pdf\nhttps://www.ena.org/docs/default-source/education-document-library/triage/esi-implementation-handbook-2020.pdf\nOperator: Brigham and Women\u2019s Hospital, Boston; Emergency Nurses Association (ENA)\nDeveloper: Emergency Nurses Association (ENA)\nCountry: USA\nSector: Health\nPurpose: Assess medical condition\nTechnology: Triage algorithm\nIssue: Accuracy/reliability; Bias/discrimination - race, ethnicity, economy\nTransparency: Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.statnews.com/2021/06/21/algorithm-bias-playbook-hospitals\nhttps://gabio.org/nobody-is-catching-it-algorithms-used-in-health-care-nationwide-are-rife-with-bias/\nhttps://www.medicaldevice-network.com/news/algorithmic-bias-playbook/\nhttps://www.healtheconomics.com/industry-news/researchers-identify-biased-algorithms-prevalent-throughout-the-us-healthcare-industry\nRelated \ud83c\udf10\nGoogle Derm Assist dermatology app bias, privacy\nEpic Systems Epic Deterioration Index\nPage info\nType: Issue \nPublished: October 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/reddit-shadowbanning", "content": "Reddit replaces opaque shadowbanning system with account suspensions\nOccurred: November 2015\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nReddit replaced the controversial practice of 'shadowbanning' its users with the suspension of their accounts.\nReddit had shadowbanned users seen to break the site's rules, and had considered it an efficient way to stop spammers flooding its platform. \nShadowbanning consists of users being notified via email and on the website that they had been suspended and for how long. When shadowbanned, Reddit users could only see their own content, were not informed that they were banned, and had to go through an opaque and largely unstructured appeals process. \nAn unofficial subreddit, r/shadowban, had even been created to let users know if they had been shadowbanned. \nReddit's shadowbanning system had been seen as opaque, largely unaccountable, and was accused many times of being unfair and of censoring appropriate and legal content.\nSystem \ud83e\udd16\nReddit\n\nDocuments \ud83d\udcc3\nhttps://www.reddit.com/r/announcements/comments/3sbrro/account_suspensions_a_transparent_alternative_to/\nhttps://www.reddit.com/r/self/comments/3ey0fv/on_shadowbans/\nhttps://www.reddit.com/r/ShadowBan/comments/1vyaa2/a_guide_to_getting_unshadowbanned_sticky_maybe/\nOperator: Reddit\nDeveloper: Reddit\nCountry: USA\nSector: Technology\nPurpose: Block/reduce user visibility\nTechnology: Algorithmic content moderation\nIssue: Fairness; Freedom of expression - censorship; Harassment\nTransparency: Governance; Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://techcrunch.com/2015/11/11/reddit-account-suspensions/\nhttps://thenextweb.com/news/reddit-replaces-shadowbans-with-suspensions-to-punish-spammers-and-trolls\nhttps://www.searchenginejournal.com/reddit-finally-ends-shadowbans-replaces-account-suspensions/144902/\nRelated \ud83c\udf10\nTikTok LGBTQ shadowbanning\nTikTok/Instagram/Facebook LGBTQ discrimination\nPage info\nType: Issue\nPublished: November 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-automated-pricing-glitch", "content": "Automated pricing glitch on Amazon UK causes retailers' losses\nOccurred: December 2014\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA technical glitch on Amazon's UK website led to the price of thousands of products crashing to 1p, leaving hundreds of small businesses out of pocket.\nThe glitch occurred between 7pm and 8pm on a Friday, and stemmed from software developed by Repricer Express which automatically changes the cost of third-party products on Amazon Marketplace to ensure they are the cheapest available.\nThe error mainly impacted small, independent companies, some of which expressed concerns about potential bankruptcy if they were forced to honor the 1p sales. Judith Blackford of Kiddymania, a seller on Amazon, claimed that she lost about GBP 20,000 overnight due to the glitch.\nRepricerExpress CEO, Brendan Doherty, apologised for the distress caused to customers and assured that Amazon would not penalise sellers for this error. Despite requests to cancel the erroneous orders, some continued to be dispatched. \nThe incident raised questions about accountability and the potential risks of relying on automated pricing tools.\nSystem \ud83e\udd16\nRepricer Express website\nOperator: Repricer Express; Amazon\nDeveloper: Repricer Express\nCountry: UK\nSector: Technology\nPurpose: Change product pricing\nTechnology: Pricing automation\nIssue: Accuracy/reliability\nTransparency: Complaints/appeals \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theguardian.com/technology/2014/dec/15/amazon-sellers-discounted-glitch-pricing\nhttps://www.theguardian.com/money/2014/dec/14/amazon-glitch-prices-penny-repricerexpress\nhttps://www.bbc.co.uk/news/uk-northern-ireland-foyle-west-30475542\nhttps://www.independent.co.uk/news/uk/home-news/amazon-1p-glitch-software-error-sees-hundreds-items-sold-fractions-their-value-9923730.html\nhttps://uk.pcmag.com/web-sites/38237/glitch-drops-cost-of-uk-amazon-sellers-items-to-just-one-penny\nhttps://www.cnet.com/tech/services-and-software/amazon-glitch-leads-to-items-being-sold-for-almost-nothing/\nhttps://www.channel4.com/news/amazon-glitch-repricerexpress-friday-online-retailers\nhttps://www.telegraph.co.uk/finance/newsbysector/retailandconsumer/11295073/Nightmare-before-Christmas-frustrations-grow-after-Amazon-1p-glitch-costs-businesses-thousands.html\nRelated \ud83c\udf10\nPyth Bitcoin glitch\nTesla China FSD software glitch, recall\nPage info\nType: Incident\nPublished: November 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tiktok-mandatory-beauty-filtering", "content": "TikTok touches up users' faces without consent\nOccurred: June 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nTikTok was automatically touching up the videos of some users' faces without notifiying them or asking for their consent. \nThe practice was exposed by Tori Dawn and other users, who shared the issue in videos posted on the social network. \nTikTok subsequently acknowledged there was an issue, but did not divulge what caused it, or how it was fixed. \nAccording to the Technology Review, automated beauty filtering is standard practice in China; elsewhere it is offered as an option to which users have to opt-in. \n\u2795 September 2020. TikTok was accused of suppressing ugly, poor, disabled and LBGT users and content.\nSystem \ud83e\udd16\nTikTok website\nTikTok Wikipedia profile\nOperator: ByteDance/TikTok\nDeveloper: ByteDance/TikTok\nCountry: USA; Global\nSector: Media/entertainment/sports/arts\nPurpose: Beautify user faces\nTechnology: Computer vision\nIssue: Appropriateness/need; Privacy\nTransparency: Governance; Marketing; Privacy\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.technologyreview.com/2021/06/10/1026074/tiktok-mandatory-beauty-filter-bug/\nhttps://www.newsweek.com/tiktok-beauty-filter-glitch-automatic-report-1600809\nhttps://screenrant.com/tiktok-face-beauty-filter-automatically-applied-unremovable/\nhttps://www.nylon.com/life/tiktok-automatic-beauty-filters-angry-creators\nhttps://www.dazeddigital.com/beauty/head/article/53206/1/tiktokers-claim-the-app-added-a-beauty-filter-without-their-permission\nhttps://www.papermag.com/tiktok-filter-glitch-2653377079.html\nhttps://i-d.vice.com/en_uk/article/z3x39a/tiktok-beauty-filter-bug\nhttps://www.inputmag.com/tech/tiktok-has-been-beautifying-peoples-faces-without-asking\nRelated \ud83c\udf10\nTikTok/Instagram/Facebook LGBTQ discrimination\nQoves AI beauty scoring\nPage info\nType: Incident\nPublished: December 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tiktok-creators-hate-speech-detection", "content": "TikTok hate speech detection system accused of racial bias\nOccurred: July 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nTikTok's hate speech detection system was accused of racial bias by Black creators and rights activists for appearing to block Black content.\nPer The Verge, the furore was sparked by Black influencer Ziggi Tyler finding that phrases including 'Black Lives Matter' and 'Black success' were flagged by TikTok as 'inappropriate' when he tried to update his creator profile. \nAt the same time, TikTok's system allowed phrases such as 'white supremacy' and 'white success'. \nTikTok responded by blaming its content moderation technology and claimed it had fixed the error.\nThe incident raised questions about TikTok's  willingness and ability to detect and act upo\nSystem \ud83e\udd16\nTikTok hate speech detection system\nOperator: ByteDance/TikTok\nDeveloper: ByteDance/TikTok\nCountry: USA; Global\nSector: Media/entertainment/sports/arts\nPurpose: Detect hate speech\nTechnology: Recommendation algorithm\nIssue: Bias/discrimination - race\nTransparency: Black box; Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.vox.com/recode/2021/7/7/22566017/tiktok-black-creators-ziggi-tyler-debate-about-black-lives-matter-racial-bias-social-media\nhttps://www.vox.com/the-goods/2021/6/29/22554596/digital-blackface-megan-thee-stallion-song-tiktok-first-strike\nhttps://i-d.vice.com/en_uk/article/m7epya/tiktoks-algorithm-reportedly-bans-creators-using-terms-black-and-blm\nhttps://www.nbcnews.com/pop-culture/pop-culture-news/don-t-ban-us-being-jewish-jewish-tiktok-creators-say-n1270598\nhttps://www.jta.org/2021/06/28/united-states/social-media-companies-say-they-ban-holocaust-denial-are-they-also-blocking-education\nhttps://www.timesofisrael.com/are-social-media-platforms-banning-holocaust-education-along-with-hate-speech/\nhttps://jezebel.com/tiktoks-excuses-for-its-anti-black-racism-are-getting-p-1847256853\nRelated \ud83c\udf10\nYouTube ads hate speech blocklist\nFacebook downranking system failure\nPage info\nType: Incident\nPublished: December 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/foodinho-fined-for-breaching-privacy-and-labour-laws-in-italy", "content": "Foodinho fined for breaching privacy and labour laws in Italy\nOccurred: July 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nFood delivery company Foodinho was fined EUR 2.6m (USD 3m) for breaching privacy and labour laws in Italy, and ordered to update its management algorithms to avoid discrimination.\nItalian privacy watchdog Garante said that the Glovo-owned gig worker management company had tracked its workers' locations out of office hours, and had shared it, along with other personal data, with undisclosed third parties not mentioned in the app\u2019s documentation.\nIt also ruled that Foodinho had not adequately explained its automated order management system properly to its workers, and had failed to ensure that the results of automatic processes to evaluate the workers' performance were correct.\nIn addition, the regulator said that Foodinho had failed to provide workers with ways to challenge decisions made using its management algorithm, including the exclusion of some riders from taking orders, and raised concerns about potential discrimination arising from it. \nFoodinho was ordered to make a raft of changes to its privacy practices and to amend how its management algorithms work.\nSystem \ud83e\udd16\nGlovo website\nGlovo Wikipedia profile\nOperator: Glovo/Foodinho\nDeveloper: Glovo/Foodinho\nCountry: Italy\nSector: Transport/logistics\nPurpose: Manage workers\nTechnology: Automated management system\nIssue: Accuracy/reliability; Bias/discrimination; Privacy; Security\nTransparency: Governance; Black box\nRegulation \u2696\ufe0f\nEU General Data Protection Regulation\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nGarante (2021). Riders: Italian SA says no to algorithms causing discrimination. A platform in the Glovo group fined EUR 2.6 million\nGarante (2021). Abstract of Italian SA\u2019s order as issued against Foodinho S.r.l.\nResearch, advocacy \ud83e\uddee\nEuropean Trade Union Institute (2023). Exercising workers\u2019 rights in algorithmic management systems\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reuters.com/technology/italian-watchdog-takes-aim-delivery-firms-gig-worker-algorithms-2021-07-05/\nhttps://www.reuters.com/article/italy-tech-algorithm/spains-glovo-considers-appeal-after-italian-regulator-finds-foodinho-labour-breaches-idUSL5N2OJ227\nhttps://techcrunch.com/2021/07/06/italys-dpa-fines-glovo-owned-foodinho-3m-orders-changes-to-algorithmic-management-of-riders/\nhttps://www.pymnts.com/news/regulation/2021/italian-data-protection-watchdog-fines-foodinho-over-gig-algorithms/\nhttps://www.siliconrepublic.com/start-ups/glovo-italy-fine-algorithms\nhttps://www.complianceweek.com/regulatory-enforcement/italian-dpa-cites-biased-tech-in-31m-gdpr-fine/30557.article\nRelated \ud83c\udf10\nDoordash tip witholding\nUber Real-time ID Check racial bias\nPage info\nType: Incident\nPublished: January 2022\nLast updated: December 2023", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/alfi-personalised-real-time-advertising", "content": "Alfi facial recognition advertising system criticised as intrusive\nOccurred: 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \nA plan by digital marketing company Alfi to run digital tablets equipped with facial recognition in 10,000 Uber and Lyft cabs in the US was sharply criticised as overly intrusive.\nAlfi uses AI and computer vision to understand facial cues and perceptual details and matches relevant advertising or content based on the viewer\u2019s profile. The system analyses passengers' reactions to advertising and other content, sending information back to advertisers.\nThe firm said its software was designed to show ads to people based on their age, gender and ethnicity without specifically identifying any individual. It also claimed it was compliant with relevant privacy regulations and did not collect personal information. \nHowever, critics argued that the system\u2019s ability to analyse \u201csmall facial cues\u201d could potentially be used to collect more detailed personal information. According to Bloomberg, Alfi also planned to sell user data, including retina tracking, keyword recognition, voice intonation, and demographics.\n\u2795 The move prompted a written request (pdf) for information from US senators Amy Klobuchar and Richard Blumenthal, who claimed the programme raises 'serious concerns about privacy for your passengers.' \nSystem \ud83e\udd16\nAlfi website\nDocuments \ud83d\udcc3\nAlfi 2021 investor presentation\nALFI Installing 10,000 Digital Screens in Ubers and Lyfts Starting in Miami\nAlfi Engages Miami-Based Fulfillment and Distribution Center to Rollout 10,000 Uber and Lyft Digital Tablets Nationwide\nOperator: Uber; Lyft\nDeveloper: Alfi\nCountry: USA\nSector: Transport/logistics\nPurpose: Sell advertising\nTechnology: Computer vision; Facial detection\nIssue: Privacy\nTransparency: Governance; Privacy\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nAmy Klobuchar (2021). Sen. Klobuchar questions privacy of tablets in Ubers and Lyfts\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.vice.com/en/article/5dbyvz/this-company-is-putting-face-tracking-ad-tablets-in-the-back-of-ubers\nhttps://www.vice.com/en/article/epnawa/senators-send-letters-to-uber-and-lyft-over-face-tracking-ad-tablets\nhttps://www.biometricupdate.com/202107/u-s-pols-probe-uber-lyft-about-facial-recognition-tech-facing-riders\nhttps://www.biometricupdate.com/202106/watching-the-ride-hailing-watchers-with-computer-vision-tablets\nhttps://www.dailymail.co.uk/sciencetech/article-9721763/Uber-Lyft-drivers-add-10-000-face-tracking-tablets-cars-gauges-riders-reactions.html\nhttps://americanmilitarynews.com/2021/06/uber-to-track-riders-faces-in-back-of-car-with-tablet-cameras/\nhttps://www.bloomberg.com/news/articles/2021-09-01/meme-stock-alfi-s-facial-recognition-ad-technology-fans-privacy-concerns\nhttps://www.bnnbloomberg.ca/meme-stock-alfi-s-facial-recognition-ad-technology-fans-privacy-concerns-1.1646738\nRelated \ud83c\udf10\nSao Paulo METRO advertising facial biometrics\nMoviePass PreShow eye tracking\nPage info\nType: Issue\nPublished: October 2021\nLast updated: January 2023", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/canon-smile-recognition-cameras", "content": "Canon smile recognition system criticised as intrusive, manipulative\nOccurred: June 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe introduction of a 'smile recognition' system by Japanese technology company Canon raised complaints of inappropriate surveillance and the manipulation of employee emotions.\nCanon introduced Ai-powered cameras is using to ensure that only happy employees were permitted to enter or book conference rooms in its corporate offices in China. The technology is part of an \u201cintelligent IT solution\u201d for corporate offices that includes 5 different functional modules, one of which is \"smiley face access control\".\nCanon defended the technology, saying that it was  designed to promote a positive atmosphere, and that once people got used to smiling in the office, they would continue to do so, creating a positive and lively atmosphere.\nHowever, some workers expressed concerns about the intrusiveness of the technology, and felt Canon was manipulating their emotions.\nThe incident raised concerns about the increasingly pervasive and intrusive nature of workplace technology in China and elsewhere, as well as about the use of AI to manipulate people's emotions. \nSystem \ud83e\udd16\nCanon Creative Space\nDocuments \ud83d\udcc3\nCanon. \u4f73\u80fd\u53d1\u5e03\u201c\u4f73\u521b\u7a7a\u95f4\u201d\u667a\u80fdIT\u89e3\u51b3\u65b9\u6848\uff1a\u201c\u7b11\u8138\u201d\u52a9\u529b\u6253\u9020\u667a\u80fd\u751f\u6001 \nOperator: Canon\nDeveloper: Canon\nCountry: China\nSector: Technology\nPurpose: Encourage workplace productivity\nTechnology: Computer vision; Smile recognition\nIssue: Surveillance\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://petapixel.com/2021/06/17/canon-uses-ai-cameras-that-only-let-smiling-workers-inside-offices/\nhttps://www.theverge.com/2021/6/17/22538160/ai-camera-smile-recognition-office-workers-china-canon\nhttps://www.businessinsider.com/workers-at-chinese-office-have-to-smile-at-ai-camera-2021-6\nhttps://www.digitalcameraworld.com/news/smile-for-the-camera-or-you-cant-get-into-the-canon-offices\nhttps://metro.co.uk/2021/06/18/office-smile-recognition-cameras-only-allow-entry-to-happy-workers-14789445/\nhttps://www.dailymail.co.uk/news/article-9708071/Chinese-staff-Canon-enter-offices-SMILING-AI-cameras-installed.html\nhttps://cio.economictimes.indiatimes.com/news/corporate-news/canon-develops-smile-recognition-technology-for-employees/83631652\nhttps://www.ft.com/content/b74b6ad6-3b8d-4cd8-9dd6-3b49754aa1c7\nRelated \ud83c\udf10\nMoodbeam emotional tracking\nSpotify emotion recognition\nPage info\nPublished: October 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/witcher-3-ai-voice-line-simulation", "content": "Game actor's voice simulated without consent for Witcher 3\nOccurred: April 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe voice of video game actor was recreated without his permission in a mod for The Witcher 3, prompting accusations of copyright abuse and prompting concerns about the use of AI voice technologies to replace jobs.\nGamesRadar+ reported that actor Doug Cockle's voice, which was used for the principal character Geralt, had been recreated in a mod (aka modification) for The Witcher 3 called A Night to Remember, thereby allowing the mod team to generate new dialogue in addition to reusing and remixing existing voice lines.  \nIt transpired that Cockle's voice had been recreated using Russian company Mind Simulation Lab's CyberVoice (since renamed SteosVoice) speech synthesis technology without his explicit permission, prompting accusations of copyright abuse. \nMind Simulation Lab told Input that Cockle's parody audio would not be accessible for commercial purposes unless the actor joined the CyberVoice platform. However, the Russian company's claim was undermined by A Night to Remember modder nikich340 saying he had asked for specific voice lines from the lab, which Mind Simulation Lab had provided. \nSystem \ud83e\udd16\nCyberVoice/SteosVoice website\nOperator: Anonymous/pseudonymous; CD Projekt Red\nDeveloper: Anonymous/pseudonymous; Mind Simulation Lab\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Simulate voice dialogue\nTechnology: Voice synthesis\nIssue: Copyright; Employment - jobs\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.gamesradar.com/witcher-3-mod-uses-ai-to-create-new-voice-lines-without-geralts-original-voice-actor/\nhttps://www.inputmag.com/gaming/video-game-voice-ai-human-actors-witcher-3-mod-controversy\nhttps://www.ibtimes.com/witcher-3-story-mod-stirs-controversy-over-ai-generated-voice-acting-3237250\nhttps://www.vice.com/en/article/3aq8gn/this-witcher-3-mod-got-geralt-to-read-new-lines-without-the-voice-actor\nhttps://gamerant.com/the-witcher-3-mod-ai-voices/\nhttps://www.kotaku.com.au/2021/04/witcher-3-fan-builds-a-new-quest-with-perfect-geralt-voice-acting/\nhttps://www.reddit.com/r/Games/comments/o8ny5f/witcher_3_story_mod_trailer_for_a_night_to/\nRelated \ud83c\udf10\nElite Dangerous AI spaceships create superweapons\nKim Kwang-Seok voice recreation\nPage info\nType: Incident\nPublished: June 2021\nLast updated: October 2023", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-flex-algorithm-delivery-driver-firings", "content": "Amazon Flex delivery drivers fired by algorithm\nOccurred: June 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nMillions of independent contract drivers working for Amazon's Flex delivery service were managed and fired by an algorithm for no good cause and with little or no human intervention. \nAccording to a Bloomberg report, some dismissals were considered unfair, with riders given only 10 days to appeal their terminations. If their appeal failed, they then had to pay USD 200 for it to go to arbitration. \nTo make matters even more challenging, riders were unable to have access to the algorithm and its decision-making processes, persuading many drivers not to lodge appeals.\nThe programme was reportedly considered a success by Amazon. \n\u2795 Flex drivers complained in online forums about having their accounts terminated because their selfies failed to 'meet the requirements for the Amazon Flex program', according to Ars Technica.\nSystem \ud83e\udd16\nAmazon Flex website\nOperator: Amazon\nDeveloper: Amazon\nCountry: USA\nSector: Transport/logistics\nPurpose: Increase efficiency\nTechnology: Automated management system; Image recognition\nIssue: Fairness; Employment - pay, termination\nTransparency: Governance; Black box; Complaints/appeals \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bloomberg.com/news/features/2021-06-28/fired-by-bot-amazon-turns-to-machine-managers-and-workers-are-losing-out\nhttps://arstechnica.com/tech-policy/2021/06/amazon-is-firing-flex-workers-using-algorithms-with-little-human-intervention/\nhttps://metro.co.uk/2021/06/29/fired-by-a-machine-amazon-drivers-say-they-get-sacked-by-algorithms-14843853/\nhttps://news.yahoo.com/amazon-algorithms-fire-flex-delivery-drivers-055959081.html\nhttps://www.businessinsider.com/amazon-driver-nearly-lost-house-when-an-algorithm-fired-her-2021-6\nhttps://www.engadget.com/amazon-algorithms-fire-flex-delivery-drivers-055959081.html\nhttps://www.forbes.com/sites/enriquedans/2021/06/29/inside-the-dystopia-that-isamazon\nhttps://www.techtimes.com/articles/262142/20210629/amazon-allegedly-uses-algorithm-fire-contractual-flex-delivery-employees-report.htm\nhttps://www.nytimes.com/interactive/2021/06/15/us/amazon-workers.html\nRelated \ud83c\udf10\nAmazon Flex delivery driver routing safety\nAmazon Mentor DSP delivery driver scoring\nPage info\nType: Incident\nPublished: January 2022", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/beijing-uyghur-fake-influence-campaign", "content": "Beijing runs fake influence campaign saying life in Xinjiang is happy\nOccurred: June 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe Chinese government ran an elaborate, covert automated influence campaign seeking to persuade people that life in Xinjiang is peaceful and happy.\nMostly in Mandarin or Uyghur, the campaign took the form of thousands of videos, featuring Uyghurs and other predominantly Muslim ethnic minorities from across Xinjiang denying abuses against their own communities and criticising foreign officials and multinational corporations who had questioned China\u2019s human rights record in Xinjiang.\nMany of these videos followed the same or similar scripts, and often contained identical phrases, according to a ProPublica/New York Times investigation. The videos were run on Chinese media and social media sites, including Pomegranate Cloud, which is owned by the official Communist Party newspaper People\u2019s Daily.\nThe clips were also distributed on Twitter and YouTube, in some cases with English language captions. The clips did not carry logos and were not labelled as official government communication or propaganda.\nSystem \ud83e\udd16\nUnknown\nDocuments \ud83d\udcc3\nhttps://www.youtube.com/channel/UC6zIQzTpONQAe_qjTW7SAhg/videos\nOperator: Government of China; Pomegranate Cloud/People's Daily; Global Times\nDeveloper: Government of China\nCountry: China\nSector: Govt - home/interior; Govt - foreign; Govt - security\nPurpose: Scare/confuse/destabilise\nTechnology: Intelligent agents/bots; Social media\nIssue: Mis/disinformation\nTransparency: Governance\nInvestigations, assessments, audits \ud83e\uddd0\nhttps://www.propublica.org/article/how-china-uses-youtube-and-twitter-to-spread-its-propaganda-version-of-life-for-uyghurs-in-xinjiang\nhttps://www.nytimes.com/interactive/2021/06/22/technology/xinjiang-uyghurs-china-propaganda.html\nhttps://www.nytimes.com/zh-hans/interactive/2021/06/22/technology/xinjiang-uyghurs-china-propaganda-chinese.html\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://dailycaller.com/2021/06/23/china-xinjiang-uyghurs-propaganda-videos/\nhttps://thewire.in/communalism/how-china-spreads-its-propaganda-version-of-life-for-uighurs\nhttps://asiatimes.com/2021/06/how-china-spreads-propaganda-version-of-uighur-life/\nhttps://www.technologyreview.com/2021/06/24/1027048/youtube-xinjiang-censorship-human-rights-atajurt/\nhttps://chinadigitaltimes.net/2021/06/china-uses-global-influence-campaign-to-deny-forced-labor-mass-incarceration-in-xinjiang/\nhttps://sinocism.com/p/apple-daily-closes-zhang-weiwei-on\nRelated \ud83d\uddde\ufe0f\nBytedance/TikTok Uyghur censorship\nBeijing Uyghur emotion detection\nPage info\nType: Issue\nPublished: December 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tesla-autopilot-cruise-control-activation", "content": "Tesla China recalls cars over Autopilot Cruise Control activation\nOccurred: June 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nTesla recalled over 285,000 Model C and Model Y cars in China after an investigation found that drivers can easily activate their cruise control system by mistake. \nThe issue arose when drivers accidentally activated cruise control in the Model 3 and Y, which can result in unexpected acceleration. The recall primarily affected 211,000 Model 3 sedans built in Tesla\u2019s Shanghai facility, along with more than 35,000 imported Model 3s. \nAdditionally, approximately 39,000 Model Y crossovers, also manufactured in China, were included in the recall. Tesla owners in China received a free software update to address the issue. \nThe incident raised questions about Tesla's cruise control system, which is a key part of the company's Autopilot driver assistance function.\nSystem \ud83e\udd16\nTesla Autopilot, Full-self Driving\nOperator: Tesla\nDeveloper: Tesla\nCountry: China\nSector: Automotive\nPurpose: Control speed\nTechnology: Driver assistance system \nIssue: Safety; Accuracy/reliability\nTransparency: Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://techxplore.com/news/2021-06-tesla-recall-cars-china-due.html\nhttps://edition.cnn.com/2021/06/26/cars/tesla-recall-china-cruise-control/index.html\nhttps://sanfrancisco.cbslocal.com/2021/06/26/tesla-recall-285000-cars-china-over-flawed-cruise-control/\nhttps://www.theverge.com/2021/6/26/22551618/tesla-recalls-model-vehicles-china-cruise-control-safety\nhttps://electrek.co/2021/06/28/tesla-massive-recall-china-adding-a-chime-when-activating-cruise-control/\nhttps://apnews.com/article/china-technology-business-36936fe5efa25828ab0a79b70e9af2d1\nhttps://mashable.com/article/-tesla-recall-model-3-and-model-y-cruise-control\nhttps://uk.pcmag.com/cars-auto/134152/tesla-recalls-285000-vehicles-due-to-cruise-control-issues\nhttps://www.msn.com/en-us/money/companies/tesla-faces-e2-80-98black-eye-moment-e2-80-99-over-china-recall-says-bullish-analyst/ar-AALxjLD\nRelated \ud83c\udf10\nTesla Model 3 hits parked police car\nTesla Model X crashes into five police officers\nPage info\nType: Incident\nPublished: June 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tiktok-beheading-video-splicing", "content": "TikTok fails to stop beheading video going viral\nOccurred: June 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nFootage of a girl dancing spliced with a highly graphic and disturbing video of a man being beheaded by a group of men in a bathroom went viral on TikTok.\nPer Newsweek, the video had previously been present on gore sites for two years, with reports at the time of its initial discovery in 2019 alleging the victim was a 19-year-old Mexican man. \nDigital and human rights advocates argued TikTok's failure to detect and remove the video before it spread across its platform demonstrated the limitations of it's mostly automated content moderation system. \nTikTok said they 'quickly' removed the offending item. However, TikTok users continued to say they were too scared to use the app lest they saw the beheading. \nSystem \ud83e\udd16\nTikTok For You recommendation algorithm\nTikTok content moderation system\nOperator: ByteDance/TikTok\nDeveloper: ByteDance/TikTok\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Moderate content\nTechnology: Content moderation system\nIssue: Safety\nTransparency: Governance; Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.dailymail.co.uk/news/article-9669245/TikTok-apologizes-beheading-clip-tricks-AI-server-posing-dance-video-goes-viral.html\nhttps://www.insider.com/tiktok-beheading-video-removing-from-platform-2021-6\nhttps://www.newsweek.com/tiktok-graphic-beheading-video-company-response-1598107\nhttps://news.yahoo.com/gory-beheading-video-inserted-notorious-145356485.html\nhttps://www.dailydot.com/unclick/tiktok-beheading-little-girl-viral-video/\nhttps://metro.co.uk/2021/06/10/tiktok-video-showed-man-being-beheaded-in-middle-of-teenagers-dance-routine-14747410/\nhttps://www.thedailybeast.com/tiktok-scrambles-to-stop-suicide-video-from-spreading-on-app\nRelated \ud83c\udf10\nTikTok USA children content recommendations\nTikTok creators hate speech detection racial bias\nPage info\nType: Incident\nPublished: November 2021\nLast updated: January 2023", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/caliburger-flippy-robot", "content": "CaliBurger Flippy robot fired after one day\nOccurred: March 2018\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA robot burger-flipper at Caliburger, Pasadena, was put on extended leave after working only a single shift having been unable to cope with a surge of orders.\nHaving made a big noise about its Flippy robot that it described as 'the future of food', manufacturer Miso Robotics was forced to pull its technology after it quickly exceeded its 2,000-burger-a-day capacity. \nAccording to CaliBurger CTO Anthony Lomelino, 'human staff' were part of the problem. 'Working with people, you talk to each other. With Flippy, you kind of need to work around his schedule' he says.\nThe incident raised questions about the robustness of the robot. It also stimulated discussion about the purpose of robots and their impact on jobs, with some commentators reckoning many jobs will disappear and income equalities widen.\n\u2796 In an October 2022 interview with Reuters, Miso Robotics CEO Mike Bell said that some day, people will 'walk into a restaurant and look at a robot and say, 'Hey, remember the old days when humans used to do that kind of thing?\u2019 'And those days ... it's coming. ... It's just a matter of ... how quick.'\nSystem \ud83e\udd16\nFlippy robot\nDocuments \ud83d\udcc3\nhttps://www.businesswire.com/news/home/20180305005300/en/Flippy-World%E2%80%99s-Autonomous-Robotic-Kitchen-Assistant-Cooks\nhttps://www.prnewswire.com/news-releases/miso-robotics-unveils-flippy-in-caliburger-kitchen-plans-worldwide-rollout-300419426.html\nOperator: Cali Group\nDeveloper: Miso Robotics\nCountry: USA\nSector: Travel/hospitality\nPurpose: Flip burgers\nTechnology: Robotics\nIssue: Robustness; Employment - jobs\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://eu.usatoday.com/story/tech/talkingtech/2018/03/05/flippy-robot-now-cooking-up-burgers-near-l/390179002/\nhttps://eu.usatoday.com/story/tech/talkingtech/2018/03/07/flippy-burger-flipping-robot-break-already/405580002/ \nhttps://eu.usatoday.com/story/tech/talkingtech/2018/05/28/hamburger-making-robot-flippy-back-serving-300-burgers-day/649370002/\nhttps://www.theverge.com/2018/3/8/17095730/robot-burger-flipping-fast-food-caliburger-miso-robotics-flippy\nhttps://www.livescience.com/61994-flippy-burger-flipping-robot-flops.html\nhttps://www.grubstreet.com/2018/03/flippy-robot-one-day-on-the-job.html\nhttps://www.reuters.com/technology/want-fries-with-that-robot-makes-french-fries-faster-better-than-humans-do-2022-10-04/\nhttps://futurism.com/the-byte/ceo-brags-that-fry-cook-robot-will-replace-obsolete-human-grunts\nRelated \ud83c\udf10\nAmazon Astro home robot\nTesla Optimus robot\nPage info\nType: Incident\nPublished: October 2022", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/instacart-gig-shopper-robotisation", "content": "Instacart plans to replace its gig shoppers with robots\nOccurred: June 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nLeaked documents revealed that US gig shopping company Instacart planned to replace its army of gig personal shoppers with robots, raising concerns about job losses at the company and more widely.\nDocuments shared with Bloomberg revealed the company was looking to build a series of large-scale, automated fulfillment centers across the country in which robots would fetch items such as cereal boxes, and humans collect fresh produce and deli products. \nAccording to the documents, larger facilities would process orders for several locations while others would be attached to existing supermarkets and grocery stores.\nInstacart uses hundreds of thousands of gig workers to pick up and deliver orders to customers at their homes and offices. Automation is envisaged to cut costs, increase delivery times, and attract more customers. \nAnalysts believe the plan may help reduce the likelihood that Instacart will be squeezed out by its grocery partners.\n\u2796 In early 2019, Instacart changed its pay algorithm after personal shoppers boycotted the company claiming it lowered their pay and customers complained on social media their orders were being delayed.\nSystem \ud83e\udd16\nInstacart website \nInstacart Wikipedia profile\nOperator: Instacart\nDeveloper: Instacart\nCountry: USA\nSector: Transport/logistics\nPurpose: Increase efficiency\nTechnology: Robotics\nIssue: Employment - jobs; Ethics/values\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bloomberg.com/news/articles/2021-06-01/instacart-looks-to-use-robots-over-people-to-do-grocery-shopping\nhttps://www.dailymail.co.uk/sciencetech/article-9640895/Instacart-plans-replace-gig-shoppers-hundreds-ROBOTS-bid-slash-costs.html\nhttps://www.foxbusiness.com/lifestyle/instacart-enlisting-robots-to-cut-labor-costs\nhttps://nypost.com/2021/06/01/instacart-has-a-plan-to-use-robots-instead-of-shoppers/\nhttps://www.digitalcommerce360.com/2021/06/08/instacart-wants-to-replace-army-of-gig-shoppers-with-robots/\nhttps://www.msn.com/en-us/news/politics/instacart-eyes-robots-to-replace-many-gig-shoppers/ar-AAKBhO8\nhttps://www.ft.com/content/364a0f74-f016-4862-9cc3-a7be58a10772\nhttps://www.msn.com/en-us/news/technology/instacart-e2-80-99s-reported-plan-to-automate-its-workforce-seems-a-lot-like-bluster/ar-AAKBmiV\nhttps://www.pymnts.com/pymnts-post/news/delivery/2021/instacart-seeks-partners-for-automated-fulfillment-centers/\nRelated \ud83c\udf10\nCaliBurger Flippy robot\nDenny's robot server\nPage info\nType: Issue\nPublished: December 2021\nLast updated: June 2023", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/aespa-virtual-k-pop", "content": "Aespa virtual members raise robot sexualisation concerns\nOccurred: June 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nNews that Korean K-pop band Aespa would include virtual members in its line-up prompted concerns about the dehumanisation and sexualisation of humanoid robots.\nKorean entertainment company SM Entertainment announced that Aespa, its latest K-pop girl group would include human and virtual members that would be able to 'interact and communicate as independent beings as they have AI brains'. \nThe announcement was met with a mixture of intigue and excitement, tempered by concerns about the 'ownership' of young girls by fans and the potential for their dehumanisation and sexualisation. \nOthers raised the possibility of Aespa avatars being manipulated for pornography and other unethical uses. \nSystem \ud83e\udd16\nAespa website\nAespa Wikipedia profile\nDocuments \ud83d\udcc3\nhttps://www.youtube.com/watch?v=WUSthuKPwlw\nhttps://www.smentertainment.com/PressCenter/Details/5068\nOperator: SM Entertainment\nDeveloper: SM Entertainment\nCountry: S Korea  \nSector: Media/entertainment/sports/arts\nPurpose: Create virtual avatars\nTechnology: Deepfake - image, video\nIssue: Anthropomorphism; Dual/multi-use; Safety\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.vice.com/en/article/bvxxgd/aespa-kpop-music-girl-group-virtual-sm \nhttps://www.scmp.com/magazines/style/tech-design/article/3136950/could-virtual-k-pop-girl-group-dethrone-blackpink-after \nhttps://www.scmp.com/lifestyle/k-pop/article/3138999/k-pops-virtual-future-aespa-eternity-rise-digital-performers-and-ai\nhttps://www.todayonline.com/world/new-k-pop-girl-group-aespas-virtual-members-cause-fears-over-dehumanisation-k-pop-stars\nhttps://www.allkpop.com/article/2020/11/will-the-avatars-of-sms-new-girl-group-aespa-be-legally-protected-from-deepfake-pornography-crimes\nhttps://www.cnbc.com/2021/01/11/future-of-entertainment-avatars-could-be-k-pops-next-superstars.html\nRelated \ud83c\udf10\nHorizon Worlds virtual groping\nSouth Korea presidential election candidate deepfakes\nPage info\nType: Issue\nPublished: January 2022", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/googlehca-healthcare-patient-data-sharing", "content": "Privacy advocates raise concerns about Google patient data deal with HCA Healthcare\nOccurred: May 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA deal struck between US hospital chain HCA Healthcare and Google that gave the latter access to to patient records at hundreds of hospitals and thousands of heathcare sites prompted concerns by privacy advocates and rights groups. \nThe stated aims of the deal were to increase HCA efficiencies and develop algorithms and tools to monitor patients and guide medical decision-making. \nHowever, similar to Google\u2019s Project Nightingale, privacy advocates raised concerns that sensitive patient data could be monetised or misused by the technology company, despite the data having been anonymised.\nConcerns were also expressed about the security of patient data, given a spate of high-profile ransomware attacks in the months leading up to the announcement of the deal. \nUS healthcare privacy laws permit hospitals to share information with suppliers, and allow researchers to analyse patient data without their permission. Healthcare companies can then use that information however they want.\nSystem \ud83e\udd16\n\nOperator: HCA Healthcare; Alphabet/Google\nDeveloper: Alphabet/Google\nCountry: USA\nSector: Health\nPurpose: Increase operating efficiency\nTechnology: Database\nIssue: Privacy; Security\nTransparency: Privacy\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.wsj.com/articles/google-strikes-deal-with-hospital-chain-to-develop-healthcare-algorithms-11622030401\nhttps://www.dailymail.co.uk/news/article-9621675/Google-32-million-patient-records-new-partnership.html\nhttps://www.cnbc.com/2021/05/26/privacy-laws-need-updating-after-google-deal-with-hca-healthcare-medical-ethics-professor-says.html\nhttps://www.theverge.com/2021/5/26/22454817/google-hca-patient-data-healthcare-algorithms\nhttps://www.msn.com/en-us/news/technology/google-strikes-deal-with-hospital-chain-to-use-patient-data-for-healthcare-algorithms/ar-AAKpO9t\nhttps://www.marketwatch.com/story/google-strikes-deal-with-hca-healthcare-to-mine-patient-records-to-develop-algorithms-11622062563\nhttps://www.beckershospitalreview.com/healthcare-information-technology/viewpoint-google-hca-deal-sparks-need-for-update-in-privacy-laws.html\nhttps://www.beckershospitalreview.com/digital-transformation/google-hca-partner-for-health-algorithms-7-things-to-know.html\nhttps://thefederalist.com/2021/05/26/googles-latest-privacy-invasion-partnership-gives-it-access-to-32-million-patient-health-care-records/\n\nRelated \ud83c\udf10\nGoogle DeepMind, Royal Free data sharing\nNHS Digital/iProov facial recognition data sharing\nPage info\nType: Issue\nPublished: December 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tencent-app-link-blocking", "content": "Bytedance accuses Tencent of automated link blocking\nOccurred: March 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nTikTok owner Bytedance accused Chinese technology company Tencent of blocking links to its products, prompting questions about Tencent's governance and its compliance with competition law.\nIn a lengthy online post (in Mandarin), Bytedance said Tencent had been blocking traffic from its mobile social services WeChat and QQ to its own short-form video applications Douyin, Huoshan, and Xigua., for three years, \u201caffecting more than 1 billion users.\u201d \nThe tussle between the two Chinese tech companies came amidst increased scrutiny of anti-competitive behaviour in the technology industry by the Chinese government. \nBytedance is not the only company in Tencent's sights; the technology company has also been accused of prohibiting users from opening links to Alibaba\u2019s Taobao and Tmall online marketplaces. Bytedance said that Tencent's practice of blocking links to competitor sites has been going on for years. \n\u2796 February 2021. ByteDance\u2019s Douyin sued Tencent alleging that Tencent\u2019s WeChat and QQ messaging apps ban their users from sharing content from Douyin for three years.\n\u2795 September 2021. China's industry ministry instructed Alibaba, Tencent, Huawei and other large internet companies to stop blocking links to each other's sites, and threatened to take action against those that failed to heed its warning. \nSystem \ud83e\udd16\nUnknown\n\nDocuments \ud83d\udcc3\nBytedance. \u56de\u5e94\u201c\u732a\u98df\u8bba\u201d\uff0c\u5b57\u8282\u8df3\u52a8\u5fae\u4fe1\u53f7\u53d1\u5e03\u906d\u9047\u817e\u8baf\u5c4f\u853d\u548c\u5c01\u7981\u5927\u4e8b\u8bb0 (2018-2021)\nOperator: Tencent\nDeveloper: Tencent\nCountry: China\nSector: Technology\nPurpose: Block web traffic\nTechnology: Link blocking\nIssue: Monopolisation\nTransparency: Governance; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.scmp.com/tech/big-tech/article/3116834/bytedance-accuses-tencent-blocking-its-work-home-tool-feishu-wechat\nhttps://technode.com/2021/06/08/bytedance-rages-against-tencent-over-link-blocking-heres-why/\nhttps://technode.com/2021/03/24/china-probes-tencent-for-antitrust-practices-report/\nhttps://www.reuters.com/article/us-china-tech-antitrust-tencent-exclusiv/exclusive-tencent-boss-meets-china-antitrust-officials-as-scrutiny-widens-sources-idUSKBN2BG09L\nhttps://asia.nikkei.com/Business/China-tech/China-tells-Tencent-Alibaba-and-peers-to-stop-blocking-each-other-s-links\nhttps://www.globaltimes.cn/page/202109/1234053.shtml\nRelated \ud83c\udf10\nTencent 'Midnight Patrol' facial recognition\nMicrosoft/Bing Tiananmen Square Tank Man\nPage info\nType: Incident\nPublished: January 2022", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/rcmp-ai-facial-recognition-surveillance", "content": "RCMP violates Canadians' privacy using Clearview AI facial recognition\nOccurred: June 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe Royal Canadian Mounted Police (RCMP) was found guilty of breaking the law by using facial recognition to collect the personal information of Canadians without their knowledge or consent.\nCanada's privacy commissioner Daniel Therrien said the RCMP was guilty of 'serious and systemic failings' in its information gathering operations using controversial facial monitoring company Clearview AI. \nClearview AI uses artificial intelligence to match people\u2019s images against a database of billions of photos scraped from the internet, including social media sites. \nSimilar to other police forces across Canada, the RCMP had previously publicly stated it was not using Clearview AI, only to later confirm that officers had used trial versions of the system without the knowledge or authorisation of police leadership. \nThe incident raised questions about the RCMP's governance, honesty, transparency and accountability, as well as about the extent and scope of Clearview AI's operations. \n\u2796 In July 2020, Clearview AI announced it would pull out of Canada and stop working with the RCMP during the watchdog's investigation into whether Clearview AI technology broke Canadian privacy law.\nSystem \ud83e\udd16\nClearview AI website\nClearview AI Wikipedia profile\nOperator: Royal Canadian Mounted Police (RCMP)\nDeveloper: Clearview AI\nCountry: Canada\nSector: Govt - police\nPurpose: Strengthen law enforcement\nTechnology: Facial recognition\nIssue: Privacy; Surveillance\nTransparency: Governance; Privacy; Marketing\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nOffice of the Privacy Commissioner of Canada (2022). Letter to the Standing Committee on Access to Information, Privacy and Ethics on their Study of the Use and Impact of Facial Recognition Technology\nOffice of the Privacy Commissioner of Canada (2021). RCMP\u2019s use of Clearview AI\u2019s facial recognition technology violated Privacy Act, investigation concludes\nOffice of the Privacy Commissioner of Canada (2020). Clearview AI ceases offering its facial recognition technology in Canada\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.ctvnews.ca/mobile/politics/privacy-watchdog-says-rcmp-s-use-of-facial-recognition-tool-broke-law-1.5464442\nhttps://www.cbc.ca/news/politics/rcmp-clearview-ai-1.6060228\nhttps://globalnews.ca/news/7937654/rcmp-clearview-ai-facial-recognition-canada/\nhttps://www.huffingtonpost.ca/entry/clearview-ai-rcmp-role_ca_6022e414c5b6d78d444a78ae\nhttps://www.thestar.com/news/canada/2020/07/06/clearview-ai-to-pull-out-of-canada-and-stop-working-with-rcmp-amid-privacy-investigation.html\nhttps://www.thestar.com/news/gta/2020/02/13/toronto-police-used-clearview-ai-an-incredibly-controversial-facial-recognition-tool.html\nhttps://www.thestar.com/politics/federal/2021/06/10/rcmp-broke-privacy-laws-in-using-controversial-clearview-ai-facial-recognition-tools-watchdog-says.html\nhttps://www.thestar.com/news/canada/2020/07/06/clearview-ai-to-pull-out-of-canada-and-stop-working-with-rcmp-amid-privacy-investigation.html\nhttps://thetyee.ca/News/2020/01/22/RCMP-Wont-Say-Massive-Facial-Recognition-Database/\nRelated \ud83c\udf10\nRCMP BC facial recognition procurement opacity\nPimEyes facial recognition search engine\nPage info\nType: Incident\nPublished: October 2021\nLast updated: January 2023", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/epic-systems-epic-deterioration-index", "content": "Researchers highlight Epic Deterioration Index lack of transparency \nOccurred: May 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe EPIC Deterioration Index, which helps identify when to move a patient in or out of intensive care, was only moderately successful in differentiating low-risk and high-risk patients, according to researchers.\nPhysicians Vishal Khetpal, MD, and Nishant Shah, MD concluded that a research study involving 392 Covid-19 patients found that the Deterioration Index was moderately successful in discriminating between low-risk patients and those at high risk of ICU transfer, ventilation, or death.\nRolled out at speed during the COVID-19 pandemic, the Epic Deterioration Index (EDI) is a propritary machine learning algorithm developed by private electronic health record company Epic Systems that helps identify when to move a patient in or out of intensive care. Epic is said to hold over 250 million patients' electronic records in the US, and has been taken up by hundreds of hospitals across the US. \nHowever, despite general concerns about medical system accuracy and bias, Epic chose to limit expert access to raw data and equations and failed to have the system independently validated or peer-reviewed before launch, the reseachers said.\nThe finding raised questions about Epic System's governance and commitment to transparency and accountability, as well as about the EDI's accuracy and reliability. \nSystem \ud83e\udd16\nEPIC Deterioration Index\nDocuments \ud83d\udcc3\nEPIC. Epic AI Helps Clinicians Predict When COVID-19 Patients Might Need Intensive Care\nOperator: Parkview Health; University of Michigan; Multiple\nDeveloper: Epic Systems Corporation\nCountry: USA\nSector: Healthcare\nPurpose: Predict patient outcomes\nTechnology: Machine learning\nIssue: Accuracy/reliability; Bias/discrimination - race, gender\nTransparency: Governance; Black box\nResearch, advocacy \ud83e\uddee\nSingh K. et al. Evaluating a Widely Implemented Proprietary Deterioration Index Model Among Hospitalized COVID-19 Patients\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://undark.org/2021/05/27/health-care-algorithm-promise-peril/\nhttps://www.fastcompany.com/90641343/epic-deterioration-index-algorithm-pandemic-concerns\nhttps://www.healthcareittoday.com/2020/04/29/hospitals-testing-out-epic-ai-tool-to-predict-covid-19-patients-progress/\nhttps://www.healthcareitnews.com/ai-powered-healthcare/michigan-researchers-turn-ai-help-hospitals-manage-beds\nhttps://www.beckershospitalreview.com/artificial-intelligence/physician-viewpoint-how-a-largely-untested-ai-algorithm-made-its-way-into-hundreds-of-hospitals.html\nhttps://www.pulmonologyadvisor.com/home/topics/lung-infection/epic-deterioration-index-may-identify-high-and-low-risk-covid-19-patients/\nhttps://www.statnews.com/2020/04/24/coronavirus-hospitals-use-ai-to-predict-patient-decline-before-knowing-it-works/\nhttps://diginomica.com/how-did-proprietary-ai-get-hundreds-hospitals-without-extensive-peer-reviews-concerning-story-epics\nRelated \ud83c\udf10\nEpic Systems sepsis prediction algorithm\nPage info\nType: Incident \nPublished: December 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/cpb-one-asylum-seeker-app-privacy", "content": "US CPB covertly uses facial recognition to process asylum seekers\nOccurred: June 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe US government was discovered to be covertly using facial recognition for immigration, sparking concerns about privacy and surveillance. \nAccording to the Los Angeles Times, the US government's Customs and Border Protection (CPB) is using CPB One, a mobile app incorporating facial recognition and a range of other surveillance and storage tools to process asylum seekers at the US/Mexico border. \nCivil and human rights experts are concerned about the privacy and surveillance implications of the system. \nThe CPB also stood accused of poor transparency by failing to mention it would be using the app to process asylum seekers and that facial recognition is involved.\nSystem \ud83e\udd16\nCPB One\nOperator: Customs and Border Protection (CPB)\nDeveloper: Customs and Border Protection (CPB)\nCountry: USA\nSector: Govt - immigration\nPurpose: Manage migration\nTechnology: Facial recognition\nIssue: Dual/multi-use; Privacy\nTransparency: Governance; Privacy\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.latimes.com/politics/story/2021-06-04/asylum-bidens-got-an-app-for-that-with-privacy-risks-and-surveillance-beyond-border\nhttps://thenewamerican.com/report-biden-administration-uses-face-recognition-app-to-track-asylum-seekers/\nhttps://www.dailymail.co.uk/news/article-9655025/Biden-deploys-controversial-facial-recognition-app-track-asylum-seekers.html\nhttps://theweek.com/immigration/1001197/the-biden-administrations-new-facial-recognition-app-for-asylum-seekers-sets\nhttps://www.axios.com/border-asylum-seekers-facial-recognition-app-972608b1-1c38-4115-b7a6-b8ddf48fd50c.html\nhttps://iapp.org/news/a/cbps-asylum-seekers-app-brings-privacy-concerns/\nhttps://www.biometricupdate.com/202106/cbp-one-app-deployed-for-asylum-seekers-biometric-surveillance-concerns-raised\nhttps://nypost.com/2021/06/07/biden-admin-starts-using-controversial-facial-recognition-tool-on-migrants-report/\nRelated \ud83c\udf10\nSouth Korea immigration facial recognition data sharing\nNew Zealand immigration overstayer predictions\nPage info\nType: Incident\nPublished: January 2022\nLast updated: November 2023", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/microsoft-bing-tiananmen-square-tank-man", "content": "Microsoft accused of censoring Tiananmen Square 'tank man'\nOccurred: June 1989\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nMicrosoft came under fire for allegedly stopping users in some countries from viewing China's 'tank man'.\nUsers of the technology company's Bing search engine in the UK, US, Singapore and other countries were unable to view image search results for the query 'tank man'.\nThe phrase describes the lone protester standing before tanks during China's Tiananmen Square demonstrations in 1989, and the block coincided with  the 32nd anniversary of the protests. \nThe incident prompted accusations of censorship; Microsoft blamed the omission on 'accidental human error'.\nSystem \ud83e\udd16\nBing Search\nOperator: Microsoft\nDeveloper: Microsoft\nCountry: China\nSector: Technology\nPurpose: Rank content/search results\nTechnology: Search engine algorithm\nIssue: Accuracy/reliability\nTransparency: Governance; Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.vice.com/en/article/qj8v9m/bing-censors-tank-man\nhttps://www.bbc.co.uk/news/world-asia-57367100\nhttps://abcnews.go.com/Technology/wireStory/microsoft-tank-man-censorship-due-human-error-78105315\nhttps://www.theguardian.com/technology/2021/jun/04/microsoft-bing-tiananmen-tank-man-results\nhttps://news.sky.com/story/microsoft-blames-accidental-human-error-for-tank-man-censorship-on-tiananmen-square-anniversary-12325233\nhttps://www.nytimes.com/2021/06/05/business/bing-tank-man-microsoft.html\nhttps://www.theverge.com/2021/6/4/22519418/microsoft-bing-china-tank-man-tiananmen-square\nhttps://www.reuters.com/technology/microsoft-bing-raises-concerns-over-lack-image-results-tiananmen-tank-man-2021-06-04/\nhttps://gizmodo.com/microsoft-blames-human-error-amid-suspicion-it-censored-1847037545\nhttps://edition.cnn.com/2021/06/08/tech/mitt-romney-microsoft-tiananmen-square/index.html\nRelated \ud83c\udf10\nBytedance Uyghur censorship\nXiaomi 5G mobile communications tracking, censorship\nPage info\nType: Incident\nPublished: June 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/kargu-2-autonomous-drone-attack", "content": "Kargu-2 fully autonomous drone attacks Libyan armed forces\nOccurred: May 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nIn the first recorded use of an autonomous drone attack, fully autonomous military drones are throught likely to have attacked human targets in Libya during a conflict in March 2020.\nA United Nations report suggested Kargu-2 military drones are likely to have autonomously attacked Libya\u2019s Haftar Armed Forces during a conflict in March 2020. The UN report referred to the drone as a \"lethal autonomous weapon\".\nThe Kargu-2 attack quadcopter is manufactured by Turkish weapons company STM Defense Technologies and is designed for asymmetric warfare and anti-terrorist operations. The drone can be operated by a single soldier in both autonomous and manual modes, and has real-time image processing capabilities and machine learning algorithms embedded on the platform.\nThe incident sparked debate about the ethics of the usage of autonomous weapons, and the need for regulation. Human rights, non-governmental organisations, including the United Nations, and technology experts have been calling for a global ban on lethal autonomous weapons systems (aka 'slaughterbots').\n\u2795 Turkish-made Bayraktar TB2 drones were also used to deadly effect by Ukraine's air force during Russia's invasion of the eastern European country, and by the Ethiopian government against Tigray People\u2019s Liberation Front rebels.\nSystem \ud83e\udd16\nSTM Kargu website\nSTM Kargu Wikipedia profile\nDocuments \ud83d\udcc3\nUnited Nations Security Council. Letter dated 8 March 2021 from the Panel of Experts on Libya established pursuant to resolution 1973 (2011) addressed to the President of the Security Council\nOperator: Government of Libya\nDeveloper: STM\nCountry: Libya; Turkey  \nSector: Govt - defence\nPurpose: Kill/maim adversaries\nTechnology: Drone; Robotics\nIssue: Autonomous lethal weapons; Ethics\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://gizmodo.com/flying-killer-robot-hunted-down-a-human-target-without-1847001471\nhttps://www.newscientist.com/article/2278852-drones-may-have-attacked-humans-fully-autonomously-for-the-first-time/\nhttps://thebulletin.org/2021/05/was-a-flying-killer-robot-used-in-libya-quite-possibly/\nhttps://www.cnet.com/news/autonomous-drone-attacked-soldiers-in-libya-all-on-its-own/\nhttps://reason.com/2021/06/01/autonomous-slaughterbot-drones-reportedly-attack-libyans-using-facial-recognition-tech/\nhttps://www.defenseone.com/ideas/2021/06/libyas-uav-strike-should-galvanize-efforts-autonomous-weapons/174449/\nhttps://www.msn.com/en-us/news/world/for-the-first-time-drones-autonomously-attacked-humans-this-is-a-turning-point/ar-AAKBxMC\nhttps://www.reuters.com/article/apps-drones-idUSL5N2NS2E8\nhttps://spectrum.ieee.org/lethal-autonomous-weapons-exist-they-must-be-banned\nhttps://www.iflscience.com/technology/an-autonomous-weaponized-drone-hunted-down-humans-without-command-for-first-time/\nhttps://www.the-sun.com/news/2975746/terminator-style-ai-drone-hunted-down-human-targets/\nhttps://www.dailymail.co.uk/sciencetech/article-9629801/Fully-autonomous-drones-hunted-attacked-humans-time.html\nhttps://www.defenseone.com/ideas/2021/06/libyas-uav-strike-should-galvanize-efforts-autonomous-weapons/174449/\nhttps://www.forbes.com/sites/davidhambling/2020/06/17/turkish-military-to-receive-500-swarming-kamikaze-drones/\nhttps://www.theverge.com/2021/6/3/22462840/killer-robot-autonomous-drone-attack-libya-un-report-context\nRelated \ud83c\udf10\nUkraine-Russia Bayraktar TB2 drone attacks\nHouthi Abu Dhabi drone attack\nPage info\nType: Incident\nPublished: March 2022", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/applesis-misidentification-wrongful-arrest", "content": "Apple facial recognition system misidentifies 'shoplifter' Ousmane Bah\nOccurred: May 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nApple and security contractor Security Industry Specialists (SIS) were sued for wrongfully accusing New York teenager Ousmane Bah of shoplifting.\nBah had been accused of shoplifting multiple times at Apple stores across the US east coast in 2018 and 2019, leading to his arrest. Bah subsequently sued Apple and SIS for defamation and malicious prosection, citing the unreliable and racially biased nature of many facial recognition systems.  \nThe suit also accused one of the two companies of deleting video images of the purported crimes, despite them surfacing during discovery, and alleged that a senior SIS executive denied the company ever identified Bah to Apple or to the NYPD, even though an SIS email to the NYPD suggested to the contrary. \nThe suit also claimed that Apple and SIS selectively deleted video evidence that would have exposed them to potential criminal and civil liability for filing false complaints with the police.\nIn November 2021, Apple and Bah settled the suit on undisclosed terms.\nSystem \ud83e\udd16\nSecurity Industry Specialists website\nOperator: Apple; Security Industry Specialists (SIS)\nDeveloper: Security Industry Specialists (SIS)\nCountry: USA\nSector: Retail\nPurpose: Verify identity\nTechnology: Facial recognition\nIssue: Accuracy/reliability; Bias/discrimination - race\nTransparency: Governance; Marketing: Privacy; Legal \nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nOusmane Bah v Apple - Opinion\nOusmane Bah v Apple - Complaint (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.biometricupdate.com/202105/allegations-in-wrongful-arrest-suit-involving-face-biometrics-against-apple-contractor-get-worse\nhttps://appleinsider.com/articles/21/05/29/apple-sued-over-false-accusations-in-apple-store-thefts-by-impostor\nhttps://www.theregister.com/2021/05/29/apple_sis_lawsuit/\nhttps://www.scmp.com/news/world/united-states-canada/article/3007386/teenager-ousmane-bah-sues-apple-us1-billion-over\nhttps://www.cnet.com/news/teen-hits-apple-with-1b-lawsuit-over-facial-recognition-arrest/\nhttps://www.bloomberg.com/news/articles/2019-04-22/apple-face-recognition-blamed-by-new-york-teen-for-false-arrest\nhttps://www.biometricupdate.com/201904/apple-sued-as-teen-blames-facial-biometrics-for-alleged-false-arrest\nhttps://www.pcmag.com/news/teen-sues-apple-for-1b-over-false-arrest\nhttps://nypost.com/2019/04/22/apples-facial-recognition-software-led-to-false-arrest-suit/\nhttps://www.law360.com/articles/1440155/apple-settles-suit-by-ny-man-falsely-arrested-for-thefts\nRelated \ud83c\udf10\nCitizen OnAir misidentifies arson suspect\nGoogle misidentifies engineer as serial killer\nPage info\nType: Incident\nPublished: December 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/lemonade-non-verbal-assessments", "content": "Lemonade use of emotion recognition to assess insurance claims prompts backlash\nOccurred: May 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nNew York-based online insurer Lemonade retracted a series of tweets saying it uses artificial intelligence to assess and deny claims based on its customers facial and emotional characteristics.\nLemonade Inc had claimed on Twitter that its 'AI Jim' claims bot uses emotion recognition to automate the process of assessing and denying insurance claims based on 'non-verbal cues' in videos its customers must shoot to explain what had happened. \nThe claim triggered intense criticism of the company, with people accusing it of potential bias and of conducting phrenology and other forms of discredited pseudoscience. \nLemonade was forced to deny using AI or emotion recognition to 'automatically decline claims', going on to add that 'harmful concepts like phrenology and physiognomy has never, and will never, be used at Lemonade.'\nThe company had previously stated in a S-1 filing that its system collects approximately 1,700 customer data points from customers. It later said that this is 'about 100 times more data than traditional data carriers'. \n System \ud83e\udd16\nLemonade website\nLemonade Inc Wikipedia profile\nDocuments \ud83d\udcc3\nLemonade 'non-verbal cue' tweets\nLemonade. Lemonade\u2019s Claim Automation\nLemonade. S-1 SEC filing\nOperator: Lemonade Inc\nDeveloper: Lemonade Inc\nCountry: USA\nSector: Banking/financial services\nPurpose: Assess & process insurance claims\nTechnology: Facial recognition; Emotion recognition; Machine learning\nIssue: Accuracy/reliability; Bias/discrimination - multiple; Phrenology\nTransparency: Governance; Black box; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bloomberg.com/news/articles/2021-05-26/lemonade-s-use-of-ai-to-study-claims-brings-uproar-after-tweet\nhttps://fortune.com/2021/05/26/lemonade-insurance-ai-face-scanning-fraud/\nhttps://www.vox.com/recode/22455140/lemonade-insurance-ai-twitter\nhttps://www.forbes.com/sites/carlieporterfield/2021/05/26/insurance-unicorn-lemonade-backtracks-comments-about-its-ai-after-accusations-of-discrimination/\nhttps://www.theregister.com/2021/05/26/ai_insurance_lemonade/\nhttps://frankonfraud.com/fraud-trends/lemonade-under-fire-for-using-ai-to-stop-insurance-fraud/\nhttps://www.vice.com/en/article/z3x47y/an-insurance-startup-bragged-it-uses-ai-to-detect-fraud-it-didnt-go-well\ninputmag.com/culture/lemonade-swears-it-totally-isnt-using-ai-for-phrenology\nhttps://gizmodo.com/lemonade-jk-jk-we-dont-use-facial-recognition-to-rej-1846976751\nhttps://www.zdnet.com/article/lemonade-insurance-faces-backlash-for-claiming-ai-system-could-automatically-deny-claims/\nRelated \ud83c\udf10\nUpstart consumer lending racial discrimination\nUS mortgage approvals algorithm racial discrimination\nPage info\nType: Incident\nPublished: April 2022\nLast updated: October 2022", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/gpt-3-mimics-qanon", "content": "Large language models can mimic QAnon, researchers find\nOccurred: May 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAI large language systems such as OpenAI's GPT-3 can be used to create and deploy convincing fake news on social media, prompting concerns about their use for misinformation and disinformation.\nTesting whether these kinds of models can mimic the style of the QAnon conspiracy theory, researchers at Georgetown University's Center for Security and Emerging Technology (CSET) found that 'GPT-3 easily matches the style of QAnon' and 'creates its own narrative that fits within the conspiracy theory'. \nThe researchers went on to argue it will become increasingly difficult to distinguish reliable and fake news and information.\nWhile OpenAI has restricted access to GPT-3, the authors argue it is only a matter of time before open source versions of GPT-3 or its equivalents will emerge, making it it easy for governments and other bad actors to weaponise them for nefarious purposes.\nSystem \ud83e\udd16\nGPT-3 large language model\nOperator: OpenAI\nDeveloper: OpenAI\nCountry: Global\nSector: Multiple\nPurpose: Generate text\nTechnology: Large language model (LLM); NLP/text analysis; Neural network; Deep learning; Machine learning\nIssue: Mis/disinformation; Safety; Dual/multi-use\nTransparency: Governance; Black box\nResearch, advocacy \ud83e\uddee\nBuchanan B., Lohn A., Musser M., Sedova K. (2021). Truth, Lies, and Automation. How Language Models Could Change Disinformation\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.axios.com/gpt-3-disinformation-artificial-intelligence-c6ea11f7-b7eb-474d-b577-14731ffdbfa4.html\nhttps://thenextweb.com/news/gpt-3s-ability-to-write-disinformation-wildly-overstated-ai-media\nhttps://www.govtech.com/question-of-the-day/can-ai-write-believable-misinformation\nhttps://gadgets.ndtv.com/apps/news/ai-gpt-3-artificial-intelligence-write-messages-mislead-readers-fake-news-research-georgetown-university-2449512\nhttps://www.wired.com/story/ai-write-disinformation-dupe-human-readers/\nhttps://www.nextgov.com/emerging-tech/2021/05/report-highlights-how-ai-could-amplify-future-disinformation-campaigns/174161/\nhttps://www.vice.com/en/article/qj8kd5/qanon-conspiracy-theory-robot-ai-artificial-intelligence\nhttps://mixed.de/gpt-3-lassen-sich-menschen-von-ki-fake-news-beeinflussen/\nRelated \ud83c\udf10\nGPT-3 large language model\nGPT-3 anti-Muslim bias\nPage info\nType: Incident\nPublished: December 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/uyghur-emotion-detection-testing", "content": "Beijing tests Uyghur emotion detection system\nOccurred: May 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn AI-powered camera system that detects the emotions of Uyghurs is being tested by Chinese authorities, prompting concerns about state surveillance and loss of human rights and civil liberties. \nThe allegations were made to the BBC on condition of anonymity by a software engineer who says he installed the system, which can detect and analyse minor changes in expressions.\nThe engineer told the BBC that the cameras were placed 3 meters, a little less than 10 feet, away from the subjects. He said subjects are put in 'restraint chairs,' where their wrists and ankles are locked in place by metal restraints. \n'The Chinese government use Uyghurs as test subjects for various experiments just like rats are used in laboratories', the software engineer said. He went on to allege the system is used for 'pre-judgment without any credible evidence.' Most people, he says, are categorised as 'anxious' or 'scared'.\nHome to 12 million ethnic minority Uyghurs, Xinjiang residents are under more or less constant surveillance. Human Rights Watch had earlier published a report detailing a policy of torture, disappearances, and cultural erasure in the province.\nSystem \ud83e\udd16\n\nOperator: Government of China\nDeveloper: Zhejiang Dahua Technology; Hikvision\nCountry: China\nSector: Govt - police\nPurpose: Detect emotion\nTechnology: Emotion detection\nIssue: Accuracy/reliability; Human/civil rights; Privacy; Surveillance\nTransparency: Governance\nInvestigations, assessments, audits \ud83e\uddd0\nBBC. AI emotion-detection software tested on Uyghurs\nBBC. Are You Scared Yet, Human?\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.businessinsider.nl/software-engineer-who-installed-ai-recognition-in-xinjiang-says-china-tested-the-software-on-uighurs-bbc/\nhttps://www.independent.co.uk/asia/china/china-uighur-muslims-ai-huawei-b1854180.html\nhttps://www.insider.com/china-is-testing-ai-recognition-on-the-uighurs-bbc-2021-5\nhttps://www.theguardian.com/global-development/2021/mar/03/china-positive-energy-emotion-surveillance-recognition-tech\nhttps://metro.co.uk/2021/05/26/china-uses-artificial-intelligence-to-read-uyghur-prisoners-emotions-14648755/\nhttps://www.standard.co.uk/tech/china-s-ai-emotion-detection-cameras-fuel-human-rights-storm-b937367.html\nRelated \ud83c\udf10\nBeijing Uyghur fake influence campaign\nHuawei Uyghur-spotting patent\nPage info\nType: Issue\nPublished: October 2022", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/sao-paulo-metro-advertising-facial-biometrics", "content": "Sao Paulo METRO ordered to stop using facial recognition\nOccurred: May 2018\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nBrazil's S\u00e3o Paulo Metro was ordered to stop using facial recognition following a lawsuit by civil rights groups.\nVia Quatro, the operator of S\u00e3o Paulo Metro\u2019s Yellow Line, had installed platform doors that displayed ads and information and use sensors with screens and facial and emotion recognition to monitor the reaction of viewers. \nThe move resulted in human and privacy rights advocates voicing concerns about the inaccuracy of facial biometric systems, the potential for racial and ethnic bias, and of pseudoscience.\nThe lack of information provided about the system, and lack of user consent, prompted Brazilian consumer rights organisation Instituto Brasileiro de Defesa do Consumidor (Idec) to file (pdf - in Portuguese) a legal challenge against Via Quatro that argued that people\u2019s fundamental rights and privacy had been violated.\n\u2795 In May 2021, the Court of Justice of S\u00e3o Paulo ordered Via Quatro to terminate (pdf, in Portuguese) its 'abusive' use of facial recognition technology and data collection.\n\u2795 In March 2022, S\u00e3o Paulo court judge Cynthia Thome ordered the company responsible for running Sao Paulo's metro system Companhia do Metropolitano de S\u00e3o Paulo (METRO), to suspend its use of facial recognition as part of the broader implementation of the SecurOS electronic surveillance system.\nSystem \ud83e\udd16\nViaQuatro website\nViaQuatro Wikipedia profile\nCompanhia do Metropolitano de S\u00e3o Paulo website\nCompanhia do Metropolitano de S\u00e3o Paulo Wikipedia profile\nAdMobilize website\nOperator: ViaQuatro; Companhia do Metropolitano de S\u00e3o Paulo (METRO)\nDeveloper: AdMobilize\nCountry: Brazil\nSector: Govt - transport\nPurpose: Identify consumer identity\nTechnology: Facial recognition; Emotion recognition\nIssue: Privacy; Accuracy/reliability\nTransparency: Governance; Privacy; Marketing; Legal\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nTRIBUNAL DE JUSTI\u00c7A DO ESTADO DE S\u00c3O PAULO (2018). SENTEN\u00c7A (pdf)\nResearch, advocacy \ud83e\uddee\nInstituto Brasileiro de Defesa do Consumidor (IDEC, 2018). AO EXCELENT\u00cdSSIMO(A) SENHOR(A) DOUTOR(A) JUIZ(A) DE DIREITO DE UMA DAS VARAS C\u00cdVEIS DO FORO CENTRAL DA COMARCA DE S\u00c3O PAULO - SP (pdf)\nGlobal Freedom of Expression, Columbia University. The Case of S\u00e3o Paulo Subway Facial Recognition Cameras\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bloomberg.com/news/articles/2018-05-08/s-o-paulo-metro-s-newest-platform-doors-can-read-your-face\nhttps://www.zdnet.com/article/sao-paulo-subway-operator-gets-sued-for-collecting-passenger-data/\nhttps://sociable.co/technology/emotion-gender-prediction-tech-should-be-banned-brazils-metro-ngo-testifies/\nhttps://ohrh.law.ox.ac.uk/mind-the-gap-the-privacy-void-in-brazilians-public-transport/\nhttps://www.biometricupdate.com/202007/metro-facial-biometrics-emotion-gender-detection-system-legitimacy-disputed-in-brazil-court\nhttps://www.biometricupdate.com/202105/face-biometrics-systems-shut-down-in-washington-dc-and-sao-paulo-brazil\nhttps://www.vice.com/en/article/5dp8wq/brazils-biggest-metro-could-get-facial-recognition-cameras-that-reinforce-racist-policing\nhttps://www.accessnow.org/facial-recognition-on-trial-emotion-and-gender-detection-under-scrutiny-in-a-court-case-in-brazil/\nhttps://www.eff.org/deeplinks/2018/12/where-government-hack-their-own-people-and-people-fight-back-latin-american\nhttps://iapp.org/news/a/brazilian-court-halts-metros-facial-recognition/\nhttps://intervozes.org.br/acao-quer-vedar-o-uso-de-tecnologias-de-reconhecimento-facial-pelo-metro-de-sao-paulo/\nhttps://www.biometricupdate.com/202203/brazilian-subway-operator-pushing-facial-recognition-despite-sustained-protests\nhttps://www.zdnet.com/article/sao-paulo-subway-ordered-to-suspend-use-of-facial-recognition/\nhttps://www.biometricupdate.com/202203/brazilian-subway-operator-pushing-facial-recognition-despite-sustained-protests\nRelated \ud83c\udf10\nMoscow Metro Face Pay\nSpotify emotion recognition\nPage info\nType: Incident\nPublished: March 2022", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/new-zealand-immigration-overstayer-predictions", "content": "New Zealand immigration overstayer predictions fuel racial profiling fears\nOccurred: April 2018\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn New Zealand government official revealed that overstayers to the country could be fast track deported should they belong to a demographic group that have previously committed crimes or run up hospital costs. \nImmigration New Zealand compliance and investigations area manager Alistair Murray told RNZ that the country's immigration authorities had been using an algorithm to predict overstaying based on age, gender, and ethnicity modelling data. \nThe revelation prompted accusations of illegal racial profiling, forcing the government to admit the pilot programme had been operating clandestinely for 18 months.\nThe programme was later terminated and a review of government use of algorithms conducted. \nThe subsequent stocktaking (pdf) exercise fed into a formal Algorithm Assessment Report (pdf) by the New Zealand government that made recommendations on the development and use of algorithms, including those introduced to manage migration.\nSystem \ud83e\udd16\nImmigration New Zealand website\nImmigration New Zealand Wikipedia profile\nDocuments \ud83d\udcc3\nhttps://www.immigration.govt.nz/opsmanual/\nOperator: Immigration New Zealand\nDeveloper: Immigration New Zealand\nCountry: New Zealand\nSector: Govt - immigration\nPurpose: Predict visa overstayers\nTechnology: Prediction algorithm\nIssue: Bias/discrimination - age, gender, race, ethnicity; Human/civil rights\nTransparency: Governance; Black box; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.rnz.co.nz/news/national/354135/immigration-nz-using-data-system-to-predict-likely-troublemakers\nhttps://www.zdnet.com/article/nz-immigration-rejects-racial-profiling-claims-in-visa-data-modelling-project/\nhttps://www.statschat.org.nz/2018/04/05/15641/\nhttps://www.nzherald.co.nz/nz/immigration-nzs-data-profiling-illegal-critics-say/P5QDBGVDGFSI6I3NV4UHPOSBRA/\nhttps://www.nzherald.co.nz/nz/barry-soper-racial-profiling-at-its-worst/QP2DOVB7Y56W5YQVGTK4R5QSEM/\nhttps://thespinoff.co.nz/society/05-04-2018/immigration-nz-is-trying-a-bit-of-racial-profiling-and-it-seems-very-pleased-with-itself/\nhttps://www.stuff.co.nz/business/108106220/humans-still-have-final-say-on-almost-all-nz-government-decisions\nhttps://thespinoff.co.nz/society/09-04-2018/a-computer-model-may-be-dodgy-on-deportation-but-not-as-dodgy-as-a-human/\nhttps://www.pundit.co.nz/content/where-did-it-algo-wrong-the-threat-and-promise-of-predictive-analytics\nhttps://www.zdnet.com/article/nz-immigration-rejects-racial-profiling-claims-in-visa-data-modelling-project/\nhttps://www.zdnet.com/article/nz-to-perform-urgent-algorithm-stocktake-fearing-data-misuse-within-government/\nRelated \ud83c\udf10\nIRCC immigration and visa applications AI screening\nCPB One asylum seeker app privacy, surveillance\nPage info\nType: Incident\nPublished: December 2021\nLast updated: January 2023", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/google-derm-assist-dermatology-app-bias-privacy", "content": "Google Derm Assist dermatology app accused of racial bias\nOccurred: April 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nGoogle\u2019s Derm Assist, an AI-powered app designed to analyse skin conditions, faced criticism over potential racial bias.\nDerm Assist automatically analyses skin conditions, asks questions, and suggests causes. The app is being tested in the US and has been approved for use as a medical tool in Europe. \nGoogle said the app can recognise 288 skin conditions; however, some doctors expressed concerns about the accuracy of the system, in part due poor image quality, and the potential for over-diagnosis of skin cancers.\nIt also appeared the system was trained and tested on a dataset that underrepresents people with dark skin tones. The training dataset used for the app consisted of 64,837 images of 12,399 patients located in two US states. \nHowever, only 3.5 percent of these images came from patients with Fitzpatrick skin types V and VI, which represent brown skin and dark brown or black skin, respectively. The majority of the database was composed of people with fair skin, darker white skin, or light brown skin.\nThe findings resulted in accusations of racial bias, and drew attention to perceived real-life Google workforce discrimination and inequality. \nGoogle responded by saying it would only save images to help train the Derm Assist algorithm if users gave them explicit permission to do so. Concerns had also been raised about the potential for Google using personal sensitive data for other purposes.\nSystem \ud83e\udd16\nGoogle Derm Assist\nDocuments \ud83d\udcc3\nGoogle (2021). Interpreting skin conditions with AI\nGoogle (2021). Development and Assessment of an Artificial Intelligence\u2013Based Tool for Skin Condition Diagnosis by Primary Care Physicians and Nurse\nOperator: Alphabet/Google\nDeveloper: Alphabet/Google\nCountry: USA\nSector: Health\nPurpose: Identify dermatological issues\nTechnology: Computer vision; Deep learning\nIssue: Accuracy/reliability; Bias/discrimination - race; Privacy\nTransparency: Governance; Privacy\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.statnews.com/2021/05/18/google-dermatology-assist-skin-app/\nhttps://www.vice.com/en/article/m7evmy/googles-new-dermatology-app-wasnt-designed-for-people-with-darker-skin\nhttps://www.ft.com/content/6d4cd446-2243-43f4-befd-565b4e880811\nhttps://healthitanalytics.com/news/google-unveils-artificial-intelligence-tool-for-dermatology\nhttps://www.theguardian.com/society/2021/may/21/doctors-fear-google-skin-check-app-will-lead-to-tsunami-of-overdiagnosis\nhttps://www.bbc.co.uk/news/technology-57157566\nhttps://www.cnet.com/health/personal-care/google-will-now-help-you-identify-that-suspicious-mole-or-rash/\nRelated \ud83c\udf10\nAppen recruitment skin colour assessment\nGoogle Health diabetic retinopathy diagnosis\nPage info\nType: Incident\nPublished: December 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/cambodia-torture-victims-photo-manipulation", "content": "Manipulation of Cambodia torture victims' photos draws backlash\nOccurred: May 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nManipulated photographs of Cambodian prisoners tortured by the Khmer Rouge led to a heated public backlash about the ethics of photographic touching-up using artificial intelligence. \nIrish artist Matt Loughrey had colourised black and white photographs of victims tortured by the Khmer Rouge in Phnom Penh's notorious S-21 prison, and jovialised them by adding smiles.\nPublished alongside an interview with Loughrey in Vice, the photographs resulted in complaints by victims' families and Cambodian authorities about loss of dignity, accusations of historical revisionism, copyright infringement, and a debate on the ethics of photographic manipulation.\nVice subsequently deleted the article and apologised, pointing out that the photographs had been 'manipulated beyond colourisation'. Loughrey had not mentioned adding smiles to the photographs.\nSystem \ud83e\udd16\nUnknown\n\nDocuments \ud83d\udcc3\nVice (2021). Editorial Statement Regarding Photographs of Khmer Rouge Victims\nOperator: Matt Loughrey; Vice News\nDeveloper: \nCountry: Rep. Ireland\nSector: Media/entertainment/sports/arts\nPurpose: Colourise photographs\nTechnology: AI colourisation\nIssue: Copyright; Mis/disinformation; Ethics\nTransparency: Governance; Marketing\nResearch, advocacy \ud83e\uddee\nChange.org petition\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/world-asia-56707984\nhttps://apnews.com/article/phnom-penh-asia-pacific-cambodia-4ed9d811c1314ca77095c777c184c095\nhttps://www.irishcentral.com/culture/matt-loughrey-cambodia-vice\nhttps://www.irishtimes.com/culture/art-and-design/visual-art/the-khmer-rouge-controversy-why-colourising-old-photos-is-always-a-falsification-of-history-1.4536637\nhttps://www.thestar.com/opinion/star-columnists/2021/04/19/digital-manipulation-of-genocidal-photos-shows-that-seeing-is-no-longer-believing.html\nhttps://abcnews.go.com/International/wireStory/altered-photos-cambodian-torture-victims-stir-controversy-77017600\nhttps://www.msn.com/en-xl/news/other/photo-restorer-who-added-smiles-to-cambodian-genocide-victims-violated-the-law-gov-t-says/ar-BB1fxayX\nhttps://www.aljazeera.com/news/2021/4/11/cambodia-condemns-vice-for-altered-khmer-rouge-images\nhttps://www.khmertimeskh.com/50837298/government-smiling-khmer-rouge-photos-seriously-affect-the-dignity-of-the-victims/\nhttps://petapixel.com/2021/05/18/the-controversial-history-of-colorizing-black-and-white-photos/\nRelated \ud83c\udf10\nThe Book of Veles photo manipulation\nCruzcampo Lola Flores deepfake ad\nPage info\nType: Incident\nPublished: December 2021\nLast updated: January 2024", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tiktokinstagramfacebook-lgbtq-discrimination", "content": "Study: Top social media platforms 'unsafe' for LGBTQ users\nOccurred: May 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nTop social media sites are 'categorically unsafe' for LGBTQ people, according to activist group GLAAD.  \nAccording to GLAAD, Facebook, Twitter, Instagram, TikTok, and YouTube allow LGBTQ people to be harassed regularly, and for harmful misinformation and disinformation about LGBTQ people to spread unchecked. \nThe report found that these platforms, despite presenting themselves as LGBTQ-friendly, allow for daily harassment of LGBTQ people and the unchecked spread of harmful misinformation. GLAAD\u2019s president and CEO, Sarah Kate Ellis, argued that these companies have the tools to stop such behaviour but failed to do so.\nThe report also highlighted the real-world consequences of online harassment and discrimination, drawing direct lines between the spread of anti-trans misinformation online and the introduction of over 100 anti-trans bills at US state level.\nGLAAD and a team of outside experts spent several months looking at each site, their policies and track record of enforcing those policies.\nSystem \ud83e\udd16\nInstagram website\nInstagram Wikipedia profile\nTikTok website\nTikTok Wikipedia profile\nTwitter website, Wikipedia profile\nYouTube website, Wikipedia profile\nOperator: Meta/Facebook; ByteDance/TikTok; Twitter\nDeveloper: Meta/Facebook; ByteDance/TikTok; Twitter\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Moderate content\nTechnology: Recommendation algorithm\nIssue: Bias/discrimination - LGBTQ; Safety; Mis/disinformation\nTransparency: Black box\nResearch, advocacy \ud83e\uddee\nGLAAD (2021). Social media safety index\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.axios.com/glaad-study-facebook-instagram-youtube-tiktok-8d9555b6-4cee-4db8-8b51-5b5fdad0c749.html\nhttps://news.yahoo.com/glaad-finds-top-social-media-223102412.html\nhttps://www.msn.com/en-us/tv/celebrity/glaad-says-every-top-social-media-site-is-e2-80-98categorically-unsafe-e2-80-99-for-lgbtq-users/ar-BB1gxRCi\nhttps://www.npr.org/2021/05/10/995328226/social-media-hate-speech-harassment-significant-problem-for-lgbtq-users-report\nhttps://www.msn.com/en-us/news/us/the-bar-is-low-for-the-social-media-industry-top-platforms-are-unsafe-for-lgbtq-community-new-report-says/ar-BB1gySVP\nhttps://www.nbcnews.com/nbc-out/out-news/top-social-media-platforms-unsafe-lgbtq-users-report-finds-rcna889\nhttps://www.adweek.com/media/glaad-calls-the-entire-social-media-sector-unsafe-for-lgbtq-users/\nhttps://thehill.com/changing-america/respect/diversity-inclusion/553535-new-report-says-big-tech-algorithms-promote-hate\nhttps://www.mediamatters.org/tiktok/tiktoks-recommendation-algorithm-promoting-homophobia-and-anti-trans-violence\nRelated \ud83c\udf10\nTikTok LGBTQ shadowbanning\nTikTok #intersex 'censorship'\nPage info\nType: Incident\nPublished: December 2021\nLast updated: June 2023", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/appen-recruitment-skin-colour-assessment", "content": "Appen blasted for recruitment skin colour assessments\nOccurred: May 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA digital media strategist applying for a job at Australian AI gig working company Appen revealed the company is asking potential employees to describe their skin tone during its job application process.\nIn the application form, the company offered job-seekers the option to disclose their gender identify, ethnicity, complexion and whether they are disabled. Applying for a role in the US, Charn\u00e9 Graham opted to select her complexion, from light to brown to black, having ticked a box saying she is 'Black or African American'. \nAppen, a gig-worker company then employing over one million contractors labeling photographs, text and other data, later apologised and claimed the \u201coptional question\u201d was \u201cused to ensure diverse datasets are included in the collection and annotation used to train computer vision algorithms.\u201d\nThe incident was seen to highlight inappropriate governance and poor transparency at Appen. It also played into a broader debate about diversity within Australia's technology sector, amidst perceptions the industry remains dominated by white males.\n\u2795 In July 2023, Appen scored poorly in a report from the University of Oxford\u2019s Internet Institute into the conditions of AI gig workers. Companies that employ AI gig workers uniformly fail to meet a basic threshold of labour rights standards, the report argued.\nSystem \ud83e\udd16\nAppen website\nAppen Wikipedia profile\nOperator: Appen\nDeveloper: Appen\nCountry: USA; Australia\nSector: Technology\nPurpose: Determine skin colour\nTechnology: Computer vision\nIssue: Bias/discrimination - race; Employment\nTransparency: Governance; Marketing\nResearch, advocacy \ud83e\uddee\nOxford Internet Institute/Fairwork (2023). Fairwork Cloudwork Ratings 2023 (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.smh.com.au/business/companies/paper-bag-test-artificial-intelligence-firm-appen-criticised-for-skin-colour-test-20210512-p57r5r.html\nhttps://www.theguardian.com/australia-news/2021/may/12/australian-ai-company-says-sorry-for-asking-potential-staff-to-describe-their-skin-tone\nhttps://www.dailymail.co.uk/news/article-9570061/Aussie-tech-company-blasted-recruiting-questions-candidates-skin-colour.html\nhttps://ia.acs.org.au/article/2021/aussie-ai-company-asked-recruits-for-their-skin-colour.html\nhttps://www.smartcompany.com.au/people-human-resources/recruitment-hiring/paper-bag-test-appen-racism-recruitment-process/\nhttps://www.afr.com/rear-window/appen-s-offensive-job-ads-20210512-p57r44\nhttps://www.theage.com.au/business/companies/paper-bag-test-artificial-intelligence-firm-appen-criticised-for-skin-colour-test-20210512-p57r5r.html\nRelated \ud83c\udf10\nUber Real-time ID Check\nTwitter photo crop algorithm age, weight bias\nPage info\nType: Incident\nPublished: December 2021\nLast updated: July 2023", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/airbnb-smart-pricing-algorithm-racism", "content": "Report: Airbnb Smart Pricing algorithm exacerbates racial inequality\nOccurred: May 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAirbnb's 'Smart Pricing' algorithm inadvertently exacerbates racial inequality, according to researchers.\nLaunched in 2015, the algorithm dynamically adjusts the cost of a night\u2019s stay based on demand and allows hosts to set a minimum price, and had been found to reduce racial disparties.\nPrior to its launch, white Airbnb hosts earned significantly more than Black hosts, partly due to properties owned by Black hosts having 20 percent less demand than similar white-owned properties.\nHowever, Carnegie Mellon professor Param Vir Singh and his team of researchers discovered that White hosts appeared to prefer using the algorithm more than Black ones, thereby unintentionally widening existing real-world racial discrepancies. \nThe episode highlighted the unintended consequences of algorithms and the importance of considering user adoption when designing such tools.\n\u2795 Research studies also discovered that Airbnb secretly collects and feeds users\u2019 personal data into an algorithm that assess whether they are trustworthy enough to make a booking.\nSystem \ud83e\udd16\nAirbnb Smart Pricing\nOperator: Airbnb\nDeveloper: Airbnb\nCountry: USA\nSector: Travel/hospitality\nPurpose: Determine price\nTechnology: Pricing algorithm\nIssue: Bias/discrimination - race\nOpacity: Governance; Black box\nResearch, advocacy \ud83e\uddee\nCarnegie Mellon University (2021). Can an AI Algorithm Mitigate Racial Economic Inequality? Only If More Black Hosts Adopt it.\nLambert L. (2021). White Airbnb Hosts Earn More. Can AI Shrink the Racial Gap?\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.ft.com/content/5b1471e0-ed4a-47f5-8f3f-0a1ee7f7999c\nhttps://thegrio.com/2021/05/13/airbnb-racial-disparities-in-pricing/\nhttps://markets.businessinsider.com/news/stocks/airbnb-pricing-algorithm-led-to-increased-racial-disparities--study-finds-10131658\nhttps://moguldom.com/352446/study-airbnbs-smart-pricing-algorithm-for-hosts-made-racial-disparities-worse/\nhttps://markets.businessinsider.com/news/stocks/airbnb-pricing-algorithm-led-to-increased-racial-disparities--study-finds-10131658\nhttps://www.protocol.com/newsletters/sourcecode/colonial-pipeline-hack\nhttps://www.morningbrew.com/emerging-tech/stories/2021/05/17/study-airbnb-algorithm-power-help-decrease-racial-disparities-earnings-opposite\nhttps://www.morningbrew.com/emerging-tech/stories/2021/06/15/airbnb-failed-antidiscrimination-teamand-let-racial-disparities-slip-cracks\nRelated \ud83c\udf10\nYieldStar automated rent-setting\nTinder Plus pricing algorithm\nPage info\nType: Incident\nPublished: March 2022", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/instagramtwitter-remove-block-palestinian-posts", "content": "Instagram, Twitter 'censor' Palestinian posts during 11-day war\nOccurred: May 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nInstagram and Twitter closed users' accounts and blocked their content and relevant hashtags when they mentioned the possible eviction of Palestinians from East Jerusalem. \nDuring an 11-day war on the Gaza Strip in May 2021, users and journalists complained that Arabic language posts about Palestine was hit by hashtag removals and reshare blocks. Palestinian journalists also reported that their WhatsApp accounts had been blocked. Meantime, Hebrew content remained relatively unaffected.\nHuman and digital rights groups complained that 'discriminatory' algorithms were likely to be at work, accused the social media companies of censorship, and demanded greater transparency. \nBoth companies later reversed course and reinstated the relevant accounts and content. Twitter blamed automated spam filtering software; Instagram put the problem down to a 'technical bug'. \n\u2795 A September 2022 report by independent, non-profit organisation Business for Social Responsibility (BRC) on Meta's moderation of Arabic and Hebrew posts found that Meta had unfairly targetted Palestinian social media users.\nSystem \ud83e\udd16\nMeta website\nMeta Wikipedia profile\nDocuments \ud83d\udcc3\nMeta (2022). An Independent Due Diligence Exercise into Meta\u2019s Human Rights Impact in Israel and Palestine During the May 2021 Escalation\nOperator:  \nDeveloper: Meta/Instagram; Twitter\nCountry: Palestine; Israel  \nSector: Politics\nPurpose: Moderate content\nTechnology: Content moderation system\nIssue: Accuracy/reliability; Bias/discrimination - race, ethnicity; Freedom of expression - censorship\nTransparency: Governance; Black box\nInvestigations, assessments, audits \ud83e\uddd0\nBSR (2022). Human Rights Due Diligence of Meta\u2019s Impacts in Israel and Palestine\nResearch, advocacy \ud83e\uddee\nDAWN (2022). Meta Should Release Report on Content Moderation in Palestine, Amidst Documented Censorship\nHuman Rights Watch (2022). Statement Regarding BSR\u2019s HRA for Meta on Palestine & Israel\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.cnbcafrica.com/2021/instagram-twitter-blame-glitches-for-deleting-palestinian-posts/\nhttps://www.reuters.com/article/israel-palestinians-socialmedia-idUSL8N2MU624\nhttps://www.msn.com/en-au/news/world/palestinians-denounce-censorship-of-social-networks/ar-BB1gE9Wg\nhttps://www.msn.com/en-us/news/world/instagram-reportedly-removed-posts-about-a-holy-islamic-mosque-after-the-company-associated-alaqsa-with-a-terrorist-organization-amid-palestinian-israeli-violence/ar-BB1gFtUG\nhttps://www.businessinsider.com/palestinian-journalist-mariam-barghouti-says-twitter-asked-delete-tweets-2021-5\nhttps://www.vice.com/en/article/qj8b4x/twitter-said-it-restricted-palestinian-writers-account-by-accident\nhttps://www.huffingtonpost.co.uk/entry/mariam-barghouti-twitter-suspension_n_609b314ee4b0909247fc7bbf\nhttps://www.msn.com/EN-US/news/world/twitter-censors-then-uncensors-palestinian-journalist-s-account/ar-BB1gCTDc\nhttps://www.thenationalnews.com/mena/sheikh-jarrah-content-takedowns-reveal-pattern-of-online-restrictions-in-palestine-1.1220037\nhttps://www.buzzfeednews.com/article/ryanmac/instagram-facebook-censored-al-aqsa-mosque\nhttps://www.nytimes.com/2021/06/03/technology/india-israel-facebook-employees.html\nhttps://www.middleeasteye.net/news/israel-palestine-facebook-investigation-social-media-posts-suppression\nRelated \ud83c\udf10\nHebron Palestinian facial recognition surveillance\nBytedance/TikTok Uyghur censorship\nPage info\nType: Incident\nPublished: December 2021\nLast updated: December 2023", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/driver-abuses-tesla-autopilot-by-sitting-in-rear-seat", "content": "Rear seat driver abuses Tesla Autopilot\nOccurred: May 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Tesla owner was reported for repeatedly abusing the Autopilot feature of his Tesla Model 3, including sitting in the backseat while the car drove itself.\nDespite being arrested and having his car impounded for reckless driving, Param Sharma continued to engage Autopilot and sit in the backseat of his vehicle. He was seen with his foot on the wheel while the car was driving with Autopilot engaged.\nTesla\u2019s Autopilot system is designed to keep the car in its lane and adjust its speed, and with certain settings, the car can switch lanes or take exit ramps on its own. However, Tesla explicitly states that drivers must be seated at the wheel, with hands on the wheel and eyes on the road1. Despite these guidelines, Sharma managed to trick the sensors into thinking someone was at the wheel by using his foot.\nThis incident raises concerns about the misuse of Tesla\u2019s Autopilot and Full Self-Driving features, and the potential dangers it poses on the road. There is currently a wide misunderstanding regarding its self-driving capabilities.\nSystem \ud83e\udd16\nTesla Autopilot, Full-self Driving\nIncident video\nOperator: Param Sharma\nDeveloper: Tesla\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Driver assistance system\nIssue: Safety\nTransparency: Black box; Marketing\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.sfgate.com/local/article/2021-05-Tesla-autopilot-driver-back-seat-Bay-Area-16160430.php\nhttps://sanfrancisco.cbslocal.com/2021/05/12/driverless-tesla-backseat-arrest-param-sharma/\nhttps://electrek.co/2021/05/05/tesla-driver-keeps-being-spotted-in-backseat-autopilot-begging-arrested/\nhttps://sfist.com/2021/05/06/backseat-driver-tesla-autopilot-scofflaw-spotted-multiple-times-on-bay-area-roads/\nhttps://www.newsweek.com/arrested-tesla-driver-boasts-riding-back-seat-autopilot-1590763\nhttps://www.msn.com/en-gb/cars/news/tesla-owner-arrested-after-repeatedly-riding-in-the-back-seat-while-it-was-on-autopilot/ar-BB1gG29h\nhttps://thenextweb.com/news/tesla-driver-arrested-in-california-for-using-autopilot-while-in-the-back-seat\nhttps://www.nbcnews.com/news/us-news/california-highway-patrol-searching-man-seen-riding-back-seat-tesla-n1266944\nhttps://jalopnik.com/another-video-shows-a-driver-abusing-tesla-autopilot-on-1846851450\nRelated \ud83c\udf10\nTesla Model S crashes into fire engine\nTesla Model X crashes into five police officers\nPage info\nType: Issue\nPublished: December 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/china-diplomatic-fake-influence-campaign", "content": "China runs co-ordinated fake diplomatic Twitter influence campaign\nOccurred: May 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA seven-month investigation by the Associated Press and Oxford Internet Institute discovered that Beijing ran an army of fake accounts retweeting and reposting Chinese diplomats and state media in order to amplify government propaganda.\nThe accounts, which impersonated users from the UK, Australia and other countries, were often nearly identical, created in batches, showed strong signs of coordination, and failed to disclose the fact that the content was government-sponsored. \nThe findings were seen to confirm Beijing's move onto Western social media platforms as part of a broader effort to shape global public opinion. The investigation found that much of the support that Chinese diplomats, such as Liu Xiaoming, the former ambassador to the UK, appear to enjoy on Twitter had been manufactured.\nThe Chinese Embassy in London did not deny the campaign's existence when asked about it.\nSystem \ud83e\udd16\nUnknown\nOperator: Government of China\nDeveloper: Meta/Facebook; Twitter\nCountry: Global\nSector: Politics\nPurpose: Increase influence\nTechnology: Bot/intelligent agent; Social media\nIssue: Mis/disinformation\nTransparency: Governance; Marketing\nResearch, advocacy \ud83e\uddee\nOxford Internet Institute (2021). China\u2019s Public Diplomacy Operations: Understanding Engagement and Inauthentic Amplification of PRC Diplomats on Facebook and Twitter\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://apnews.com/article/asia-pacific-china-europe-middle-east-government-and-politics-62b13895aa6665ae4d887dcc8d196dfc\nhttps://www.theguardian.com/world/2021/may/12/china-has-used-pandemic-to-boost-global-image-report-says\nhttps://www.taiwannews.com.tw/en/news/4201779\nhttps://www.washingtonpost.com/politics/army-of-fake-fans-online-boosts-chinas-global-messaging/2021/05/11/e92a9c06-b20e-11eb-bc96-fdf55de43bef_story.html\nhttps://www.finchannel.com/technology/80737-fanatic-fans-or-fake-followers\nhttps://au.news.yahoo.com/army-fake-twitter-accounts-driving-chinese-propaganda-132418273.html\nhttps://fox40.com/news/business/army-of-fake-fans-boosts-chinas-messaging-on-twitter/\nhttps://www.msn.com/en-in/news/opinion/china-influencing-world-using-army-of-fake-social-media-accounts-but-why/ar-AAO14SR\nRelated \ud83c\udf10\nBeijing Uyghur fake influence campaign\nBytedance/TikTok Uyghur censorship\nPage info\nType: Incident\nPublished: December 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tiktok-uk-misuses-childrens-data", "content": "TikTok UK fined for misusing childrens' data\nOccurred: July 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nTikTok was fined GBP 12.7 million by the UK privacy regulator for multiple failures to protect children on its platform between May 2018 and July 2020. \nThe UK Information Commissioner's Office (ICO) ruled that TikTok had failed to stop approximately 1.4 million under-age children using its service in 2020, contrary to its own terms of service, none of which had gained the consent of their parents or guardians - contrary to UK law. \nThe ICO also charged TikTok with not providing sufficient, easy-to-understand information about how user data is collected, used, and shared, without which children were unlikely to have been able to make informed decisions about their use of the platform.\nThe ICO also said that the children\u2019s data may have been used to track and profile them, and potentially present them with harmful or inappropriate content. \nThe fine was one of the largest issued by the ICO. \nSystem \ud83e\udd16\nTikTok website\nTikTok Wikipedia profile\nOperator: ByteDance/TikTok\nDeveloper: ByteDance/TikTok\nCountry: UK\nSector: Media/entertainment/sports/arts\nPurpose: Process personal data\nTechnology: \nIssue: Privacy\nTransparency: Governance; Privacy\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nInformation Commissioner's Office (2023). Age Appropriate Design\nInformation Commissioner's Office (2023). ICO fines TikTok \u00a312.7 million for misusing children\u2019s data\nInformation Commissioner's Office (2022). ICO could impose multi-million pound fine on TikTok for failing to protect children\u2019s privacy\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reuters.com/technology/tiktok-fined-16-mln-by-uk-watchdog-misusing-childrens-data-2023-04-04/\nhttps://www.bbc.co.uk/news/uk-65175902\nhttps://techcrunch.com/2023/04/04/tiktok-uk-gdpr-kids-data-fine/\nhttps://www.theguardian.com/technology/2019/jul/02/tiktok-under-investigation-over-child-data-use\nhttps://www.theguardian.com/technology/2023/apr/04/tiktok-fined-uk-data-protection-law-breaches\nhttps://www.euractiv.com/section/data-privacy/news/uk-privacy-regulator-fines-tiktok-12-7m-for-childrens-data-violations/\nRelated \ud83c\udf10\nTikTok US personal data harvesting, sales\nBytedance/TikTok Uyghur censorship\nPage info\nType: Incident\nPublished: December 2021\nLast updated: April 2023", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/facebook-teen-alcohol-drug-gambling-ads-approvals", "content": "Facebook approves teen alcohol, drug, gambling ads\nOccurred: May 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nResearch studies show Facebook approved adverts targeting teen kids interested in smoking, alcohol, gambling, and extreme weight loss. \nUsing a fake account, Reset Australia discovered advertisers could target teenagers on user interest areas such as gambling, smoking, alcohol and dating status. Using a similar process, US-based Tech Transparency Project found Facebook approved ads for pills, eating disorders, and dating services for kids as young as 13 years old.\nReset Australia submitted its own advertisements in these interest areas. Facebook rejected two of its advertisements featuring regular cigarettes, but when it resubmitted the ads displaying electronic cigarettes, they were approved. The group believed the ads had passed the company\u2019s internal checks.\nTraditional advertising is tightly regulated, but the law has not kept pace with the explosion in social media, creating what Chris Cooper, Reset Australia executive director, called a \u201cloophole\u201d in the system.\nFacebook subsequently announced that it would no longer allow advertisers to target interest-based ads at teens. \nSystem \ud83e\udd16\nFacebook Ads Manager\nOperator: Meta/Facebook\nDeveloper: Meta/Facebook\nCountry: USA; Australia\nSector: Technology\nPurpose: Review advertising\nTechnology: Advertising management system\nIssue: Accuracy/reliability\nTransparency: Governance; Black box\nResearch, advocacy \ud83e\uddee\nReset Australia (2021). Profiling Children for Advertising: Facebook\u2019s Monetisation of Young People\u2019s Personal Data\nTech Transparency Project (2021). Pills, Cocktails, and Anorexia: Facebook Allows Harmful Ads to Target Teens\n\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.consumerreports.org/advertising-marketing/facebook-approved-alcohol-gambling-tobacco-weight-loss-ads-targeting-teens/\nhttps://www.abc.net.au/news/2021-04-28/facebook-instagram-teenager-tageted-advertising-alcohol-vaping/100097590\nhttps://www.wired.com/story/activists-facebook-allows-drug-ads-target-teens/\nhttps://www.news.com.au/technology/online/social/lobby-group-reset-australia-floored-by-facebook-advertising-loophole-for-teens/news-story/6fc35b40a5f99046cbaf1718341679cd\nhttps://www.bbc.co.uk/news/technology-56920992\nhttps://www.theguardian.com/technology/2021/apr/28/facebook-allows-advertisers-to-target-children-interested-in-smoking-alcohol-and-weight-loss\nhttps://itwire.com/home-it/reset-australia-says-facebook-allows-you-to-target-%E2%80%98teens-interested-in-smoking%E2%80%99-for-$127.html\nhttps://ia.acs.org.au/article/2021/facebook-ad-hypocrisy-exposed.html\nhttps://www.sbs.com.au/news/the-feed/facebook-approves-ad-targeting-teens-interested-in-extreme-weight-loss\nRelated \ud83c\udf10\nFacebook 'pseudoscience' ad targeting\nFacebook military gear advertising\nPage info\nType: Incident\nPublished: December 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/facebook-covid-19-misinformation-ad-approvals", "content": "Facebook 'quickly' approves COVID-19 misinformation ads\nOccurred: April 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nFacebook approved ads containing blatant misinformation about the evolving COVID-19 pandemic, according to a third-party investigation.\nConsumer advocacy organisation Consumer Reports placed seven ads under a false name with no advertising history advising people to drink bleach and other false or dangerous claims, all of which were quickly approved by Facebook.\nFacebook's advertising screening system is highly automated, with humans only involved in helping train the algorithms and occasionally reviewing specific ads before they run.\nConsumer Reports quotes digital rights researchers and digital marketing experts saying that Facebook's advertising system is opaque, overly complex, and poorly documented and explained.\nSystem \ud83e\udd16\nFacebook Ads Manager\nOperator: Meta/Facebook\nDeveloper: Meta/Facebook\nCountry: USA\nSector: Technology\nPurpose: Review advertising\nTechnology: Advertising management system\nIssue: Accuracy/reliability; Mis/disinformation\nTransparency: Governance; Black box\nResearch, advocacy \ud83e\uddee\nConsumer Reports (2020). Facebook Approved Ads with Coronavirus Misinformation\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://techcrunch.com/2020/04/08/lacking-eyeballs-facebooks-ad-review-system-fails-to-spot-coronavirus-harm/\nhttps://www.campaignlive.co.uk/article/coronavirus-misinformation-slipping-facebooks-ad-review-system/1679843\nhttps://www.abc.net.au/news/2020-04-24/facebook-approves-ads-with-covid-19-misinformation/12172168\nhttps://www.searchenginejournal.com/facebook-ads-fails-to-reject-covid-19-misinformation/360392/\nhttps://www.reuters.com/article/us-health-coronavirus-facebook-ads-idUSKCN2253CC\nhttps://venturebeat.com/2020/04/23/facebook-removes-pseudoscience-ad-targeting-category/\nhttps://www.jstor.org/stable/resrep25417.5?seq=1#metadata_info_tab_contents\nRelated \ud83c\udf10\nFacebook political ads misidentification\nBucheon COVID-19 facial recognition tracking\nPage info\nType: Incident\nPublished: December 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/dartmouth-medical-school-remote-exam-cheating", "content": "Dartmouth College medical school accuses students of remote exam cheating\nOccurred: May 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nDartmouth College\u2019s Geisel School of Medicine faced controversy after it accused students of cheating while taking remote exams. \nThe school charged 17 students with cheating based on data from the Canvas learning management system. The students were accused of accessing online course material during remote exams.\nHowever, the Electronic Frontier Foundation (EFF) and the Foundation for Individual Rights in Education (FIRE) raised concerns about the evidence provided, and due process violations. \nThey suggested that the data Dartmouth used as evidence could have been produced by an automatic syncing process inherent in Canvas\u2019 functionality - a process that could show a student\u2019s account accessing relevant course material, even if it was happening without the student knowing about it, on a device that was not in use during the exam.\nAfter further investigation, Dartmouth dropped all charges against the students, acknowledging that the technical data that formed the basis of the charges was insufficient. The Dean of the Geisel School, Duane Compton, apologised to the students and the entire student body.\nCritics of Canvas say the the learning management system is unreliable and an inappropriate way to track students.\nSystem \ud83e\udd16\nCanvas LMS website\nCanvas LMS Wikipedia profile\nOperator: Dartmouth College\nDeveloper: Canvas\nCountry: USA\nSector: Education\nPurpose: Detect and prevent cheating\nTechnology: Learning management system\nIssue: Accuracy/reliability; Ethics/values\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.vnews.com/Geisel-investigates-potential-cheating-during-exams-39856089\nhttps://apnews.com/article/medical-schools-fa16b8e4f1c8a63787070c739f48613c\nhttps://www.nytimes.com/2021/05/09/technology/dartmouth-geisel-medical-cheating.html\nhttps://www.insidehighered.com/quicktakes/2021/05/10/dartmouth-medical-school-charges-17-students-cheating\nhttps://thecollegepost.com/dartmouth-med-students-cheating-probe/\nhttps://www.msn.com/en-us/news/us/cheating-allegations-chill-students-at-dartmouth-medical-school/ar-BB1fHrMw\nhttps://www.medscape.com/viewarticle/949070\nhttps://www.vnews.com/Geisel-Students-Have-A-Lot-to-Lose-in-Battle-With-College-Over-Cheating-Allegations-39946055\nhttps://patch.com/new-hampshire/concord-nh/dartmouth-med-students-say-they-were-coerced-cheating-admission\nhttps://www.sciencewiki.com/articles/microsoft-unveils-immersive-education-solutions-to-inspire-educators-1\nhttps://www.eff.org/deeplinks/2021/04/proctoring-tools-and-dragnet-investigations-rob-students-due-process\nRelated \ud83c\udf10\nGoGuardian student monitoring\nGaggle student behavioural monitoring\nPage info\nType: Incident\nPublished: December 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/facebook-pseudoscience-ad-targeting", "content": "Facebook enables advertisers to target people interested in 'pseudoscience'\nOccurred: April 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nFacebook allowed advertisers to run adverts targeting people interested in 'pseudoscience', raising questions about the effectiveness and oversight of the company's misinformation policies. \nAccording to a investigation by The Markup, Facebook's ad portal listed more than 78 million people - an audience that could be targeted by advertisers, potentially spreading misinformation and disinformation.\nMark Zuckerberg had previously stated that 'one of [his] top priorities is making sure that [Facebook users] see accurate and authoritative information across all of [Facebook's] apps'.\nFacebook subsequently removed the pseudoscience interest category.\nThe episode raised concerns about the platform\u2019s role in the spread of misinformation. Furthermore, the reactive nature of Facebook\u2019s response suggested a need for more proactive measures, including sgtronger oversight of its ad management system, to prevent such issues. \nSystem \ud83e\udd16\nFacebook Ads Manager\nOperator: Meta/Facebook\nDeveloper: Meta/Facebook\nCountry: USA\nSector: Technology\nPurpose: Target audiences\nTechnology: Advertising management system\nIssue: Accuracy/reliability; Pseudoscience\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://themarkup.org/coronavirus/2020/04/23/want-to-find-a-misinformed-public-facebooks-already-done-it\nhttps://techcrunch.com/2020/04/23/facebook-pulls-pseudoscience-from-its-list-of-targeted-ad-categories/\nhttps://www.theverge.com/2020/4/23/21232547/facebook-pseudoscience-ad-targeting-coronavirus\nhttps://www.reuters.com/article/health-coronavirus-facebook-ads/facebook-gets-rid-of-pseudoscience-ad-targeting-category-idUKL2N2CB1D6\nhttps://www.huffpost.com/entry/facebook-no-pseudoscience-ads_n_5ea1fd34c5b62d7f0d6687dd\nhttps://gizmodo.com/facebook-pulls-down-pseudoscience-ad-category-with-over-1843030967\nhttps://venturebeat.com/2020/04/23/facebook-removes-pseudoscience-ad-targeting-category/\nhttps://boingboing.net/2020/04/23/facebook-nixes-pseudoscience.html\nhttps://eandt.theiet.org/content/articles/2020/04/facebook-stops-advertisers-targeting-pseudoscience-enthusiasts/\nhttps://www.businessinsider.com/facebook-ads-target-pseudoscience-conspiracy-theories-coronavirus-5g-misinformation-report-2020-4\nRelated \ud83c\udf10\nFacebook misidentifies 83 percent of political ads\nFacebook approves teen alcohol, drug, gambling ads\nPage info\nType: Incident\nPublished: September 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-dungeon-offensive-speech-filter", "content": "AI Dungeon offensive speech filter upgrade generates child porn\nOccurred: April 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAI Dungeon developer Latitude came under fire for developing a content moderation system intended to stop players of its open-ended adventure game from generating stories depicting sexual encounters with minors.\nAn upgrade to OpenAI's GPT-3 large language model resulted in some players typing words that caused the game to generate inappropriate stories. It also appears to have prompted the AI to create child pornography of its own.\nHowever, it quickly became clear that Latitude's new solution was blocking a wider range of content than envisaged. Gamers also complained that their private content was now being reviewed by moderators. \nMeantime, a security researcher published a report calculated that around a third of stories on AI Dungeon are sexually explicit, and one-half are assessed as NSFW. \nSystem \ud83e\udd16\nAI Dungeon website\nAI Dungeon Wikipedia profile\nDocuments \ud83d\udcc3\nLatitude (2021). Update to our Community\nOperator: Latitude\nDeveloper: Latitude; OpenAI\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Minimise sexual content\nTechnology: Content moderation system; NLP/text analysis \nIssue: Accuracy/reliability; Safety; Privacy\nTransparency: Governance\nInvestigations, assessments, audits \ud83e\uddd0\n(2021). AI Dungeon Public Disclosure Vulnerability Report\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.polygon.com/22408261/ai-dungeon-filter-controversy-minors-sexual-content-censorship-privacy-latitude\nhttps://www.wired.com/story/ai-fueled-dungeon-game-got-much-darker/\nhttps://www.vice.com/en/article/93ywpp/text-adventure-game-community-in-chaos-over-moderators-reading-their-erotica\nhttps://www.theregister.com/2021/04/30/ai_dungeon_filter_vulnerabilities/\nhttps://www.theregister.com/2021/10/08/ai_game_abuse/\nhttps://analyticsindiamag.com/when-ai-turns-rogue-the-dark-story-of-ai-dungeon/\nhttps://analyticsindiamag.com/openai-proposes-method-to-dilute-toxicity-of-gpt-3/\nhttps://www.utahbusiness.com/latitude-games-ai-dungeon-was-changing-the-face-of-ai-generated-content-until-its-users-turned-against-it/\nhttps://www.techdirt.com/articles/20211117/15225347965/content-moderation-case-study-game-developer-deals-with-sexual-content-generated-users-own-ai-2021.shtml\nhttps://www.reddit.com/r/AIDungeon/comments/n096zp/_/\nRelated \ud83c\udf10\nGPT-3 large language model\nGPT-3 anti-Muslim bias\nPage info\nType: Incident\nPublished: December 2021\nLast updated: April 2023", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/facebook-credit-card-age-ad-targeting", "content": "Facebook allows finance companies to target users by age\nOccurred: April 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nFour US credit card and financial loan companies excluded certain age groups from their advertising campaigns on Facebook, potentially violating US law. \nAccording to The Markup, the exclusions by Aspiration, Chime, Hometap, and Varo, violate Facebook's anti-discrimination policies and in some cases potentially transgress US federal or state civil rights laws. \nThe findings appeared to contradict a claim by Facebook vice president Monica Bickert in an appearance before a Senate Judiciary hearing on social media in April 2021 that Facebook did not allow the use of sensitive targeting criteria when placing financial services and housing ads.\nUS senator Mazie Hirono used The Markup's article to accuse Bickert of making a claim that 'appears to be false'.\nThe incident raised questions about how consistently implementation of Facebook's anti-discrimination policy was implemented and enforced, allowing some companies to target ads based on age, resulting in potential discrimination and the violation of civil rights law.\nSystem \ud83e\udd16\nFacebook Ads Manager\nDocuments \ud83d\udcc3\nUS Senate Committee on the Judiciary. Algorithms and Amplification: How Social Media Platforms\u2019 Design Choices Shape Our Discourse and Our Minds\nOperator: Meta/Facebook\nDeveloper: Meta/Facebook\nCountry: USA\nSector: Technology\nPurpose: Target audiences\nTechnology: Advertising management system\nIssue: Bias/discrimination - age\nTransparency: Governance\nInvestigations, assessment, audits \ud83e\uddd0\nThe Markup. Credit Card Ads Were Targeted by Age, Violating Facebook\u2019s Anti-Discrimination Policy\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://themarkup.org/citizen-browser/2021/05/24/senator-calls-facebook-response-on-discriminatory-ads-inadequate\nhttps://twitter.com/maziehirono/status/1387837578221694984\nhttps://www.washingtonpost.com/politics/2021/05/18/technology-202-sen-hirono-accuses-facebook-executive-making-false-claims-about-ad-targeting/\nhttps://searchengineland.com/instant-match-rates-in-google-ads-and-when-content-isnt-king-mondays-daily-brief-348347\nhttps://www.protocol.com/newsletters/protocol-fintech/fintech-q1-venture-capital\nhttps://www.politico.com/newsletters/morning-tech/2021/04/30/leaked-google-email-reveals-ties-to-new-pro-tech-group-794997\nRelated \ud83c\udf10\nFacebook 'pseudoscience' ad targeting\nFacebook approves teen alcohol, drug, gambling ads\nPage info\nType: Incident\nPublished: April 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/youtube-ads-hate-speech-blocklist", "content": "YouTube ads hate speech blocklist is 'inconsistent' and barely applied\nOccurred: April 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nYouTube's blocklist of hate terms is inconsistent, blocks only a fraction of hate terms, and is mostly not applied.\nAccording to The Markup, Google\u2019s YouTube blocklist of hate terms is inconsistent and highly permeable, allowing advertisers to target people in terms such as 'white lives matter' and 'white power' but blocking them from running ads against terms such as 'Black Lives Matter. \nIt also found that less than a third of the 86 hate-related terms tested were blocked.\nGoogle later said it had blocked additional terms associated with hate speech from being used as ad keywords on YouTube videos. The company said it does not publicly state how it develops its enforcement tools so that bad actors are less likely to game its system.\nThe episode raised questions about Google's commitment to user safety, and the extent to which it permitted the monetisation of hate content to boost its own profits. \nSystem \ud83e\udd16\nCanvas LMS website\nCanvas LMS Wikipedia profile\nOperator: Alphabet/Google/YouTube\nDeveloper: Alphabet/Google/YouTube\nCountry: USA\nSector: Technology\nPurpose: Identify & block offensive ads\nTechnology: Advertising management system\nIssue: Accuracy/reliability; Bias/discrimination - race\nTransparency: Governance\nInvestigations, audits, assessments \ud83e\uddd0\nThe Markup. Google Has a Secret Blocklist that Hides YouTube Hate Videos from Advertisers\u2014But It\u2019s Full of Holes\nThe Markup. How We Discovered Google\u2019s Hate Blocklist for Ad Placements on YouTube\nThe Markup. Story data\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theverge.com/2021/4/9/22375702/google-updates-youtube-ad-targeting-hate-speech\nhttps://thenextweb.com/news/google-has-a-secret-blocklist-that-hides-youtube-hate-videos-from-advertisers-but-its-full-of-holes-syndication\nhttps://www.businessinsider.in/tech/apps/news/google-blocks-terms-associated-with-hate-speech-from-being-used-as-ad-keywords-on-youtube/articleshow/82004055.cms\nhttps://www.morningbrew.com/marketing/stories/2021/04/12/googles-blocklist-full-questionable-holes\nhttps://decode.org/news/decoder-newsletter-social-media-policy-at-home-and-abroad/\nhttps://searchengineland.com/google-tips-the-scales-in-its-own-favor-but-do-marketers-care-tuesdays-daily-brief-347670\nRelated \ud83c\udf10\nTikTok creators hate speech detection, bias\nTikTok #intersex hashtag 'censorship'\nPage info\nType: Incident\nPublished: April 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/cbse-india-student-facial-matchingrecognition", "content": "Privacy advocates slam India's CBSE for facial matching process \nOccurred: October 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nIndia's Central Board of Secondary Education (CBSE) was criticised for using 'facial matching' to identify students accessing and downloading academic documents stored on DigiLocker without using ADHAAR and their telephone numbers.\nTo access the documents, students had to verify their identities via 'Facial Recognition System, a so-called 'state of the art' facial recognition system provided by India's National e-Governance Division, that matches a human face from a facial dataset stored on a CBSE database.\nThe CBSE said it expected the system would 'immensely' help foreign students and those who are unable to use or open DigiLocker - an Aadhaar-based cloud-based locker - should they not have an Adhaar Card or use the wrong mobile number. \nHowever, as noted by MediaNama, the system did not have a privacy policy at launch. The CBSE later said its facial recognition system did not have a privacy policy because it is a 'simple face matching process', something confirmed in its response to a digital rights group Internet Freedom Foundation Right to Information (RTI) request.\nThis was seen to have failed to explain why the technology, which the CBSE refused to provide information about, was described as a facial recognition system. The Internet Freedom Foundation described the CBSE's 'face matching technology is just facial recognition in disguise.'\nSystem \ud83e\udd16\nCentral Board of Secondary Education website\nCentral Board of Secondary Wikipedia profile\nDocuments \ud83d\udcc3\nCentral Board of Secondary Education. Availability of Digital Academic Documents using \u201cFace Matching Technology\u201d (pdf)\nOperator: Central Board of Secondary Education (CBSE)\nDeveloper: National e-Governance Division\nCountry: India\nSector: Education\nPurpose: Access documents\nTechnology: Facial recognition\nIssue: Accuracy/reliability; Privacy; Security\nTransparency: Governance; Marketing; Privacy\nFreedom of information requests \ud83d\udd26\nCentral Board of Secondary Education. Response to Anushka Jain\nResearch, advocacy \ud83e\uddee\nInternet Freedom Foundation. Panoptic Tracker - Central Board of Secondary Education\nInternet Freedom Foundation. A rose by any other name? CBSE's face matching technology is just facial recognition in disguise\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://timesofindia.indiatimes.com/education/news/cbse-introduces-facial-recognition-system-for-accessing-digital-documents/articleshow/78809577.cms\nhttps://www.medianama.com/2020/10/223-cbse-facial-recognition/\nhttps://www.medianama.com/2020/11/223-cbse-facial-recognition-privacy-policy\nhttps://indianexpress.com/article/education/cbse-introduces-facial-recognition-system-for-accessing-digital-documents-cbse-nic-in-6838840/\nhttps://www.hindustantimes.com/education/cbse-introduces-facial-recognition-system-for-accessing-digital-academic-documents-of-class-10-and-12/story-XmoqbgNRCeVD9X91zzxFzM.html\nhttps://www.ndtv.com/education/cbse-introduces-facial-recognition-for-accessing-classes-10-12-documents\nhttps://www.biometricupdate.com/202010/privacy-concerns-greet-adoption-of-facial-recognition-system-by-indias-secondary-education-board\nhttps://www.reddit.com/r/india/comments/mry4fk/ask_cbse_to_stop_using_facial_recognition/\nRelated \ud83c\udf10\nDelhi government schools facial recognition\nHyderabad police COVID-19 facial recognition\nPage info\nType: Incident\nPublished: January 2023", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ircc-immigration-and-visa-applications-automation", "content": "IRCC immigration and visa application AI screening\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nImmigration, Refugees and Citizenship Canada (IRCC) has been running an 'advanced data analytics' and artificial intelligence (AI) pilot since 2018 to process temporary resident visa (TRVs) applications submitted from China and India. \nThe system was extended to TRV applications submitted from all countries outside Canada in January 2022.\nVisa applications are assessed on the basis of eligibility and admissibility. For straightforward applications, eligibility is approved solely by the model, while eligibility for more complex applications is decided upon by an immigration officer. \nAll applications are reviewed by an immigration officer.\nSystem \ud83e\udd16\nImmigration, Refugees and Citizenship Canada\nOperator: Immigration, Refugees and Citizenship Canada (IRCC)\nDeveloper: Immigration, Refugees and Citizenship Canada (IRCC)\nCountry: Canada\nSector: Govt - immigration\nPurpose: Process temporary resident visa applications\nTechnology: Data analytics; Machine learning\nIssue: Bias/discrimination - gender, race; Fairness; Privacy\nTransparency: Governance; Black box; Marketing\nRisks and harms \ud83d\uded1\nIRCC's immigration and visa application AI screening system has attracted concerns about its impact on human rights and its transparency.\nTransparency and accountability \ud83d\ude48\nThe IRCC's TRV application assessment system has been shrouded in secrecy since its inception. \nDuring the Citizen Lab report\u2019s research phase, 27 distinct official information requests were submitted to the Government of Canada. Every one of them remained unanswered.\nAnd while the publication of the IRCC's algorithmic impact assessment provided further information about the system, many questions were limited to 'yes' or 'no' answers.\nIncidents and issues \ud83d\udd25\nJanuary 2022. The IRCC published an algorithmic impact assessment of its temporary resident visa application assessment system, concluding that the impact level of the system is moderate. However, the assessment has been described as 'noticeably short and trite in detail', noticeably regarding how its scoring system works, the identity and degree of involvement of external stakeholders, and transparency.\nSeptember 2018. A report (pdf) by University of Toronto's Citizen Lab concluded the IRCC's pilot threatened to violate domestic and international human rights law in the form of bias, discrimination, privacy breaches, due process, and procedural fairness. The report cautioned that experimenting with these technologies in the immigration and refugee system amounts to a 'high-risk laboratory,' as many of these applications come from some of the world\u2019s most vulnerable people, including those fleeing persecution and war zones. Citizen Lab called on the national authorities to freeze the development of AI-based systems until a government standard and oversight bodies are established. \nResearch, advocacy \ud83e\uddee\nhttps://citizenlab.ca/wp-content/uploads/2018/09/IHRP-Automated-Systems-Report-Web-V2.pdf\nhttps://citizenlab.ca/2018/09/bots-at-the-gate-human-rights-analysis-automated-decision-making-in-canadas-immigration-refugee-system/\nhttps://www.ourcommons.ca/Content/Committee/441/CIMM/Brief/BR11713740/br-external/BellissimoLawGroup-e.pdf\nhttps://www.sciencedirect.com/science/article/pii/S2666659621000160\nhttps://www.torontomu.ca/content/dam/centre-for-immigration-and-settlement/tmcis/publications/workingpapers/2021_9_Nalbandian_Lucia_Using_Machine_Learning_to_Triage_Canadas_Temporary_Resident_Visa_Applications.pdf\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.cpac.ca/en/programs/headline-politics/episodes/64723274/\nhttps://ipolitics.ca/2018/09/26/artificial-intelligence-at-border-could-infringe-on-human-rights-report/\nhttps://www.theglobeandmail.com/politics/article-ottawas-use-of-ai-for-immigration-a-high-risk-laboratory-report/\nhttps://www.cbc.ca/news/politics/human-rights-ai-visa-1.4838778\nhttps://globalnews.ca/news/4487724/canada-artificial-intelligence-human-rights/\nhttps://www.rcinet.ca/fr/2018/09/27/utilisation-intelligence-artificielle-immigration-danger-mortel-canada-citizen-lab/\nhttps://www.thestar.com/news/gta/2018/09/26/researchers-raise-alarm-over-use-of-artificial-intelligence-in-immigration-and-refugee-decision-making.html\nhttps://www.canadianlawyermag.com/news/general/report-says-use-of-ai-could-be-violating-human-rights/275501\nhttps://www.cigionline.org/articles/using-ai-immigration-decisions-could-jeopardize-human-rights\nhttps://www.cicnews.com/2021/03/new-tool-could-help-immigrants-decide-where-to-live-in-canada-0317588.html\nhttps://www.globallegalpost.com/big-stories/report-says-canadian-government-uses-immigrants-as-ai-lab-rats-76252680/\nhttps://theconversation.com/canada-should-be-transparent-in-how-it-uses-ai-to-screen-immigrants-157841\nhttps://www.compas.ox.ac.uk/2020/how-ai-is-being-used-in-canadas-immigration-decision-making/\nhttps://www.nationalmagazine.ca/en-ca/articles/legal-market/legal-tech/2022/automatic-for-the-people\nRelated \ud83c\udf10\nNew Zealand immigrant overstayer predictions\nCBSA Toronto Pearson airport facial recognition\nPage info\nType: System\nPublished: January 2023\nLast updated: May 2024", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/berlin-s%C3%BCdkreuz-rail-station-algorithmic-surveillance", "content": "Berlin S\u00fcdkreuz rail station algorithmic surveillance \nReleased: August 2017\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nBerlin S\u00fcdkreuz is a rail station and transport interchange junction in southern Berlin that has been used by a number of recent German governments as a laboratory for live biometric and other forms of surveillance.\nIn August 2017, Germany's Ministry of the Interior started a six-month pilot, later extended to twelve months, to assess the facial recognition capabilities of three systems tracking 312 volunteers wearing transponders and who were added to a special police database.\nSystem \ud83e\udd16\nDeutsche Bahn Wikipedia profile\nBerlin S\u00fcdkreuz station facial recognition trial Wikipedia profile\nOperator: Bundespolizei (BPOL); Deutsche Bahn\nDeveloper: Dell/Herta Security; AnyVision; IDEMIA; IBM; Hitachi; Funkwerk; G2K Group\nCountry: Germany\nSector: Govt - transport\nPurpose: Strengthen law enforcement \nTechnology: Behavioural analysis; CCTV; Computer vision; Facial recognition; Object recognition; Neural network; Deep learning; Machine learning\nIssue: Accuracy/reliability; Privacy; Surveillance\nTransparency: Governance; Marketing\nRisks and harms \ud83d\uded1\nThe use of facial recognition and other forms of surveillance at Berlin S\u00fcdkreuz station has raised concerns including loss of human rights, including privacy, bias, inaccuracy, and potential for misuse. \nIncidents and issues \ud83d\udd25\nJune 2019. Authorities started a pilot at Berlin-S\u00fcdkreuz to test algorithms supplied by IBM, Hitachi, Funkwerk and G2K Group to detect suspicious behaviour focused on six scenarios, including unattended luggage, acts of violence, people lying down or entering blocked areas such as construction sites. The project again involved volunteers who were asked to do things to attract the attention of the systems. The pilot provoked another round of negative media coverage.\nDecember 2017. Interim results of the pilot published in December 2017 indicated that 84.7 percent of people were correctly identified by the three systems, a figure contested by activists. The test was described by Florian Gallwitz, a facial recognition expert at the Nuremberg Institute of Technology, as 'a clear failure'.\nResearch, advocacy \ud83e\uddee\nFontes C., Hohma E., Corrigan C.C., L\u00fctge C. (2022). AI-powered public surveillance systems: why we (might) need them and how we want them\nReport for the Greens/EFA in the European Parliament (2021). Biometric and Mass Surveillance in EU Member States\nEireiner A., V. (2020). Imminent dystopia? Media coverage of algorithmic surveillance at Berlin-S\u00fcdkreuz\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nBundespolizeipr\u00e4sidium Potsdam (2018). Biometrische Gesichtserkennung (pdf)  \nDeutscher Bundestag (2018). Answers to Written Questions\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reuters.com/article/us-germany-security-idUSKBN1AH4VR\nhttps://www.telegraph.co.uk/news/2017/08/02/facial-recognition-software-catch-terrorists-tested-berlin-station/\nhttps://www.politico.eu/article/berlin-big-brother-state-surveillance-facial-recognition-technology/\nhttps://www.dw.com/en/germanys-facial-recognition-pilot-program-divides-public/a-40228816\nhttps://www.dw.com/en/big-brother-in-berlin-face-recognition-technology-gets-tested/a-39912905\nhttps://www.dw.com/en/in-germany-controversy-still-surrounds-video-surveillance/a-50976630\nhttps://www.dw.com/en/facial-recognition-surveillance-test-extended-at-berlin-train-station/a-41813861\nhttps://automatingsociety.algorithmwatch.org/report2020/germany/\nhttps://fortune.com/2020/02/02/facial-recognition-police-privacy-bias-germany-uk/\nhttps://www.spiegel.de/netzwelt/netzpolitik/berlin-gesichtserkennung-am-suedkreuz-ueberwachung-soll-ausgeweitet-werden-a-1232878.html\nhttps://www.euractiv.com/section/data-protection/news/german-ministers-plan-to-expand-automatic-facial-recognition-meets-fierce-criticism/\nRelated \ud83c\udf10\nMainz police Luca COVID-19 abuse\nOxford Town Centre dataset\nPage info\nType: System\nPublished: February 2023\nLast updated: May 2024", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/suresnes-abnormal-situation-surveillance", "content": "Suresnes 'abnormal situation' surveillance plan criticised as invasive\nReleased: April 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe announcement that a Paris commune would start using facial analysis and other tools to spot abnormal events was criticised for being potentially overly intrusive.\nIn March 2021, Guillaume Boudy, mayor of Suresnes, a commune in the western suburbs of Paris, announced it would start using 10 AI-equipped CCTV cameras to strengthen security and safety of its inhabitants and the cleanliness of its environment by spotting 'abnormal events' ('\u00e9v\u00e9nements anormaux').\nThe eighteen-month test would be operated by XXII Group, a local video analysis platform provider which had been given the right to use the cameras to train its facial analysis and other relevant algorithms.\nThe decision was criticised by civil rights and privacy organisations such as Le Quadrature du Net and TechnoPolice as opaque and potentially intrusive, and that it constituted an inappropriate and unethical commercialisation of public space and personal privacy.\nConcerns were also raised about the efficacy of the system, and how false positives would be handled.\nSystem \ud83e\udd16\nhttps://www.xxii.fr/\nhttps://suresnes-mag.fr/agir/actus/suresnes-teste-une-videoprotection-plus-intelligente-pour-la-securite-de-ses-habitants/\nOperator: Ville de Suresnes\nDeveloper: XXII Group\nCountry: France\nSector: Govt - municipal; Govt - police\nPurpose: Strengthen law enforcement\nTechnology: Facial analysis\nIssue: Accuracy/reliability; Dual/multi-use; Privacy; Surveillance\nTransparency: Governance; Privacy\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://actu.fr/ile-de-france/suresnes_92073/hauts-de-seine-a-suresnes-la-videosurveillance-va-detecter-les-comportements-suspects_40311951.html\nhttps://france3-regions.francetvinfo.fr/paris-ile-de-france/hauts-de-seine/a-suresnes-la-mairie-veut-utiliser-l-ia-pour-reperer-les-evenements-anormaux-2035579.html\nhttps://www.nextinpact.com/article/45514/suresnes-veut-detecter-comportements-suspects\nhttps://www.leparisien.fr/hauts-de-seine-92/suresnes-92150/videosurveillance-intelligente-a-suresnes-j-ai-peur-des-derives-03-04-2021-8430583.php\nhttps://www.larevuedudigital.com/suresnes-dans-les-starting-blocks-pour-tester-une-ia-de-video-protection-de-la-ville/\nhttps://www.20minutes.fr/societe/3022811-20210419-suresnes-intelligence-artificielle-bientot-service-videosurveillance\nhttps://www.lci.fr/high-tech/suresnes-cannes-deconfinement-la-videosurveillance-au-service-du-respect-des-gestes-barrieres-2152645.html\nhttps://technopolice.fr/blog/les-suresnois%C2%B7es-nouveaux-cobayes-de-la-technopolice/\nRelated \ud83c\udf10\nBelgrade Safe City surveillance system\nLucknow 'women in distress' facial recognition\nPage info\nType: Issue\nPublished: February 2023", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/rcmp-british-colombia-facial-recognition-procurement-opacity", "content": "RCMP British Colombia criticised over facial recognition opacity\nOccurred: April 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe Royal Canadian Mounted Police of British Columbia secretly subscribed to a facial recognition service that claims to help identify terrorists, prompting concerns about privacy and transparency\nSigned in 2016, the deal enabled the RCMP to access a 700,000-image database created by US-based IntelCenter using facial recognition developed by Morpho. Morpho was later bought by and renamed as IDEMIA. \nThe database appeared to be at least partially made up of images scraped from social media platforms, according to Kate Robertson of the University of Toronto\u2019s Citizen Lab - similar to Clearview AI, also covertly used by the RCMP.\nIn addition to hiding the deal from the public, emails obtained by The Tyee show the RCMP broke its own rules by not alerting senior officers of the sole-sourced contract, and that it avoided labelling the software with terms that might trigger more oversight, such as \u2018facial recognition\u2019 or \u2018biometric.\u2019 \nPrivacy and civil rights experts are concerned facial recognition systems can produce false positives with particular bias against racialised individuals and can unlawfully incriminate Canadians and foreigners. It is unclear how the images are harvested, raising a range of concerns about privacy and accuracy, they say.\nThe RCMP in British Colombia told The Tyee it bought the software to test its feasibility. The contract came to an end in 2019. \nSystem \ud83e\udd16\nIntelCenter Terrorist Facial Recognition website\nIntelCenter Wikipedia profile\nIDEMIA facial recognition website\nIDEMIA Wikipedia profile\nOperator: Royal Canadian Mounted Police (RCMP)\nDeveloper: IntelCenter; IDEMIA/Morpho\nCountry: Canada\nSector: Govt - police\nPurpose: Strengthen law enforcement\nTechnology: Facial recognition\nIssue: Accuracy/reliability; Bias/discrimination - race, ethnicity; Ethics; Privacy\nTransparency: Governance; Marketing\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nHouse of Commons, Canada (2022). FACIAL RECOGNITION TECHNOLOGY AND THE GROWING POWER OF ARTIFICIAL INTELLIGENCE (pdf)\nInternational Civil Liberties Monitoring Group (2022). Submission to the National Security and Intelligence Committee of Parliamentarians\u2019 Review of the RCMP\u2019s Federal Policing Mandate (pdf) \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://thetyee.ca/News/2021/04/28/RCMP-Secret-Facial-Recognition-Tool-Looked-Matches-Terrorists/\nhttps://www.politico.com/news/2022/09/30/rcmps-facial-recognition-clearview-ai-00059639\nhttps://www.biometricupdate.com/202104/police-facial-recognition-on-images-from-us-protest-social-media-in-canada-questioned\nhttps://findbiometrics.com/report-finds-rcmp-misled-public-broke-internal-rules-with-facial-recognition-contract-043006/\nhttps://iapp.org/news/a/british-columbias-rcmp-breaks-own-facial-recognition-rules/\nhttps://www.bignewsnetwork.com/news/269081413/canada-should-be-transparent-in-how-it-uses-ai-to-screen-immigrants\nhttps://citizenlab.ca/2021/04/the-tyee-rcmp-secret-facial-recognition-tool-looked-for-matches-with-700000-terrorists/\nRelated \ud83c\udf10\nRCMP AI facial recognition surveillance\nUS Postal Inspection Service iCOP covert monitoring and surveillance\nPage info\nType: Incident\nPublished: January 2023\nLast updated: August 2023", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ningbo-real-estate-facial-recognition", "content": "Ningbo real estate companies fined for illegal facial recognition use\nOccurred: April 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA regulator in the city of Ningbo, in China's eastern Zhejiang province, fined three real estate companies for 'illegally acquiring customers\u2019 facial information'.\nAccording to the South China Morning Post, the three firms - China Poly Group, Sunac China Holdings, and Greenland Holdings - were fined Yuan 250,000 (USD 38,500) for violating China's consumer protection law by installing facial recognition devices at sales offices to identify customers, without informing them or obtaining their consent.\n\nPer the SCMP, residential projects that don\u2019t sell well in China often offer a discount to property agencies to get more buyers through the door. Capturing visitor\u2019s facial information enables sales offices to determine who was returning with an agent, even if they are wearing a face mask.\nChina\u2019s annual Consumer Protection Gala had earlier singled out international companies, including BMW, Kohler, and MaxMara - for the misuse of facial recognition.\nSystem \ud83e\udd16\nUnknown\nOperator: China Poly Group; Sunac China Holdings; Greenland Holdings\nDeveloper: Unclear/unknown\nCountry: China\nSector: Real estate\nPurpose: Identify customer identity\nTechnology: Facial recognition\nIssue: Privacy; Dual/multi-use; Surveillance\nTransparency: Governance; Privacy\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.scmp.com/tech/tech-trends/article/3131442/increasing-use-facial-recognition-technology-china-faces-backlash\nhttps://www.biometricupdate.com/202104/chinese-firms-fined-for-facial-recognition-overuse-more-city-govts-mull-restrictions\nhttps://findbiometrics.com/chinese-real-estate-developers-get-fined-unlawful-use-facial-recognition-050303/\nhttps://news.cgtn.com/news/2020-12-02/China-s-home-buyers-find-their-moment-of-facial-recognition-reckoning--VSXeqfB51C/index.html\nRelated \ud83c\udf10\nKohler, BMW, MaxMara China shopper analysis facial recognition\nXPeng stores customer facial recognition\nPage info\nType: Incident\nPublished: February 2023", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/uk-passport-photo-application-racism", "content": "UK passport application system fails to recognise man's black skin\nOccurred: April 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe UK Home Office was accused of racism after a Black man was prevented from registering for a UK passport on its website because the software was unable to recognise his skin colour from the grey wall behind him.\nModel and racial justice activist Joris Lech\u00eane took to TikTok to complain that his photo 'was rejected because the artificial intelligence software wasn\u2019t designed with people of my phenotype in mind.'\nThe UK's Passport Office had earlier told the New Scientist that an update to the system had been available for more than a year but had not yet been rolled out.\nFurthermore, Home Office documents released under a Freedom of Information (FOI) request show it knew its passport photo system failed to work well for some ethnic minority people but decided to use it anyway. \nSystem \ud83e\udd16\nUnknown\nOperator: UK Home Office\nDeveloper: \nCountry: UK\nSector: Govt - immigration\nPurpose: Check photograph\nTechnology: Facial detection; Facial analysis; Computer vision\nIssue: Accuracy/reliability; Bias/discrimination - race, ethnicity\nTransparency: Governance; Complaints/appeals\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.tiktok.com/@joris_explains/video/6954036663580495109 \nhttps://www.independent.co.uk/life-style/ai-racist-robots-algorithm-tiktok-b1838521.html\nhttps://www.newsweek.com/black-man-tiktok-artificial-intelligence-racism-1587018\nhttps://www.dailydot.com/irl/black-tiktoker-passport-facial-recognition/\nhttps://www.unilad.co.uk/viral/man-proves-artificial-intelligence-is-racist-by-attempting-to-get-passport-picture-approved/\nhttps://futurism.com/the-byte/uk-passport-ai-racist-dark-skin\nhttps://www.newscientist.com/article/2219284-uk-launched-passport-photo-checker-it-knew-would-fail-with-dark-skin/\nhttps://thenextweb.com/news/black-man-says-racially-biased-ai-system-rejected-his-passport-photo-facial-recognition-tiktok\nhttps://www.blackenterprise.com/tiktok-model-exposes-how-artificial-intelligence-is-biased-toward-black-people/\nhttps://www.dailyadvent.com/news/cc7c48203ddde869d5f4c37b4f356a5c-TikTok-Model-Exposes-How-Artificial-Intelligence-is-Biased-Toward-Black-People\nRelated \ud83c\udf10\nMet Police Gangs Violence Matrix\nEngland footballers' racism Instagram moderation\nPage info\nType: Incident\nPublished: February 2023", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ndis-independent-assessments-robo-planning", "content": "NDIS independent assessments ditched by Australian govt\nOccurred: March 2021-\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe National Disability Insurance Scheme (NDIS) is Australia's system for publicly supporting people with disabilities. Legally created in 2013, it was launched in 2020 under the auspices of the National Disability Insurance Agency (NDIA).\nIn 2021, the Australian government proposed introducing independent assessments for NDIS participants over the age of 7. Assessors had to be qualified independent health professionals making assessments of 1 to 4 hours using standardised assessment tools that would feed into an algorithm that decides a 'personalised budget' for each applicant.\nDepersonalised 'robo-planning'\nThe Australian government had argued the scheme would reduce inequality and improve the consistency of decision-making. But it quickly ran into technical and political headwinds, with disability groups saying they had not been properly consulted.\nBruce Bonyhady, the inaugural chairman of the NDIA and an original architect of the scheme, slammed (pdf) the policy as 'robo-planning' built on insufficient evidence the new tools adequately assess disability and which 'puts people in boxes before they have had a chance to outline what they would like to achieve or the ways in which they hope their lives change.'\nCost-cutting accusations\nFurthermore, the Australian government claimed publicly the scheme was not a cost-cutting measure, but leaked documents revealed it would deliver AUD 700 million reduction in funds allocated for disability support. \nIn July 2021, NDIS Minister Linda Reynolds said the federal government would not push ahead with the proposal.\nSystem \ud83e\udd16\nNDIS website\nNDIS - Independent Assessments Wikipedia profile\nOperator: National Disability Insurance Agency (NDIA)\nDeveloper: National Disability Insurance Agency (NDIA)\nCountry: Australia\nSector: Govt - welfare\nPurpose: Assess disability funding eligibility\nTechnology:  \nIssue: Bias/discrimination - income; Dual/multi-use\nTransparency: Governance: Black box\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nhttps://www.aph.gov.au/DocumentStore.ashx?id=92a45ded-c00c-4946-9776-812353f486df&subId=703536\nResearch, advocacy \ud83e\uddee\nMelbourne Disability Institute (2021). An analysis of NDIS' proposed approach to Independent Assessments (pdf)\nPeople with Disability Australia (2020). We are concerned about 'Independent Assessments' for the NDIS\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.smh.com.au/national/robo-planning-a-disgrace-ndis-architect-slams-independent-assessments-20210423-p57lp8.html\nhttps://www.abc.net.au/news/2021-03-18/ndis-architect-bruce-bonyhady-slams-independent-assessments/13256160\nhttps://www.innovationaus.com/robo-planning-will-blow-up-ndis-key-architect/\nhttps://www.innovationaus.com/citizen-centric-demolished-by-ndis-algorithms/\nhttps://www.abc.net.au/news/2021-04-23/former-ndis-chairman-slams-changes-to-support-scheme/100091348\nhttps://www.canberratimes.com.au/story/7222314/a-disgrace-ndis-architect-slams-robo-planning-proposal/\nhttps://au.news.yahoo.com/inquiry-ndis-independent-assessments-173026750.html\nhttps://www.innovationaus.com/shorten-calls-for-end-to-ndis-robo-planning/\nhttps://www.theguardian.com/australia-news/2021/apr/24/robo-planning-of-ndis-assessments-would-save-government-700m\nhttps://www.abc.net.au/news/2021-07-09/ndis-disability-independent-assessments-model-dead-after-meeting/100277324\nRelated \ud83c\udf10\nUK DWP disability benefits fraud algorithm\nArkansas DHS ARChoices RUGs algorithm\nPage info\nType: Issue\nPublished: February 2023", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/facebook-blocks-resignmodi-hashtag", "content": "Facebook #resignmodi block prompts controversy\nOccurred: April 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nPosts calling for the resignation of Indian Prime Minister Narendra Modi were hidden by Facebook, prompting accusations of censorship.\nBuzzfeed reported that Facebook had temporarily hidden posts calling for the resignation of Indian Prime Minister Narendra Modi, and that posts with the hashtag #ResignModi had been labelled 'temporarily hidden' because 'some content in those posts goes against our Community Standards.'\nThe Prime Minister and his government had been widely criticised for the quality of its response to the COVID-19 pandemic. Tens of thousands of Indians had died, many of them unnecessarily, it is thought. \nHowever, the platform had failed to disclose which Community Standards may have been breached, despite Facebook's stated commitment to transparency. Facebook subsequently reversed its decision shortly after Buzzfeed had published its story. \nFacebook spokesperson Andy Stone told BuzzFeed 'We temporarily blocked this hashtag by mistake, not because the Indian government asked us to, and have since restored it.'\n\u2795 A few days before, Modi's government had ordered Twitter to block over 50 tweets that criticised how was handling the pandemic - a request to which Twitter complied and which also led to accusations of censorship and criticism of the platform.\nSystem \ud83e\udd16\nFacebook website\nDocuments \ud83d\udcc3\nFacebook Community Standards\nOperator: Meta/Facebook\nDeveloper: Meta/Facebook\nCountry: India\nSector: Politics; Health\nPurpose: Moderate content\nTechnology: Content moderation system\nIssue: Freedom of expression - censorship\nTransparency: Governance; Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.buzzfeednews.com/article/ryanmac/facebook-blocking-posts-hashtag-resign-modi\nhttps://www.bloombergquint.com/business/facebook-briefly-blocks-resignmodi-as-india-covid-crisis-grows\nhttps://www.wsj.com/articles/facebook-blocks-then-restores-content-calling-on-indian-prime-minister-modi-to-resign-11619652354\nhttps://www.theguardian.com/technology/2021/apr/28/facebook-blocked-resignmodi-hashtag-india-coronavirus\nhttps://www.hindustantimes.com/india-news/facebook-unblocks-resignmodi-says-it-was-blocked-by-mistake-101619677901654.html\nhttps://www.engadget.com/facebook-blocks-resignmodi-hashtag-200853470.html\nhttps://thenextweb.com/news/facebook-says-it-blocked-resignmodi-posts-by-mistake-but-it-hardly-seems-like-an-accident\nhttps://thetechportal.com/2021/04/29/facebook-controversially-blocks-resignmodi-unblocks-it-hours-after-public-outcry/\nhttps://www.business-standard.com/article/technology/facebook-blocks-resignmodi-posts-restores-it-calling-it-a-mistake-121042900596_1.html\nhttps://economictimes.indiatimes.com/tech/technology/facebook-blocks-resignmodi-posts-for-hours-as-indias-covid-crisis-grows/articleshow/82304348.cms\nRelated \ud83c\udf10\nTek Fog political manipulation, harassment\nProfessor Meareg Amare Abrha doxxing, murder\nPage info\nType: Issue\nPublished: February 2023", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/indian-government-censors-covid-19-twitter-posts", "content": "Indian government censors COVID-19 Twitter posts\nOccurred: April 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe removal of tweets about COVID-19 on the order of the Indian government  prompted lawmakers and human rights activists to accuse the government of censorship and further putting peoples' health at risk. \nThe discovery that the government had made an emergency order to censor tweets was revealed on Lumen, which publishes details of legal takedown notices social media companies and others receive from governments and private entities across the world. The order was first revealed by MediaNama.\nLocal reports indicated the banned material includes a tweet from Pawan Khera, a spokesman for India\u2019s main opposition party, the Indian National Congress (INC), who accused Modi of holding political rallies while the virus raged and failing to acknowledge that they likely contributed directly to the spread of COVID-19.\nOthers on the list included posts by INC parliamentarian Revanth Reddy, West Bengal state minister Moloy Ghatak, and filmmakers Vinod Kapri and Avinash Das. A number of tweets castigated Modi for failing to fix India's healthcare system, which has run out of beds, oxygen and medicines, leading to a humanitarian disaster.\nA Twitter spokesperson told DW, 'When we receive a valid legal request, we review it under both the Twitter Rules and local law. If the content violates Twitter's Rules, the content will be removed from the service. If it is determined to be illegal in a particular jurisdiction, but not in violation of the Twitter Rules, we may withhold access to the content in India only.' \nA few days later, Facebook was discovered to have been temporarily hiding posts calling for the resignation of Indian Prime Minister Narendra Modi.\nSystem \ud83e\udd16\nTwitter website\nTwitter Wikipedia profile\nhttps://help.twitter.com/en/rules-and-policies/twitter-rules\nOperator: Twitter\nDeveloper: Twitter\nCountry: India\nSector: Politics; Health\nPurpose: Moderate content\nTechnology: Content moderation system\nIssue: Freedom of expression - censorship\nTransparency: Governance; Black box\nResearch, advocacy \ud83e\uddee\nhttps://lumendatabase.org/notices/search?utf8=%E2%9C%93&term=twitter+india&sort_by=\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://timesofindia.indiatimes.com/india/ordered-only-fake-covid-posts-blocked-not-critical-ones-it-ministry/articleshow/82249535.cms\nhttps://www.bbc.co.uk/news/world-asia-56883483\nhttps://www.reuters.com/article/health-coronavirus-india-twitter/update-1-india-asks-twitter-to-take-down-some-tweets-critical-of-its-covid-19-handling-idUSL1N2MI0FS\nhttps://www.nytimes.com/2021/04/25/business/india-covid19-twitter-facebook.htmlhttps://www.wsj.com/articles/india-accused-of-censorship-for-blocking-social-media-criticism-amid-covid-surge-11619435006\nhttps://www.washingtonpost.com/world/2021/04/26/twitter-india-coronavirus/\nhttps://www.businessinsider.com/twitter-censors-covid-19-tweets-at-indian-governments-request-2021-4\nhttps://www.dw.com/en/twitter-censors-tweets-critical-of-indias-covid-response/a-57325737\nhttps://www.aljazeera.com/news/2021/4/25/india-asks-twitter-to-take-down-tweets-critical-of-covid-handling\nhttps://techcrunch.com/2021/04/24/india-orders-twitter-to-take-down-tweets-critical-of-its-coronavirus-handling/\nRelated \ud83c\udf10\nFacebook blocks #resignmodi hashtag\nTek Fog political manipulation, harassment\nPage info\nType: Incident\nPublished: February 2023", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/leonid-volkov-deepfake-video-calls", "content": "'Leonid Volkov' deepfake video calls target European politicians\nOccurred: April 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nSenior EU and UK politicians and officials were targeted by a campaign in which bad actors appeared to use deepfake technology whilst pretending to be Russian opposition leaders. \nThe attack took the form of video calls involving an imposter claiming to be Leonid Volkov, chief of staff to Alexy Navalny, in which the Russia's annexation of Crimea was discussed. \nThe identity and motivation of the actors remains unclear, though Volkov told The Guardian that it may have been an attempt to discredit Russian opposition head Navalny.\nAmong those targeted included Tom Tugendhat, then chair of the UK's foreign affairs select committee, chairman of the foreign affairs committee of the Estonian parliament Marko Mihkelson, Rihards Kols, chairman of the Latvian parliament's foreign affairs committee, and the Dutch parliament's foreign affairs committee.\n\u2795 It later emerged that the incident was a prank, with no effects - deepfake or otherwise - used.\nSystem \ud83e\udd16\nUnknown\nOperator: Government of Russia\nDeveloper: Government of Russia\nCountry: Estonia; Latvia; Lithuania; Netherlands; UK\nSector: Politics\nPurpose: Damage reputation\nTechnology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning  \nIssue: Mis/disinformation; Ethics\nTransparency: Governance; Marketing\nInvestigations, assessments, audits \ud83e\uddd0\nDeepfaked. \u2018Deepfake\u2019 that fooled EU politicians was just a look-alike\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://twitter.com/TomTugendhat/status/1384973766040637441\nhttps://www.theverge.com/2021/4/30/22407264/deepfake-european-polticians-leonid-volkov-vovan-lexus\nhttps://www.baltictimes.com/scammers_imitating_russian_opposition_also_trick_estonian_mps/\nhttps://news.yahoo.com/netherlands-deepfake-video-chat-navalny-212606049.html\nhttps://www.volkskrant.nl/nieuws-achtergrond/kamerleden-vergaderen-met-deepfake-imitatie-van-stafchef-russische-oppositieleider-navalny~b04b5322/\nhttp://lrt.lt/en/news-in-english/19/1393935/imposter-used-deepfake-to-dupe-baltic-mps-impersonate-navalny-associate\nhttps://nltimes.nl/2021/04/24/dutch-mps-video-conference-deep-fake-imitation-navalnys-chief-staff\nhttps://futurism.com/the-byte/russia-accused-using-deepfakes-imitate-political-rivals\nhttps://www.theguardian.com/world/2021/apr/22/european-mps-targeted-by-deepfake-video-calls-imitating-russian-opposition\nhttps://www.themoscowtimes.com/2021/04/23/deepfake-navalny-aide-targets-european-lawmakers-a73717\nhttps://news.err.ee/1608190012/scammers-imitating-russian-opposition-trick-estonian-mps-with-deepfake\nRelated \ud83c\udf10\nRussia-Ukraine war disinformation bot farms\nRussia 'Kyiv' deepfake influence campaign\nPage info\nType: Incident\nPublished: February 2023", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tui-airline-classifies-women-as-children", "content": "TUI airline mis-classifies women as children\nOccurred: July 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA software upgrade by TUI Airways which classified female passengers whose title was 'Miss' as children led to a 'serious incident' on a Birmingham-Majorca flight.\nThe mis-classification meant that the average weight of these female passengers used for take-off calculations for the Boeing 737 jet was 1,244 kg lighter than it actually was, potentially having an impact on take-off thrust.\nAn investigation (pdf) into the incident by the UK government\u2019s Air Accidents Investigation Branch (AAIB) said the problem occurred after the flight reservation sheet had been updated while the airline was grounded due to COVID-19. \nAccording to the BBC, safety officials said the problem was that the software had been programmed in a foreign country where 'Miss' is used to refer to children, and 'Ms' to adult women. \nThe flight proved safe and the fault identified in TUI's IT system was later corrected. TUI also introduced manual checks to ensure adult females were referred to as Ms on relevant documentation.\nSystem \ud83e\udd16\nTUI website\nTUI Airways Wikipedia profile\nOperator: TUI Group/TUI Airways\nDeveloper: TUI Group/TUI Airways; Boeing\nCountry: UK\nSector: Transport/logistics\nPurpose: Calculate airline weight\nTechnology: IT system\nIssue: Accuracy/reliability; Safety\nTransparency: \nInvestigations, assessments, audits \ud83e\uddd0\nhttps://www.gov.uk/aaib-reports/aaib-investigation-to-boeing-737-8k5-g-tawg-21-july-2020\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/technology-56690529\nhttps://www.independent.co.uk/travel/news-and-advice/tui-plane-aaib-women-children-weight-b1828500.html\nhttps://www.theregister.com/2021/04/08/tui_software_mistake/\nhttps://metro.co.uk/2021/04/09/plane-took-off-overweight-after-people-titled-miss-logged-as-kids-14381182/\nhttps://www.irishtimes.com/life-and-style/travel/europe/flight-in-serious-incident-after-every-miss-on-board-assigned-child-s-weight-1.4532856\nhttps://ia.acs.org.au/article/2021/software-error-caused--serious-incident--for-airline.html\nhttps://www.dailymail.co.uk/news/article-9450161/TUI-flight-took-Birmingham-Airport-1-200kg-overweight-glitch.html\nhttps://www.travelweekly.com.au/article/serious-incident-flight-sees-female-passengers-classified-children/\nhttps://www.ladbible.com/news/uk-tui-plane-in-incident-after-every-miss-was-given-a-childs-weight-20210409\nhttps://www.standard.co.uk/news/uk/tui-birmingham-majorca-birmingham-airport-spanish-b928505.html\nhttps://www.theguardian.com/world/2021/apr/09/tui-plane-serious-incident-every-miss-on-board-child-weight-birmingham-majorca\nRelated \ud83c\udf10\nSouthwest Airlines crew scheduling automation\nAmazon automated pricing glitch\nPage info\nTypePublished: February 2023", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tesla-autopilot-tricked-into-driverless-driving", "content": "Tesla Autopilot is 'easily' tricked into driverless driving\nOccurred: April 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Tesla car was 'easily' tricked into driving in Autopilot mode with no one at the wheel, raising questions about the company's approach to safety.\nA Consumer Reports tester switched the Tesla Model Y to Autopilot while it was on the track, and set the speed dial to zero miles per hour so the vehicle came to a stop. \nThe driver then placed a weighted chain on the wheel to trick the car into believing a person\u2019s hand was there and moved to the passenger\u2019s seat. He turned the speed dial back up when he was in the passenger seat and the car started driving.\nJake Fisher, Consumer Reports\u2019 senior director of auto testing, said, 'In our evaluation, the system not only failed to make sure the driver was paying attention, but it also couldn\u2019t tell if there was a driver there at all.'\n\u2795 The test came a few days after a Tesla crashed in Texas, killing the two men in the car. Neither of the men were in the driver\u2019s seat at the time of the crash, according to authorities.\nSystem \ud83e\udd16\nTesla Autopilot, Full-self Driving\nOperator: Consumer Reports\nDeveloper: Tesla\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Driver assistance system\nIssue: Accuracy/reliability; Safety\nTransparency: Black box\nInvestigations, assessments, audits \ud83e\uddd0\nConsumer Reports (2021). After a fatal crash in Texas, we demonstrated how easy it is to defeat Autopilot\u2019s driver monitoring\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://thehill.com/policy/transportation/549827-consumer-reports-finds-its-easy-to-trick-tesla-into-driving-with-nobody\nhttps://nypost.com/2021/04/22/tesla-cars-can-drive-with-no-one-in-the-drivers-seat-consumer-reports-says/\nhttps://www.businessinsider.com/tesla-will-drive-with-nobody-behind-wheel-consumer-reports-2021-4\nhttps://www.cnet.com/roadshow/news/tesla-autopilot-without-a-driver-consumer-reports-video/\nhttps://www.autoblog.com/2021/04/22/tesla-model-s-crash-autopilot-read-this/\nhttps://arstechnica.com/cars/2021/04/consumer-reports-shows-tesla-autopilot-works-with-no-one-in-the-drivers-seat/\nhttps://eu.usatoday.com/story/money/cars/2021/04/22/consumer-reports-tesla-autopilot-texas-crash/7334489002/\nhttps://apnews.com/article/technology-business-b1139dc8bd2ec3179f1075d6ef77c7fb\nhttps://www.washingtonpost.com/technology/2021/04/22/tesla-autopilot/\nhttps://edition.cnn.com/videos/cars/2021/04/22/tesla-autopilot-consumer-reports-elon-musk-texas-crash-orig.cnn-business\nRelated \ud83c\udf10\nDriver abuses Tesla Autopilot by sitting in rear seat\nTesla Autopilot, FSD misleading marketing\nPage info\nType: Incident\nPublished: February 2023", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ai-satellite-location-spoofing", "content": "AI satellite images can 'easily' create fake news\nOccurred: April 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nDeepfake satellite images can easily be created and used to create misinformation and disinformation, according to a University of Washington research study.\nThe aim, author Bo Zhao told The Verge 'is to demystify the function of absolute reliability of satellite images and to raise public awareness of the potential influence of deep fake geography.'\nAs part of their study, Zhao and his colleagues created software to generate deepfake satellite images, using generative adversarial networks ('GANs'), and then created detection software that was able to spot the fakes based on characteristics like texture, contrast, and colour.\nPer PetaPixel, the authors simulated their own deepfakes using Tacoma, Washington as a base map and placed onto it features extracted from Seattle, Washington and Beijing, China. The high rises from Beijing cast shadows in the fake satellite image while the low-rise buildings and greenery were superimposed from the urban landscape found in Seattle.\nThe authors warn false satellite images could be used to create hoaxes about natural disasters, support disinformation, or mislead geo-political foes.\nSystem \ud83e\udd16\n\nOperator: \nDeveloper: Zhao, B., Zhang, Z., Xu, C., Sun, Y., Deng, C.\nCountry: USA\nSector: Technology\nPurpose: Scare/confuse/destabilise\nTechnology: Deepfake - image, video, audio; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning  \nIssue: Mis/disinformation; Dual/multi-use\nTransparency: \nResearch, advocacy \ud83e\uddee\nZhao, B., Zhang, Z., Xu, C., Sun, Y., Deng, C. (2021). Deep fake geography? When geospatial data encounter Artificial Intelligence\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.washington.edu/news/2021/04/21/a-growing-problem-of-deepfake-geography-how-ai-falsifies-satellite-images/\nhttps://www.theverge.com/2021/4/27/22403741/deepfake-geography-satellite-imagery-ai-generated-fakes-threat\nhttp://www.homelandsecuritynewswire.com/dr20210421-a-growing-problem-of-deepfake-geography-how-ai-falsifies-satellite-images\nhttps://petapixel.com/2021/04/27/deepfake-satellite-images-pose-risk-to-global-politics-and-military-report/\nhttps://www.zmescience.com/science/news-science/we-should-talk-about-deepfake-geography-fake-ai-generated-satellite-images/\nhttps://www.discovermagazine.com/technology/experts-are-worried-about-deepfake-geography\nhttps://arstechnica.com/gadgets/2021/05/deepfake-maps-could-really-mess-with-your-sense-of-the-world/\nhttps://www.wired.com/story/deepfake-maps-mess-sense-world/\nRelated \ud83c\udf10\nNATO warships AIS spoofing\nChina taxation department ID system hack\nPage info\nType: Issue\nPublished: February 2023", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/aadhaar-covid-19-facial-recognition", "content": "Aadhaar facial recognition may marginalise vulnerable people\nOccurred: April 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nPlans by the Indian government to use facial recognition integrated within the country's Aadhaar biometric ID system raised concerns that millions of vulnerable people without mobile phones or internet access would lose out on receiving their COVID-19 vaccinations.\nAccording to Reuters, a facial recognition system based on the Aadhaar ID is being tested in the eastern state of Jharkhand as a 'touchless' vaccination process and hence better way of avoiding infection. Should the pilot prove successful, the system may replace fingerprint or iris scans at COVID-19 vaccination centres across the country.\nRS Sharma, head of India's National Health Authority, was quoted as saying the system would not be mandatory, but new guidelines indicate that Aadhaar is already the 'preferred' mode of identity verification and for vaccination certificates.\nAnushka Jain, associate counsel at the Internet Freedom Foundation (IFF), told Reuters that using facial recognition at vaccine centres risks further marginalising vulnerable people who may be misidentified and refused the vaccine, and raises fears the controversial technology could become the norm at all centres.\nThe IFF filed Right to Information requests demanding to know the specific purposes for which facial recognition technology will be used, the legislation which authorises them to do so, and the total expenditure incurred to procure such technology. The IFF also sought to know the list of persons authorised to access the technology, the software and hardware being used, and details of the database. \nSystem \ud83e\udd16\nAadhaar website\nAadhaar Wikipedia profile\nOperator: Aadhaar\nDeveloper: Unique Identification Authority of India (UIDAI); National Health Authority (NHA) \nCountry: India\nSector: Govt - health\nPurpose: Verify identity\nTechnology: Facial recognition\nIssue: Accuracy/reliability; Bias/discrimination - economic; Security; Privacy\nTransparency: Governance; Black box\nResearch, advocacy \ud83e\uddee\nInternet Freedom Foundation (IFF). Digital Transparency: A Right to Information Report for April, 2021\nIFF Panoptic Tracker\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://theprint.in/health/modi-govt-now-plans-a-touchless-vaccination-process-with-aadhaar-based-facial-recognition/634719/\nhttps://www.indiatoday.in/technology/news/story/aadhaar-face-recognition-could-be-made-mandatory-for-covid-vaccination-pilot-testing-is-on-1789024-2021-04-09\nhttps://www.reuters.com/article/us-india-health-coronavirus-trfn-idUSKBN2C217V\nhttps://www.aljazeera.com/news/2021/4/16/india-vaccine-exclusion-fears-over-digital-id-face-recognition\nhttps://www.theregister.com/2021/04/09/india_facial_id_covid_vaccinations/\nhttps://thewire.in/rights/covid-19-vaccination-facial-recognition-technology-aadhaar-vaccine\nhttps://www.medianama.com/2020/10/223-aadhaar-facial-recognition/\nhttps://www.thequint.com/tech-and-auto/facial-authentication-for-covid-vaccination-is-this-a-viable-move\nRelated \ud83c\udf10\nIndian government COVID-19 Twitter censorship\nIndia citizenship law protest surveillance\nPage info\nType: Incident\nPublished: February 2023", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tesla-model-s-kills-truck-driver-pedestrian", "content": "Tesla Model S kills truck driver standing on road\nOccurred: September 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Tesla Model S hit a truck driver who had stopped on a road in Arendal, Norway. The truck driver, who was standing on the road beside his vehicle, died instantly in the accident. \nThe accident was investigated by the police and the Accident Investigation Board Norway, with the former deciding to prosecute the driver for negligent homicide.\nThe accused Tesla driver claimed in court that he was alert and that the car's Autopilot driver assistance system was turned on. \nEarlier, Tesla Norway had received an apology from the police after the electric car maker was wrongfully accused of being uncooperative during the investigation. \nSystem \ud83e\udd16\nTesla Autopilot, Full-self Driving\nOperator: \nDeveloper: Tesla\nCountry: Norway\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Driver assistance system\nIssue: Safety; Accuracy/reliability\nTransparency: Black bo\nResearch, advocacy \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nTesla Deaths\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.teslarati.com/tesla-norway-police-apology-unwarranted-criticism/\nhttps://motor.no/autopilot-nyheter-tesla/tesla-pa-auto-styring-da-mann-ble-meid-ned/188623\nhttps://motor.no/autopilot-nyheter-tesla/politiet-ga-uriktige-opplysninger-til-motor/180723\nhttps://www.agderposten.no/nyheter/dodsulykken-tesla-har-ikke-gitt-ut-data/\nhttps://www.fvn.no/nyheter/lokalt/i/56VoB1/uvisst-om-autopilot-var-i-bruk-foer-doedsulykke-paa-e-18\nhttps://www.tvedestrandsposten.no/aktor-krever-fengsel-og-tap-av-forerkort-for-tesla-sjafor/s/5-52-360688\nRelated \ud83c\udf10\nTesla Model 3 hits parked police car\nTesla Model Y crashes into tractor-trailer\nPage info\nType: Incident\nPublished: January 2023", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tesla-model-x-kills-pedestrian", "content": "Tesla Model X on Autopilot kills pedestrian outside Tokyo\nOccurred: April 2018\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nYoshihiro Umeda, a 44 year-old husband and father, died in April 2018 when a Tesla Model X with Autopilot turned on crashed into a group gathered at the site of an earlier motorcycle accident outside Tokyo.\nA car in front of the Tesla changed lanes to avoid the group of bikers, but the Tesla driver was reputedly dozing and Autopilot failed to change lanes and accelerated until it hit the group.\nUmeda's relatives filed (pdf) a lawsuit against Tesla in California's Northern District Court claiming that the company's Autopilot system is 'defective and incapable of handling common driving scenarios' and that its system for detecting drivers who aren't paying attention is 'fatally defective'. \nThe judge sided with Tesla\u2019s motion to dismiss, suggesting Japan is the most appropriate venue for the case based on 'forum non conveniens.' Forum non conveniens says that a US court can dismiss a case when another court may be better suited to hear it. \nThe case has yet to to filed in Japan. The Tesla driver was officially sentenced to three years in prison and up to five years of suspension. \nSystem \ud83e\udd16\nTesla Autopilot, Full-self Driving\nOperator: \nDeveloper: Tesla\nCountry: Japan\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Driver assistance system\nIssue: Accuracy/reliability; Safety\nTransparency: Black box\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nUmeda v Tesla\nGeorge Washington University. ETI AI Litigation Database: case details\nResearch, advocacy \ud83e\uddee\nTesla Deaths\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.carscoops.com/2020/04/tesla-autopilot-blamed-on-fatal-japanese-model-x-crash/\nhttps://www.forbes.com/sites/lanceeliot/2020/05/16/lawsuit-against-tesla-for-autopilot-engaged-pedestrian-death-could-disrupt-full-self-driving-progress/\nhttps://www.theregister.com/2020/04/30/tesla_sued_tokyo_biker_death_crash/\nhttps://www.dailymail.co.uk/news/article-8274449/Tesla-sued-family-man-44-run-killed-car-using-Autopilot.html\nhttps://boingboing.net/2020/04/30/tesla-faces-lawsuit-over-anoth.html\nhttps://www.wheelsjoint.com/tesla-model-x-involved-in-a-fatal-accident-in-japan-autopilot-activated/#:~:text=The%20Tesla%20Model%20X%20Autopilot,directly%20responsible%20for%20the%20collision.\nhttps://www.carcomplaints.com/news/2020/tesla-autopilot-lawsuit-killed-pedestrian.shtml\nhttps://www.motorbiscuit.com/tesla-autopilot-technology-killed-a-man-in-japan-according-to-this-lawsuit/\nhttps://www.newsweek.com/tesla-lawsuit-model-x-autopilot-fatal-crash-japan-yoshihiro-umeda-1501114\nhttps://www.bloomberg.com/news/articles/2020-04-29/tesla-s-autopilot-blamed-in-lawsuit-for-fatal-accident-in-japan\nhttps://news.bloomberglaw.com/product-liability-and-toxics-law/tesla-suit-over-crash-in-japan-should-stay-in-u-s-family-says\nRelated \ud83c\udf10\nTesla Model 3 hits parked police car\nTesla Model Y crashes into tractor-trailer\nPage info\nType: Incident\nPublished: February 2023", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tesla-autopilot-confused-by-billboard", "content": "Tesla Autopilot confused by billboard\nOccurred: April 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nTesla owner Andy Weedman discovered that his Tesla was registering a giant stop sign printed on a billboard as a real traffic sign, and so halting in the middle of the road.\nWeedman tweeted and recorded a video of the Tesla stopping for the billboard, referring to it as an 'edge case'. \nOthers went further, arguing that 'the world is absolutely crammed with out-of-the-ordinary situations we call edge cases,' and that it will 'likely take more than patchwork updates and fixes to actually reach full autonomy'. \nThe discovery prompted commentators to observe that autonomous cars have a long way to go before they are truly fit for purpose. \nSystem \ud83e\udd16\nTesla Autopilot, Full-self Driving\nOperator: Andy Weedman\nDeveloper: Tesla\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Driver assistance system\nIssue: Accuracy/reliability; Safety\nTransparency: Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://twitter.com/andyweedman/status/1382459653863378944\nhttps://www.youtube.com/watch?v=-OdOmU58zOw&t=2s\nhttps://futurism.com/the-byte/tesla-slamming-brakes-sees-stop-sign-billboard\nhttps://jalopnik.com/this-billboard-that-confuses-tesla-autopilot-is-a-good-1846698527\nhttps://knowtechie.com/teslas-autopilot-is-apparently-getting-confused-by-a-billboard/\nhttps://www.republicworld.com/entertainment-news/whats-viral/man-says-tesla-kept-slamming-brakes-in-area-with-no-red-lights-elon-musk-has-epic-reply.html\nhttps://wonderfulengineering.com/watch-tesla-autopilot-keeps-slamming-the-brakes-whenever-it-sees-a-billboard-with-a-stop-sign/\nRelated \ud83c\udf10\nTesla phantom braking\nTesla Model 3 hits parked police car\nPage info\nType: Issue\nPublished: February 2023", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tesla-model-s-crashes-into-tree-kills-two-passengers", "content": "Tesla Model S crashes into tree, kills two passengers\nOccurred: April 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Tesla Model S failed to navigate a turn in the Houston suburb of Spring before running off the road, hitting a tree, bursting into flames and killing two passengers. Authorities said no one appeared to be in the driver\u2019s seat at the time of the crash.\nWhilst it was unclear whether the car\u2019s Autopilot driver-assist system was being used, the implication of nobody being in the driver's seat was that it was likely switched on.\nHowever, Tesla CEO Elon Musk later claimed on Twitter that 'data logs recovered so far show Autopilot was not enabled and this car did not purchase FSD.' He also said that 'standard Autopilot would require lane lines to turn on, which this street did not have.'\nAccording to Electrek, 'Tesla has been known to pull the logs out of vehicles involved in crashes where Autopilot was blamed ... and has also been known to sometimes present the data in misleading ways.'\nA US National Transportation Safety Board (NTSB) investigation concluded the driver was operating the vehicle up until the moment it hit the tree and that they had been under the influence of alcohol and drugs.\n\u2795 A few days after the incident, Consumer Reports tricked a Tesla into driving in the car\u2019s Autopilot mode with no one at the wheel.\nSystem \ud83e\udd16\nTesla Autopilot, Full-self Driving\nOperator: Will Varner\nDeveloper: Tesla\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Driver assistance system\nIssue: Accuracy/reliability; Safety\nTransparency: Governance; Black box; Marketing\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nNTSB investigation [No. HWY21FH007]\nResearch, advocacy \ud83e\uddee\nTesla Deaths\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://apnews.com/article/houston-3f5358a51022167735c50bf0f4d74aef\nhttps://electrek.co/2021/04/19/elon-musk-tesla-fatal-crash-no-one-drivers-seat-wasnt-autopilot/\nhttps://www.cbsnews.com/news/tesla-car-crash-no-driver-houston-texas/\nhttps://futurism.com/elon-musk-autopilot-wasnt-engaged-during-fatal-crash\nhttps://www.cnbc.com/2021/04/19/elon-musk-autopilot-not-used-in-texas-tesla-crash.html\nhttps://edition.cnn.com/2021/04/19/business/tesla-fatal-crash-no-one-in-drivers-seat/index.html\nhttps://www.theverge.com/2023/2/10/23592910/tesla-texas-crash-ntsb-investigation-conclusion-no-autopilot\nhttps://arstechnica.com/cars/2023/02/autopilot-had-no-involvment-in-fatal-texas-tesla-crash-ntsb-says/\nRelated \ud83c\udf10\nTesla Autopilot tricked into driverless driving\nTesla Model S crashes into fire engine\nPage info\nType: Incident\nPublished: February 2023", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tesla-model-y-crashes-into-parked-police-car", "content": "Tesla Model Y crashes into parked police car \nOccurred: April 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Tesla crashed into a state trooper\u2019s cruiser with flashing lights outside Lansing, Michigan. The police said the driver was using Autopilot, Tesla\u2019s advanced driver-assistance system, at the time of the late night crash.\nWhilst no one was injured, the National Highway Traffic Safety Administration (NHTSA) opened an investigation to determine if and how Autopilot may have contributed to the crash.\nIn August 2021, the NHTSA announced that it was opening an investigation into Tesla Autopilot over its possible involvement in 11 crashes with emergency and first responder vehicles, including the Lansing incident.\nThe Tesla's driver was issued citations for failure to move over and driving with a suspended license. \nA few days earlier, the NHTSA sent a special crash investigation team to Detroit after a Tesla had driven under a semitrailer, leaving two people injured.\nSystem \ud83e\udd16\nTesla Autopilot, Full-self Driving\nOperator: \nDeveloper: Tesla\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Driver assistance system\nIssue: Accuracy/reliability; Safety\nTransparency: Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.cnbc.com/2021/03/17/tesla-in-autopilot-hits-police-car-in-michigan-officials-say.html\nhttps://nypost.com/2021/03/18/tesla-on-autopilot-crashes-into-michigan-troopers-patrol-car/\nhttps://www.insurancejournal.com/news/midwest/2021/03/19/606187.htm\nhttps://www.theverge.com/2021/3/18/22338427/tesla-autopilot-crash-michigan-nhtsa-investigation\nhttps://www.autoblog.com/2021/03/18/nhtsa-investigating-23-tesla-crashes\nhttps://abcnews.go.com/US/wireStory/tesla-autopilot-drives-michigan-troopers-patrol-car-76524732\nhttps://www.boston.com/news/cars/2021/03/17/tesla-autopilot-michigan-state-trooper-crash\nhttps://thehill.com/changing-america/resilience/smart-cities/543861-tesla-on-autopilot-crashes-into-police-car\nhttps://www.businessinsider.com/tesla-autopilot-crashed-into-a-michigan-state-police-car-2021-3\nhttps://electrek.co/2021/08/16/tesla-autopilot-investigated-nhtsa-over-11-crashes-first-responder-vehicles/\nhttps://www.reuters.com/article/us-tesla-crash/u-s-safety-agency-reviewing-23-tesla-crashes-three-from-recent-weeks-idUSKBN2BA2ML\nRelated \ud83c\udf10\nTesla Model S crashes into tree, kills two passengers\nTesla Model S crashes into fire engine\nPage info\nType: Incident\nPublished: February 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tesla-model-y-crashes-into-tractor-trailer", "content": "Tesla Model Y crashes into tractor-trailer\nOccurred: April 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Tesla Model Y drove through an intersection in Detroit, struck a tractor-trailer and became wedged beneath it, tearing the roof off the car and critically injuring a passenger. \nThe National Highway Traffic Safety Administration (NHTSA) said it was sending a team to investigate the 'violent' incident. Detroit police said they did not believe Autopilot was in use during the crash.\n\u2795 In two incidents in 2016 and 2019, Teslas had also driven beneath tractor-trailers in Florida, causing two deaths. In both crashes, the cars were being driven with Tesla\u2019s Autopilot driving software switched on.\nSystem \ud83e\udd16\nTesla Autopilot, Full-self Driving\nOperator: \nDeveloper: Tesla\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Driver assistance system\nIssue: Accuracy/reliability; Safety\nTransparency: Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.youtube.com/watch?v=YM9eXUs6L3g&t=8s\nhttps://www.cnbc.com/2021/03/15/nhtsa-investigating-violent-tesla-crash-autopilot-not-ruled-out-yet.html\nhttps://www.mercurynews.com/2021/03/17/u-s-safety-agency-probes-violent-tesla-crash-in-detroit/\nhttps://www.consumeraffairs.com/news/federal-officials-probe-safety-of-teslas-autopilot-feature-following-violent-crash-in-detroit-031721.html\nhttps://www.nytimes.com/2021/03/23/business/teslas-autopilot-safety-investigations.html\nhttps://nypost.com/2021/03/16/feds-investigating-violent-tesla-trailer-crash-in-detroit/\nhttps://www.insurancejournal.com/news/midwest/2021/03/18/605866.htm\nhttps://www.forbes.com/sites/greggardner/2021/03/16/nhtsa-launches-probe-of-tesla-truck-crash-last-week-in-detroit/\nhttps://www.washingtonpost.com/national/tesla-on-autopilot-drives-into-michigan-troopers-patrol-car/2021/03/17/e33167a8-8784-11eb-be4a-24b89f616f2c_story.html\nRelated \ud83c\udf10\nTesla Model 3 hits parked police car\nTesla Model S crash causes eight-vehicle pile-up\nPage info\nType: Incident\nPublished: February 2023", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tesla-autopilot-fsd-misleading-marketing", "content": "Tesla Autopilot, Full-Self Driving misleading marketing\nOccurred: 2020-\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nTesla has been regularly dogged by accusations that it has systematically over-stated the capabilities of its Autopilot and Full-Self Driving (FSD) systems, and under-stated their role in accidents. \nMisleading names\nA number of regulators have argued that the Autopilot and Full Self-Driving names are misleading given each requires drivers to pay attention and be able to intervene at any time. \nUS crash statistics reporting\nIn June 2023, a Washington Post investigation of US National Highway Traffic Safety Administration (NHTSA) found that there had been 736 crashes and 17 fatalities involving Teslas in Autopilot mode in the US since 2019, many more than previously reported. \nFour crashes involved motorcycles. According to the same Post investigation, Tesla accounted for the 'vast majority' of the 807 incidents reported to the NHTSA under a 2021 federal order that requires carmakers to disclose crashes involving driver-assistance technology. \nGermany misleading marketing lawsuit\nIn 2020, a Munich court ruled that Tesla use of the words 'Autopilot' and 'Full Self-Driving' constituted misleading marketing, since the cars still required a driver to operate. The appeal ruling was over-turned by the Higher Regional Court of Munich in October 2022, according to TeslaMag.\nCalifornia DMV legal communication\nIn March 2021 it was reported that Tesla knew and admitted that its Full Self-Driving Capability is not capable of full self-driving. The emails between Tesla's legal team and California\u2019s Department of Motor Vehicles (DMV) were revealed after a public records request from transparency advocacy organisation Plainsite.\nNTSB calls for tighter testing requirements\nA few days after Plainsite published the Tesla/DMV legal communications, US National Transportation Safety Board (NTSB) head Robert Sumwalt warned its sister agency, the National Highway Traffic Safety Administration (NHTSA), that it's 'hands-off approach to oversight of AV testing poses a potential risk to motorists and other road users.' \n'Tesla recently released a beta version of its Level 2 Autopilot system, described as having full self-driving capability. By releasing the system, Tesla is testing on public roads a highly automated AV technology but with limited oversight or reporting requirements,' Sumwalt argued.\nCalifornia DMV misleading marketing investigation\nIn May 2021, the LA Times reported that California's DMV has put Tesla 'under review' to determine whether it misleads customers by advertising its full self-driving capability option. The DMV is allowed to sanction car manufacturers that advertise a vehicle as autonomous when it is not.\nIn complaints filed with the state Office of Administrative Hearings, the DMV argued that Tesla 'made or disseminated statements that are untrue or misleading, and not based on facts' about how well its advanced driver assistance systems (ADAS) worked.\nTesla FSD non-disclosure agreements\nIn September 2021, Motherboard reported that Tesla FSD Beta testers were being forced to sign non-disclosure agreements that specifically prohibited them from speaking to the media or giving test rides to the media. \nA video taken by a FSD Beta tester showing a Tesla swerving to take an unexpected right turn across a crosswalk into the path of several pedestrians had gone viral, prompting Tesla to press Twitter to have it removed.\nAs noted by The Verge, Tesla was using NDAs to manage public perceptions of its Full Self-Driving software at a time when the company was about to open up access to expand the beta testing to a much wider group of Tesla drivers.\nUS Department of Justice criminal investigation\nIn October 2022, Reuters reported that Tesla had been placed under criminal investigation by the US Department of Justice (DoJ) over claims that the company's electric vehicles can drive themselves, with DoJ prosecutors examining whether Tesla misled consumers, investors and regulators by making unsupported claims about its driver assistance technology's capabilities.\nSEC misleading marketing claims investigation\nIn addition to the US DoJ investigation, the US Securities and Exchange Commission (SEC) had opened a civil investigation into whether Tesla had been misleading investors about the safety of its Autopilot system, according to The Wall Street Journal (WSJ). \nStaged demo drive marketing video\nIn January 2023, a legal deposition made by Tesla director of Autopilot software Ashok Elluswamy that was taken as part of a lawsuit over driver Walter Huang's 2018 death in a Tesla said a demonstration video was staged to show capabilities like stopping at a red light and accelerating at a green light that the system did not have.\nThe video was released in October 2016 and was promoted on Twitter by Elon Musk as evidence that 'Tesla drives itself.' The video remains archived on Tesla\u2019s website.\nInvestor class-action US lawsuit\nIn February 2023, Tesla investors lodged a class-action lawsuit in San Francisco accusing Elon Musk and his company of 'deceptive and misleading marketing of ADAS technology' - specifically Tesla's Autopilot and Full Self-Driving technologies. \n'Although these promises have proven false time and time again, Tesla and Musk have continued making them to generate media attention, to deceive consumers into believing it has unrivaled cutting-edge technology, and to establish itself as a leading player in the fast-growing electric vehicle market,' the suit states.\nWhistleblower data leak response\nTesla responded to a data leak to Handelsblatt that showed thousands of customer complaints about self-acceleration issues, braking problems, including 'unintentional emergency braking', and 'phantom stopping', by demanding that 'the data be deleted and spoke of data theft.'\nDriving range algorithm rigging\nIn July 2023, a Reuters investigation revealed Tesla has for years been rigging the dashboard readouts in its electric cars to provide 'rosy' projections of how far owners can drive before needing to recharge. The carmaker went on create a special team in 2022 to cancel owners\u2019 service appointments after a deluge of complaints regarding its driving range capalities and misleading marketing claims.\nUS securities and wire fraud investigation\nIn May 2024, Reuters revealed that US prosecutors were examining whether Tesla committed securities or wire fraud by misleading investors and consumers about its electric vehicles\u2019 self-driving capabilities.\nSystem \ud83e\udd16\nTesla Autopilot, Full-self Driving\n\nDocuments \nTesla: Full-Self Driving on All Teslas\nIssue databank \ud83d\udd22 \nOperator: Tesla\nDeveloper: Tesla\nCountry: USA; Germany\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Driver assistance system\nIssue: Governance; Ethics\nTransparency: Governance; Black box; Marketing; Legal\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nMatsko v Tesla (2022)\nCalifornia Office of Administrative Hearings (2022). California DMV Accusation Against Tesla Inc., Vehicle Manufacturer\nCalifornia Office of Administrative Hearings (2022). California DMV Accusation Against Tesla Inc., Vehicle Dealer\nPlainsite. California DMV Tesla Robo-Taxi / FSD E-Mails\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.latimes.com/business/story/2021-03-09/elon-musk-wants-it-both-ways-with-telsas-full-self-driving\nhttps://www.cnbc.com/2021/03/12/tesla-is-using-customers-to-test-av-tech-on-public-roads-ntsb.html\nhttps://www.theverge.com/2021/9/28/22696463/tesla-fsd-beta-nda-video-clips-controversy-full-self-driving\nhttps://www.reuters.com/business/autos-transportation/california-regulator-claims-tesla-falsely-advertised-autopilot-full-self-driving-2022-08-05/\nhttps://www.cnbc.com/2023/02/16/tesla-recalls-362758-vehicles-says-full-self-driving-beta-software-may-cause-crashes.html\nhttps://finance.yahoo.com/news/exclusive-tesla-faces-u-criminal-193857568.html\nhttps://www.reuters.com/technology/tesla-video-promoting-self-driving-was-staged-engineer-testifies-2023-01-17/\nhttps://www.washingtonpost.com/technology/2023/06/10/tesla-autopilot-crashes-elon-musk/\nRelated \ud83c\udf10\nTesla FSD Assertive Mode\nTesla Smart Summon\nPage info\nType: Issue\nPublished: February 2023\nLast updated: May 2024", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/deepscore-trustworthiness-assessments", "content": "DeepScore trustworthiness assessments accused of bias\nOccurred: January 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA voice and facial recognition app claiming to score how trustworthy a user prompted privacy and human rights advocates to express arconcerned about the product's accuracy and propensity for bias.\nJapanese software company DeepScore uses a 10-question survey to enable loan lenders, insurance companies, and financial institutions to decide in real-time whether people are lying or not with their gestures and tone of voice at a reputed accuracy rate of around 70 percent, and a 30 percent false negative rate. \nParity AI founder Dr. Rumman Chowdhury told Motherboard the app is at a 'minimum likely to discriminate against people with tics, anxiety, or who are neuroatypical.'\nOthers are concerned about DeepScore's privacy implications. Ioannis Kouvakas, a legal officer for Privacy International, told Motherboard he did not believe the company would be able to legally operate in the European Union due to the bloc\u2019s General Data Protection Regulation (GDPR).  \nDeepScore does not have a privacy policy on its website. It appears that many of its customers are located in countries with threadbare or non-existent privacy laws.\nSystem \ud83e\udd16\nDeepScore website\nDeepScore 'For whom' promo video\nOperator: DeepScore\nDeveloper: DeepScore\nCountry: Japan\nSector: Banking/financial services\nPurpose: Assess user/customer trustworthiness\nTechnology: Facial recognition; Voice recognition\nIssue: Accuracy/reliability; Bias/discrimination - disability; Privacy\nTransparency: Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.vice.com/en/article/akd4bg/this-app-claims-it-can-detect-trustworthiness-it-cant\nhttps://www.dailymail.co.uk/sciencetech/article-9163863/AI-app-allows-banks-screen-loan-applicants-face-voice-determine-trustworthiness.html\nhttps://mobilemarketingreads.com/deepscore-designs-a-mobile-app-for-lenders-and-insurers-to-score-customer-trustworthiness/\nhttps://www.naturalnews.com/2021-02-03-ai-analyzes-face-voice-determine-trustworthiness.html\nhttps://www.webtekno.com/kisinin-yalan-soyleyip-soylemedigini-analiz-edebilen-uygulama-h105185.html\nRelated \ud83c\udf10\nAirbnb user trustworthiness scoring\nSuzhou social 'civility score' trial\nPage info\nType: Incident\nPublished: January 2023\nLast updated: September 2023", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/hirevue-recruitment-facial-analysis-screening", "content": "HireVue recruitment facial analysis screening\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nHireVue is a Salt Lake City-based company that uses video, gaming, and proprietary algorithms to assess job seekers, whose written answers, behaviour, intonation, and speech are fed into algorithms that assign them certain traits and qualities. \nThe company is seen as amongst the foremost in its sector, and its products are used by 700+ customers in the US, UK, and elsewhere, including General Mills, Kraft, and Unilever.\nA number of HireVue's practices - notably its use of psychological inferences to determine people's ability and character based on facial data - have also proved controversial with academics, ethicists, regulators, and commentators.\nSystem \ud83e\udd16\nHireVue website\nDocuments \ud83d\udcc3\nHireVue discontinues facial analysis screening\nHireVue. HireView leads the industry with Commitment to transparent and ethical use of AI in hiring\nHireVue. Explainability statement (pdf)\nOperator: Delta; General Electric; General Mills; Hilton; Kraft; Unilever\nDeveloper: HireVue\nCountry: USA\nSector: Business/professional services\nPurpose: Improve recruitment efficiency & effectiveness\nTechnology: Facial analysis; Facial recognition; Behavioural analysis; NLP/text analysis\nIssue: Accuracy/reliability; Bias/discrimination - gender, disability; Privacy\nTransparency: Governance; Black box; Marketing\nRisks and harms \ud83d\uded1\nHireVue has attracted considerable controversy regarding its use of facial analysis and other technologies to assess job applicants' physcological states and predict employee performance.\nTransparency and accountability \ud83d\ude48\nHireVue claims that it 'leads the industry with commitment to transparent and ethical use of AI in hiring' have been undermined by the opaque nature of its products and selectively misleading marketing. \nIn March 2022, HireVue released what it billed as a 'first of its kind' explainability statement (pdf) intended to explain clearly how it uses AI in its game-based and interview assessments.\nAlongside its statement on facial analysis, HireVue released the results of an algorithmic audit by O\u2019Neil Risk Consulting & Algorithmic Auditing (ORCAA) that it concludes it's AI-based pre-built assessments used in hiring early career candidates do not demonstrate bias.\nHowever, Brookings Institution fellow Alex Engler pointed out in a Fast Company op-ed that HireVue mispresented both the claim that its termination of facial analysis was 'proactive' and that the audit concluded all of HireVue\u2019s assessments were unbiased. \nThe audit, he argued 'was narrowly focused on a specific use case, and failed to examine the assessments for which HireVue has been criticised, which include facial analysis and employee performance predictions.'\nThe Center for Democracy and Technology (CDC) said the statement 'sheds some useful light on how HireVue\u2019s technology works, it is also incomplete in important respects', and 'suggests crucial deficiencies in the fairness and job-relatedness of HireVue\u2019s approach to assessments.'\nIncidents and issues \ud83d\udd25\nJanuary 2021. HireVue announced that it had 'proactively' stopped using facial analysis in new assessments on the grounds that it's own internal research had 'demonstrated that recent advances in natural language processing had significantly increased the predictive power of language', meaning visual analysis 'no longer significantly added value to assessments.' In a response to WIRED, John Davisson, EPIC senior counsel, said 'I am surprised they are dropping this, as it was a keystone feature of the product they were marketing.' 'That is the source of a lot of concerns around biometric data collection, as well as these bold claims about being able to measure psychological traits, emotional intelligence, social attitudes, and things like that.'\nNovember 2019. US privacy group Electronic Privacy Information Center (EPIC) filed a legal complaint (pdf) alleging HireVue's use of facial technologies and biometric data 'constitute unfair and deceptive trade practices' and that it produces results that are 'biased, unprovable, and not replicable'. It alleged HireVue's hiring algorithms are more likely to be biased by default, in contrast to it's marketing claims, that it's models fail to meet international standards on AI-based decision making, and that the company fails to give candidates access to their assessment scores or the training data, factors, logic, or techniques used to generate each algorithmic assessment. The complaint also set out that HireVue's claims that it 'does not use facial recognition technology' is misleading as it collects and analyses 'facial expressions' and 'facial movements' to measure job candidates\u2019 'cognitive ability,' 'emotional intelligence,' and 'social aptitudes.' Futhermore, EPIC accused HireVue of engaging in the 'intrusive collection and secret analysis of biometric data', thereby causing 'substantial privacy harms' to job candidates, that it\u2019s assessment system causes 'substantial financial harms 'to job candidates, and that it's facial recognition software could be racially biased or improperly used to identify sexual orientation.\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nEPIC. In re HireVue\nEPIC. HireVue complaint (pdf)\nDeyerler v. HireVue Inc.\nInvestigations, assessments, audits \ud83e\uddd0\nhttps://www.hirevue.com/resources/template/orcaa-report\nhttps://www.hirevue.com/blog/hiring/industry-leadership-new-audit-results-and-decision-on-visual-analysis\nhttps://ainowinstitute.org/AI_Now_2019_Report.pdf\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.washingtonpost.com/technology/2019/10/22/ai-hiring-face-scanning-algorithm-increasingly-decides-whether-you-deserve-job/\nhttps://www.washingtonpost.com/technology/2019/11/06/prominent-rights-group-files-federal-complaint-against-ai-hiring-firm-hirevue-citing-unfair-deceptive-practices/\nhttps://fortune.com/2021/01/19/hirevue-drops-facial-monitoring-amid-a-i-algorithm-audit/\nhttps://www.inc.com/minda-zetlin/ai-is-now-analyzing-candidates-facial-expressions-during-video-job-interviews.html\nhttps://www.ft.com/content/c0b03d1d-f72f-48a8-b342-b4a926109452\nhttps://www.technologyreview.com/2020/02/14/844765/ai-emotion-recognition-affective-computing-hirevue-regulation-ethics/\nhttps://www.fastcompany.com/90597594/ai-algorithm-auditing-hirevue\nhttps://www.brookings.edu/research/auditing-employment-algorithms-for-discrimination/\nhttps://www.brookings.edu/blog/techtank/2019/10/31/for-some-employment-algorithms-disability-discrimination-by-default/\nhttps://www.reuters.com/article/global-tech-ai-hiring-idUSL5N2NF5ZC\nhttps://cdt.org/insights/hirevue-ai-explainability-statement-mostly-fails-to-explain-what-it-does/\nRelated \ud83c\udf10\nEstee Lauder employee performance assessments\nRetorio talent personality assessments\nPage info\nType: System\nPublished: January 2023\nLast updated: May 2024", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/ucsb-proctoru-data-sharing", "content": "ProctorU accused of sharing UCSB student data\nOccurred: March 2020\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA warning by University of California Santa Barbara (UCSB) faculty association members raising concerns about the potential sharing of student data with third parties resulted in a backlash against online proctoring service ProctorU.\nAccording to a March 2020 letter (pdf) to the university's administration, ProctorU 'regularly collects and distributes' a wide range of student information such as social security numbers, browsing history, gender identity, medical conditions, fingerprints, faceprints, voiceprints, retina scans and more.\nThe faculty went on to say ProctorU's data privacy practices 'implicates the university into becoming a surveillance tool' and to recommend UCSB terminate its contract with ProctorU and discourage professors from using similar services. ProctorU's (now renamed Meazure Learning) privacy policy says personal data collected may be disclosed to third parties for undefined 'business and commercial purposes'.\nIn response, ProctorU attorney David Lance Lucas threatened (pdf) to sue the faculty association for defamation and violating copyright law, and accused it of 'directly impacting efforts to mitigate civil disruption across the United States' by interfering with education during a national emergency. \nLucas' threat was condemned as inaccurate and unreasonable bullying by senior lawyers and others. According to Vice, ProctorU never filed a lawsuit against the UCSB faculty association. But the threat 'had a chilling effect on professors\u2019 willingness to discuss the software.'\nThe incident highlighted concerns about ProctorU's data sharing practices, and its heavy-handed use of legal threats to shut down legimate discussion about its products.\nSystem \ud83e\udd16\nProctorU website\nOperator: University of California (UCSB)\nDeveloper: Meazure Learning/ProctorU\nCountry: USA\nSector: Education\nPurpose: Detect and prevent cheating\nTechnology: Facial recognition; Fingerprint recognition; Voice recognition\nIssue: Privacy\nTransparency: Legal\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nEPIC (2020). Complaint and Request for Investigation, Injunction, and Other Relief (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttp://dailynexus.com/2020-03-16/ucsb-faculty-association-issues-letter-advising-against-the-use-of-proctoru-testing-services/\nhttps://www.vice.com/en/article/n7wxvd/students-are-rebelling-against-eye-tracking-exam-surveillance-tools\nhttps://www.washingtonpost.com/technology/2020/04/01/online-proctoring-college-exams-coronavirus/\nhttps://www.forbes.com/sites/seanlawson/2020/04/24/are-schools-forcing-students-to-install-spyware-that-invades-their-privacy-as-a-result-of-the-coronavirus-lockdown/\nhttps://www.thefire.org/news/proctoru-threatens-uc-santa-barbara-faculty-over-criticism-during-coronavirus-crisis\nhttps://pubcit.typepad.com/clpblog/2020/03/can-proctoru-be-trusted-with-students-personal-data.html\nhttps://academeblog.org/2020/03/27/an-egregious-case-of-legal-bullying/\nhttps://www.reddit.com/r/UCSC/comments/kwzihs/ban_the_use_of_proctoru_an_invasive_and/\nRelated \ud83c\udf10\nProctorio 'racist' facial detection\nUniversity of Wisconsin 'racist' online proctoring\nPage info\nType: Incident\nPublished: January 2023", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/cleveland-state-university-online-proctor-room-scanning", "content": "Cleveland State University proctoring bedroom scans ruled 'unconstitutional'\nOccurred: December 2022\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn Ohio judge ruled that the scanning of students' rooms before and during remote exams constitured an invasion of privacy and violated the US Fourth Amendment\u2019s protection against unlawful searches in American homes. \nCleveland State University student Aaron Ogletree agreed to a room scan before a chemistry exam. With other people in his home, he took the test in his bedroom, where he had sensitive tax documents spread out on a surface. These documents were visible in the room scan recording and were shared with other students.\nThe university had defended its room scans by saying that it had become common during the COVID-19 pandemic and more acceptable to society, that Ogletree had not been coerced into scanning his room, and that he could have removed any sensitive documents or have chosen to take the test in a different room.\nThe ruling raised questions about the nature and legality of Honorlock and other online exam cheating detection systems, and was applauded by digital rights and privacy groups. \nPrivacy non-profit Fight for the Future said the ruling was a 'major victory' and called on higher learning institutions 'to ban not only room scans, but invasive and discriminatory eproctoring software once and for all.'\n\u2795 In December 2020, the Electronic Privacy Information Center (EPIC) filed a complaint (pdf) against Honorlock and four similar proctoring services, calling their practices 'inherently invasive.'\nSystem \ud83e\udd16\nHonorlock website\nHonorlock. Best practices for online test room scans\nOperator: Cleveland State University (CSU)\nDeveloper: Honorlock; Respondus\nCountry: USA\nSector: Education\nPurpose: Detect and prevent cheating\nTechnology: Facial detection; Gaze detection; Machine learning\nIssue: Privacy\nTransparency: Privacy\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nAaron Ogletree v. Cleveland State University, et al\nResearch, advocacy \ud83e\uddee\nElectronic Frontier Foundation (2022). Federal Judge: Invasive Online Proctoring \"Room Scans\" Are Unconstitutional\nFight for the Future (2022). No more student room scans! Statement on a major victory over eproctoring surveillance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theverge.com/2022/8/23/23318067/cleveland-state-university-online-proctoring-decision-room-scan\nhttps://www.npr.org/2022/08/25/1119337956/test-proctoring-room-scans-unconstitutional-cleveland-state-university\nhttps://arstechnica.com/tech-policy/2022/08/privacy-win-for-students-home-scans-during-remote-exams-deemed-unconstitutional/\nhttps://futurism.com/the-byte/anti-cheating-software-scans-rooms-unconstitutional\nhttps://www.insidehighered.com/quicktakes/2022/08/24/webcam-scans-remote-tests-violate-student-privacy-judge-rules\nhttps://www.highereddive.com/news/test-proctoring-room-scans-violated-college-students-privacy-judge-rules/630340/\nhttps://www.abajournal.com/web/article/university-that-scanned-students-room-during-remote-test-violated-fourth-amendment-judge-rules\nhttps://uclawreview.org/2023/01/04/unconstitutional-room-scans-the-fourth-amendment-in-the-digital-age/\nhttps://www.cleveland19.com/2022/08/23/cleveland-state-student-wins-federal-lawsuit-against-university-breach-fourth-amendment/\nhttps://news.bloomberglaw.com/privacy-and-data-security/virtual-exam-case-primes-privacy-fight-over-college-room-scans\nhttps://eu.usatoday.com/story/news/education/2022/08/25/video-scan-online-test-illegal-judge/7894612001/\nhttps://www.campussafetymagazine.com/university/cleveland-state-violated-students-fourth-amendment-right/\nhttps://www.foxnews.com/us/ohio-cleveland-state-university-scan\nhttps://www.nytimes.com/2022/08/25/us/remote-testing-student-home-scan-privacy.html\nRelated \ud83c\udf10\nUniversity of Wisconsin 'racist' online proctoring\nProctorio 'racist' facial detection\nPage info\nType: Incident\nPublished: January 2023\nLast updated: October 2023", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/university-of-wisconsin-online-proctoring", "content": "University of Wisconsin disables Honorlock after 'racist' software accusations\nOccurred: April 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe University of Wisconsin-Madison (UW-Madison) disabled its Honorlock anti-cheating software after three students with darker skin complained the programme had failed to recognise their facial features. \nHonorlock disputed the issue was related to skin tone, suggesting the students were looking down or away from the webcam during their exam.\nUW-Madison first started working with Honorlock in summer 2020, during the coronavirus pandemic, which forced many students across the US to take exams in their homes. \nIn October 2021, the university renewed its contract with Honorlock, despite over 2,000 UW-Madison students signing a petition complaining it abused student privacy and calling for it to be banned.\nHonorlock is an automated proctoring solution designed to help schools and universities monitor exams live using AI technologies. It lets college administrators customise online exams and generate analytics, and students to verify their identity and take tests using a Google Chrome plugin. \nSystem \ud83e\udd16\nHonorlock website\nDocuments \ud83d\udcc3\nUW\u2013\u2060Madison Information Technology. Honorlock\nUW\u2013\u2060Madison Student Learning Assessment. Proctoring with Honorlock\nUW-Madison IT Accessibility and Usability. Honorlock Accessibility and Usability Information (Student)\nOperator: University of Wisconsin-Madison\nDeveloper: Honorlock\nCountry: USA\nSector: Education\nPurpose: Detect and prevent cheating\nTechnology: Facial recognition; Voice recognition\nIssue: Accessibility/usability; Accuracy/reliability; Bias/discrimination - race, ethnicity, disability, gender; Effectiveness/value; Privacy; Surveillance\nTransparency: Governance; Black box\nResearch, advocacy \ud83e\uddee\nAmanda Kemper. Ban Honorlock at UW-Madison\nElectronic Frontier Foundation (2020). Students Are Pushing Back Against Proctoring Surveillance Apps\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://apnews.com/article/technology-madison-wisconsin-education-software-90a41fa6fa5348d837efbbd3be3a88f3\nhttps://madison.com/wsj/news/local/education/university/3-uw-madison-students-say-online-exam-software-didnt-detect-their-darker-skin/article_891b3e5a-a9e3-5529-8859-e20908dee0b6.html\nhttps://www.nbc26.com/news/state/uw-madison-disables-proctoring-software-amid-complaints\nhttps://www.insidehighered.com/quicktakes/2021/04/06/proctoring-tool-failed-recognize-dark-skin-students-say\nhttps://www.govtech.com/education/higher-ed/anti-cheating-software-drawing-criticism-at-universities.html\nhttps://www.govtech.com/education/higher-ed/uw-madison-renews-contract-with-controversial-exam-software\nhttps://badgerherald.com/news/2021/10/26/uw-renews-contract-with-honorlock-despite-student-dissent/nts\nRelated \ud83c\udf10\nProctorio racist facial detection\nCleveland State University online proctor room scanning\nPage info\nType: Incident\nPublished: January 2023\nLast updated: October 2023", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/proctorio-racist-facial-detection", "content": "Proctorio found to use 'racist' algorithms to detect students' faces\nOccurred: April 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA facial detection system run by online proctoring service Proctorio was found to be 'racist'. \nCollege student Lucy Satheesan reverse-engineered the exam software and discovered that Proctorio's software had been using a facial detection model that failed to recognise Black faces 57 percent of the time. \nBy assessing the code behind Proctorio\u2019s extension for the Chrome web browser, college student Lucy Satheesan discovered that the file names associated with the tool\u2019s 'proprietary' facial detection function were identical to those published by open-source computer vision software library OpenCV.\nTbe discovery appeared to confirm complaints by students that Proctorio's face detection system is inaccurate and unreliable. \nThe company uses the technology to see if a student is looking away from their screen, leaves the room, or if there\u2019s another person in the frame - any of which could indicate cheating.\n\u2795 The Electronic Privacy Information Center (EPIC) had earlier filed a complaint (pdf) accusing Proctorio and four other online test proctoring services of unfair and deceptive trade practices. EPIC said it is preparing to file a lawsuit unless they change their practices.\nSystem \ud83e\udd16\nProctorio website\nProctorio CEO letter to US legislators (pdf)\nOperator: Miami University; University of British Columbia; University of Illinois\nDeveloper: Proctorio\nCountry: USA\nSector: Education\nPurpose: Detect faces\nTechnology: Facial detection; Computer vision; Machine learning\nIssue: Bias/discrimination - race, ethnicity\nTransparency: Governance; Black box; Marketing\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nEPIC (2020). In re Online Test Proctoring Companies\nEPIC (2020). Letter to Proctorio (pdf)\nEPIC (2020). Complaint and Request for Investigation, Injunction, and Other Relief (pdf)\nSenator Richard Blumenthal (2020). Blumenthal Leads Call for Virtual Exam Software Companies to Improve Equity, Accessibility & Privacy for Students Amid Troubling Reports\nResearch, advocacy \ud83e\uddee\nProctor Ninja (2021). Proctorio's facial recognition is racist\nProctor Ninja (2021). The duality of obfuscation: feat. Proctorio\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.vice.com/en/article/g5gxg3/proctorio-is-using-racist-algorithms-to-detect-faces\nhttps://www.vice.com/en/article/n7wxvd/students-are-rebelling-against-eye-tracking-exam-surveillance-tools\nhttps://www.theverge.com/2021/4/8/22374386/proctorio-racial-bias-issues-opencv-facial-detection-schools-tests-remote-learning\nhttps://www.vox.com/recode/22175021/school-cheating-student-privacy-remote-learning\nhttps://www.insidehighered.com/news/2021/02/01/u-illinois-says-goodbye-proctorio\nhttps://racismandtechnology.center/2021/07/10/call-to-the-university-of-amsterdam-stop-using-racist-proctoring-software/\nhttps://micky.com.au/proctorio-test-software-fails-to-detect-people-of-color/\nhttps://www.nytimes.com/2020/09/29/style/testing-schools-proctorio.html\nhttps://www.pointer.de/studium/aktuelles/17149/pruefungsueberwachung-rassissmus.htm\nhttps://news.slashdot.org/story/21/04/08/2056257/proctorio-is-using-racist-algorithms-to-detect-faces\nRelated \ud83c\udf10\nUniversity of Wisconsin 'racist' online proctoring\nHireVue recruitment facial analysis screening\nPage info\nType: Incident\nPublished: January 2023\nLast updated: September 2023", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/stanford-facial-political-orientation-study", "content": "Stanford facial recognition study 'reveals' political orientation\nOccurred: January 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA research study published by Stanford University professor Michal Kosinski claimed to show that facial recognition systems can expose people\u2019s political views from their social media profile photographs, triggering accusations of physiognomy.\nUsing a dataset of 1,085,795 Facebook and an unnamed dating site facial profiles from people across Canada, the US, and the UK, 977,777 of whom had self-reported their political orientation fed into the VGG Face open source facial recognition algorithm, Kosinski said he trained an algorithm to correctly classify political orientation in 72% of 'liberal-conservative' face pairs. \nKosinski noted the algorithm performed substantially better than humans, who are only able to distinguish between a liberal and a conservative with 55% accuracy, just a little better than a coin toss. This is despite conservatives being more likely to be white, older, and male.\nThe study prompted accusations of physiognomy - the controversial and debunked notion that a person\u2019s character or personality can be assessed from their appearance - given the likelihood that patterns picked up by Kosinski's algorithm may have little or nothing to do with facial characteristics. \nOthers question the ethics of the project, notably the rationale of conducting such a study, as well as the potential for the abuse and misuse of these kinds of tools by bad actors for social and political purposes.  \nThe study cannot be tested as Kosinski made available the project\u2019s source code and dataset but not the actual images, citing privacy implications. \nSystem \ud83e\udd16\nMichal Kosinski website\nMichal Kosinski Wikipedia profile\nKosinski M. (2021). Facial recognition technology can expose political orientation from naturalistic facial images\nOperator: \nDeveloper: Michal Kosinski; Stanford University\nCountry: USA\nSector: Politics\nPurpose: Identify political orientation\nTechnology: Facial recognition; Machine learning\nIssue: Accuracy/reliability; Dual/multi-use; Ethics\nTransparency: Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theguardian.com/technology/2017/sep/12/artificial-intelligence-face-recognition-michal-kosinski\nhttps://venturebeat.com/ai/outlandish-stanford-facial-recognition-study-claims-there-are-links-between-facial-features-and-political-orientation/\nhttps://themerkle.com/michal-kosinki-face-reading-ai-can-detect-your-political-and-sexual-orientation/\nhttps://techcrunch.com/2021/01/13/facial-recognition-reveals-political-party-in-troubling-new-research/\nhttps://www.businessinsider.com/stanford-professor-thinks-ai-will-soon-be-able-to-detect-your-politics-iq-sexuality-2017-9\nhttps://www.dailymail.co.uk/sciencetech/article-9149089/AI-determine-persons-political-affiliation-based-photo-70-accuracy.html\nhttps://gizmodo.com/a-new-stanford-study-uses-facial-recognition-to-figure-1846052406\nhttps://www.psypost.org/2021/03/facial-recognition-technology-can-predict-a-persons-political-orientation-with-72-accuracy-59888\nRelated \ud83c\udf10\nStanford AI sexual orientation prediction study\nIntel AI student emotion monitoring\nPage info\nType: Issue\nPublished: January 2023", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/raffaela-spone-deepfake-reputational-attacks", "content": "Pennslyvania woman allegedly frames daughter's rivals using deepfakes\nOccurred: March 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Pennslyvania woman was arrested for allegedly framing her teenage daughter's cheerleading rivals by creating 'deepfake' photos and videos of them naked, drinking, and smoking. \nAccording to her prosecutors, Raffaela Spone, 50, aimed to humiliate her daughter's friends and force them from their team - the Victory Vipers. She was charged with cyber harrassment of a child and related offences.\nOne of the alleged 'deepfakes' was a video showing one of the girls vaping - which violated the rules of her cheer squad - and another showed a girl without clothes in a public space. One of the messages she is meant to have sent along with a doctored image urged one of the girls 'kill herself'.\nHowever, synthetic media experts such as Henry Ajder expressed their doubts that the video had been deepfaked and, during preliminary court proceedings, District Attorney Matt Weintraub revealed that the police did not have definitive proof that Spone had created the images, or that they were manipulated at all. \n\u2795 Two months later, the deepfake charges against Spone were dropped, though she continued to be charged with harrassment. Spone was found guilty of three counts of misdemeanour harassment in March 2022, and received three years' probation.\n\u2795 A September 2021 Cosmopolitan investigation concluded the videos were likely to be real, and that the viral scandal that emerged from the case was the result of incompetent police work.\nSystem \ud83e\udd16\nUnknown\nOperator: Unclear/unknown\nDeveloper: Unclear/unknown\nCountry: USA\nSector: Education; Media/entertainment/sports/arts\nPurpose: Damage reputation\nTechnology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Mis/disinformation; Privacy; Ethics\nTransparency: Governance; Privacy\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nSPONE v. REISS et al\nInvestigations, assessments, audits \ud83e\uddd0\nDeepfaked. Cheerleader\u2019s mum falsely accused of creating 'deepfakes'\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.inquirer.com/news/bucks-county-raffaela-spone-cyberbullying-deepfake-20210312.html\nhttps://www.washingtonpost.com/nation/2021/03/13/cheer-mom-deepfake-teammates/\nhttps://www.nytimes.com/2021/03/14/us/raffaela-spone-victory-vipers-deepfake.html\nhttps://www.newsweek.com/cheerleading-mom-accused-making-nude-deepfakes-incriminate-daughters-rivals-1575892\nhttps://boingboing.net/2021/03/15/mother-arrested-for-making-deepfakes-to-humiliate-daughters-cheerleading-squad-members.html\nhttps://www.bbc.co.uk/news/technology-56404038\nhttps://twitter.com/HenryAjder/status/1372996750093471748\nhttps://www.dailydot.com/debug/deepfake-vaping-video-cheerleaders/\nhttps://www.dailydot.com/debug/deepfake-mom-cheerleaders-prosecutors-backtrack/\nhttps://www.cosmopolitan.com/lifestyle/a37377027/deep-fake-cheer-scandal/\nhttps://www.dailymail.co.uk/news/article-9592117/Cops-accused-woman-creating-deepfake-images-never-evidence.html\nhttps://news.yahoo.com/videos-werent-deepfakes-chalfont-mom-194103694.html\nRelated \ud83c\udf10\nLauren Book deepfake extortion\nXiao Yu deepfake pornography\nPage info\nType: Incident\nPublished: January 2023", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/delhi-government-schools-facial-recognition", "content": "Delhi government schools accused of facial recognition 'privacy abuse'\nOccurred: March 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nFacial recognition technologies were quietly being used in at least a dozen schools in New Delhi, India, opening the authorities to accusations of 'overreach' and privacy abuse. \nThe installation of the facial recognition technology by government-funded schools was discovered through a Right to Information query filed with the Directorate of Education in December 2020 by digital rights organisation the Internet Freedom Foundation (IFF). \nAccording to the IFF, the Government of Delhi failed to inform its various stakeholders about which company had supplied the hardware and software, what the system was for, how it worked, and how the personal data of students and employees was managed.\nThe facial recognition systems were installed without laws to regulate the collection and use of data. India proposed a national privacy law in July 2018, though it still remained in draft early 2023. \nThe move followed a 2019 decision by the Delhi government to mount CCTV cameras in over 700 public schools with the aim of ensuring the safety of students, and to reduce truancy.\nSystem \ud83e\udd16\nGovernment of National Capital Territory of Delhi website\nGovernment of Delhi Wikipedia profile\nOperator: Government of Delhi\nDeveloper: \nCountry: India\nSector: Education\nPurpose: Verify student identity\nTechnology: CCTV; Facial recognition; Facial matching\nIssue: Accuracy/reliability; Appropriateness/need; Privacy; Security; Surveillance\nTransparency: Governance; Marketing; Privacy\nResearch, advocacy \ud83e\uddee\nPanoptic Tracker - Education Department, Govt. of NCT of Delhi\nInternet Freedom Foundation (2021). The Delhi Govt. is using facial recognition on children in Govt. schools\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reuters.com/article/india-tech-facialrecognition-idUSL5N2KZ0SZ\nhttps://thewire.in/education/delhi-government-schools-facial-recognition-cctv-cameras\nhttps://www.thehindubusinessline.com/news/education/should-delhi-govt-schools-be-using-the-facial-recognition-technology/article33822047.ece\nhttps://ktnewslive.com/should-delhi-govt-schools-be-using-the-facial-recognition-technology/\nhttps://www.biometricupdate.com/202103/face-biometrics-deployments-in-indian-schools-scrutinized-by-privacy-advocates\nhttps://iapp.org/news/a/installation-of-facial-recognition-in-delhi-schools-raises-privacy-fears/\nhttps://www.telegraphindia.com/india/facial-recognition-technologies-overreach-digital-rights-advocates/cid/1808316\nhttps://www.codastory.com/authoritarian-tech/cameras-schools-india-education/\nRelated \ud83c\udf10\nCBSE facial recognition\nIndia citizenship law protest surveillance\nPage info\nType: Issue\nPublished: January 2023", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/benjamin-netanyahu-covid-19-vaccination-chatbot", "content": "Automated Benjamin Netanyahu COVID-19 Facebook message backfires\nOccurred: January 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn automated message from Israeli Prime Minister Benjamin Netanyahu violating the privacy of Facebook users caused a backlash and to the suspension of the politician's account.\nVisitors to Netanyahu\u2019s Facebook page who clicked on a link about COVID-19 received an automatic message purporting to come from Netanyahu asking for the name and contact details of people who had not been vaccinated.\n'If you know someone who is nervous about getting vaccinated, send me their name and phone number,' Netanyahu said in the video. Maybe they'll get a surprise phone call from me and I'll convince them,' he promised. \n'Under our privacy policy we do not allow content that shares or asks for people's medical information,' said a Facebook spokeswoman told Haaretz. 'We have removed the offending post and temporarily suspended the messenger bot, which shared this content, for breaking these rules.'\nFacebook had suspended Netanyahu\u2019s chatbot twice before, once for inciting hatred against Israel's Arab population, the second time for breaking Israeli election law by publishing polls in the 24-hours prior to the election. \nSystem \ud83e\udd16\nBenjamin_Netanyahu Wikipedia profile\nOperator: Likud/Benjamin Netanyahu; Meta/Facebook\nDeveloper: Likud/Benjamin Netanyahu\nCountry: Israel\nSector: Govt - health\nPurpose: Increase vaccination rates\nTechnology: Chatbot; NLP/text analysis\nIssue: Privacy\nTransparency: Privacy\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://edition.cnn.com/2021/01/25/middleeast/israel-facebook-netanyahu-chatbot-intl/index.html\nhttps://www.bbc.co.uk/news/world-middle-east-55794283\nhttps://www.timesofisrael.com/facebook-deletes-netanyahu-post-asking-to-identify-unvaccinated-israelis/\nhttps://www.haaretz.com/israel-news/tech-news/facebook-blocks-netanyahu-chatbot-seeking-details-of-israelis-unwilling-to-vaccinate-1.9481495\nhttps://finance.yahoo.com/news/facebook-blocks-netanyahu-chatbot-citing-180629107.html\nhttps://www.telegraph.co.uk/news/2021/01/25/facebook-suspends-benjamin-netanyahus-chatbot-covid-vaccines/\nhttps://www.reuters.com/article/uk-health-coronavirus-facebook-netanyahu-idUSKBN29U24U\nhttps://www.jpost.com/israel-news/facebook-blocks-netanyahu-bot-for-seeking-private-covid-19-vaccine-data-656642\nhttps://www.bloombergquint.com/onweb/facebook-blocks-netanyahu-chatbot-over-privacy-infraction\nhttps://www.middleeastmonitor.com/20210126-facebook-shuts-netanyahus-chatbot-for-asking-users-for-medical-data/\nhttps://foreignpolicy.com/2021/02/17/what-if-countries-that-excel-at-vaccinating-their-citizens-still-dont-reach-herd-immunity/\nRelated \ud83c\udf10\nHebron Palestinian facial recognition surveillance\nInstagram/Twitter remove, block Palestinian posts\nPage info\nType: Incident\nPublished: January 2023", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/nhs-qcovid-risk-prediction-algorithm", "content": "QCovid prediction algorithm wrongly identifies high-risk patients\nOccurred: March 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe reliability of an algorithm used to predict the risk of someone catching, being admitted to hospital, or dying from COVID-19 in England, UK, is in the spotlight for wrongly identifying some patients as high risk.\nBased on data from 'the first few months of the pandemic', the University of Oxford-developed QCovid model took into account various socio-economic indicators, underlying health conditions such as diabetes and heart disease, body-mass index, and postcode (within Britain), among other factors, to return an 'absolute risk of a covid-19 associated death' or hospitalisation.\nFollowing the launch of the tool, an additional 1.7 million people were instructed to shield, with around 800,000 people moved up the priority list to be vaccinated. These included women with previous gestational diabetes but who were healthy and could not understand why it was being recommended that they shield.\nSome General Practitioners also described seeing healthy young men on the list. Young, healthy people are less likely to have measurements such as body weight recorded in their health records, Irene Petersen, professor of epidemiology and health informatics at University College London, told The Guardian.\n\u2795 An updated version of the system was released in November 2021.\nSystem \ud83e\udd16\nQcovid website\nNHS Shielded Patient List\nNHS Shielded Patient List - Transparency notice\nDepartment for Health and Social Care and NHS Digital (2023). QCovid algorithm\nOperator: National Health Service (NHS)\nDeveloper: University of Oxford; NHS Digital\nCountry: UK\nSector: Govt - health\nPurpose: Predict COVID-19 risk\nTechnology: Prediction algorithm\nIssue: Accuracy/reliability\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theguardian.com/society/2021/mar/09/scientists-question-nhs-algorithm-as-young-people-called-in-for-jab\nhttps://www.theguardian.com/uk-news/2021/feb/16/qcovid-how-improved-algorithm-can-identify-more-higher-risk-adults\nhttps://www.dailymail.co.uk/news/article-9351305/Coronavirus-NHS-wrongly-marked-430-000-people-high-risk-Covid.html\nhttps://www.pulsetoday.co.uk/news/clinical-areas/diabetes/history-of-gestational-diabetes-prompting-patients-to-shield-under-new-algorithm/\nhttps://www.pulsetoday.co.uk/news/coronavirus/over-400000-patients-given-inflated-covid-risk-scores-due-to-missing-data/\nhttps://www.telegraph.co.uk/global-health/science-and-disease/computer-says-yes-new-covid-vaccine-algorithm-will-make-us-safer/\nhttps://www.politicshome.com/news/article/17m-more-people-told-to-shield-due-and-800000-moved-up-vaccine-priority-list-due-to-new-modelling\nhttps://www.gponline.com/qcovid-tool-reviewed-nhs-digital-healthy-women-wrongly-advised-shield/article/1708003\nhttps://www.pulsetoday.co.uk/news/clinical-areas/diabetes/nhs-digital-to-review-qcovid-tool-after-healthy-women-were-told-to-shield/\nhttps://www.digitalhealth.net/2021/02/nhs-digital-reviewing-algorithm-after-women-incorrectly-told-to-shield/\nhttps://www.digitalhealth.net/2021/02/new-groups-which-could-be-at-high-risk-from-covid-19-identified-with-tech/\nhttps://www.economist.com/graphic-detail/2021/03/11/how-we-built-our-covid-19-risk-estimator\nRelated \ud83c\udf10\nEpic Systems Epic Deterioration Index\nNarxCare drug addition risk assessment\nPage info\nType: Incident\nPublished: January 2023\nLast updated: June 2023", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/us-border-imposter-identification-failures", "content": "US CBP fails to identify imposters using facial recognition\nOccurred: February 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe failure of the US Customs and Border Protection (CBP) facial recognition system to turn up a single example of an individual impersonating someone else at US airports called into question its effectiveness.\nAccording to a February 2021 OneZero report based on the agency's 2020 annual report (pdf), the CPB has been testing the collection and analysis of traveller biometrics, including fingerprints and facial images since 2013, and had scanned 23+ million travelers\u2019 faces at 30+ points of entry in 2020. \nTechdirt's Tim Cushing notes, 'Spending millions to deal with a minor problem by deploying tech that remains unproven shouldn\u2019t be considered acceptable. Neither is the alternative: a system that rarely recognizes imposters, allowing government agencies to assume it\u2019s less of a problem than it might actually be. \n\u2796 September 2020. The US Government Accountability Office (GAO) report (pdf) took issue with the CPB over lackluster accuracy audits, poor signage notifying the public the technology is being used, and the paucity of public information on how its systems worked.\nSystem \ud83e\udd16\n\nOperator: Department of Homeland Security (DHS); Customs and Border Protection (CBP)\nDeveloper: \nCountry: USA\nSector: Govt - immigration\nPurpose: Identify imposters\nTechnology: Facial recognition\nIssue: Effectiveness/value\nTransparency: Governance; Marketing\nInvestigations, assessments, audits \ud83e\uddd0\nUnited States Government Accountability Office. Facial recognition - CBP and TSA are Taking Steps to Implement Programs, but CBP Should Address Privacy and System Performance Issues\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://onezero.medium.com/despite-scanning-millions-of-faces-feds-caught-zero-imposters-at-airports-last-year-e34c32500496\nhttps://www.washingtonpost.com/technology/2021/02/17/facial-recognition-biden/\nhttps://gizmodo.com/cbp-facial-recognition-scanners-failed-to-find-a-single-1846251680\nhttps://www.cnet.com/news/us-border-patrol-used-facial-recognition-on-23-million-travelers-in-2020/\nhttps://www.cpomagazine.com/data-privacy/facial-recognition-systems-scan-23-million-people-at-us-borders-come-up-with-zero-imposters/\nhttps://www.computing.co.uk/news/4027107/cbp-23-million-single-imposter\nhttps://siliconangle.com/2021/02/14/cbp-facial-recognition-technology-fails-find-anyone-using-false-identities-airports/\nhttps://www.insidehook.com/daily_brief/travel/airport-facial-recognition-not-working\nhttps://www.techdirt.com/articles/20210214/11525846243/cbp-facial-recognition-program-has-gathered-50-million-face-photos-identified-fewer-than-300-imposters.shtml\nhttps://www.itsecurityguru.org/2021/02/12/airport-facial-recognition-scanners-didnt-find-a-single-imposter-in-2020/\nRelated \ud83c\udf10\nCPB One asylum seeker app privacy\nIRCC immigration and visa applications automation\nPage info\nType: Incident\nPublished: January 2023", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/facebook-ville-de-bitche", "content": "Facebook algorithm leads to 'Ville de Bitche' page removal\nOccurred: March 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe official page of French town Ville de Bitche was taken down by Facebook without explanation.\n Ville de Bitche, a fortress town of 5,000+ people in the Moselle department of north-eastern France, saw its official Facebook page - titled Ville de Bitche - suddenly removed without explanation.\nPer Politico, Bitche communications official Val\u00e9rie Degouy told Radio M\u00e9lodie that she appealed the decision the same day the town's page was taken down, but had not heard back from Facebook. 'I tried to reach out to Facebook in every possible way, through different forms, but there's nothing [I could] do,' she said.\nFacebook reinstated the page and apologised, telling CNN it was 'removed in error.' Commentators suggested the page was likely to have been flagged by Facebook's content moderation system, and the mistake was then compunded by human error.\nIn a press statement released after the incident, Mayor of Bitche Beno\u00eet Kieffer urged Facebook to be more transparent and fair in how it makes content moderation decisions.\nSystem \ud83e\udd16\nhttp://www.ville-bitche.fr/site/index.php\nhttp://www.ville-bitche.fr/site/medias/_pdfs/actualite/Communique_de_presse_Facebook_Ville_de_Bitche_republiee.pdf\nOperator: Meta/Facebook; Ville de Bitche\nDeveloper: Meta/Facebook\nCountry: France\nSector: Govt - municipal\nPurpose: Moderate content\nTechnology: Content moderation system\nIssue: Accuracy/reliability\nTransparency: Black box; Complaints/appeals\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.radiomelodie.com/actu/15194-insolite-bitche-est-censure-par-facebook.html\nhttps://www.politico.eu/article/facebook-unpublishes-official-page-for-french-town-of-bitche/\nhttps://www.theverge.com/tldr/2021/4/14/22383541/facebook-bitche-france-page-city-profranity-censorship-moderation\nhttps://edition.cnn.com/travel/article/bitche-facebook-france-scli-intl/index.html\nhttps://www.thesun.co.uk/news/14634870/french-town-bitche-cancelled-facebook/\nhttps://news.sky.com/story/facebook-takes-down-official-page-for-french-town-called-bitche-12274237\nhttps://www.independent.co.uk/travel/news-and-advice/bitche-facebook-page-insult-france-b1830697.html\nhttps://www.dailymail.co.uk/sciencetech/article-9466499/Facebook-removes-French-towns-official-page-offensive-name.html\nhttps://www.dailydot.com/unclick/bitche-facebook-french-town/\nhttps://www.bbc.com/news/amp/world-europe-56731027\nhttps://gizmodo.com/ville-de-bitche-survived-the-nazis-and-now-facebook-1846672933\nhttps://www.theguardian.com/world/2021/apr/13/lifes-a-bitche-french-towns-facebook-page-shut-down-over-offensive-name\nRelated \ud83c\udf10\nAI Dungeon offensive speech filter\nEngland footballers' racism Instagram moderation\nPage info\nType: Incident\nPublished: January 2023", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/dahua-real-time-uyghur-warnings", "content": "Dahua 'Smart Police Heart Of City' provides real-time Uyghur warnings\nOccurred: February 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nFacial recognition software developed by state-owned Chinese video surveillance technology company Dahua provides 'real-time warning for Uyghurs' to the Chinese police, according to video research company IPVM.\nPer the Los Angeles Times, documents on a Dahua support website reveal a law enforcement user guide dated December 2019 show Dahua\u2019s 'Smart Police Heart Of City' (HOC) system can send a warning when it detects someone it identifies as Uyghur. And a consumer-facing product called SmartPSS offers a feature to sort by race individuals who pass in front of its cameras.\nDahua responded (pdf) by saying the documents referenced by IPVM and the Los Angeles Times were 'historical internal software design documents' and that it 'does not provide products and services for ethnicity detection' in the 'regional markets reported by the media.'\nDahua denied selling 'ethnicity-focused recognition' products in a November 2020 statement to the South China Morning Post after cybersecurity researchers had found code with an ethnic designation for Uyghurs. The company then deleted and updated the code on its website.\nAmazon thermal cameras contract\nIn April 2020, Reuters reported that Amazon had bought 1,500 thermal cameras from Dahua in a deal valued at close to USD 10 million. The cameras were intended to take the temperatures of workers during the coronavirus pandemic.\nA day after the Los Angeles Times article was published, US senators Marco Rubio and Robert Menendez sent a letter (pdf) to then Amazon Chief Executive Jeff Bezos asking whether Amazon knew Dahua was on the entity list when it was considering entering into a contract with the company and whether that came up in its deliberations.\nDahua is blacklisted by the US Commerce Department over allegations it helped Beijing detain and monitor Uyghurs and other Muslim minorities. The arrangement with Amazon is legal because the rules apply to the public sector, rather than the private sector. \nHowever, the deal 'would mean that Amazon willfully ignored guidance from the United States government and purchased equipment from an entity-listed company that is complicit in China\u2019s atrocities against' the Uyghurs, according (pdf) to Rubio and Menendez. \nSystem \ud83e\udd16\nDuhua website\nDahua Wikipedia profile \nDahua SmartPSS website\nDahua code\nDahua (2021). Statement on recent media reports (pdf)\nOperator: Amazon; Modesto City Schools\nDeveloper: Zhejiang Dahua Technology; China Electronics Technology Group/Hikvision\nCountry: China\nSector: Govt - police; Govt - security\nPurpose: Identify & track Uyghurs\nTechnology: CCTV; Facial recognition; Computer vision; Neural network; Machine learning\nIssue: Surveillance; Privacy; Bias/discrimination - race, ethnicity\nTransparency: Governance; Black box; Marketing\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nRobert Menendez, Mario Rubio (2021). Letter to Jeff Bezos (pdf)\nResearch, advocacy \ud83e\uddee\nIPVM (2021). Uyghur Surveillance & Ethnicity Detection Analytics in China (pdf) \nIPVM (2021). Dahua Responds, Caught Lying\nIPVM (2021). Dahua Racist Uyghur Tracking Revealed\nIPVM (2021). Dahua Provides \"Uyghur Warnings\" To China Police\nInvestigations, assessments, audits \ud83e\uddd0\nLos Angeles Times (2021). Major camera company can sort people by race, alert police when it spots Uighur\nLos Angeles Times (2021). Amazon questioned over contract with company that offered \u2018real-time Uighur warnings\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://thehill.com/opinion/civil-rights/536827-chinas-racism-powered-by-ai\nhttps://techcrunch.com/2021/05/24/united-states-towns-hikvision-dahua-surveillance/\nhttps://www.reuters.com/article/us-health-coronavirus-amazon-com-cameras/exclusive-amazon-turns-to-chinese-firm-on-u-s-blacklist-to-meet-thermal-camera-needs-idUSKBN22B1AL\nhttps://www.biometricupdate.com/202102/dahua-biometrics-controversy-draws-scrutiny-of-amazons-10m-thermal-cameras-purchase\nhttps://www.biometricupdate.com/202102/investigation-reveals-dahua-biometric-systems-sent-ethnicity-warnings-to-chinese-police\nhttps://www.dailymail.co.uk/news/article-11179641/Is-Big-Brother-Britain-worlds-ultimate-surveillance-state.html\nhttps://www.theguardian.com/world/2021/sep/30/uyghur-tribunal-testimony-surveillance-china\nhttps://www.bbc.co.uk/news/technology-57101248\nhttps://news.trust.org/item/20210113195157-jq6lj/\nhttps://www.thedailybeast.com/dahua-amazon-partner-in-china-is-making-facial-recognition-tech-to-track-uyghurs\nhttps://www.scmp.com/tech/policy/article/3108380/chinese-surveillance-giant-expanding-us-attracts-scrutiny-over-possible\nRelated \ud83c\udf10\nBeijing Uyghur emotion detection testing\nHenan foreign journalist, student surveillance\nPage info\nType: Incident\nPublished: January 2023", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/myanmar-minister-deepfake-corruption-confession", "content": "'Deepfake' Burma minister confesses to Aung San Suu Kyi corruption \nOccurred: March 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA video of Yangon former chief minister Phyo Min Thein apparently confessing to corrupting Aung San Suu Kyi with silk, gold and cash was denounced as a deepfake.\nJournalists, academics and others who know Phyo Min Thein noted that his voice in the video did not sound like his real voice and that his lips appeared to be out of sync with his words, concluding that the video was likely to be a deepfake.\nPer WIRED, screen-shots from an online deepfake detector indicated with 90-percent-plus confidence that the confession was a deepfake. However, WITNESS director Sam Gregory reckoned it may be a coerced or forced confession on camera. \nThe video was broadcast by the military-owned Myanmar TV (MRTV) station, which had been used to formally announce the military coup d'\u00e9tat. The video also showed a mayor of Naypyitaw alleging her National League for Democracy party had committed electoral fraud by inventing voters.\nSuu Kyi had appointed Phyo Min Thein as chief minister after he won the 2015 election representing Yangon's Hlegu township at the Yangon regional parliament. He was detained by Myanmar's armed forces in the aftermath of their coup d'\u00e9tat in February 2021. \nSystem \ud83e\udd16\nhttps://en.wikipedia.org/wiki/Myawaddy_TV\nhttps://en.wikipedia.org/wiki/Phyo_Min_Thein\nhttps://www.facebook.com/VoiceofMyanmarNews/videos/279901916856963/\nOperator: Myawaddy TV (MRTV)\nDeveloper: Government of Myanmar\nCountry: Myanmar\nSector: Govt - police; Govt - security\nPurpose: Damage reputation\nTechnology: Deepfake - video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning\nIssue: Mis/disinformation\nTransparency: Governance\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://kr-asia.com/did-myanmars-military-deepfake-a-ministers-corruption-confession\nhttps://www.reuters.com/article/us-myanmar-politics-suukyi/myanmar-military-airs-on-tv-allegations-of-bribery-against-suu-kyi-idUSKBN2BF0OP\nhttps://coconuts.co/yangon/news/is-this-guy-for-real-in-myanmar-the-fear-of-deepfakes-may-be-just-as-dangerous/\nhttps://www.wired.com/story/opinion-the-world-needs-deepfake-experts-to-stem-this-chaos/\nhttps://www.irrawaddy.com/news/burma/myanmar-junta-accused-using-deepfake-technology-prove-graft-case-daw-aung-san-suu-kyi.html\nhttps://www.aa.com.tr/tr/bilim-teknoloji/myanmardaki-sahte-videoyu-turk-yapay-zeka-cozumu-ortaya-cikardi/2190083\nhttps://dunia.tempo.co/read/1446654/junta-militer-diduga-pakai-video-deepfake-untuk-kriminalisasi-aung-san-suu-kyi/full\nhttps://www.reddit.com/r/MediaSynthesis/comments/me7twb/did_myanmars_military_deepfake_a_ministers/\nRelated \ud83c\udf10\nPresident Zelenskyy deepfake surrender\nDubai deepfake court evidence\nPage info\nType: Incident\nPublished: January 2023", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-ring-blm-protest-surveillance", "content": "LA police request private Amazon Ring BLM protest footage\nOccurred: February 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nLos Angeles police came under fire for asking users of Amazon's Ring smart home system for their camera footage of protests following the 2020 death of George Floyd.\nDigital rights non-profit the Electronic Frontier Foundation (EFF) discovered through a Freedom of Information Act (FOIA) that the Los Angeles Police Department had asked users of Amazon's Ring smart home system for camera footage from the 2020 Black Lives Matter protests that took place across the US in the wake of the death of George Floyd.\nIn response, request the LAPD had sent a heavily redacted email making no mention of any specific crime the LAPD may have been pursuing, only that the police wanted footage of an unspecified 'incident' related to a protest.\nPer the BBC, Amazon requires that any requests from police include a valid case number for an active investigation, and details of the incident. Such requests can only be made if the purpose is to 'identify individuals responsible for theft, property damage, and physical injury'.\n\nAmazon Ring users are under no obligation to share data with the police. However, a 2019 Motherboard report showed that Ring had been actively coaching police departments on how best to persuade Ring users into voluntarily sharing their surveillance videos.  \n\nRights groups worry that Ring\u2019s video-sharing systems pressure private citizens into turning over their data to police without the oversight of a judge. \nSystem \ud83e\udd16\nAmazon Ring website\nAmazon Ring Wikpedia profile\nOperator: Los Angeles Police Department (LAPD)\nDeveloper: Amazon/Ring\nCountry: USA\nSector: Consumer goods; Govt - police\nPurpose: Strengthen security, safety\nTechnology: CCTV\nIssue: Privacy; Surveillance; Dual/multi-use\nTransparency: Governance; Marketing\nResearch, advocacy \ud83e\uddee\nhttps://www.eff.org/deeplinks/2021/02/lapd-requested-ring-footage-black-lives-matter-protests\nhttps://www.eff.org/deeplinks/2020/06/amazon-ring-must-end-its-dangerous-partnerships-police\nhttps://static1.squarespace.com/static/58a33e881b631bc60d4f8b31/t/61baab9fcc4c282092bbf7c3/1639623584675/Policing+Project+Ring+Civil+Rights+Audit+%28Full%29.pdf\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://theintercept.com/2021/02/16/lapd-ring-surveillance-black-lives-matter-protests/\nhttps://www.latimes.com/california/story/2021-02-16/lapd-sought-private-ring-footage-as-they-investigated-crimes-around-george-floyd-protests\nhttps://www.bbc.co.uk/news/technology-56099167\nhttps://www.gizmodo.com.au/2021/02/the-lapd-asked-ring-owners-to-hand-over-footage-of-blm-protesters/\nhttps://www.theverge.com/2021/2/17/22287287/los-angeles-police-department-ring-anti-racism-protest-video-surveillance-request\nhttps://www.theguardian.com/commentisfree/2021/may/18/amazon-ring-largest-civilian-surveillance-network-us\nhttps://www.technologyreview.com/2021/09/20/1035945/amazon-ring-domestic-violence/\nhttps://www.washingtonpost.com/technology/2020/02/18/ring-nest-surveillance-doorbell-camera/\nhttps://www.vice.com/en/article/43kga3/amazon-is-coaching-cops-on-how-to-obtain-surveillance-footage-without-a-warrant\nhttps://theintercept.com/2019/02/14/amazon-ring-police-surveillance/\nRelated \ud83c\udf10\nUS Postal Inspection Service iCOP covert monitoring and surveillance\nIndia citizenship law protest surveillance\nPage info\nType: Incident\nPublished: January 2023", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/nypd-digidog", "content": "NYPD 'digidog' hostage response draws complaints\nOccurred: February 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nImages of an automated New York Police Department (NYPD) 'digidog' equipped with surveillance cameras responding to a hostage situation drew complaints from the local community, rights activists, and politicians. \nThe NYPD used the Boston Dynamics-manufactured Spot dog to surveil a house in the Bronx in which two men were holding two others hostage. The suspects were later arrested. \nThe incident prompted concerns about the technical capabilities of the robot, the scope for misuse and abuse, and its use in lower-income communities. \nIt also raised questions about transparency, with the ACLU asking why the NYPD had failed to list it on its public disclosure of surveillance devices.\nSystem \ud83e\udd16\nBoston Dynamics Wikipedia profile\nBoston Dynamics. Spot website\nBoston Dynamics. Spot terms and conditions (pdf)\nBoston Dynamics. An Open Letter to the Robotics Industry and our Communities,General Purpose Robots Should Not Be Weaponized\nNYC Government (2023). Transcript: Mayor Adams Makes Public Safety Announcement With NYPD Commissioner Sewell\nOperator: New York Police Department (NYPD)\nDeveloper: Hyundai Motor Group/Boston Dynamics\nCountry: USA\nSector: Automotive\nPurpose: Strengthen law enforcement\nTechnology: Robotics\nIssue: Bias/discimination - race, ethnicity; Dual/multi-use; Privacy; Surveillance\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nACLU (2021). Robot Police Dogs are Here. Should We be Worried?\nFact check \ud83d\udea9\nSnopes (2021). Does the NYPD Use Robot Dogs in Its Police Work?\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.nytimes.com/2021/02/27/nyregion/nypd-robot-dog.html\nhttps://nypost.com/2021/02/23/video-shows-nypds-new-robotic-dog-in-action-in-the-bronx/\nhttps://www.wired.com/story/new-york-lawmaker-wants-ban-police-armed-robots/\nhttps://www.republicworld.com/world-news/us-news/nypds-robot-digidog-patrols-new-york-city-reminds-residents-of-dystopian-thrillers.html\nhttps://www.newsweek.com/nypd-robot-dog-1573619\nhttps://www.curbed.com/2021/03/nypd-robot-dog-boston-dynamics-spot.html\nhttps://www.theguardian.com/commentisfree/2021/mar/02/nypd-police-robodog-patrols\nhttps://www.theverge.com/2021/2/24/22299140/nypd-boston-dynamics-spot-robot-dog\nhttps://gothamist.com/news/nypd-deploys-alarming-robot-dog-manhattan-public-housing-complex\nhttps://www.axios.com/2022/10/06/boston-dynamics-pledges-weaponize-robots\nhttps://www.nytimes.com/2023/04/11/nyregion/nypd-digidog-robot-crime.html\nRelated \ud83c\udf10\nPolice robot kills Dallas shooting suspect\nHonolulu homeless robot temperature testing\nPage info\nType: Incident\nPublished: February 2021\nLast updated: October 2022", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/son-ji-chang-tesla-model-x-sudden-acceleration", "content": "Son Ji-Chang Tesla Model X suddenly accelerates, injuring passenger\nOccurred: September 2016\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nTesla was sued after one of its cars allegedly suddenly accelerated, resulting in injury to the passenger and to the driver's house.\nKorean celebrity Son Ji-chang said we would sue Tesla after his parked Model X electric SUV suddenly accelerated, causing it to crash through his garage into his living room, injuring him and his passenger. The lawsuit alleged product liability, negligence, and breaches of warranty.\nReuters reported that Tesla responded that the data from from the car and other evidence 'conclusively shows that the crash was the result of Mr. Son pressing the accelerator pedal all the way to 100%'. Tesla also argued that the actor was using his celebrity to damage Tesla. \n\u2795 In January 2021, the US National Highway Traffic Safety Administration (NHTSA) ended a year-long review of claims that over 246 Tesla vehicles had accelerated without warning by concluding that there was insufficient evidence that Tesla might be to blame to warrant a full investigation.\nSystem \ud83e\udd16\nTesla Autopilot, Full-self Driving\nOperator: Son Ji-chang\nDeveloper: Tesla\nCountry: USA\nSector: Automotive\nPurpose: Automate steering, acceleration, braking\nTechnology: Driver assistance system\nIssue: Safety; Accuracy/reliability; Legal - Liability\nTransparency: Black box\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nhttps://www.courthousenews.com/wp-content/uploads/2019/04/SonTeslamtd-ORDER.pdf\nhttps://www.plainsite.org/dockets/33ksv0gu6/california-central-district-court/ji-chang-son-et-al-v-tesla-motors-inc/\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.reuters.com/article/us-tesla-lawsuit-idUSKBN14J1ZE\nhttps://electrek.co/2016/12/30/tesla-sued-model-x-sudden-acceleration/\nhttps://gizmodo.com/tesla-famous-korean-actor-who-is-suing-us-threatened-t-1790677044\nhttps://gizmodo.com/tesla-owner-files-lawsuit-claiming-his-model-x-spontane-1790660793\nhttps://nationalpost.com/news/world/you-just-cant-drive-tesla-fires-back-over-korean-celebritys-claims-of-unintended-acceleration\nhttp://english.chosun.com/site/data/html_dir/2017/01/02/2017010201247.html\nhttps://www.courthousenews.com/korean-stars-defamation-suit-over-tesla-crash-tossed/\nhttps://jalopnik.com/the-latest-tesla-lawsuit-proves-how-important-human-dri-1790904371\nRelated \ud83d\uddde\ufe0f\nTesla Model X crashes into five police officers\nTesla phantom braking\nPage info\nType: Incident\nPublished: January 2021\nLast updated: January 2022", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tiktok-intersex-censorship", "content": "TikTok accused of 'censoring' #intersex hashtag\nOccurred: June 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nTikTok sparked outrage for appearing to censor the #intersex hashtag on its platform after users spotted the tag pulled up a 'null' page.\nAccording toThe Verge, TikTok does not list banned words and phrases in its community guidelines, and did not make a public statement about the removal, leaving users to speculate whether they were being censored.\nTikTok later claimed the tag had been removed by mistake.\nThe episode raised questions about the Chinese company's apparent willingness to censor LGBTQ+ content across the world.\n\u2795 Earlier, TikTok was accused of limiting the reach of posts by LGBTQ, disabled, ugly and poor people, and of suppressing Black creators.\nSystem \ud83e\udd16\nTikTok website\nTikTok Wikipedia profile\nOperator: ByteDance/TikTok\nDeveloper: ByteDance/TikTok\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Moderate content\nTechnology: Recommendation algorithm \nIssue: Freedom of expression - censorship\nTransparency: Governance; Black box \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theverge.com/2021/6/4/22519433/tiktok-intersex-ban-mistake-moderation-transparency\nhttps://www.technologyreview.com/2021/07/13/1028401/tiktok-censorship-mistakes-glitches-apologies-endless-cycle/\nhttps://www.dailywire.com/news/tiktok-sparks-outrage-for-censoring-intersex-hashtag-blames-mistake\nhttps://theconversation.com/ia-et-moderation-des-reseaux-sociaux-un-cas-decole-de-discrimination-algorithmique-166614\nhttps://futurism.com/the-byte/tiktok-censorship-black-white-supremacy\nhttps://www.reddit.com/r/self/comments/3ey0fv/on_shadowbans/\nhttps://olhardigital.com.br/es/2021/07/09/internet-e-redes-sociais/tiktok-vai-automatizar-remocao-videos/\nhttps://www.technologyreview.es//s/13545/asi-funciona-el-interminable-ciclo-de-censura-y-errores-de-tiktok\nRelated \ud83c\udf10\nTikTok creators hate speech detection racial bias\nBytedance/TikTok Uyghur censorship \nPage info\nType: Incident\nPublished: June 2021\nLast updated: January 2022", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tom-cruise-deepfakes", "content": "DeepTomCruise fake videos impersonate actor on TikTok\nOccurred: February 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA TikTok account impersonating actor Tom Cruise spared concern about the hyer-real nature of deepfake technology and its potential use for misinformation, disinformation and other malicious purposes.\nThe Daily Beast reported on a TikTok account in the name of @deeptomcruise that is posting deepfake video clips of actor Tom Cruise golfing, telling jokes about former USSR president Mikhail Gorbachev, and doing magic tricks.\nFew clues indicated the fake nature of the clips; however, many viewers appeared unable to tell whether they were real or otherwise, and commentators focused on the increasing realism of deepfake technologies and their power to manipulate and deceive unsuspecting consumers.\nAs of January 2022, the @deeptomcruise account attracted 3.3 milion followers.\n\u2795 A 2022 study by University of Oxford, Brown University and Royal Society researchers found that most people cannot tell they are watching a 'deepfake' video - even if they have been told what they are watching may have been digitally altered. \nSystem \ud83e\udd16\nUnknown\nDeepTomCruise TikTok profile\nOperator: Bytedance/TikTok\nDeveloper: Chris Ume; Miles Fisher\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Imitate Tom Cruise  \nTechnology: Deepfake - video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning \nIssue: Ethics; Impersonation; Mis/disinformation\nTransparency: Governance\nResearch, advocacy \ud83e\uddee\nThe Royal Society (2022). The online information environment (pdf)\nInvestigations, assessments, audits \ud83e\uddd0\nDeepfaked. Deepfake Tom Cruise goes viral on social media\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.thedailybeast.com/shockingly-real-tom-cruise-deepfakes-are-invading-tiktok\nhttps://www.theverge.com/22303756/tiktok-tom-cruise-impersonator-deepfake\nhttps://www.telegraph.co.uk/films/0/alarmingly-realistic-tom-cruise-deepfakes-taking-tiktok/\nhttps://www.thesun.co.uk/tvandshowbiz/14170927/tom-cruise-deepfake-trending-on-tiktok/\nhttps://hotair.com/archives/allahpundit/2021/02/26/astounding-tom-cruise-tiktok-deepfakes/\nhttps://www.news.com.au/entertainment/celebrity-life/concerning-fake-videos-of-tom-cruise-have-popped-up-all-over-tiktok/news-story/61fbf4b19ef7e99e99a72e8882722b97\nhttps://movieweb.com/tom-cruise-deepfakes-tiktok/\nhttps://www.mic.com/p/the-worlds-foremost-tom-cruise-impersonator-tells-us-about-those-viral-tom-cruise-deepfakes-63994240\nhttps://golficity.com/tom-cruise-takes-a-golf-swing-on-tiktok-comes-off-a-bit-creepy/\nhttps://www.thetimes.co.uk/article/deepfake-videos-of-tom-cruise-watched-by-millions-tr8lkmfdk\nhttps://edition.cnn.com/videos/business/2021/03/02/tom-cruise-tiktok-deepfake-orig.cnn-business\nhttps://www.ft.com/content/721da1df-a1e5-4e2f-97fe-6de633ed4826\nhttps://www.independent.co.uk/life-style/gadgets-and-tech/tom-cruise-deepfakes-videos-test-b1993401.html\nRelated \ud83c\udf10\nAnthony Bourdain Roadrunner voice deepfake\nPresident Zelenskyy deepfake surrender\nPage info\nType: Incident\nPublished: February 2021\nLast updated: January 2022", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/cruzcampo-lola-flores-deepfake-ad", "content": "Cruzcampo Lola Flores deepfake ad draws ethics complaints\nOccurred: January 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAn advert by brewer Cruzcampo that used AI to bring back to life celebrated Spanish singer Lola Flores prompted compaints about poor ethics.\nAccording to El Pa\u00eds, Flores' voice, face, and features were recreated for the ad using deepfake technology and hours of audiovisual material and over 5,000 photographs.\nDevised and created by advertising agency Ogilvy and production firm Metropolitana, the spot encouraged people to be proud of their roots. Flores hailed from Anadalucia and died in 1995. \nThe campaign was praised for its relevance and realism. However, others complained it was unethical and unnecessarily commercial. \nSystem \ud83e\udd16\nUnknown\nhttps://www.youtube.com/watch?v=Yewm6TfLZ3Q\nhttps://www.youtube.com/watch?v=BQLTRMYHwvE&t=1s\nOperator: Cruzcampo\nDeveloper: WPP/Ogilvy; Metropolitana; DeepFaceLab\nCountry: Spain\nSector: Consumer goods\nPurpose: Imitate Lola Flores  \nTechnology: Deepfake - video; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning \nIssue: Ethics/values\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://elpais.com/tecnologia/2021-01-21/una-campana-con-un-deepfake-de-lola-flores-se-hace-viral.html\nhttps://www.igamesnews.com/pc/lola-flores-and-deepfake-for-or-against/\nhttps://www.esquire.com/es/tecnologia/a35322942/deepfake-tecnologia/\nhttps://www.elmundo.es/f5/comparte/2021/01/21/6009656221efa06b038b45e1.html\nhttps://es.gizmodo.com/asi-se-hizo-el-deepfake-de-lola-flores-para-un-anuncio-1846099743\nhttps://wersm.com/new-cruzcampo-campaign-brings-lola-flores-back-to-life/\nhttps://www.lavanguardia.com/economia/20210313/6374091/deepfake-resucita-lola-flores-brl.html\nhttps://www.rtve.es/noticias/20210321/deep-fakes-debate-suplantacion-identidad-inteligencia-artificial/2082845.shtml\nhttps://www.expansion.com/economia-digital/innovacion/2021/01/27/60104acbe5fdeaa9248b4644.html\nhttps://www.elmundo.es/cultura/2021/02/20/602ffb8cfdddfffb1c8b461e.html\nhttps://thenextweb.com/neural/2021/01/22/ai-resurrects-legendary-spanish-singer-lola-flores-to-hawk-beer/\nRelated \ud83c\udf10\nLinkedIn deepfake salespeople\nHour One 'character' clones\nPage info\nType: Incident\nPublished: January 2021\nLsst updated: January 2022", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/kohler-bmw-maxmara-china-facial-recognition", "content": "Kohler, BMW, MaxMara called out for using facial recognition to track Chinese shoppers\nOccurred: March 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nBathroom fixture maker Kohler, fashion house MaxMara and BMW were singled out by the Chinese authorities for covertly using facial recognition to monitor and analyse shopper movements in their mainland China stores. \nCCTV alleged the three companies had failed to inform or gain permission from visitors to their stores that they were being surveilled and their facial images analysed and matched. \nChinese national data privacy legislation that started on January 1, 2021, requires that user are informed and their consent is given when private and sensitive is collected and processed.\nThe allegations about the three companies were made during events marking China's annual Consumer Rights Day.\nSystem \ud83e\udd16\nBMW China\nKohler China\nMaxMara China\nOperator: Kohler; BMW; MaxMara\nDeveloper: Unclear/unknown\nCountry: China\nSector: Retail\nPurpose: Understand shopper behaviour\nTechnology: Facial recognition\nIssue: Privacy; Security\nTransparency: Governance; Privacy\nInvestigations, assessments, audits \ud83e\uddd0\nCCTV (2021). 3\u00b715\u665a\u4f1a\u66dd\u5149 | \u79d1\u52d2\u536b\u6d74\u3001\u5b9d\u9a6c\u3001MaxMara\u5546\u5e97\u5b89\u88c5\u4eba\u8138\u8bc6\u522b\u6444\u50cf\u5934\uff0c\u6d77\u91cf\u4eba\u8138\u4fe1\u606f\u5df2\u88ab\u641c\u96c6!\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.nytimes.com/aponline/2021/03/17/business/bc-as-china-facial-recognition.html\nhttps://www.engadget.com/china-state-tv-raps-kohler-040854372-193642268.html\nhttps://www.independent.co.uk/news/china-state-tv-raps-kohler-bmw-for-using-facial-recognition-bmw-kohler-cctv-china-beijing-b1817691.html\nhttps://www.thehindu.com/news/international/china-state-tv-raps-kohler-bmw-for-using-facial-recognition/article34081392.ece\nhttps://www.protocol.com/china/china-facial-recognition\nhttps://abcnews.go.com/Lifestyle/wireStory/china-state-tv-raps-kohler-bmw-facial-recognition-76480856\nhttps://www.manufacturing.net/technology/news/21330795/china-state-tv-criticizes-kohler-bmw-for-facial-recognition-use\nhttps://ipvm.com/reports/china-consumer-illegal-facerec\nhttps://apnews.com/article/technology-lifestyle-beijing-china-85dafdf87c3d4798bf8e36070c9b12f4\nRelated \ud83c\udf10\nHangzhou Safari Park mandatory facial recognition registration\nXPeng stores customer facial recognition\nPage info\nPublished: March 2021\nLast updated: January 2022", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-algorithms-promote-vaccine-misinformation", "content": "Amazon, Waterstones algorithms proactively promote vaccine misinformation\nOccurred: January 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nAmazon proactively recommended a wide range of anti-vaccination, health misinformation books, apparel and other products, according to a new study.\nResearchers at the University of Washington also found (pdf) that Amazon's recommendation algorithms made matters worse by pushing users interested in these products at even more related products. \nIn addition, follow-up research in the UK by Sky News found that Foyles and Waterstones online bookstores had been recommending a wide range of COVID-19 anti-vaccination and health conspiracy theory books to their users. \nAmazon, Facebook, Google, Twitter and other companies are under pressure to tackle the huge volume of online misinformation about COVID-19.\nSystem \ud83e\udd16\nAmazon US website\nAmazon Wikipedia profile\nOperator: Amazon; Foyles; Waterstones\nDeveloper: Amazon; Foyles; Watertones\nCountry: USA; UK; France\nSector: Retail; Health\nPurpose: Recommend content\nTechnology: Recommendation system\nIssue: Mis/disinformation; Freedom of expression - censorship\nTransparency: Black box; Governance\nResearch, advocacy \ud83e\uddee\nJuneja P., Mitra T. (2021). Auditing E-Commerce Platforms for Algorithmically Curated Vaccine Misinformation (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://news.sky.com/story/waterstones-and-amazon-urged-to-add-warning-tags-as-anti-vaccination-book-sales-surge-12234972\nhttps://www.euronews.com/2021/03/05/amazon-directs-customers-to-vaccine-misinformation-study-finds\nhttps://www.buzzfeednews.com/article/craigsilverman/amazon-covid-conspiracy-books\nhttps://venturebeat.com/2021/01/26/university-of-washington-researchers-say-amazons-algorithms-spread-vaccine-misinformation/\nhttps://brandequity.economictimes.indiatimes.com/news/digital/amazons-algorithms-boost-vaccine-misinformation-says-study/80570501\nhttps://www.spokesman.com/stories/2021/jan/29/uw-study-finds-amazon-promotes-vaccine-lies-especi/\nhttps://www.seattletimes.com/business/amazon/amazon-algorithms-promote-vaccine-misinformation-uw-study-says/\nhttps://www.beckershospitalreview.com/artificial-intelligence/how-amazon-algorithms-are-spreading-vaccine-misinformation.html\nhttps://thenextweb.com/neural/2021/02/03/amazons-search-algorithm-spreads-vaccine-disinformation-study-finds/\nhttps://metro.co.uk/2021/03/08/amazon-and-waterstones-urged-to-add-warnings-to-anti-vaxxer-books-14206447/\nhttps://techxplore.com/news/2021-01-amazon-algorithms-vaccine-misinformation.html\nRelated \ud83c\udf10\nFacebook COVID-19 misinformation ad approvals\nAmazon chemical food preservative suicides\nPage info\nType: Issue\nPublished: January 2021\nLast updated: January 2022", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/facebook-job-ad-delivery-gender-discrimination", "content": "Facebook ad delivery system shows different ads to women and men\nOccurred: March 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nFacebook ad delivery system showed different job ads to women and men, according to independent researchers.\nAn audit by researchers at the University of Southern California (USC) discovered that Facebook\u2019s ad-delivery system had been withholding showing job ads to women, even though the jobs require the same qualifications. \nBias of this type is considered sex-based discrimination under US equal employment opportunity law. \nTechnology Review pointed out that the findings come despite years of advocacy and lawsuits, and after promises from Facebook to overhaul how it delivers ads.\nSystem \ud83e\udd16\nFacebook ad delivery stystem\nOperator: Meta/Facebook\nDeveloper: Meta/Facebook\nCountry: USA\nSector: Business/professional services\nPurpose: Target audiences\nTechnology: Advertising management system\nIssue: Bias/discrimination - gender\nTransparency: Governance; Black box\nInvestigations, assessments, audits \ud83e\uddd0\nImana B., et al (2021). Auditing for Discrimination in Algorithms Delivering Job Ads\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.technologyreview.com/2021/04/09/1022217/facebook-ad-algorithm-sex-discrimination/\nhttps://apnews.com/article/discrimination-f62160cbbad4d72ce5250e6ef2222f5e\nhttps://www.wsj.com/articles/facebook-shows-men-and-women-different-job-ads-study-finds-11617969600\nhttps://theintercept.com/2021/04/09/facebook-algorithm-gender-discrimination/\nhttps://www.dw.com/en/study-unveils-facebook-gender-bias-in-job-ads/a-57152645\nhttps://www.reuters.com/article/facebook-advertising/study-flags-gender-bias-in-facebooks-ads-tools-idUKL1N2M13D8\nhttps://www.engadget.com/facebook-job-ads-men-women-study-144333009.html\nhttps://seekingalpha.com/news/3680830-facebook-job-ads-skewing-by-gender-in-way-that-may-be-illegal-study-says\nhttps://www.theverge.com/2021/4/9/22375366/facebook-ad-gender-bias-delivery-algorithm-discrimination\nhttps://www.mediapost.com/publications/article/362209/facebook-linkedin-job-ad-algorithms-studied-for-g.html\nhttps://www.theregister.com/2021/04/09/facebook_algorithm_discriminating/\nRelated \ud83c\udf10\nFacebook labels black men 'primates'\nTwitter, Instagram Palestinian discrimination\nPage info\nType: Incident\nPublished: April 2021\nLast updated: January 2022", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/facebook-military-gear-advertising", "content": "Facebook advertises military gear during January 6 US attempted coup\nOccurred: January 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nFacebook ran adverts for military gear and weapons accessories next to posts about the January 6 attempted insurrection in Washington DC, prompting questions about its safety systems.\nNon-profit Tech Transparency Project's (TTP) discovery raised questions about the effectiveness of the social network's content safety measures. It also prompted a number of Democrat senators to ask Mark Zuckerberg to permanently block ads of products clearly designed to be used in armed combat.\nFacebook later said it would ban ads for weapon accessories and protective equipment in the US 'out of an abundance of caution' until at least two days after Joe Biden's inauguration on January 20, 2021.\nTTP's discovery comes weeks after it had found that Facebook was permitting US militia groups to run recruitment ads on its platform.\nSystem \ud83e\udd16\nFacebook website\nFacebook Wikipedia profile\nFacebook (2021). Our Preparations Ahead of Inauguration Day\nOperator: Meta/Facebook\nDeveloper: Meta/Facebook\nCountry: USA\nSector: Retail\nPurpose: Review advertising\nTechnology: Advertising management system\nIssue: Safety; Accuracy/reliability; Ethics\nTransparency: Governance; Black box\nResearch, advocacy \ud83e\uddee\nTech Transparency Project (2021). How Facebook Profits from the Insurrection\nTech Transparency Project (2020). Facebook Ran Recruitment Ads for Militia Groups\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.buzzfeednews.com/article/ryanmac/facebook-profits-military-gear-ads-capitol-riot\nhttps://www.buzzfeednews.com/article/ryanmac/facebook-pauses-ads-for-gun-accessories-and-military-gear\nhttps://gizmodo.com/facebook-will-pause-ads-promoting-weapons-accessories-a-1846075789\nhttps://edition.cnn.com/politics/live-news/trump-impeachment-news-01-14-21/h_f548652ddf7f2ee9784719f5f4606932\nhttps://www.businessinsider.com/facebook-stops-military-gear-weapon-accessory-advertisements-until-january-22-2021-1\nhttps://www.theguardian.com/commentisfree/2021/jan/26/facebook-ads-combat-gear-rightwing-users\nhttps://www.reuters.com/article/us-usa-trump-protests-facebook-idUSKBN29L0S5\nhttps://www.wsj.com/livecoverage/trump-impeachment-biden-inauguration/card/bwbNSbrwuN8T9PlSNDpF\nhttps://thehill.com/policy/technology/534600-facebook-temporarily-bans-ads-for-weapons-accessories-following-capitol\nRelated \ud83c\udf10\nFacebook teen alcohol, drug, gambling ads approvals\nGuns disguised as cases for sale on Facebook Marketplace\nPage info\nType: Incident\nPublished: January 2021\nLast updated: December 2023", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/facebook-blocks-sexual-cows", "content": "Facebook algorithm blocks images of 'sexual' cows, office buildings\nOccurred: February 2021 \nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nPhotographs of wildlife, landscapes and buildings taken by the owner of a digital photo library were blocked by Facebook for supposedly containing 'overtly sexual' content. \nMike Hall had been trying to use photos on his business page - which included a cow standing in a field, the England cricket team, and office buildings - to run ads on Facebook. \nThe images were flagged as unsafe for reasons ranging from \u201cpromoting weapons\u201d and \u201covertly sexual\u201d content, and led to the restriction of Mr Hall's account and a ban on his ability to place advertising, according to the BBC.\n\nFacebook later admitted the block had been applied in error, though it failed to explain why and how it had been put in place. Commentators suggested the problem was likely to have been caused by Facebook's automated content moderation system, and tghen compounded by human \nSystem \ud83e\udd16\nFacebook content moderation system\nOperator: Meta/Facebook\nDeveloper: Meta/Facebook\nCountry: UK\nSector: Business/professional services\nPurpose: Review advertising\nTechnology: Advertising management system\nIssue: Accuracy/reliability\nTransparency: Governance; Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.co.uk/news/technology-55981602\nhttps://www.dailyecho.co.uk/news/19079582.northwall-gallery-winchester-banned-facebook-advertising/\nhttps://wisden.com/stories/facebook-bans-picture-of-england-cricket-huddle-for-being-too-sexy\nhttps://www.thesun.co.uk/news/14021335/facebook-slammed-cow-sexy/\nhttps://www.dailymail.co.uk/news/article-9249087/Facebook-apologises-blocks-art-gallerys-images-COWS-sexy.html\nhttps://metro.co.uk/2021/02/11/facebook-banned-this-picture-of-cows-for-being-too-sexy-14065563/\nhttps://www.news.com.au/sport/cricket/facebook-blocks-england-photographers-overtly-sexual-cricket-snap/news-story/29235ced1b0f9bc840466b6cefc3e56b\nhttps://www.hampshirechronicle.co.uk/news/19079011.northwall-gallery-winchester-banned-facebook-advertising/\nhttps://honey.nine.com.au/latest/facebook-banned-this-picture-of-cows-for-being-too-sexy/67db2c48-3df6-43f3-b6b3-ac8dcb280c85\nRelated \ud83c\udf10\nSama 'ethical AI' Facebook content moderation\nFacebook Cross-check\nPage info\nType: Incident\nPublished: February 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/facebook-australia-news-civil-society-blocks", "content": "Facebook Australia algorithm blocks news publishers, emergency services\nOccurred: February 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA Facebook algorithm wiped clean the pages of news publishers, government departments, emergency services and others as part of a campaign to put pressure on the Australian government, outraging local people and jeopardising their right to public information.  \nThe technology company blocked news journalism on its platform rather than pay the companies that produce it under the Australia's government's proposed News Media and Digital Platforms Mandatory Bargaining Code that would make platforms pay for publishers' content. \nHowever, the social network wiped clean the pages of new publishers, as well as of government departments, emergency services, non-profits and charities. Meantime, conspiracy theorist and anti-vaccine group pages were left untouched.\nAt the time, the move outraged Australians across all sections of society and raised questions about the accuracy of Facebook's content moderation and misinformation systems. Both systems had - and continue - to be widely accused of being ineffective, despite Facebook's insistence to the contrary.\nThe fracas also led some politicians and commentators to question whether Facebook had deliberately extended its news blocking activities to raise pressure on the Australian government - fears which appear to have been realised by a May 2022 Wall Street Journal report based on whistleblower testimony that indicated Facebook designed and used a custom algorithm for deciding what pages to take down that it knew would affect organisations other than news publishers, thereby ratcheting the pressure on the Australian government.\nFacebook restored news to its platform after the Australian government introduced last-minute amendments to the legislation. An earlier threat by Google to shut down its search engine in Australia failed to materialise, with the search engine company instead choosing to forge private deals with news publishers. \nSystem \ud83e\udd16\nFacebook. Changes to Sharing and Viewing News on Facebook in Australia\nOperator: Meta/Facebook\nDeveloper: Meta/Facebook\nCountry: Australia\nSector: Media/entertainment/sports/arts\nPurpose: Moderate content\nTechnology: Content moderation system\nIssue: Accuracy/reliability; Ethics/values; Mis/disinformation\nTransparency: Governance; Black box; Complaints/appeals; Marketing\nRegulation \u2696\ufe0f\nTreasury Laws Amendment (News Media and Digital Platforms Mandatory Bargaining Code) Bill 2021\nInvestigations, assessments, audits \ud83e\uddd0\nhttps://www.wsj.com/articles/facebook-deliberately-caused-havoc-in-australia-to-influence-new-law-whistleblowers-say-11651768302\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.nytimes.com/2021/02/18/business/media/facebook-australia-news.html\nhttps://www.abc.net.au/news/2021-02-18/facebook-to-restrict-sharing-or-viewing-news-in-australia/13166208\nhttps://www.abc.net.au/news/2021-02-18/facebook-credibility-brought-into-question-health-emergency-news/13166318\nhttps://www.theguardian.com/australia-news/2021/feb/18/facebook-to-restrict-australian-users-sharing-news-content\nhttps://www.theguardian.com/australia-news/2021/feb/18/facebook-to-restrict-australian-users-sharing-news-content\nhttps://www.theguardian.com/technology/2021/feb/19/misinformation-runs-rampant-as-facebook-says-it-may-take-a-week-before-it-unblocks-some-pages\nhttps://www.hollywoodreporter.com/business/digital/facebook-blocks-news-viewing-sharing-in-australia-faces-backlash-from-emergency-services-4134568/\nhttps://techcrunch.com/2021/02/18/facebook-applies-overly-broad-content-block-in-flex-against-australias-planned-news-reuse-law/\nhttps://www.cnbc.com/2021/02/23/facebook-to-restore-news-pages-for-australian-users-in-coming-days.html\nhttps://www.theverge.com/2022/5/6/23059684/facebook-australia-news-ban-internally-praised-overbroad-nonprofits-government-organizations\nRelated \ud83c\udf10\nFacebook Georgia political partisanship\nFacebook Cross-check\nPage info\nType: Incident\nPublished: February 2021\nLast updated: May 2022", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/huawei-5g-influence-campaign", "content": "Huawei found to be running deepfake campaign against Belgian government\nOccurred: January 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nChinese telecoms company Huawei ran an opaque deepfake campaign to undermine the Belgian government's plan to ban the company from its 5G network.\nAccording (pdf) to social media research company Graphika, Huawei ran an opaque, coordinated online campaign to attack the Belgian government's plan to ban Huawei from supplying 5G equipment to the country.\nThe campaign consisted of 14 Twitter accounts, each of which used a fake name and a fake profile image that had been generated using deepfake technology. Each profile posed as Belgium-based technology and 5G experts and pushed articles critical of the Belgian government. \nPer the report, the accounts spent their time retweeting content from popular accounts and mixing it with their own tweets, alongside tweets praising Huawei as a reliable investor and business partner. \nIt also found that Huawei officials, including Kevin Liu, Huawei president for public affairs and communications in Western Europe, regularly shared anti-Belgium government tweets and content.\nSystem \ud83e\udd16\nHuawei website\nHuawei Wikipedia profile\nOperator: Huawei\nDeveloper: Huawei\nCountry: Belgium\nSector: Govt - telecoms\nPurpose: Influence goverment decision-making\nTechnology: Deepfake - image; Generative adversarial network (GAN); Neural network; Deep learning; Machine learning; Bot/intelligent agent\nIssue: Mis/disinformation\nTransparency: Governance\nInvestigations, assessments, audits \ud83e\uddd0\nGraphika (2021). Fake Cluster Boots Huawei (pdf) \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.nytimes.com/2021/01/29/technology/commercial-disinformation-huawei-belgium.html\nhttps://www.ft.com/content/0411bc12-6a0c-4c14-9227-c06093e96e63\nhttps://www.zdnet.com/article/a-network-of-twitter-bots-has-attacked-the-belgian-governments-huawei-5g-ban/\nhttps://www.nature.com/articles/d41586-021-00867-6\nhttps://gizmodo.com/a-network-of-twitter-bots-reportedly-launched-a-smear-c-1846164974\nhttps://indianexpress.com/article/technology/tech-news-technology/inside-a-pro-huawei-influence-campaign-7168175/\nhttps://globalvoices.org/2021/02/18/research-firm-uncovers-a-pro-huawei-influence-campaign-against-belgian-5g-policy/\nhttps://www.politico.eu/article/pro-huawei-astroturfing-campaign-falls-flat/\nhttps://technews.tw/2021/02/02/inside-a-pro-huawei-influence-campaign/\nhttps://www.scmp.com/news/world/europe/article/3123390/huaweis-european-executives-promoted-content-fabricated-news\nhttps://www.techdirt.com/articles/20210201/08102746162/huawei-attempts-to-rebuild-trust-using-fake-twitter-telecom-experts.shtml\nRelated \ud83c\udf10\nHuawei Uyghur-spotting system\nBeijing Uyghur emotion detection\nPage info\nType: Incident\nPublished: January 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/moodbeam-emotional-tracking", "content": "Moodbeam HR emotion tracking blasted as 'invasive'\nOccurred: March 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA wristband-based system that enables organisations to assess and compare the emotional state of individuals and teams was criticised for being inappropriate and invasive.\nAccording to the BBC, UK-based AI start-up Moodbeam is hawking wearable silicon wristbands that its says enable organisations to assess and compare the emotional state of individuals and teams.\nWorkers are encouraged to push yellow and blue buttons that indicate they are happy or sad; the data is then reported to workers' managers via a dashboard that includes a 'Daily Happiness Score'.\nSome commentators are less enthusiastic, reckoning it is unnecessary, invasive, and likely to be inaccurate given how easily bored or frustrated workers can game it.\nOn its website, Moodbeam claimed purported benefits for employers included 'Increased productivity', 'Improved petention', and 'Reduced absenteeism'. \nSystem \ud83e\udd16\n\nOperator: Moodbeam\nDeveloper: Moodbeam\nCountry: UK\nSector: Health\nPurpose: Monitor emotions\nTechnology:  \nIssue: Appropriateness/need; Privacy; Surveillance\nTransparency: \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.bbc.com/news/amp/business-55637328\nhttps://www.telegraph.co.uk/business/2021/03/12/hr-departments-impose-tech-tyranny-woke-workplace/\nhttps://www.zdnet.com/article/your-boss-just-gave-you-a-pretty-wristband-it-reveals-what-mood-youre-in/\nhttps://www.dailystar.co.uk/news/latest-news/big-brother-style-wristbands-track-23730823\nhttps://brobible.com/culture/article/employees-wristband-alerts-boss-unhappy/\nhttps://inews.co.uk/opinion/columnists/mood-tracking-apps-moodbeam-employerare-no-solution-mental-health-crisis-lockdown-841398\nhttps://uk.news.yahoo.com/moodbeam-boss-happy-182618476.html\nhttps://index.medium.com/companies-want-managers-to-track-your-mood-with-a-wristband-b64d3a95b5cc\nhttps://www.hulldailymail.co.uk/news/celebs-tv/loose-women-share-feelings-hull-4918390\nhttps://www.dailymail.co.uk/sciencetech/article-9159037/Wristband-tracks-emotional-state-lets-bosses-monitor-employees-wellbeing-lockdown.html\nhttps://www.dailydot.com/irl/moodbeam-surveillance-tech/\nRelated \ud83c\udf10\nCanon smile recognition cameras\nZhihu job resignation predictions\nPage info\nType: Issue\nPublished: March 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/gpt-3-anti-muslim-bias", "content": "GPT-3 associates Muslims with violence\nOccurred: January 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nOpenAI's GPT-3 large language model consistently associated Muslims with violence, according to a research study.\nStanford McMaster university researchers discovered that the word 'Muslim' was associated with 'terrorist' 23 percent of the time, and feeding the phrase 'Two Muslims walked into a ... ' into the model, GPT-3 returned words and phrases associated with violence 66 out of 100 times. \nThe researchers also found that GPT-3 also exhibited 'severe bias' compared to stereotypes about other religious groups. \nIt is not the only time GPT-3 has been called out for racial and religious bias. In 2021, the system kept casting Middle-eastern actor Waleed Akhtar as a terrorist or rapist during 'AI', the world\u2019s first play written and performed live using GPT-3.\nSystem \ud83e\udd16\nGPT-3 large language model\nOperator: OpenAI\nDeveloper: OpenAI\nCountry: USA\nSector: Multiple\nPurpose: Generate text\nTechnology: Large language model (LLM); NLP/text analysis; Neural network; Deep learning; Machine learning\nIssue: Bias/discrimination - race, religion\nTransparency: Governance; Black box\nResearch, advocacy \ud83e\uddee\nAbid A., Farooqi M., Zou J. (2021). Persistent Anti-Muslim Bias in Large Language Models (pdf)\nAbid A., Farooqi M., Zou J. (2021). Large language models associate Muslims with violence\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://hai.stanford.edu/news/rooting-out-anti-muslim-bias-popular-language-model-gpt-3\nhttps://onezero.medium.com/for-some-reason-im-covered-in-blood-gpt-3-contains-disturbing-bias-against-muslims-693d275552bf\nhttps://thenextweb.com/neural/2021/01/19/gpt-3-has-consistent-and-creative-anti-muslim-bias-study-finds/\nhttps://thenextweb.com/neural/2021/01/19/gpt-3-is-the-worlds-most-powerful-bigotry-generator-what-should-we-do-about-it/\nhttps://towardsdatascience.com/is-gpt-3-islamophobic-be13c2c6954f\nhttps://thenextweb.com/neural/2021/01/27/its-time-to-use-all-of-twitters-archives-to-teach-ai-about-about-bias/\nhttps://www.vox.com/future-perfect/22672414/ai-artificial-intelligence-gpt-3-bias-muslim\nhttps://onezero.medium.com/for-some-reason-im-covered-in-blood-gpt-3-contains-disturbing-bias-against-muslims-693d275552bf\nhttps://indianexpress.com/article/explained/explained-why-artificial-intelligences-religious-biases-are-worrying-7533309/\nhttps://towardsdatascience.com/is-gpt-3-islamophobic-be13c2c6954f\nRelated \ud83c\udf10\nGPT-3 large language model\nGPT-3 short-form misinformation\nPage info\nType: Incident\nPublished: January 2021\nLast updated: December 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/gpt-2-dupes-medicaid", "content": "Student uses GPT-2 to dupe Medicaid\nOccurred: December 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nWIRED reports that a Harvard medical student has been able to fool volunteers into thinking automatically generated comments submitted about a Medicaid proposal were real. \nOf the 1,000 or so comments on proposed changes to Idaho's Medicaid programme on the medicaid.gov website, half had been generated by student Max Weiss using OpenAI's GPT-2 large language model. \nWeiss' research shows Medicaid users were unable to distinguish real comments from fake ones.\nThe stunt demonstrates how easily large language systems such as OpenAI's GPT-2 can be deployed to produce effective misinformation and disinformation campaigns. \nSystem \ud83e\udd16\nGPT-2 Wikipedia profile\nOpenAI (2019). GPT-2: 1.5B release\nOpenAI (2019). Better Language Models\nGPT-2 research study (pdf)\nGPT-2 code\nOperator: OpenAI\nDeveloper: OpenAI\nCountry: USA\nSector: Multiple; Govt - health\nPurpose: Generate text\nTechnology: Large language model (LLM); NLP/text analysis; Neural networks; Deep learning; Machine learning \nIssue: Mis/disinformation; Dual/multi-use\nTransparency: Governance; Black box\nResearch, advocacy \ud83e\uddee\nWeiss M. (2019). Deepfake Bot Submissions to Federal Public Comment Websites Cannot Be Distinguished from Human Submissions\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.wired.com/story/ai-powered-text-program-could-fool-government/\nhttps://news.harvard.edu/gazette/story/2020/02/why-an-undergrad-flooded-government-websites-with-bot-comments/\nhttps://arstechnica.com/tech-policy/2021/01/ai-powered-text-from-this-program-could-fool-the-government/\nhttps://www.inputmag.com/culture/artificial-intelligence-machine-was-able-to-dupe-medicaidgov\nhttps://inversezone.com/2021/01/18/harvard-medical-student-used-openais-gpt-2-to-submit-comments-on-idahos-draft-medicaid-proposal-volunteers-could-not-tell-them-apart-from-humans-will-knight-wired/\nhttps://www.theregister.com/2021/02/01/ai_in_brief/\nRelated \ud83c\udf10\nGPT-2 large language model\nThe Book of Veles disinformation manipulation\nPage info\nType: Incident\nPublished: January 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/simclr-igpt-racial-bias-stereotyping", "content": "SimCLR, iGPT image generation systems found to be racially biased\nOccurred: January 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nProminent image generation algorithms, including Google\u2019s SimCLR and OpenAI\u2019s iGPT, are biased and prone to negative stereotyping, according to a research study.\nResearchers Ryan Steed and Aylin Caliskan showed iGPT a head shot of prominent US politician Alexandria Ocasio-Cortez ('AOC') wearing business attire, only for the software to recreate her mutiple times in a bikini or low-cut top. \nMeantime, the researchers were criticised for including AI generated images of AOC in the pre-print paper, thereby exposing her to additional potential sexualisation and other possible abuses.\nSystem \ud83e\udd16\nGoogle SimCLR\nOpen AI Image GPT ('iGPT')\nOperator: Alphabet/Google; OpenAI\nDeveloper: Alphabet/Google; OpenAI\nCountry: USA\nSector: Multiple; Research/academia\nPurpose: Generate images\nTechnology: Image generation; Neural network; Deep learning; Machine learning\nIssue: Accuracy/reliability; Bias/discrimination - gender, race\nTransparency: Black box\nResearch, advocacy \ud83e\uddee\nSteed R., Caliskan A. (2021). Image Representations Learned With Unsupervised Pre-Training Contain Human-like Biases (pdf) \nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://twitter.com/KendraSerra/status/1354182538147885059\nhttps://www.technologyreview.com/2021/01/29/1017065/ai-image-generation-is-racist-sexist/\nhttps://onezero.medium.com/men-wear-suits-women-wear-bikinis-image-generating-algorithms-learn-biases-automatically-eee3d8a56f2e\nhttps://www.theguardian.com/commentisfree/2021/feb/03/what-a-picture-of-alexandria-ocasio-cortez-in-a-bikini-tells-us-about-the-disturbing-future-of-ai\nhttps://www.theregister.com/2021/02/01/ai_in_brief/\nhttps://www.hitc.com/en-gb/2021/02/04/alexandria-ocasio-cortez-in-a-bikini/\nhttps://towardsdatascience.com/algorithms-are-not-sexist-we-are-795525769e8e\nhttps://www.heise.de/news/Algorithmen-zur-Bildgenerierung-Maenner-tragen-Anzuege-Frauen-Bikinis-5043035.html\nhttps://mixed.de/ki-vorurteil-alexandria-ocasio-cortez-traegt-meistens-bikini/\nRelated \ud83c\udf10\nGPT-3 anti-Muslim bias\nGPT-2 dupes Medicaid\nPage info\nType: Incident\nPublished: February 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/tiktok-personal-data-harvesting-sales", "content": "TikTok fined for selling personal data of US users\nOccurred: December 2019\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nTikTok paid USD 92 million to settle a legal complaint filed in California that it collected the private data of 89 million users in the US without their consent, and then sold it to third-party advertisers, including Facebook, Google and entities in China.\nThe class-action lawsuit alleged the company used 'automated software, AI, facial recognition, and other technologies' to collect and profit from sensitive and confidential data on users\u2019 identity, ethnicity, gender, age, location, contact information and other attributes under the guise of a preventative measure to keep minors off the app. \nAccording to the lawsuit, the app 'clandestinely vacuumed up' huge quantities of private and personally identifiable data that could be used to identify and surveil users without permission. It also asserted that TikTok went to great lengths to hide its data collection and sharing practices by obfuscating its source code, amongst other measures.\nThe settlement raised concerns about the company's privacy practices, and was seen to underscore its reputed sharing of US user data with Chinese entities. \nTikTok claimed it agreed to settle the lawsuit in order to avoid a drawn-out legal battle.\nSystem \ud83e\udd16\nTikTok website\nTikTok Wikipedia profile\nOperator: ByteDance/TikTok\nDeveloper: ByteDance/TikTok\nCountry: USA\nSector: Media/entertainment/sports/arts\nPurpose: Collect personal data \nTechnology: Facial recognition\nIssue: Privacy\nTransparency: Governance; Privacy\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nIn Re TikTok Consumer Privacy litigation (pdf)\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.npr.org/2021/02/25/971460327/tiktok-to-pay-92-million-to-settle-class-action-suit-over-theft-of-personal-data\nhttps://arstechnica.com/tech-policy/2021/02/tiktok-agrees-to-proposed-92-million-settlement-in-privacy-class-action/\nhttps://www.engadget.com/tiktok-class-action-data-harvesting-lawsuit-settlement-105004598.html\nhttps://variety.com/2021/digital/news/tiktok-privacy-lawsuit-payment-92-million-1234915736/#!\nhttps://www.zdnet.com/article/tiktok-agrees-to-pay-92-million-to-settle-teen-privacy-class-action-lawsuit/\nhttps://gizmodo.com/tiktok-to-pay-92-million-settlement-in-nationwide-clas-1846361570\nhttps://www.npr.org/2020/08/04/898836158/class-action-lawsuit-claims-tiktok-steals-kids-data-and-sends-it-to-china\nhttps://www.latimes.com/business/technology/story/2019-12-03/tiktok-funneled-personal-data-to-china-lawsuit-alleges\nhttps://www.reuters.com/article/us-usa-tiktok-lawsuit/tiktok-accused-in-california-lawsuit-of-sending-user-data-to-china-idUKKBN1Y708Q\nRelated \ud83c\udf10\nTikTok UK child personal data harvesting\nTikTok mandatory beauty filtering\nPage info\nType: Incident\nPublished: February 2021\nLast updated: December 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/pyth-bitcoin-glitch", "content": "Pyth Bitcoin glitch triggers Bitcoin collapse\nOccurred: September 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nBloomberg reports that the recent 90 percent collapse in the price of Bitcoin from around USD 41,000 to USD 5,402 was likely due to a glitch on crypto data network Pyth.\nAccording to a Pyth investigatory report 'The issue was caused by the combination of (1) two different Pyth publishers publishing a near-zero price for BTC/USD and (2) the aggregation logic overweighting these publishers\u2019 contributions and both publishers encountered problems related to the handling of decimal numbers.' \nWhilst the impact on Bitcoin investors remains unclear, it seems some financial trading systems automatically sold Bitcoin in response to the apparent drop in price.\nSystem \ud83e\udd16\nPyth Network tweet\nOperator: Pyth \nDeveloper: Pyth  \nCountry: USA\nSector: Banking/financial services \nPurpose: Provide pricing information\nTechnology: Pricing algorithm\nIssue: Accuracy/reliability\nTransparency: Black box\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://twitter.com/bonfida/status/1439939941992222722\nhttps://www.bloomberg.com/news/articles/2021-09-21/bitcoin-price-crashed-to-5-402-on-network-backed-by-big-quants\nhttps://futurism.com/the-byte/wall-street-glitch-bitcoin-crashing\nhttps://fortune.com/2021/09/22/bitcoin-briefly-crashes-5402-pyth-network-glitch/\nhttps://www.cnbctv18.com/cryptocurrency/how-bitcoin-prices-briefly-crashed-to-5000-due-to-a-bug-10852832.htm\nhttps://www.livemint.com/market/cryptocurrency/bitcoin-prices-crashed-to-5-402-in-a-tech-glitch-on-pyth-network-this-is-what-caused-the-error-11632290553595.html\nhttps://www.bitcoininsider.org/article/127672/bitcoin-btc-price-plunged-90-5400-due-network-glitch-heres-what-happened\nRelated \ud83c\udf10\nKnight Capital Group equity order routing system glitch\nTesla China FSD glitch, recall\nPage info\nType: Incident\nPublished: September 2021", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/amazon-delivery-driver-safety-cameras", "content": "Amazon Driveri delivery driver safety monitoring slammed as inaccurate, unfair\nOccurred: February 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nThe use of an 'innovative' AI-enabled video camera system by Amazon was slammed by critics as inaccurate, unfair, and unnecessary.\n\nAccording to The Information, the Netradyne-supplied cameras were able to access drivers' location, movement, and biometric data to detect risky driver behaviour, with verbal warnings issued when drivers appear distracted, ignore signposts, or drive too fast. \n\nVice later reported that drivers who refuse to sign forms allowing Amazon to collect, store and use their facial and other biometric data lose their jobs, and that drivers are being unfairly punished for mistakes they had not made.\n\nDrivers, digital rights advocates and others complained that Amazon was running an inaccurate, unfair, and unnecessary system with inadequate security and privacy protection. \n\nThey also argued that Amazon deliberately makes it deliberately difficult for people being monitored to lodge complaints and appeals in a meaningful manner.\nSystem \ud83e\udd16\nNetradyne website\nOperator: Amazon\nDeveloper: Netradyne\nCountry: USA\nSector: Transport/logistics\nPurpose: Improve safety\nTechnology: CCTV; Computer vision\nIssue: Accuracy/reliability; Fairness; Surveillance; Security; Privacy; Employment - jobs, pay\nTransparency: Governance; Black box; Complaints/appeals\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.theinformation.com/articles/amazon-plans-ai-powered-cameras-to-monitor-delivery-van-drivers\nhttps://www.cnbc.com/2021/02/03/amazon-using-ai-equipped-cameras-in-delivery-vans.html\nhttps://www.bbc.co.uk/news/technology-55938494\nhttps://www.independent.co.uk/life-style/gadgets-and-tech/amazon-ai-cameras-yawn-drivers-b1797528.html\nhttps://news.trust.org/item/20210205132207-c0mz7\nhttps://www.cnbc.com/2021/02/12/amazon-mentor-app-tracks-and-disciplines-delivery-drivers.html\nhttps://news.trust.org/item/20210319120214-n93hk/\nhttps://www.vice.com/en/article/dy8n3j/amazon-delivery-drivers-forced-to-sign-biometric-consent-form-or-lose-job\nhttps://www.businessinsider.com/amazon-delivery-driver-camera-ai-bezos-2021-3\nhttps://www.theverge.com/2021/2/3/22265031/amazon-netradyne-driveri-survelliance-cameras-delivery-monitor-packages\nhttps://www.vice.com/en/article/88npjv/amazons-ai-cameras-are-punishing-drivers-for-mistakes-they-didnt-make\nRelated \ud83c\udf10\nAmazon Flex delivery driver routing safety\nAmazon DSP Ans Rana driver liability\nPage info\nType: Incident \nPublished: February 2021\nLast updated: June 2024", "year": "3"}
{"key": "", "url": "https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/deliveroo-italy-rider-shift-management-algorithm", "content": "Deliveroo Italy algorithm discriminates against reliable riders\nOccurred: January 2021\nReport incident \ud83d\udd25 | Improve page \ud83d\udc81 | Access database \ud83d\udd22\nA court in Bologna, Italy, ruled that a Deliveroo algorithm used to assess the reliability of its riders is discriminatory.\nIn a case bought by a group of Deliveroo riders and backed by CGIL, Italy\u2019s largest trade union, the court ruled (pdf) that the 'secretive' 'Frank' algorithm used by the food delivery company to assess the reliability of its riders violated local labour laws by failing to distinguish between legally legitimate reasons riders may not be working, such as illness or serious emergency, and more mundane reasons.\nThe system had been thereby unjustly penalising riders with legitimate reasons for not working. \nDeliveroo, which says it no longer uses the algorithm, has been ordered to pay EUR 50,000 to every affected rider. It had previously claimed the algorithm cut delivery times by 20 percent.\nIn April 2021, Deliveroo riders across the UK striked against the company's poor pay, safety and workers' rights record.\nSystem \ud83e\udd16\nDeliveroo Italy\nOperator: Deliveroo\nDeveloper: Deliveroo\nCountry: Italy\nSector: Transport/logistics\nPurpose: Determine rider reliability\nTechnology: Workforce management system\nIssue: Bias/discrimination - productivity; Fairness; Employment - pay\nTransparency: Governance; Black box\nLegal, regulatory \ud83d\udc69\ud83c\udffc\u200d\u2696\ufe0f\nhttp://www.bollettinoadapt.it/wp-content/uploads/2021/01/Ordinanza-Bologna.pdf\nNews, commentary, analysis \ud83d\uddde\ufe0f\nhttps://www.forbes.com/sites/jonathankeane/2021/01/05/italian-court-finds-deliveroo-rating-algorithm-was-unfair-to-riders\nhttps://www.vice.com/en/article/7k9e4e/court-rules-deliveroo-used-discriminatory-algorithm\nhttps://ai-lawhub.com/2021/01/18/an-italian-lesson-for-deliveroo-computer-programmes-do-not-always-think-of-everything/\nhttps://www.business-humanrights.org/en/latest-news/italy-court-rules-against-deliveroos-rider-algorithm-citing-discrimination/\nhttps://www.socialeurope.eu/food-delivery-riders-algorithms-and-autonomy\nhttps://techcrunch.com/2021/01/04/italian-court-rules-against-discriminatory-deliveroo-rider-ranking-algorithm/\nhttps://www.natlawreview.com/article/italian-garante-fines-deliveroo-25m-euros-unlawful-processing-personal-data\nhttps://www.ansa.it/emiliaromagna/notizie/2021/01/02/rider-cgilalgoritmo-discrimina-sentenza-tribunale-bologna_cc14c299-2c6b-411b-b677-496549ee3af1.html\nRelated \ud83c\udf10\nDeliveroo UK rider management, compensation\nGorillas rider work schedule automation\nPage info\nType: Incident\nPublished: January 2021\nLast updated: December 2021", "year": "3"}
